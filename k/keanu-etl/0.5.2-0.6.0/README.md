# Comparing `tmp/keanu_etl-0.5.2-py3-none-any.whl.zip` & `tmp/keanu_etl-0.6.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,28 +1,34 @@
-Zip file size: 27882 bytes, number of entries: 26
--rw-r--r--  2.0 unx       78 b- defN 20-Mar-12 10:31 keanu/__init__.py
--rw-r--r--  2.0 unx     2467 b- defN 20-Mar-26 15:02 keanu/batch.py
--rw-r--r--  2.0 unx     7390 b- defN 20-Mar-31 11:03 keanu/cli.py
--rw-r--r--  2.0 unx     3857 b- defN 20-Mar-18 16:10 keanu/config.py
--rw-r--r--  2.0 unx      479 b- defN 20-Mar-18 16:10 keanu/data_store.py
--rw-r--r--  2.0 unx     1386 b- defN 20-Mar-26 15:02 keanu/db.py
+Zip file size: 43806 bytes, number of entries: 32
+-rw-rw-r--  2.0 unx      110 b- defN 20-Aug-28 10:05 keanu/__init__.py
+-rw-rw-r--  2.0 unx     3665 b- defN 20-Aug-28 10:05 keanu/batch.py
+-rw-rw-r--  2.0 unx    13009 b- defN 20-Aug-28 10:05 keanu/cli.py
+-rw-rw-r--  2.0 unx     3937 b- defN 20-May-18 18:25 keanu/config.py
+-rw-rw-r--  2.0 unx      554 b- defN 20-Aug-28 10:05 keanu/data_store.py
+-rw-rw-r--  2.0 unx     1386 b- defN 20-Apr-23 06:31 keanu/db.py
 -rw-r--r--  2.0 unx     1356 b- defN 20-Mar-24 20:17 keanu/db2.py
--rw-r--r--  2.0 unx      571 b- defN 20-Mar-26 15:02 keanu/db_destination.py
--rw-r--r--  2.0 unx      802 b- defN 20-Mar-26 15:02 keanu/db_source.py
+-rw-rw-r--  2.0 unx      614 b- defN 20-Apr-23 06:31 keanu/db_destination.py
+-rw-rw-r--  2.0 unx      802 b- defN 20-Apr-23 06:31 keanu/db_source.py
 -rw-r--r--  2.0 unx     2997 b- defN 20-Mar-10 12:34 keanu/load_module.py
 -rw-r--r--  2.0 unx    17803 b- defN 20-Jan-13 14:56 keanu/metabase.py
--rw-r--r--  2.0 unx     7022 b- defN 20-Mar-31 11:46 keanu/py_loader.py
+-rw-rw-r--  2.0 unx     7022 b- defN 20-Apr-23 06:31 keanu/py_loader.py
 -rw-r--r--  2.0 unx     3852 b- defN 20-Mar-10 17:20 keanu/py_loader_2.py
--rw-r--r--  2.0 unx      648 b- defN 20-Mar-18 16:10 keanu/py_transform.py
--rw-r--r--  2.0 unx     1256 b- defN 20-Mar-18 13:50 keanu/run_statement.py
--rw-r--r--  2.0 unx     8110 b- defN 20-Mar-18 16:10 keanu/sql_loader.py
--rw-r--r--  2.0 unx      780 b- defN 20-Mar-18 16:10 keanu/sql_transform.py
+-rw-rw-r--  2.0 unx      648 b- defN 20-Apr-07 15:35 keanu/py_transform.py
+-rw-rw-r--  2.0 unx     1306 b- defN 20-Aug-28 10:05 keanu/run_statement.py
+-rw-rw-r--  2.0 unx     8510 b- defN 20-Aug-04 12:52 keanu/sql_loader.py
+-rw-rw-r--  2.0 unx      780 b- defN 20-Apr-07 15:35 keanu/sql_transform.py
+-rw-rw-r--  2.0 unx     2706 b- defN 20-Aug-28 10:05 keanu/test.py
 -rw-r--r--  2.0 unx      931 b- defN 20-Mar-11 22:44 keanu/trace.py
--rw-r--r--  2.0 unx     1087 b- defN 20-Mar-18 16:10 keanu/tracing.py
+-rw-rw-r--  2.0 unx     1087 b- defN 20-Apr-07 15:35 keanu/tracing.py
 -rw-r--r--  2.0 unx      192 b- defN 20-Jan-15 09:37 keanu/transform.py
--rw-r--r--  2.0 unx     1867 b- defN 20-Mar-18 16:10 keanu/util.py
--rw-r--r--  2.0 unx     8350 b- defN 20-Apr-01 18:43 keanu_etl-0.5.2.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 20-Apr-01 18:43 keanu_etl-0.5.2.dist-info/WHEEL
--rw-r--r--  2.0 unx       41 b- defN 20-Apr-01 18:43 keanu_etl-0.5.2.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        6 b- defN 20-Apr-01 18:43 keanu_etl-0.5.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     1952 b- defN 20-Apr-01 18:43 keanu_etl-0.5.2.dist-info/RECORD
-26 files, 75372 bytes uncompressed, 24806 bytes compressed:  67.1%
+-rw-rw-r--  2.0 unx     1867 b- defN 20-Apr-07 15:35 keanu/util.py
+-rw-rw-r--  2.0 unx      146 b- defN 20-Jun-22 13:41 keanu/metabase/__init__.py
+-rw-rw-r--  2.0 unx     4999 b- defN 20-Jun-22 13:41 keanu/metabase/client.py
+-rw-rw-r--  2.0 unx       99 b- defN 20-Jun-22 13:41 keanu/metabase/helpers.py
+-rw-rw-r--  2.0 unx    28634 b- defN 20-Aug-28 10:05 keanu/metabase/metabase_io.py
+-rw-rw-r--  2.0 unx     4354 b- defN 20-Jul-31 08:33 keanu/metabase/splitter.py
+-rw-rw-r--  2.0 unx    15100 b- defN 20-Aug-28 10:06 keanu_etl-0.6.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 20-Aug-28 10:06 keanu_etl-0.6.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       41 b- defN 20-Aug-28 10:06 keanu_etl-0.6.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        6 b- defN 20-Aug-28 10:06 keanu_etl-0.6.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     2438 b- defN 20-Aug-28 10:06 keanu_etl-0.6.0.dist-info/RECORD
+32 files, 131043 bytes uncompressed, 39988 bytes compressed:  69.5%
```

## zipnote {}

```diff
@@ -45,35 +45,53 @@
 
 Filename: keanu/sql_loader.py
 Comment: 
 
 Filename: keanu/sql_transform.py
 Comment: 
 
+Filename: keanu/test.py
+Comment: 
+
 Filename: keanu/trace.py
 Comment: 
 
 Filename: keanu/tracing.py
 Comment: 
 
 Filename: keanu/transform.py
 Comment: 
 
 Filename: keanu/util.py
 Comment: 
 
-Filename: keanu_etl-0.5.2.dist-info/METADATA
+Filename: keanu/metabase/__init__.py
+Comment: 
+
+Filename: keanu/metabase/client.py
+Comment: 
+
+Filename: keanu/metabase/helpers.py
+Comment: 
+
+Filename: keanu/metabase/metabase_io.py
+Comment: 
+
+Filename: keanu/metabase/splitter.py
+Comment: 
+
+Filename: keanu_etl-0.6.0.dist-info/METADATA
 Comment: 
 
-Filename: keanu_etl-0.5.2.dist-info/WHEEL
+Filename: keanu_etl-0.6.0.dist-info/WHEEL
 Comment: 
 
-Filename: keanu_etl-0.5.2.dist-info/entry_points.txt
+Filename: keanu_etl-0.6.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: keanu_etl-0.5.2.dist-info/top_level.txt
+Filename: keanu_etl-0.6.0.dist-info/top_level.txt
 Comment: 
 
-Filename: keanu_etl-0.5.2.dist-info/RECORD
+Filename: keanu_etl-0.6.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## keanu/__init__.py

```diff
@@ -1,6 +1,7 @@
 
 import dotenv
 dotenv.load_dotenv()
 
 from . import util
 from . import tracing
+from .test import BatchTestCase
```

## keanu/batch.py

```diff
@@ -1,11 +1,30 @@
 import operator
+import click
+from time import sleep
 from . import util
 from .tracing import tracer
 
+from signal import signal, SIGUSR1
+
+sighup_received = False
+def sighup(_a, _b):
+    click.echo("\rðŸ›¬ Received SIGUSR1, stopping as soon as possible...")
+    global sighup_received
+    sighup_received = True
+
+signal(SIGUSR1, sighup)
+
+class RetryScript(Exception):
+    pass
+
+RETRY_COUNT = 3
+RETRY_SLEEP = 10
+
+
 class Batch:
     def __init__(_, mode):
         """
         mode - running mode of this batch. Map containing:
         = {
           incremental - boolean - whether to avoid updating data that did not change (optimization)
           order - string - order specification, limiting scripts to run
@@ -52,27 +71,44 @@
         _.scripts = _.sort(_.scripts)
         if _.mode['rewind']:
             _.scripts.reverse()
 
     def execute(_):
         with tracer.start_active_span('batch', tags=_.tracer_tags):
             for scr in _.scripts:
-                if _.mode['rewind'] == False:
-                    for e,d in scr.execute():
-                        yield e, d
-                else:
-                    for e,d in scr.delete():
-                        yield e, d
+                for tries in range(RETRY_COUNT):
+                    try:
+                        if _.mode['rewind'] == False:
+                            for e,d in scr.execute():
+                                yield e, d
+                        else:
+                            for e,d in scr.delete():
+                                yield e, d
+
+                        if sighup_received:
+                            raise click.Abort("Stopped gracefully due to USR1 signal")
+
+                        break # from retry loop
+                    except RetryScript as rse:
+                        if tries + 1 == RETRY_COUNT:
+                            raise click.Abort("Too many retries, aborting") from rse
+                        else:
+                            click.echo("Encountered error that can be retried: {}.\nðŸ˜´  Sleeping 10 seconds....".format(rse.__cause__))
+                            sleep(RETRY_SLEEP)
+                            click.echo("Retrying....")
 
     def find_source(_, criteria):
         for s in reversed(_.sources):
             if criteria(s):
                 return s
         return None
 
+    def find_source_by_name(_, name):
+        return _.find_source(lambda s: s.name == name)
+
     @staticmethod
     def sort(scripts):
         scripts.sort(key=operator.attrgetter('order'))
         return scripts
 
     @property
     def tracer_tags(_):
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## keanu/cli.py

```diff
@@ -1,37 +1,39 @@
 from dotenv import load_dotenv
 load_dotenv()
 
 import click
+from click_aliases import ClickAliasedGroup
 import json
 from glob import glob
 from os import environ
 from . import db, util, metabase, config
 from .sql_loader import SqlLoader
 from .db_destination import DBDestination
+from .test import TestLoaders
 from pymysql.err import MySQLError
 from sqlalchemy.exc import IntegrityError, InternalError, ProgrammingError, DataError
 import re
 import sys
 import traceback
 import logging
 
 
-@click.group()
+@click.group(cls=ClickAliasedGroup)
 def cli():
     pass
 
 def positive_int(ctx, param, value):
     v = int(value)
     if v >= 1:
         return v
     else:
         raise click.BadParameter("-t thread_number must be a positive integer")
 
-@cli.command()
+@cli.command(aliases=['l'])
 @click.option('-i', '--incremental', is_flag=True, default=False, help='incremental load')
 @click.option('-n', '--dry-run', is_flag=True, default=False, help='dry run')
 @click.option('-o', '--order', default='0:', help='specify order of files to run by (eg. 10 or 10,12 or 10:15,60 etc)')
 @click.option('-d', '--display', is_flag=True, default=False, help='display SQL')
 @click.option('-W', '--warn', is_flag=True, default=False, help='display SQL warnings')
 @click.option('-t', '--threads', default=1, callback=positive_int, help='Number of threads for parallel python scripts')
 @click.option('-v', '--verbose', is_flag=True, default=False, help="More logging")
@@ -91,15 +93,15 @@
                 scr.filename,
                 data['time']
             ))
 
 
 
 
-@cli.command()
+@cli.command(aliases=['d'])
 @click.option('-n', '--dry-run', is_flag=True, default=False, help='dry run')
 @click.option('-o', '--order', default='0:', help='specify order of files to run by (eg. 10 or 10,12 or 10:15,60 etc)')
 @click.option('-d', '--display', is_flag=True, default=False, help='display SQL')
 @click.option('-W', '--warn', is_flag=True, default=False, help='display SQL warnings')
 @click.argument('config_or_dir', default='keanu.yaml', type=click.Path(exists=True))
 def delete(order, display, dry_run, warn, config_or_dir):
     mode = {
@@ -124,69 +126,187 @@
         elif event.startswith('py.script.start'):
             click.echo("ðŸ’¨ [{:3d}] {}".format(
                 scr.order,
                 scr.filename))
 
 
 
-@cli.command()
+@cli.command(aliases=['s'])
 @click.option('-D', '--drop', is_flag=True, default=False, help='DROP TABLEs before running the script')
-@click.option('-L', '--load', default=None, help='Load this SQL file')
+@click.option('-L', '--loads', default=[], multiple=True, help='Load this SQL file')
 @click.argument('database_url')
-def schema(drop, load, database_url):
+def schema(drop, loads, database_url):
     dest = DBDestination({'url': environ['DATABASE_URL']})
     connection = dest.connection()
 
     if drop:
         for (table, _) in connection.execute("show full tables where Table_Type = 'BASE TABLE'"):
             connection.execute('SET FOREIGN_KEY_CHECKS = 0')
             click.echo('ðŸ’¥ Dropping table {}'.format(table))
             connection.execute('DROP TABLE {}'.format(table))
 
-    if load:
-        script = SqlLoader(load, {}, None, dest)
-        script.replace_sql_object('keanu', dest.schema)
-        click.echo("ðŸšš Loading {}...".format(script.filename))
-        with connection.begin() as tx:
-            for event, data in script.execute():
-                scr = data['script']
-                if event.startswith('sql.statement.start'):
-                    click.echo("ðŸ“¦ {0}...".format(
-                        util.highlight_sql(
-                            scr.statement_abbrev(data['sql']))),
-                               nl=False)
-                elif event.startswith('sql.statement.end'):
-                    click.echo("\râœ…ï¸ {} rows in {:0.2f}s {:}".format(
-                        data['result'].rowcount,
-                        data['time'],
-                        util.highlight_sql(scr.statement_abbrev(data['sql']))
-                    ))
+    if loads:
+        for load in loads:
+            script = SqlLoader(load, {}, None, dest)
+            script.replace_sql_object('keanu', dest.schema)
+            click.echo("ðŸšš Loading {}...".format(script.filename))
+            with connection.begin() as tx:
+                for event, data in script.execute():
+                    scr = data['script']
+                    if event.startswith('sql.statement.start'):
+                        click.echo("ðŸ“¦ {0}...".format(
+                            util.highlight_sql(
+                                scr.statement_abbrev(data['sql']))),
+                                   nl=False)
+                    elif event.startswith('sql.statement.end'):
+                        click.echo("\râœ…ï¸ {} rows in {:0.2f}s {:}".format(
+                            data['result'].rowcount,
+                            data['time'],
+                            util.highlight_sql(scr.statement_abbrev(data['sql']))
+                        ))
 
-
-@cli.group('metabase')
+@cli.group('metabase', cls=ClickAliasedGroup, aliases=['mb'])
 def metabase_cli():
   pass
 
-@metabase_cli.command('export')
+@metabase_cli.command('export', aliases=['e'])
 @click.option('-c', '--collection', help="Name of the collection to export")
-def metabase_export(collection):
+@click.option('-j', '--json-file', default=None, help="path to JSON file to import")
+@click.option('-y', '--yaml-dir', default=None, help="path to directory with yaml files")
+@click.option('-v', '--verbose', is_flag=True, default=False, help="More logging")
+def metabase_export(collection, json_file, yaml_dir, verbose):
+    set_verbose(verbose)
     client = metabase.Client()
     mio = metabase.MetabaseIO(client)
     result = mio.export_json(collection)
-    print(json.dumps(result, indent=2))
+    if json_file:
+        with open(json_file, 'w') as out:
+            out.write(json.dumps(result, indent=2, sort_keys=True))
+    if yaml_dir:
+        splitter = metabase.Splitter(yaml_dir)
+        splitter.store(result)
+    if not (json_file or yaml_dir):
+        print(json.dumps(result, indent=2, sort_keys=True))
 
-@metabase_cli.command('import')
+@metabase_cli.command('import', aliases=['i'])
 @click.option('-c', '--collection', help="Name of the collection to import into")
-@click.option('-j', '--json-file', help="path to JSON file to import")
+@click.option('-j', '--json-file', default=None, help="path to JSON file to import")
+@click.option('-y', '--yaml-dir', default=None, help="path to directory with yaml files")
 @click.option('-m', '--metadata', is_flag=True, help="Also import metadata before importing the collection")
-def metabase_import(collection, json_file, metadata):
+@click.option('-o', '--overwrite', is_flag=True, help="Overwrite cards")
+@click.option('-D', '--db-map', multiple=True, help="Map Metabase database names fromname:toname.")
+@click.option('-V', '--validate', is_flag=True,help="Validate JSON before load")
+@click.option('-v', '--verbose', is_flag=True, default=False, help="More logging")
+def metabase_import(collection, json_file, metadata, overwrite, db_map, validate, yaml_dir, verbose):
+    set_verbose(verbose)
     client = metabase.Client()
     mio = metabase.MetabaseIO(client)
-    with open(json_file, 'r') as f:
-        source = json.loads(f.read())
-        mio.import_json(source, collection, metadata)
+    db_mapping = {d1: d2 for (d1,d2) in map(lambda x: x.split(":"), db_map)}
+    if json_file:
+        with open(json_file, 'r') as f:
+            source = json.loads(f.read())
+    elif yaml_dir:
+        splitter = metabase.Splitter(yaml_dir)
+        source = splitter.load()
+    else:
+        raise click.Abort("You need to specify json file with -j or yaml directory with -y")
 
+    if validate:
+        if verbose:
+            click.echo("ðŸ” Validating import file coherence...")
+        broken_cards = metabase.broken_cards(source['items'], source['datamodel'])
+        if len(broken_cards) > 0:
+            print("There are broken cards:")
+            for bc in broken_cards:
+                print("{}: {}".format(*bc))
+
+        broken_dashboards = metabase.broken_dashboards(source['items'])
+        if len(broken_dashboards) > 0:
+            print("There are broken dashboards (with questions outside of imported collection):")
+            for bd in broken_dashboards:
+                print("{}: {} (missing card id {})".format(*bd))
+
+        broken_datamodel = metabase.broken_datamodel(source['datamodel'])
+        if len(broken_datamodel) > 0:
+            print("This is broken in the data model:")
+            for bd in broken_datamodel:
+                print("{}: {} - {}".format(*bd))
+
+        if len(broken_cards) > 0 or len(broken_dashboards) > 0 or len(broken_datamodel) > 0:
+            return 1
+
+
+    mio.import_json(source, collection, metadata,
+                    overwrite,
+                    db_mapping)
+
+
+@cli.command(aliases=['t'])
+@click.option('--no-fixtures', is_flag=True, help="Do not load configured fixtures before running the test suite")
+@click.argument('test_config', default='keanu-test.yaml', type=click.Path(exists=True))
+@click.argument('test_dir', default='tests', type=click.Path(exists=True))
+@click.argument('spec',  required=False)
+def test(test_config, test_dir, spec, no_fixtures):
+    """
+    Run tests from TEST_DIR (default tests), using batch configuration from TEST_CONFIG (default keanu-test.yaml).
+    You should configure the batch in test file in a way that is similar to your production setup. Each test will make a full load of loaders defined by script ORDER variable, similar to load -o option format.
+
+    Use spec to limit test files, it defaults to test*.py when omitted.
+    """
+    configuration = config.configuration_from_argument(test_config)
+    suite = TestLoaders(configuration, no_fixtures)
+    result = suite.run(test_dir, spec)
+
+    # if result.wasSuccessful():
+    #     click.echo("All {} tests pass".format(result.testsRun))
+    #     return 0
+
+    # for (t, tb) in result.errors:
+    #     click.echo("ðŸ’”  Error in {}\n{}".format(t,tb))
+
+    # for (t, tb) in result.failures:
+    #     click.echo("ðŸ˜ž  Failure in {}\n{}".format(t, tb))
+
+    # click.echo("{} tests failed".format(len(result.failures)))
+    # sys.exit(1)
+
+@metabase_cli.command('query', aliases=['q'])
+@click.option('-j', '--json', is_flag=True, default=False, help="Print in JSON instead of Python")
+@click.argument('model')
+@click.argument('oid', default=None, required=False)
+@click.argument('sub', default=None, required=False)
+def metabase_query(model, oid, sub, **opts):
+    from pprint import pprint
+    client = metabase.Client()
+
+    r = client.get(model, oid, sub)
+
+    if opts['json']:
+        print(json.dumps(r, indent=2))
+    else:
+        pprint(r)
+
+@metabase_cli.command('split', aliases=['s'])
+@click.option('-j', '--json-file', help="path to JSON file to import")
+@click.option('-y', '--yaml-dir', help="path to directory with yaml files")
+def metabase_split(json_file, yaml_dir):
+    data = json.load(open(json_file))
+    splitter = metabase.Splitter(yaml_dir)
+    splitter.store(data)
+
+@metabase_cli.command('join', aliases=['j'])
+@click.option('-j', '--json-file', default=None, help="path to JSON file to import")
+@click.option('-y', '--yaml-dir', help="path to directory with yaml files")
+def metabase_split(json_file, yaml_dir):
+    splitter = metabase.Splitter(yaml_dir)
+    data = splitter.load()
+    if json_file:
+        with open(json_file, 'w') as out:
+            out.write(json.dumps(data, indent=2, sort_keys=True))
+    else:
+        print(json.dumps(data, indent=2, sort_keys=True))
 
 def set_verbose(verbose):
     if verbose:
         logging.basicConfig()
         logging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)
+        logging.getLogger('metabase.io').setLevel(logging.INFO)
```

## keanu/config.py

```diff
@@ -24,14 +24,16 @@
                                      param_hint='SOURCE')
 
         conf = [
             {'source': {'db': {'schema': environ['SOURCE']}}},
             {'destination': {'db': {'url': environ['DATABASE_URL']}}},
             {'transform': {'sql': {'directory': file_or_dir}}}
         ]
+    else:
+        raise click.Abort("Cannot use config {}".format(file_or_dir))
     return conf
 
 def load(file_path):
     with open(file_path) as f:
         txt = f.read()
         def get_var(m):
             return environ[m.group(1)]
```

## keanu/data_store.py

```diff
@@ -10,7 +10,10 @@
         _.local = False
         _.spec = db_spec
         _.dry_run = dry_run
         _.batch = None
 
     def set_batch(_, b):
         _.batch = b
+
+    def use(_):
+        _.connection().execute("USE {}".format(_.schema))
```

## keanu/db_destination.py

```diff
@@ -2,14 +2,15 @@
 from . import db
 
 class DBDestination(DataStore):
     def __init__(_, db_spec, name=None, dry_run=False):
         super().__init__(name, db_spec, dry_run)
 
         _.url = db_spec['url']
+        _.schema = db.url_to_schema(_.url)
         if not _.local:
             _.engine = db.get_engine(_.url, _.dry_run)
 
 
     def connection(_):
         if not _.local:
             conn = db.get_connection(_.engine)
```

## keanu/run_statement.py

```diff
@@ -11,14 +11,15 @@
         connection_id, = connection.execute('SELECT connection_id()').fetchone()
         result = None
         with warnings.catch_warnings():
             if not warn:
                 warnings.simplefilter("ignore", category=Warning)
             try:
                 for sql in statements:
+                    sql = sql.replace(":", "\\:")
                     yield 'sql.statement.start', {'sql': sql, 'script': _ }
                     start_time = time()
                     result = connection.execute(text(sql))
                     yield 'sql.statement.end', { 'sql': sql, 'script': _,
                                                  'time': time() - start_time, 'result': result }
             except (KeyboardInterrupt, exceptions.Abort) as ki:
                 echo("ðŸ”« Killing sql process {0} ðŸ”«".format(connection_id))
```

## keanu/sql_loader.py

```diff
@@ -1,25 +1,27 @@
 import operator
 import click
 from glob import glob
 import re
 from sqlalchemy import text
 import click
 from .run_statement import RunStatement
+from .batch import RetryScript
 from . import util
 from . import tracing
 import os
+import shutil
 from pymysql.err import MySQLError
 from sqlalchemy.exc import IntegrityError, InternalError, ProgrammingError, DataError
 
 class SqlLoader(RunStatement, tracing.Tags):
     """
     Class that runs load scripts, that is SQL that loads some data in keanu database.
     It can read extra metadata from the script comments.
-
+    
     Pass path to file of SQL script.
 
     options can be:
     incremental - run incremental variant of the script (no by default)
     display - displays full SQL while executing (no by default)
     warn - do show warnings from mysql driver (no by default)
     """
@@ -185,15 +187,16 @@
         after = "`{}`".format(after)
         _.statements = list(map(lambda st: st.replace(before, after), _.statements))
 
     def statement_abbrev(_, statement):
         if _.options['display']:
             return statement
 
-        trim_to = 50
+        trim_to = int(shutil.get_terminal_size((200, 20)).columns * 0.7)
+
         lines = statement.split("\n")
         lines = filter(lambda x: not re.match(r" *--", x) and not re.match(r"\s*$", x), lines)
         try:
             first = next(lines)
             if len(first) > trim_to:
                 first =  first[0:trim_to] + '...'
             return first
@@ -214,14 +217,20 @@
                     for event, data in super().execute(connection, _.deleteSql, warn=_.options['warn']):
                         yield event, data
                 except KeyboardInterrupt as ctrlc:
                     transaction.rollback()
                     raise ctrlc
                 yield 'sql.script.end.delete', { 'script': _ }
 
+    def display_error(_, e):
+        msg = str(e.args[0])
+        msg = msg.replace('\\n', "\n")
+        click.echo(message=msg, err=True)
+        return msg
+
     def execute(_):
         if len(_.statements) == 0:
             return
 
         connection = _.destination.connection()
         with tracing.tracer.start_active_span(
                 'script.{}'.format(_.filename.replace('/', '.')),
@@ -231,14 +240,18 @@
                     yield 'sql.script.start', { 'script': _ }
                     for event, data in super().execute(connection, _.statements, warn=_.options['warn']):
                         yield event, data
                     yield 'sql.script.end', { 'script': _ }
                 except KeyboardInterrupt as ctrlc:
                     transaction.rollback()
                     raise click.Abort("aborted.")
-                except (ProgrammingError, IntegrityError, MySQLError, InternalError, DataError) as e:
+                except (ProgrammingError, MySQLError, DataError) as e:
                     transaction.rollback()
-                    msg = str(e.args[0])
-                    msg = msg.replace('\\n', "\n")
-                    click.echo(message=msg, err=True)
-                    raise click.Abort(msg)
+                    raise click.Abort(_.display_error(e))
+                except InternalError as e:
+                    if 'Lock wait timeout exceeded' in e.orig.args[1]:
+                        raise RetryScript() from e
+                    else:
+                        transaction.rollback()
+                        raise click.Abort(_.display_error(e))
+
```

## Comparing `keanu_etl-0.5.2.dist-info/RECORD` & `keanu_etl-0.6.0.dist-info/RECORD`

 * *Files 23% similar despite different names*

```diff
@@ -1,26 +1,32 @@
-keanu/__init__.py,sha256=cCMeC4a3jVP5irWEXT88EdLUYFqHdplafI_fLqVGU1Q,78
-keanu/batch.py,sha256=XZPYHL5aiPbJs2qQ_RsG5jTSca9LmLCojs0HQ4XRdQw,2467
-keanu/cli.py,sha256=rIZxIZ7a_EhPeH9_1_3LDCaHeADARjs3RYeTijut9qI,7390
-keanu/config.py,sha256=ZrDCTPAB-SJFp2rrsQasWSP1SMg_kSrxBLc0wBn6xqQ,3857
-keanu/data_store.py,sha256=fQU9Abp0wOugAkcdYPa1K5HzHAkObVC2dTfFdYelpyQ,479
+keanu/__init__.py,sha256=XQ75sfgPVG3yvcGnlnXSibKz51g5oCMhCty8EV9Dqzw,110
+keanu/batch.py,sha256=V_vtyqOhc3lvMER9pgyrbwpO8RcPgbcKGD4JL29U5Rg,3665
+keanu/cli.py,sha256=5U34KZm_9Zj0VQoYDL-gC3orGrEjHVFT4zP9nUfPWBM,13009
+keanu/config.py,sha256=qrBU_yV7e_Gx4245dJgP46upNra77WU8PWP2um0Mr90,3937
+keanu/data_store.py,sha256=cWexMX58_o2LLa5qN45Z_tv4wvnlkFCEVaNkhe9hKeg,554
 keanu/db.py,sha256=CrO9708c34i-C6qSoKzFbFinm4T5VwFT-FGl51BvPHM,1386
 keanu/db2.py,sha256=IVQ2I8ZdoAff_1uwpah9h-7Zk1w-UPcGXtTR8Or5M3U,1356
-keanu/db_destination.py,sha256=o5TfDhAHuZvaHeMjGD2eTSC5LKlTKFNvRg1iEvssRXc,571
+keanu/db_destination.py,sha256=tvBFEnDsUlsxbcPT12N4jZbNOGpDDsfL9CFljLdzJZ4,614
 keanu/db_source.py,sha256=levneBsuj9Awcz98vSbwzGdmpqYJwUQothe-fjdMdSc,802
 keanu/load_module.py,sha256=GLNbMDVRRduhlagv_FGThTog4_4BZLEGBc6CT8BZbDQ,2997
 keanu/metabase.py,sha256=i7kRaFTipldvoXr4vwuVeg5encAu5tDE_fMaTyO0qMI,17803
 keanu/py_loader.py,sha256=gDWXkVE9Fk4ecZW3UOuy1LMq-BsWjFvUJb0iAB7m6_A,7022
 keanu/py_loader_2.py,sha256=sk88ha4AEBJMRznHhAAYcPyf1LxMZUHxxR87JnWa_dw,3852
 keanu/py_transform.py,sha256=Yeg_J4LQ27cskBIjwZTWSKZL_EYmXf4ynSaaNyEHTbw,648
-keanu/run_statement.py,sha256=xlhD6fnny8NvRacz85XdB4Kk09UE_0Nobkrz8pbC1Zw,1256
-keanu/sql_loader.py,sha256=P83OE2a76MjHj6UuJH1J2xXTYJf42VsB9AeVbj6zCZo,8110
+keanu/run_statement.py,sha256=v6ViNRI7GyqnrkhUDMsT7FbDHK8bO5RsGCBwq8lP9bs,1306
+keanu/sql_loader.py,sha256=CWrjQTUYFJpDoJ9SAjT1fC9ARffXJl5C2ARNydM_MUw,8510
 keanu/sql_transform.py,sha256=hinwx3xg303FQxUzZ88GIjHOMBGZyAY_ncyODHJknBc,780
+keanu/test.py,sha256=rSDx9b3M7bjiz0vCYJTg0VEGFV94mOeWNSnVQk2es4I,2706
 keanu/trace.py,sha256=VCtoyhfBSiOu5B3ztiaggf07hmesH4tx9N-bGHqesNU,931
 keanu/tracing.py,sha256=FNEBEM9Lft5Tty2SntIrA-LZ6os-MxIbMANs07uJe8g,1087
 keanu/transform.py,sha256=fAVWRQIvWY6FBWoqvKNx7nj9bUZemNr2fpacJcvNhlw,192
 keanu/util.py,sha256=2N2EoUprXTps2BZazqF-znfIQoMM7-LpcDIZOERoEWY,1867
-keanu_etl-0.5.2.dist-info/METADATA,sha256=If6Xce7QP7h0v04VDP7HA9UNLXvqbeMzcLBcagULHQs,8350
-keanu_etl-0.5.2.dist-info/WHEEL,sha256=p46_5Uhzqz6AzeSosiOnxK-zmFja1i22CrQCjmYe8ec,92
-keanu_etl-0.5.2.dist-info/entry_points.txt,sha256=wOuFupQO9f_GJx4RPIeXHLlutKOKr4eJk1B0Kb4kpfc,41
-keanu_etl-0.5.2.dist-info/top_level.txt,sha256=gbucPIxoXTUYjV9Emqipap2Tg5ojIjcsLCAHzTh1dKk,6
-keanu_etl-0.5.2.dist-info/RECORD,,
+keanu/metabase/__init__.py,sha256=9DhdkLtZRXOl1xVJpF350FxTx0rapMdxAhy2ytoaCts,146
+keanu/metabase/client.py,sha256=bx4uT-cHZWWHpo5lEsketFi1Sc0hVUQ64DtKGYCYgS8,4999
+keanu/metabase/helpers.py,sha256=pSMo0c9_B_E5ljWV_CVXfxP8fMCOPzsu9wyxw2IIJ_U,99
+keanu/metabase/metabase_io.py,sha256=AbnOdbysARk9yNF0Wrg33-cbFxh9diL-gt0LO4ycCOk,28634
+keanu/metabase/splitter.py,sha256=5-JFksmaDXB0PRhpV5DZnOrNI14dum595zjQ0BNY4O8,4354
+keanu_etl-0.6.0.dist-info/METADATA,sha256=BHvUjwVgXhxFasTpy5NqCabuEdGFpIUxnLYJ7WbPTOU,15100
+keanu_etl-0.6.0.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
+keanu_etl-0.6.0.dist-info/entry_points.txt,sha256=wOuFupQO9f_GJx4RPIeXHLlutKOKr4eJk1B0Kb4kpfc,41
+keanu_etl-0.6.0.dist-info/top_level.txt,sha256=gbucPIxoXTUYjV9Emqipap2Tg5ojIjcsLCAHzTh1dKk,6
+keanu_etl-0.6.0.dist-info/RECORD,,
```

