# Comparing `tmp/lightbeam-0.1.1-py3-none-any.whl.zip` & `tmp/lightbeam-0.1.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,22 +1,22 @@
-Zip file size: 37171 bytes, number of entries: 20
+Zip file size: 37794 bytes, number of entries: 20
 -rw-r--r--  2.0 unx        0 b- defN 22-Sep-16 20:03 lightbeam/__init__.py
 -rw-r--r--  2.0 unx     5915 b- defN 23-Oct-16 15:41 lightbeam/__main__.py
--rw-r--r--  2.0 unx    17611 b- defN 23-Oct-16 15:41 lightbeam/api.py
+-rw-r--r--  2.0 unx    16486 b- defN 24-Apr-19 21:05 lightbeam/api.py
 -rw-r--r--  2.0 unx     3516 b- defN 23-Oct-16 15:41 lightbeam/count.py
 -rw-r--r--  2.0 unx    12784 b- defN 23-Oct-16 15:41 lightbeam/delete.py
--rw-r--r--  2.0 unx     5536 b- defN 24-Feb-16 19:38 lightbeam/fetch.py
+-rw-r--r--  2.0 unx     5362 b- defN 24-Apr-19 21:05 lightbeam/fetch.py
 -rw-r--r--  2.0 unx      672 b- defN 22-Sep-19 14:49 lightbeam/hashlog.py
 -rw-r--r--  2.0 unx     9611 b- defN 23-Oct-16 15:41 lightbeam/lightbeam.py
--rw-r--r--  2.0 unx    11923 b- defN 24-Feb-16 19:03 lightbeam/send.py
+-rw-r--r--  2.0 unx    11458 b- defN 24-Apr-19 21:05 lightbeam/send.py
 -rw-r--r--  2.0 unx     4066 b- defN 23-Oct-16 15:41 lightbeam/truncate.py
--rw-r--r--  2.0 unx     1431 b- defN 24-Feb-16 19:03 lightbeam/util.py
--rw-r--r--  2.0 unx     7051 b- defN 23-Oct-16 15:41 lightbeam/validate.py
+-rw-r--r--  2.0 unx     3107 b- defN 24-Apr-19 21:05 lightbeam/util.py
+-rw-r--r--  2.0 unx     7423 b- defN 24-Apr-19 21:05 lightbeam/validate.py
 -rw-r--r--  2.0 unx        0 b- defN 22-Jun-28 14:58 lightbeam/resources/__init__.py
--rwxrwxrwx  2.0 unx    11349 b- defN 24-Feb-16 19:39 lightbeam-0.1.1.dist-info/LICENSE
--rw-r--r--  2.0 unx    19903 b- defN 24-Feb-16 19:39 lightbeam-0.1.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Feb-16 19:39 lightbeam-0.1.1.dist-info/WHEEL
--rwxrwxrwx  2.0 unx      132 b- defN 24-Feb-16 19:39 lightbeam-0.1.1.dist-info/dependency_links.txt
--rwxrwxrwx  2.0 unx       54 b- defN 24-Feb-16 19:39 lightbeam-0.1.1.dist-info/entry_points.txt
--rwxrwxrwx  2.0 unx       10 b- defN 24-Feb-16 19:39 lightbeam-0.1.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1596 b- defN 24-Feb-16 19:39 lightbeam-0.1.1.dist-info/RECORD
-20 files, 113252 bytes uncompressed, 34591 bytes compressed:  69.5%
+-rwxrwxrwx  2.0 unx    11349 b- defN 24-Apr-19 21:45 lightbeam-0.1.2.dist-info/LICENSE
+-rw-r--r--  2.0 unx    20205 b- defN 24-Apr-19 21:45 lightbeam-0.1.2.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-19 21:45 lightbeam-0.1.2.dist-info/WHEEL
+-rwxrwxrwx  2.0 unx      132 b- defN 24-Apr-19 21:45 lightbeam-0.1.2.dist-info/dependency_links.txt
+-rwxrwxrwx  2.0 unx       54 b- defN 24-Apr-19 21:45 lightbeam-0.1.2.dist-info/entry_points.txt
+-rwxrwxrwx  2.0 unx       10 b- defN 24-Apr-19 21:45 lightbeam-0.1.2.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     1596 b- defN 24-Apr-19 21:45 lightbeam-0.1.2.dist-info/RECORD
+20 files, 113838 bytes uncompressed, 35214 bytes compressed:  69.1%
```

## zipnote {}

```diff
@@ -33,29 +33,29 @@
 
 Filename: lightbeam/validate.py
 Comment: 
 
 Filename: lightbeam/resources/__init__.py
 Comment: 
 
-Filename: lightbeam-0.1.1.dist-info/LICENSE
+Filename: lightbeam-0.1.2.dist-info/LICENSE
 Comment: 
 
-Filename: lightbeam-0.1.1.dist-info/METADATA
+Filename: lightbeam-0.1.2.dist-info/METADATA
 Comment: 
 
-Filename: lightbeam-0.1.1.dist-info/WHEEL
+Filename: lightbeam-0.1.2.dist-info/WHEEL
 Comment: 
 
-Filename: lightbeam-0.1.1.dist-info/dependency_links.txt
+Filename: lightbeam-0.1.2.dist-info/dependency_links.txt
 Comment: 
 
-Filename: lightbeam-0.1.1.dist-info/entry_points.txt
+Filename: lightbeam-0.1.2.dist-info/entry_points.txt
 Comment: 
 
-Filename: lightbeam-0.1.1.dist-info/top_level.txt
+Filename: lightbeam-0.1.2.dist-info/top_level.txt
 Comment: 
 
-Filename: lightbeam-0.1.1.dist-info/RECORD
+Filename: lightbeam-0.1.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## lightbeam/api.py

```diff
@@ -49,75 +49,32 @@
         all_endpoints = self.get_sorted_endpoints()
 
         # filter down to only selected endpoints
         self.lightbeam.endpoints = self.apply_filters(all_endpoints)
 
 
     def apply_filters(self, endpoints=[]):
-        selected_endpoints = self.parse_endpoint_string(self.lightbeam.selector, endpoints=endpoints, all_on_empty=True)
-
-        # make sure all selectors resolve to an endpoint
-        unknown_endpoints = list(set(selected_endpoints).difference(endpoints))
-        if unknown_endpoints:
-            self.logger.critical("no match for selector(s) [{0}] to any endpoint in your API; check for typos?".format(", ".join(unknown_endpoints)))
-
-        excluded_endpoints = self.parse_endpoint_string(self.lightbeam.exclude, endpoints=selected_endpoints)
+        # apply filters
+        my_endpoints = util.apply_selections(endpoints, self.lightbeam.selector, self.lightbeam.exclude)
         
         # make sure we have some endpoints to process
-        my_endpoints = list(set(selected_endpoints).difference(excluded_endpoints))
         if not my_endpoints:
             self.logger.critical("selector filtering left no endpoints to process; check your selector for typos?")
 
+        # make sure all selectors resolve to an endpoint
+        unknown_endpoints = set(my_endpoints).difference(endpoints)
+        if unknown_endpoints:
+            self.logger.critical("no match for selector(s) [{0}] to any endpoint in your API; check for typos?".format(", ".join(unknown_endpoints)))
+
         # all the list(set()) stuff above can mess up the ordering of the endpoints (which must be in dependency-order)... this puts them back in dependency-order
         final_endpoints = [x for x in endpoints if x in my_endpoints]
         
         return final_endpoints
 
 
-    @staticmethod
-    def parse_endpoint_string(full_endpoint_string: str, endpoints=[], all_on_empty=False):
-        """
-        Possible endpoint strings:
-        - "students"
-        - "students,schools"
-        - "student*"
-        - "student*,schools"
-        - "*Associations"
-        - "*Associations,schools"
-        """
-        # If no string is provided, return all or no endpoints, depending on use-case.
-        if not full_endpoint_string:
-            if all_on_empty:
-                return endpoints
-            else:
-                return []
-        
-        # Asterisk wildcards to all endpoints.
-        if full_endpoint_string == "*":
-            return endpoints
-        
-        # Otherwise, a comma-separated list of endpoints is expected.
-        return_endpoints = set()
-
-        for endpoint_string in full_endpoint_string.split(","):
-
-            if endpoint_string.startswith("*"):  # left wildcard: "*Associations"
-                return_endpoints.update(
-                    filter(lambda endpoint: endpoint.endswith(endpoint_string.lstrip("*")), endpoints)
-                )
-            elif endpoint_string.endswith("*"):  # right wildcard: "student*"
-                return_endpoints.update(
-                    filter(lambda endpoint: endpoint.startswith(endpoint_string.rstrip("*")), endpoints)
-                )
-            else:  # no wildcard: "students"
-                return_endpoints.add(endpoint_string)
-        
-        return list(return_endpoints)
-
-
     # Returns a client object with exponential retry and other parameters per configs
     def get_retry_client(self):
         return RetryClient(
             timeout=aiohttp.ClientTimeout(sock_connect=self.lightbeam.config['connection']["timeout"]),
             retry_options=ExponentialRetry(
                 attempts=self.lightbeam.config['connection']["num_retries"],
                 factor=self.lightbeam.config['connection']["backoff_factor"],
@@ -340,16 +297,29 @@
         if "Descriptor" in endpoint: swagger = self.descriptors_swagger
         else: swagger = self.resources_swagger
         definition = util.camel_case(self.lightbeam.config["namespace"]) + "_" + util.singularize_endpoint(endpoint)
         return self.get_required_params_from_swagger(swagger, definition)
 
     def get_required_params_from_swagger(self, swagger, definition, prefix=""):
         params = {}
-        for requiredProperty in swagger["definitions"][definition]["required"]:
-            if "$ref" in swagger["definitions"][definition]["properties"][requiredProperty].keys():
-                sub_definition = swagger["definitions"][definition]["properties"][requiredProperty]["$ref"].replace("#/definitions/", "")
+        use_definitions = False
+        if "definitions" in swagger.keys():
+            schema = swagger["definitions"][definition]
+            use_definitions = True
+        elif "components" in swagger.keys() and "schemas" in swagger["components"].keys():
+            schema = swagger["components"]["schemas"][definition]
+        else:
+            self.logger.critical(f"Swagger contains neither `definitions` nor `components.schemas` - check that the Swagger is valid.")
+        
+        for requiredProperty in schema["required"]:
+            if "$ref" in schema["properties"][requiredProperty].keys():
+                sub_definition = schema["properties"][requiredProperty]["$ref"]
+                if use_definitions:
+                    sub_definition = sub_definition.replace("#/definitions/", "")
+                else:
+                    sub_definition = sub_definition.replace("#/components/schemas/", "")
                 sub_params = self.get_required_params_from_swagger(swagger, sub_definition, prefix=requiredProperty+".")
                 for k,v in sub_params.items():
                     params[k] = v
-            elif swagger["definitions"][definition]["properties"][requiredProperty]["type"]!="array":
+            elif schema["properties"][requiredProperty]["type"]!="array":
                 params[requiredProperty] = prefix + requiredProperty
         return params
```

## lightbeam/fetch.py

```diff
@@ -82,26 +82,22 @@
                         self.logger.warn(f"Unable to load records for {endpoint}... {status} API response.")
                     else:
                         if response.content_type == "application/json":
                             values = json.loads(body)
                             if type(values) != list:
                                 self.logger.warn(f"Unable to load records for {endpoint}... API JSON response was not a list of records.")
                             else:
+                                payload_keys = list(values[0].keys())
+                                final_keys = util.apply_selections(payload_keys, self.lightbeam.keep_keys, self.lightbeam.drop_keys)
+                                do_key_filtering = len(payload_keys) != len(final_keys)
                                 for v in values:
-                                    if self.lightbeam.keep_keys!="":
-                                        row = {}
-                                        for key in self.lightbeam.keep_keys.split(','):
-                                            row.update({key: v[key]})
+                                    if do_key_filtering: row = {k: v[k] for k in final_keys}
                                     else: row = v
-                                    # delete_keys (id, _etag, _lastModifiedDate)
-                                    for key in self.lightbeam.drop_keys.split(','):
-                                        if key in row.keys():
-                                            del row[key]
                                     if file_handle: file_handle.write(json.dumps(row)+"\n")
-                                    else: self.lightbeam.results.append(v)
+                                    else: self.lightbeam.results.append(row)
                                     self.lightbeam.increment_status_counts(status)
                                 break
                         else:
                             self.logger.warn(f"Unable to load records for {endpoint}... API response was not JSON.")
 
             except RuntimeError as e:
                 await asyncio.sleep(1)
```

## lightbeam/send.py

```diff
@@ -1,7 +1,8 @@
+import re
 import os
 import time
 import json
 import asyncio
 import datetime
 
 from lightbeam import util
@@ -52,37 +53,36 @@
         self.metadata.update({
             "completed_at": self.end_timestamp.isoformat(timespec='microseconds'),
             "runtime_sec": (self.end_timestamp - self.start_timestamp).total_seconds(),
             "total_records_processed": sum(item['records_processed'] for item in self.metadata["resources"].values()),
             "total_records_skipped": sum(item['records_skipped'] for item in self.metadata["resources"].values()),
             "total_records_failed": sum(item['records_failed'] for item in self.metadata["resources"].values())
         })
-        # total up counts by message and status
-        for resource, resource_metadata in self.metadata["resources"].items():
-            if "failed_statuses" in resource_metadata.keys():
-                for status, status_metadata in resource_metadata["failed_statuses"].items():
-                    total_num_errs = 0
-                    for message, message_metadata in status_metadata.items():
-                        for file, file_metadata in message_metadata["files"].items():
-                            num_errs = len(file_metadata["line_numbers"])
-                            file_metadata.update({
-                                "count": num_errs,
-                                "line_numbers": ",".join(str(x) for x in file_metadata["line_numbers"])
-                            })
-                            total_num_errs += num_errs
-                    status_metadata.update({"count": total_num_errs})
+        # sort failing line numbers
+        for resource in self.metadata["resources"].keys():
+            if "failures" in self.metadata["resources"][resource].keys():
+                for idx, _ in enumerate(self.metadata["resources"][resource]["failures"]):
+                    self.metadata["resources"][resource]["failures"][idx]["line_numbers"].sort()
+        
+        
+        # helper function used below
+        def repl(m):
+            return re.sub(r"\s+", '', m.group(0))
         
         ### Create structured output results_file if necessary
         if self.lightbeam.results_file:
             
             # create directory if not exists
             os.makedirs(os.path.dirname(self.lightbeam.results_file), exist_ok=True)
             
             with open(self.lightbeam.results_file, 'w') as fp:
-                fp.write(json.dumps(self.metadata, indent=4))
+                content = json.dumps(self.metadata, indent=4)
+                # failures.line_numbers are split each on their own line; here we remove those line breaks
+                content = re.sub(r'"line_numbers": \[(\d|,|\s|\n)*\]', repl, content)
+                fp.write(content)
 
         if self.metadata["total_records_processed"] == self.metadata["total_records_skipped"]:
             self.logger.info("all payloads skipped")
             exit(99) # signal to downstream tasks (in Airflow) all payloads skipped
         
         if self.metadata["total_records_processed"] == self.metadata["total_records_failed"]:
             self.logger.info("all payloads failed")
@@ -108,34 +108,34 @@
         # process each file
         data_files = self.lightbeam.get_data_files_for_endpoint(endpoint)
         tasks = []
         total_counter = 0
         for file_name in data_files:
             with open(file_name) as file:
                 # process each line
-                for line in file:
+                for line_counter, line in enumerate(file):
                     total_counter += 1
                     data = line.strip()
                     # compute hash of current row
                     hash = hashlog.get_hash(data)
                     # check if we've posted this data before
                     if self.lightbeam.track_state and hash in self.hashlog_data.keys():
                         # check if the last post meets criteria for a resend
                         if self.lightbeam.meets_process_criteria(self.hashlog_data[hash]):
                             # yes, we need to (re)post it; append to task queue
                             tasks.append(asyncio.create_task(
-                                self.do_post(endpoint, file_name, data, total_counter, hash)))
+                                self.do_post(endpoint, file_name, data, line_counter, hash)))
                         else:
                             # no, do not (re)post
                             self.lightbeam.num_skipped += 1
                             continue
                     else:
                         # new, never-before-seen payload! append it to task queue
                         tasks.append(asyncio.create_task(
-                            self.do_post(endpoint, file_name, data, total_counter, hash)))
+                            self.do_post(endpoint, file_name, data, line_counter, hash)))
                 
                     if total_counter%self.lightbeam.MAX_TASK_QUEUE_SIZE==0:
                         await self.lightbeam.do_tasks(tasks, total_counter)
                         tasks = []
                     
                 if self.lightbeam.num_skipped>0:
                     self.logger.info("skipped {0} of {1} payloads because they were previously processed and did not match any resend criteria".format(self.lightbeam.num_skipped, total_counter))
@@ -172,27 +172,31 @@
                         self.lightbeam.num_finished += 1
                         
                         # warn about errors
                         if response.status not in [ 200, 201 ]:
                             message = str(response.status) + ": " + util.linearize(json.loads(body).get("message"))
 
                             # update run metadata...
-                            failed_statuses_dict = self.metadata["resources"][endpoint].get("failed_statuses", {})
-                            if response.status not in failed_statuses_dict.keys():
-                                failed_statuses_dict.update({response.status: {}})
-                            if message not in failed_statuses_dict[response.status].keys():
-                                failed_statuses_dict[response.status].update({message: {}})
-                            if "files" not in failed_statuses_dict[response.status][message].keys():
-                                failed_statuses_dict[response.status][message].update({"files": {}})
-                            if file_name not in failed_statuses_dict[response.status][message]["files"].keys():
-                                failed_statuses_dict[response.status][message]["files"].update({file_name: {}})
-                            if "line_numbers" not in failed_statuses_dict[response.status][message]["files"][file_name].keys():
-                                failed_statuses_dict[response.status][message]["files"][file_name].update({"line_numbers": []})
-                            failed_statuses_dict[response.status][message]["files"][file_name]["line_numbers"].append(line)
-                            self.metadata["resources"][endpoint]["failed_statuses"] = failed_statuses_dict
+                            failures = self.metadata["resources"][endpoint].get("failures", [])
+                            do_append = True
+                            for index, item in enumerate(failures):
+                                if item["status_code"]==response.status and item["message"]==message and item["file"]==file_name:
+                                    failures[index]["line_numbers"].append(line)
+                                    failures[index]["count"] += 1
+                                    do_append = False
+                            if do_append:
+                                failure = {
+                                    'status_code': response.status,
+                                    'message': message,
+                                    'file': file_name,
+                                    'line_numbers': [line],
+                                    'count': 1
+                                }
+                                failures.append(failure)
+                            self.metadata["resources"][endpoint]["failures"] = failures
 
                             # update output and counters
                             self.lightbeam.increment_status_reason(message)
                             if response.status==400:
                                 raise Exception(message)
                             else: self.lightbeam.num_errors += 1
```

## lightbeam/util.py

```diff
@@ -1,9 +1,10 @@
 import re
 import json
+import itertools
 
 # Strips newlines from a string
 # Replace single-quotes with backticks
 def linearize(string: str) -> str:
     exp = re.compile(r"\s+")
     return exp.sub(" ", string).replace("'", "`").strip()
 
@@ -36,8 +37,40 @@
             value = value[key]
         params[k] = value
     return params
 
 def url_join(*args):
     return '/'.join(
         map(lambda x: str(x).rstrip('/'), filter(lambda x: x is not None, args))
-    )
+    )
+
+# Returns the subset of `keys` that match the `keep` and `drop` criteria, importantly
+# respecting wildcards! (so keep=["*Association,student*"] matches anything beginning
+# with "student" or ending with "Association")
+# This function is used for both the endpoint selection in apply_filters() of api.py and
+# the keep-keys and drop-keys filtering in fetch.py
+def apply_selections(keys, keep, drop):
+    # `keep` and `drop` _should_ be arrays, but in case they're strings, we split them
+    if isinstance(keep, str): keep = keep.split(",")
+    if isinstance(drop, str): drop = drop.split(",")
+    # this will be the filtered set of keys
+    final_keys = []
+    # populate `final_keys` with `keys` that match `keep`
+    if keep and keep != ["*"]:
+        for payload_key, keep_key in list(itertools.product(keys, keep)):
+            if (keys_match(payload_key, keep_key)):
+                final_keys.append(payload_key)
+    else: final_keys = keys
+    # remove from `final_keys` keys that match `drop`
+    if drop and drop != [""]:
+        for payload_key, drop_key in list(itertools.product(keys, drop)):
+            if (keys_match(payload_key, drop_key)):
+                if payload_key in final_keys: final_keys.remove(payload_key)
+    return final_keys
+
+# Compares a key like "stateAbbreviationDescriptors" with a (potentially wildcard) expression
+# like "*Descriptors" for match.
+def keys_match(key, wildcard_key):
+    if key==wildcard_key: return True
+    if wildcard_key.startswith("*") and key.endswith(wildcard_key.lstrip("*")): return True
+    if wildcard_key.endswith("*") and key.startswith(wildcard_key.rstrip("*")): return True
+    return False
```

## lightbeam/validate.py

```diff
@@ -27,16 +27,21 @@
             if "Descriptor" in endpoint: swagger = self.lightbeam.api.descriptors_swagger
             else: swagger = self.lightbeam.api.resources_swagger
             self.validate_endpoint(swagger, endpoint, local_descriptors)
 
     # Validates a single endpoint based on the Swagger docs
     def validate_endpoint(self, swagger, endpoint, local_descriptors=[]):
         definition = util.camel_case(self.lightbeam.config["namespace"]) + "_" + util.singularize_endpoint(endpoint)
-        resource_schema = swagger["definitions"][definition]
-
+        if "definitions" in swagger.keys():
+            resource_schema = swagger["definitions"][definition]
+        elif "components" in swagger.keys() and "schemas" in swagger["components"].keys():
+            resource_schema = swagger["components"]["schemas"][definition]
+        else:
+            self.logger.critical(f"Swagger contains neither `definitions` nor `components.schemas` - check that the Swagger is valid.")
+        
         resolver = RefResolver("test", swagger, swagger)
         validator = Draft4Validator(resource_schema, resolver=resolver)
         params_structure = self.lightbeam.api.get_params_for_endpoint(endpoint)
         distinct_params = []
 
         endpoint_data_files = self.lightbeam.get_data_files_for_endpoint(endpoint)
         for file in endpoint_data_files:
```

## Comparing `lightbeam-0.1.1.dist-info/LICENSE` & `lightbeam-0.1.2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `lightbeam-0.1.1.dist-info/METADATA` & `lightbeam-0.1.2.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: lightbeam
-Version: 0.1.1
+Version: 0.1.2
 Summary: Sends JSONL data into an Ed-Fi API
 Home-page: https://github.com/edanalytics/lightbeam
 Download-URL: https://github.com/edanalytics/lightbeam/archive/0.0.1.tar.gz
 Author: Tom Reitz
 Author-email: treitz@edanalytics.org
 License: Apache 2.0
 Keywords: data,transmission,api,edfi
@@ -20,21 +20,21 @@
 Classifier: Topic :: Office/Business
 Classifier: Topic :: Scientific/Engineering
 Classifier: Topic :: Utilities
 Requires-Python: >=3
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: wheel
-Requires-Dist: aiohttp >=3.8.1
-Requires-Dist: aiohttp-retry >=2.8.3
-Requires-Dist: jsonschema >=4.16.0
-Requires-Dist: python-dateutil >=2.8.2
-Requires-Dist: pyyaml >=6.0
-Requires-Dist: requests >=2.28.1
-Requires-Dist: setuptools >=44.0.0
+Requires-Dist: aiohttp (>=3.8.1)
+Requires-Dist: aiohttp-retry (>=2.8.3)
+Requires-Dist: jsonschema (>=4.16.0)
+Requires-Dist: python-dateutil (>=2.8.2)
+Requires-Dist: pyyaml (>=6.0)
+Requires-Dist: requests (>=2.28.1)
+Requires-Dist: setuptools (>=44.0.0)
 
 <!-- Logo/image -->
 ![lightbeam](https://raw.githubusercontent.com/edanalytics/lightbeam/main/lightbeam/resources/lightbeam.png)
 
 `lightbeam` transmits payloads from JSONL files into an [Ed-Fi API](https://techdocs.ed-fi.org/display/ETKB/Ed-Fi+Operational+Data+Store+and+API).
 <!-- GIF or screenshot? -->
 
@@ -150,14 +150,16 @@
 lightbeam fetch -s student* -e *Descriptors -q '{"studentUniqueId":12345}' -d id,_etag,_lastModifiedDate
 ```
 
 Optionally specify `--keep-keys id` or `-k id` to keep only specific keys from every payload. This can be useful to reduce the amount of data stored if you only need certain fields. It is used internally by `truncate` to only `fetch` the `id`s or payloads to then `delete` by `id`.
 
 Optionally specify `--drop-keys id,_etag,_lastModified` or `-d id` to remove specific keys from every payload. This can be useful if you want to `fetch` data from one Ed-Fi API and then turn around and `send` it to another.
 
+Like [selectors](#selectors), `keep-keys` and `drop-keys` are comma-separated lists of values, each of which may begin or end with an asterisk (`*`) for wildcard matching. Example: `-d _*` would remove properties beginning with an underscore (`_`) character from any `fetch`ed payloads.
+
 ## `validate`
 ```bash
 lightbeam validate -c path/to/config.yaml
 ```
 You may `validate` your JSONL before transmitting it. This checks that the payloads
 1. are valid JSON
 1. conform to the structure described in the Swagger documents for [resources](https://api.ed-fi.org/v5.3/api/metadata/data/v3/resourcess/swagger.json) and [descriptors](https://api.ed-fi.org/v5.3/api/metadata/data/v3/descriptors/swagger.json) fetched from your API
```

## Comparing `lightbeam-0.1.1.dist-info/RECORD` & `lightbeam-0.1.2.dist-info/RECORD`

 * *Files 16% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 lightbeam/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 lightbeam/__main__.py,sha256=JflnuPtIhRh7-2lfY7lHP48fc1uHHscBrRv1QlL1eo8,5915
-lightbeam/api.py,sha256=YxMtZr1etaCSBgA_2xR4t0LzwS1onEEHmHzZ0tWp7qQ,17611
+lightbeam/api.py,sha256=yoKdyw9tEMRA8Nk3Y93Pr1CMwhfrWfG1dSUrS3BnvtQ,16486
 lightbeam/count.py,sha256=AVC3CfY7IZNJwGyKEi0fK_u9nSvrFqdPlgFSUqHJle8,3516
 lightbeam/delete.py,sha256=LaN3HJ8mMiX93WGXowlPaOBLXzFA3aNCpSQfy9Vd_RA,12784
-lightbeam/fetch.py,sha256=Jw9tGCEisPDKQD4GBraDFk0dsW18AxepELSHoTLv40c,5536
+lightbeam/fetch.py,sha256=VKI79xl6jq_H-NdBHpPYG2NCnsLU8iR3J7Bq0NhHT3M,5362
 lightbeam/hashlog.py,sha256=K6A0xXhkoWgOpjg6WlF5_bp-YvinH27lOhPb815shrs,672
 lightbeam/lightbeam.py,sha256=Ibq5r-R8mvx7JiwSrf6Tev5oGw6nXKvbgmIKjSBPOto,9611
-lightbeam/send.py,sha256=9sT31giysZxjNHLk5WrCQKwDHkxOOeb-wQJ3ietKxBw,11923
+lightbeam/send.py,sha256=sKl_TW3woM9_61Mb1yitqb5KnrnUBpOXOtH5iGsfi1c,11458
 lightbeam/truncate.py,sha256=0exWnxZdhZXLzDVQLzeLIRXxHsIxodsrSnm7rZs0e1k,4066
-lightbeam/util.py,sha256=UfB2o_yidEMRQxBD1s_sVuDpCMJ4OLIWUOICi9Z0BQE,1431
-lightbeam/validate.py,sha256=Dt99_gAhDb7p_poKfUAzHgE6MXlv3GYclzr5dctAq4s,7051
+lightbeam/util.py,sha256=-LxzDFlBCO8laThBBSsK687FhpSWlQ-UNIafinvLrgM,3107
+lightbeam/validate.py,sha256=4rMT2a14HHahs3dfSGKV9kvTTUtWcQULrBdNIvzvhY4,7423
 lightbeam/resources/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-lightbeam-0.1.1.dist-info/LICENSE,sha256=QoIB1PHNf6dGs6i0VkXumtNelR-vv2rfNMAHMnlaIDw,11349
-lightbeam-0.1.1.dist-info/METADATA,sha256=m-S9henATfPxnf2M0TlvaGCxeI14kGCiMiV3OnhJME0,19903
-lightbeam-0.1.1.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
-lightbeam-0.1.1.dist-info/dependency_links.txt,sha256=zaXVnhNfDDVKDG63P2d5g3NMI6enCOEsVLmo7bmHAjA,132
-lightbeam-0.1.1.dist-info/entry_points.txt,sha256=BbLylxOffnTGU7NfApSlFKDf_C0S-fqE_mcvz0iuuKA,54
-lightbeam-0.1.1.dist-info/top_level.txt,sha256=k8DpVRedspxz4O88pMNN_ClMaC22KtuvkF1HHsxiUow,10
-lightbeam-0.1.1.dist-info/RECORD,,
+lightbeam-0.1.2.dist-info/LICENSE,sha256=QoIB1PHNf6dGs6i0VkXumtNelR-vv2rfNMAHMnlaIDw,11349
+lightbeam-0.1.2.dist-info/METADATA,sha256=1eSdnfgpC26pmDp1_QW2VRS3o7XK0jgKIhbKl3-qzxA,20205
+lightbeam-0.1.2.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+lightbeam-0.1.2.dist-info/dependency_links.txt,sha256=zaXVnhNfDDVKDG63P2d5g3NMI6enCOEsVLmo7bmHAjA,132
+lightbeam-0.1.2.dist-info/entry_points.txt,sha256=BbLylxOffnTGU7NfApSlFKDf_C0S-fqE_mcvz0iuuKA,54
+lightbeam-0.1.2.dist-info/top_level.txt,sha256=k8DpVRedspxz4O88pMNN_ClMaC22KtuvkF1HHsxiUow,10
+lightbeam-0.1.2.dist-info/RECORD,,
```

