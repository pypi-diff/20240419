# Comparing `tmp/ydf-0.4.0-cp39-cp39-win_amd64.whl.zip` & `tmp/ydf-0.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,117 +1,146 @@
-Zip file size: 5030916 bytes, number of entries: 115
--rw-rw-rw-  2.0 fat     2671 b- defN 24-Apr-12 21:05 ydf/__init__.py
--rw-rw-rw-  2.0 fat     6499 b- defN 24-Apr-12 21:05 ydf/api_test.py
--rw-rw-rw-  2.0 fat      610 b- defN 24-Apr-12 21:05 ydf/version.py
--rw-rw-rw-  2.0 fat      591 b- defN 24-Apr-12 21:05 ydf/cc/__init__.py
--rw-rw-rw-  2.0 fat 11149312 b- defN 24-Apr-12 21:05 ydf/cc/ydf.pyd
--rw-rw-rw-  2.0 fat      591 b- defN 24-Apr-12 21:05 ydf/dataset/__init__.py
--rw-rw-rw-  2.0 fat    24355 b- defN 24-Apr-12 21:05 ydf/dataset/dataset.py
--rw-rw-rw-  2.0 fat    65185 b- defN 24-Apr-12 21:05 ydf/dataset/dataset_test.py
--rw-rw-rw-  2.0 fat     1762 b- defN 24-Apr-12 21:05 ydf/dataset/dataset_with_tf_test.py
--rw-rw-rw-  2.0 fat    23287 b- defN 24-Apr-12 21:05 ydf/dataset/dataspec.py
--rw-rw-rw-  2.0 fat    13459 b- defN 24-Apr-12 21:05 ydf/dataset/dataspec_test.py
--rw-rw-rw-  2.0 fat      591 b- defN 24-Apr-12 21:05 ydf/dataset/io/__init__.py
--rw-rw-rw-  2.0 fat     5549 b- defN 24-Apr-12 21:05 ydf/dataset/io/dataset_io.py
--rw-rw-rw-  2.0 fat     1219 b- defN 24-Apr-12 21:05 ydf/dataset/io/dataset_io_test.py
--rw-rw-rw-  2.0 fat     1718 b- defN 24-Apr-12 21:05 ydf/dataset/io/dataset_io_types.py
--rw-rw-rw-  2.0 fat     1917 b- defN 24-Apr-12 21:05 ydf/dataset/io/pandas_io.py
--rw-rw-rw-  2.0 fat      983 b- defN 24-Apr-12 21:05 ydf/dataset/io/pandas_io_test.py
--rw-rw-rw-  2.0 fat     1488 b- defN 24-Apr-12 21:05 ydf/dataset/io/tensorflow_io.py
--rw-rw-rw-  2.0 fat      591 b- defN 24-Apr-12 21:05 ydf/learner/__init__.py
--rw-rw-rw-  2.0 fat    12348 b- defN 24-Apr-12 21:05 ydf/learner/custom_loss.py
--rw-rw-rw-  2.0 fat    25040 b- defN 24-Apr-12 21:05 ydf/learner/custom_loss_test.py
--rw-rw-rw-  2.0 fat     5141 b- defN 24-Apr-12 21:05 ydf/learner/distributed_learner_test.py
--rw-rw-rw-  2.0 fat    20901 b- defN 24-Apr-12 21:05 ydf/learner/generic_learner.py
--rw-rw-rw-  2.0 fat     3939 b- defN 24-Apr-12 21:05 ydf/learner/hyperparameters.py
--rw-rw-rw-  2.0 fat    30705 b- defN 24-Apr-12 21:05 ydf/learner/learner_test.py
--rw-rw-rw-  2.0 fat     1574 b- defN 24-Apr-12 21:05 ydf/learner/learner_with_tf_test.py
--rw-rw-rw-  2.0 fat   127185 b- defN 24-Apr-12 21:05 ydf/learner/specialized_learners.py
--rw-rw-rw-  2.0 fat   129404 b- defN 24-Apr-12 21:05 ydf/learner/specialized_learners_pre_generated.py
--rw-rw-rw-  2.0 fat    12862 b- defN 24-Apr-12 21:05 ydf/learner/tuner.py
--rw-rw-rw-  2.0 fat     7252 b- defN 24-Apr-12 21:05 ydf/learner/tuner_test.py
--rw-rw-rw-  2.0 fat     2155 b- defN 24-Apr-12 21:05 ydf/learner/worker.py
--rw-rw-rw-  2.0 fat      879 b- defN 24-Apr-12 21:05 ydf/learner/worker_main.py
--rw-rw-rw-  2.0 fat      591 b- defN 24-Apr-12 21:05 ydf/metric/__init__.py
--rw-rw-rw-  2.0 fat    12187 b- defN 24-Apr-12 21:05 ydf/metric/display_metric.py
--rw-rw-rw-  2.0 fat    15907 b- defN 24-Apr-12 21:05 ydf/metric/metric.py
--rw-rw-rw-  2.0 fat     8393 b- defN 24-Apr-12 21:05 ydf/metric/metric_test.py
--rw-rw-rw-  2.0 fat      591 b- defN 24-Apr-12 21:05 ydf/model/__init__.py
--rw-rw-rw-  2.0 fat     9665 b- defN 24-Apr-12 21:05 ydf/model/analysis.py
--rw-rw-rw-  2.0 fat     1467 b- defN 24-Apr-12 21:05 ydf/model/export_cc_generator.py
--rw-rw-rw-  2.0 fat    22096 b- defN 24-Apr-12 21:05 ydf/model/export_tf.py
--rw-rw-rw-  2.0 fat    37036 b- defN 24-Apr-12 21:05 ydf/model/generic_model.py
--rw-rw-rw-  2.0 fat     7293 b- defN 24-Apr-12 21:05 ydf/model/model_lib.py
--rw-rw-rw-  2.0 fat     2005 b- defN 24-Apr-12 21:05 ydf/model/model_metadata.py
--rw-rw-rw-  2.0 fat    21913 b- defN 24-Apr-12 21:05 ydf/model/model_test.py
--rw-rw-rw-  2.0 fat     2521 b- defN 24-Apr-12 21:05 ydf/model/optimizer_logs.py
--rw-rw-rw-  2.0 fat     2654 b- defN 24-Apr-12 21:05 ydf/model/optimizer_logs_test.py
--rw-rw-rw-  2.0 fat     8469 b- defN 24-Apr-12 21:05 ydf/model/template_cpp_export.py
--rw-rw-rw-  2.0 fat    43175 b- defN 24-Apr-12 21:05 ydf/model/tf_model_test.py
--rw-rw-rw-  2.0 fat      591 b- defN 24-Apr-12 21:05 ydf/model/decision_forest_model/__init__.py
--rw-rw-rw-  2.0 fat     7760 b- defN 24-Apr-12 21:05 ydf/model/decision_forest_model/decision_forest_model.py
--rw-rw-rw-  2.0 fat     5125 b- defN 24-Apr-12 21:05 ydf/model/decision_forest_model/decision_forest_model_test.py
--rw-rw-rw-  2.0 fat      591 b- defN 24-Apr-12 21:05 ydf/model/gradient_boosted_trees_model/__init__.py
--rw-rw-rw-  2.0 fat     3644 b- defN 24-Apr-12 21:05 ydf/model/gradient_boosted_trees_model/gradient_boosted_trees_model.py
--rw-rw-rw-  2.0 fat    17156 b- defN 24-Apr-12 21:05 ydf/model/gradient_boosted_trees_model/gradient_boosted_trees_model_test.py
--rw-rw-rw-  2.0 fat      591 b- defN 24-Apr-12 21:05 ydf/model/random_forest_model/__init__.py
--rw-rw-rw-  2.0 fat     5334 b- defN 24-Apr-12 21:05 ydf/model/random_forest_model/random_forest_model.py
--rw-rw-rw-  2.0 fat     3485 b- defN 24-Apr-12 21:05 ydf/model/random_forest_model/random_forest_model_test.py
--rw-rw-rw-  2.0 fat     1801 b- defN 24-Apr-12 21:05 ydf/model/tree/__init__.py
--rw-rw-rw-  2.0 fat    18431 b- defN 24-Apr-12 21:05 ydf/model/tree/condition.py
--rw-rw-rw-  2.0 fat    18098 b- defN 24-Apr-12 21:05 ydf/model/tree/condition_test.py
--rw-rw-rw-  2.0 fat     5846 b- defN 24-Apr-12 21:05 ydf/model/tree/node.py
--rw-rw-rw-  2.0 fat     1390 b- defN 24-Apr-12 21:05 ydf/model/tree/node_test.py
--rw-rw-rw-  2.0 fat     3726 b- defN 24-Apr-12 21:05 ydf/model/tree/plot.py
--rw-rw-rw-  2.0 fat     2361 b- defN 24-Apr-12 21:05 ydf/model/tree/plot_test.py
--rw-rw-rw-  2.0 fat     5317 b- defN 24-Apr-12 21:05 ydf/model/tree/tree.py
--rw-rw-rw-  2.0 fat     5604 b- defN 24-Apr-12 21:05 ydf/model/tree/tree_test.py
--rw-rw-rw-  2.0 fat     7017 b- defN 24-Apr-12 21:05 ydf/model/tree/value.py
--rw-rw-rw-  2.0 fat     4799 b- defN 24-Apr-12 21:05 ydf/model/tree/value_test.py
--rw-rw-rw-  2.0 fat      591 b- defN 24-Apr-12 21:05 ydf/utils/__init__.py
--rw-rw-rw-  2.0 fat     1020 b- defN 24-Apr-12 21:05 ydf/utils/documentation.py
--rw-rw-rw-  2.0 fat     2982 b- defN 24-Apr-12 21:05 ydf/utils/html.py
--rw-rw-rw-  2.0 fat     1634 b- defN 24-Apr-12 21:05 ydf/utils/html_test.py
--rw-rw-rw-  2.0 fat     6172 b- defN 24-Apr-12 21:05 ydf/utils/log.py
--rw-rw-rw-  2.0 fat     1840 b- defN 24-Apr-12 21:05 ydf/utils/paths.py
--rw-rw-rw-  2.0 fat     1903 b- defN 24-Apr-12 21:05 ydf/utils/paths_test.py
--rw-rw-rw-  2.0 fat     4648 b- defN 24-Apr-12 21:05 ydf/utils/string_lib.py
--rw-rw-rw-  2.0 fat     4196 b- defN 24-Apr-12 21:05 ydf/utils/string_lib_test.py
--rw-rw-rw-  2.0 fat     6127 b- defN 24-Apr-12 21:05 ydf/utils/test_utils.py
--rw-rw-rw-  2.0 fat     3031 b- defN 24-Apr-12 21:05 ydf/utils/test_utils_test.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/__init__.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/dataset/__init__.py
--rw-rw-rw-  2.0 fat    12217 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/dataset/data_spec_pb2.py
--rw-rw-rw-  2.0 fat     2952 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/dataset/example_pb2.py
--rw-rw-rw-  2.0 fat     3286 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/dataset/weight_pb2.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/learner/__init__.py
--rw-rw-rw-  2.0 fat     6638 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/learner/abstract_learner_pb2.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/learner/hyperparameters_optimizer/__init__.py
--rw-rw-rw-  2.0 fat     4481 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/learner/hyperparameters_optimizer/hyperparameters_optimizer_pb2.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/learner/hyperparameters_optimizer/optimizers/__init__.py
--rw-rw-rw-  2.0 fat     1840 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/learner/hyperparameters_optimizer/optimizers/random_pb2.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/metric/__init__.py
--rw-rw-rw-  2.0 fat    17906 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/metric/metric_pb2.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/model/__init__.py
--rw-rw-rw-  2.0 fat     5226 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/model/abstract_model_pb2.py
--rw-rw-rw-  2.0 fat     7941 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/model/hyperparameter_pb2.py
--rw-rw-rw-  2.0 fat     3413 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/model/prediction_pb2.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/model/decision_tree/__init__.py
--rw-rw-rw-  2.0 fat     7492 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/model/decision_tree/decision_tree_pb2.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/model/random_forest/__init__.py
--rw-rw-rw-  2.0 fat     2920 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/model/random_forest/random_forest_pb2.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/utils/__init__.py
--rw-rw-rw-  2.0 fat     2958 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/utils/distribution_pb2.py
--rw-rw-rw-  2.0 fat     3094 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/utils/fold_generator_pb2.py
--rw-rw-rw-  2.0 fat    10351 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/utils/model_analysis_pb2.py
--rw-rw-rw-  2.0 fat     4789 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/utils/partial_dependence_plot_pb2.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/utils/distribute/__init__.py
--rw-rw-rw-  2.0 fat     1935 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/utils/distribute/distribute_pb2.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/utils/distribute/implementations/__init__.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/utils/distribute/implementations/grpc/__init__.py
--rw-rw-rw-  2.0 fat     4791 b- defN 24-Apr-12 21:05 yggdrasil_decision_forests/utils/distribute/implementations/grpc/grpc_pb2.py
--rw-rw-rw-  2.0 fat    13759 b- defN 24-Apr-12 21:06 ydf-0.4.0.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     3890 b- defN 24-Apr-12 21:06 ydf-0.4.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 24-Apr-12 21:06 ydf-0.4.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       31 b- defN 24-Apr-12 21:06 ydf-0.4.0.dist-info/top_level.txt
--rw-rw-r--  2.0 fat    10735 b- defN 24-Apr-12 21:06 ydf-0.4.0.dist-info/RECORD
-115 files, 12163282 bytes uncompressed, 5013636 bytes compressed:  58.8%
+Zip file size: 9139153 bytes, number of entries: 144
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf-0.4.1.dist-info/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf.libs/
+-rw-rw-r--  2.0 unx    10486 b- defN 24-Apr-19 14:17 ydf-0.4.1.dist-info/RECORD
+-rw-r-----  2.0 unx    13516 b- defN 24-Apr-19 14:17 ydf-0.4.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx        4 b- defN 24-Apr-19 14:17 ydf-0.4.1.dist-info/top_level.txt
+-rw-r--r--  2.0 unx     3798 b- defN 24-Apr-19 14:17 ydf-0.4.1.dist-info/METADATA
+-rw-r--r--  2.0 unx      148 b- defN 24-Apr-19 14:17 ydf-0.4.1.dist-info/WHEEL
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/learner/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/utils/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/model/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/metric/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/dataset/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/proto/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/cc/
+-rw-r--r--  2.0 unx     2685 b- defN 24-Apr-19 14:17 ydf/__init__.py
+-rw-r--r--  2.0 unx      595 b- defN 24-Apr-19 14:17 ydf/version.py
+-rw-r--r--  2.0 unx     6303 b- defN 24-Apr-19 14:17 ydf/api_test.py
+-rw-r--r--  2.0 unx     6986 b- defN 24-Apr-19 14:17 ydf/learner/tuner_test.py
+-rw-r--r--  2.0 unx     3806 b- defN 24-Apr-19 14:17 ydf/learner/hyperparameters.py
+-rw-r--r--  2.0 unx    20199 b- defN 24-Apr-19 14:17 ydf/learner/generic_learner.py
+-rw-r--r--  2.0 unx   126989 b- defN 24-Apr-19 14:17 ydf/learner/specialized_learners_pre_generated.py
+-rw-r--r--  2.0 unx     1823 b- defN 24-Apr-19 14:17 ydf/learner/learner_with_tf_test.py
+-rw-r--r--  2.0 unx    12450 b- defN 24-Apr-19 14:17 ydf/learner/tuner.py
+-rw-r--r--  2.0 unx     2083 b- defN 24-Apr-19 14:17 ydf/learner/worker.py
+-rw-r--r--  2.0 unx    12000 b- defN 24-Apr-19 14:17 ydf/learner/custom_loss.py
+-rw-r--r--  2.0 unx      577 b- defN 24-Apr-19 14:17 ydf/learner/__init__.py
+-rw-r--r--  2.0 unx   124984 b- defN 24-Apr-19 14:17 ydf/learner/specialized_learners.py
+-rw-r--r--  2.0 unx      850 b- defN 24-Apr-19 14:17 ydf/learner/worker_main.py
+-rw-r--r--  2.0 unx     4972 b- defN 24-Apr-19 14:17 ydf/learner/distributed_learner_test.py
+-rw-r--r--  2.0 unx    24327 b- defN 24-Apr-19 14:17 ydf/learner/custom_loss_test.py
+-rw-r--r--  2.0 unx    29761 b- defN 24-Apr-19 14:17 ydf/learner/learner_test.py
+-rw-r--r--  2.0 unx     5918 b- defN 24-Apr-19 14:17 ydf/utils/test_utils.py
+-rw-r--r--  2.0 unx     2947 b- defN 24-Apr-19 14:17 ydf/utils/test_utils_test.py
+-rw-r--r--  2.0 unx     2864 b- defN 24-Apr-19 14:17 ydf/utils/html.py
+-rw-r--r--  2.0 unx     1790 b- defN 24-Apr-19 14:17 ydf/utils/paths.py
+-rw-r--r--  2.0 unx     4063 b- defN 24-Apr-19 14:17 ydf/utils/string_lib_test.py
+-rw-r--r--  2.0 unx     1852 b- defN 24-Apr-19 14:17 ydf/utils/paths_test.py
+-rw-r--r--  2.0 unx      994 b- defN 24-Apr-19 14:17 ydf/utils/documentation.py
+-rw-r--r--  2.0 unx     1579 b- defN 24-Apr-19 14:17 ydf/utils/html_test.py
+-rw-r--r--  2.0 unx      577 b- defN 24-Apr-19 14:17 ydf/utils/__init__.py
+-rw-r--r--  2.0 unx     4497 b- defN 24-Apr-19 14:17 ydf/utils/string_lib.py
+-rw-r--r--  2.0 unx     5940 b- defN 24-Apr-19 14:17 ydf/utils/log.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/model/decision_forest_model/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/model/gradient_boosted_trees_model/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/model/tree/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/model/random_forest_model/
+-rw-r--r--  2.0 unx     2544 b- defN 24-Apr-19 14:17 ydf/model/optimizer_logs_test.py
+-rw-r--r--  2.0 unx    21291 b- defN 24-Apr-19 14:17 ydf/model/model_test.py
+-rw-r--r--  2.0 unx     8212 b- defN 24-Apr-19 14:17 ydf/model/template_cpp_export.py
+-rw-r--r--  2.0 unx     7060 b- defN 24-Apr-19 14:17 ydf/model/model_lib.py
+-rw-r--r--  2.0 unx     9317 b- defN 24-Apr-19 14:17 ydf/model/analysis.py
+-rw-r--r--  2.0 unx      577 b- defN 24-Apr-19 14:17 ydf/model/__init__.py
+-rw-r--r--  2.0 unx     1417 b- defN 24-Apr-19 14:17 ydf/model/export_cc_generator.py
+-rw-r--r--  2.0 unx    21406 b- defN 24-Apr-19 14:17 ydf/model/export_tf.py
+-rw-r--r--  2.0 unx     1930 b- defN 24-Apr-19 14:17 ydf/model/model_metadata.py
+-rw-r--r--  2.0 unx    35981 b- defN 24-Apr-19 14:17 ydf/model/generic_model.py
+-rw-r--r--  2.0 unx    41994 b- defN 24-Apr-19 14:17 ydf/model/tf_model_test.py
+-rw-r--r--  2.0 unx     4223 b- defN 24-Apr-19 14:17 ydf/model/export_jax.py
+-rw-r--r--  2.0 unx     6756 b- defN 24-Apr-19 14:17 ydf/model/jax_model_test.py
+-rw-r--r--  2.0 unx     2402 b- defN 24-Apr-19 14:17 ydf/model/optimizer_logs.py
+-rw-r--r--  2.0 unx     7519 b- defN 24-Apr-19 14:17 ydf/model/decision_forest_model/decision_forest_model.py
+-rw-r--r--  2.0 unx      577 b- defN 24-Apr-19 14:17 ydf/model/decision_forest_model/__init__.py
+-rw-r--r--  2.0 unx     4983 b- defN 24-Apr-19 14:17 ydf/model/decision_forest_model/decision_forest_model_test.py
+-rw-r--r--  2.0 unx    16644 b- defN 24-Apr-19 14:17 ydf/model/gradient_boosted_trees_model/gradient_boosted_trees_model_test.py
+-rw-r--r--  2.0 unx     3526 b- defN 24-Apr-19 14:17 ydf/model/gradient_boosted_trees_model/gradient_boosted_trees_model.py
+-rw-r--r--  2.0 unx      577 b- defN 24-Apr-19 14:17 ydf/model/gradient_boosted_trees_model/__init__.py
+-rw-r--r--  2.0 unx     2278 b- defN 24-Apr-19 14:17 ydf/model/tree/plot_test.py
+-rw-r--r--  2.0 unx    17824 b- defN 24-Apr-19 14:17 ydf/model/tree/condition.py
+-rw-r--r--  2.0 unx    17539 b- defN 24-Apr-19 14:17 ydf/model/tree/condition_test.py
+-rw-r--r--  2.0 unx     1350 b- defN 24-Apr-19 14:17 ydf/model/tree/node_test.py
+-rw-r--r--  2.0 unx     5414 b- defN 24-Apr-19 14:17 ydf/model/tree/tree_test.py
+-rw-r--r--  2.0 unx     5625 b- defN 24-Apr-19 14:17 ydf/model/tree/node.py
+-rw-r--r--  2.0 unx     4615 b- defN 24-Apr-19 14:17 ydf/model/tree/value_test.py
+-rw-r--r--  2.0 unx     1755 b- defN 24-Apr-19 14:17 ydf/model/tree/__init__.py
+-rw-r--r--  2.0 unx     6771 b- defN 24-Apr-19 14:17 ydf/model/tree/value.py
+-rw-r--r--  2.0 unx     3603 b- defN 24-Apr-19 14:17 ydf/model/tree/plot.py
+-rw-r--r--  2.0 unx     5113 b- defN 24-Apr-19 14:17 ydf/model/tree/tree.py
+-rw-r--r--  2.0 unx     5158 b- defN 24-Apr-19 14:17 ydf/model/random_forest_model/random_forest_model.py
+-rw-r--r--  2.0 unx      577 b- defN 24-Apr-19 14:17 ydf/model/random_forest_model/__init__.py
+-rw-r--r--  2.0 unx     3379 b- defN 24-Apr-19 14:17 ydf/model/random_forest_model/random_forest_model_test.py
+-rw-r--r--  2.0 unx      577 b- defN 24-Apr-19 14:17 ydf/metric/__init__.py
+-rw-r--r--  2.0 unx    15362 b- defN 24-Apr-19 14:17 ydf/metric/metric.py
+-rw-r--r--  2.0 unx    11702 b- defN 24-Apr-19 14:17 ydf/metric/display_metric.py
+-rw-r--r--  2.0 unx     8045 b- defN 24-Apr-19 14:17 ydf/metric/metric_test.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/dataset/io/
+-rw-r--r--  2.0 unx    22625 b- defN 24-Apr-19 14:17 ydf/dataset/dataspec.py
+-rw-r--r--  2.0 unx    63339 b- defN 24-Apr-19 14:17 ydf/dataset/dataset_test.py
+-rw-r--r--  2.0 unx    23726 b- defN 24-Apr-19 14:17 ydf/dataset/dataset.py
+-rw-r--r--  2.0 unx      577 b- defN 24-Apr-19 14:17 ydf/dataset/__init__.py
+-rw-r--r--  2.0 unx     1692 b- defN 24-Apr-19 14:17 ydf/dataset/dataset_with_tf_test.py
+-rw-r--r--  2.0 unx    13051 b- defN 24-Apr-19 14:17 ydf/dataset/dataspec_test.py
+-rw-r--r--  2.0 unx     1853 b- defN 24-Apr-19 14:17 ydf/dataset/io/pandas_io.py
+-rw-r--r--  2.0 unx     1669 b- defN 24-Apr-19 14:17 ydf/dataset/io/dataset_io_types.py
+-rw-r--r--  2.0 unx      577 b- defN 24-Apr-19 14:17 ydf/dataset/io/__init__.py
+-rw-r--r--  2.0 unx     5377 b- defN 24-Apr-19 14:17 ydf/dataset/io/dataset_io.py
+-rw-r--r--  2.0 unx      952 b- defN 24-Apr-19 14:17 ydf/dataset/io/pandas_io_test.py
+-rw-r--r--  2.0 unx     1773 b- defN 24-Apr-19 14:17 ydf/dataset/io/tensorflow_io.py
+-rw-r--r--  2.0 unx     1180 b- defN 24-Apr-19 14:17 ydf/dataset/io/dataset_io_test.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/proto/learner/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/proto/utils/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/proto/model/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/proto/metric/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/proto/dataset/
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-19 14:17 ydf/proto/__init__.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/proto/learner/hyperparameters_optimizer/
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-19 14:17 ydf/proto/learner/__init__.py
+-rw-r--r--  2.0 unx     6553 b- defN 24-Apr-19 14:17 ydf/proto/learner/abstract_learner_pb2.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/proto/learner/hyperparameters_optimizer/optimizers/
+-rw-r--r--  2.0 unx     4430 b- defN 24-Apr-19 14:17 ydf/proto/learner/hyperparameters_optimizer/hyperparameters_optimizer_pb2.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-19 14:17 ydf/proto/learner/hyperparameters_optimizer/__init__.py
+-rw-r--r--  2.0 unx     1823 b- defN 24-Apr-19 14:17 ydf/proto/learner/hyperparameters_optimizer/optimizers/random_pb2.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-19 14:17 ydf/proto/learner/hyperparameters_optimizer/optimizers/__init__.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/proto/utils/distribute/
+-rw-r--r--  2.0 unx    10266 b- defN 24-Apr-19 14:17 ydf/proto/utils/model_analysis_pb2.py
+-rw-r--r--  2.0 unx     3094 b- defN 24-Apr-19 14:17 ydf/proto/utils/fold_generator_pb2.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-19 14:17 ydf/proto/utils/__init__.py
+-rw-r--r--  2.0 unx     4755 b- defN 24-Apr-19 14:17 ydf/proto/utils/partial_dependence_plot_pb2.py
+-rw-r--r--  2.0 unx     2958 b- defN 24-Apr-19 14:17 ydf/proto/utils/distribution_pb2.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/proto/utils/distribute/implementations/
+-rw-r--r--  2.0 unx     1935 b- defN 24-Apr-19 14:17 ydf/proto/utils/distribute/distribute_pb2.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-19 14:17 ydf/proto/utils/distribute/__init__.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/proto/utils/distribute/implementations/grpc/
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-19 14:17 ydf/proto/utils/distribute/implementations/__init__.py
+-rw-r--r--  2.0 unx     4774 b- defN 24-Apr-19 14:17 ydf/proto/utils/distribute/implementations/grpc/grpc_pb2.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-19 14:17 ydf/proto/utils/distribute/implementations/grpc/__init__.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/proto/model/decision_tree/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-19 14:17 ydf/proto/model/random_forest/
+-rw-r--r--  2.0 unx     7941 b- defN 24-Apr-19 14:17 ydf/proto/model/hyperparameter_pb2.py
+-rw-r--r--  2.0 unx     3396 b- defN 24-Apr-19 14:17 ydf/proto/model/prediction_pb2.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-19 14:17 ydf/proto/model/__init__.py
+-rw-r--r--  2.0 unx     5192 b- defN 24-Apr-19 14:17 ydf/proto/model/abstract_model_pb2.py
+-rw-r--r--  2.0 unx     7475 b- defN 24-Apr-19 14:17 ydf/proto/model/decision_tree/decision_tree_pb2.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-19 14:17 ydf/proto/model/decision_tree/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-19 14:17 ydf/proto/model/random_forest/__init__.py
+-rw-r--r--  2.0 unx     2886 b- defN 24-Apr-19 14:17 ydf/proto/model/random_forest/random_forest_pb2.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-19 14:17 ydf/proto/metric/__init__.py
+-rw-r--r--  2.0 unx    17821 b- defN 24-Apr-19 14:17 ydf/proto/metric/metric_pb2.py
+-rw-r--r--  2.0 unx     3286 b- defN 24-Apr-19 14:17 ydf/proto/dataset/weight_pb2.py
+-rw-r--r--  2.0 unx    12217 b- defN 24-Apr-19 14:17 ydf/proto/dataset/data_spec_pb2.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-19 14:17 ydf/proto/dataset/__init__.py
+-rw-r--r--  2.0 unx     2952 b- defN 24-Apr-19 14:17 ydf/proto/dataset/example_pb2.py
+-rwxr-xr-x  2.0 unx 28886744 b- defN 24-Apr-19 14:17 ydf/cc/ydf.so
+-rw-r--r--  2.0 unx      577 b- defN 24-Apr-19 14:17 ydf/cc/__init__.py
+144 files, 29887452 bytes uncompressed, 9119429 bytes compressed:  69.5%
```

## zipnote {}

```diff
@@ -1,346 +1,433 @@
-Filename: ydf/__init__.py
+Filename: ydf-0.4.1.dist-info/
 Comment: 
 
-Filename: ydf/api_test.py
+Filename: ydf/
 Comment: 
 
-Filename: ydf/version.py
+Filename: ydf.libs/
 Comment: 
 
-Filename: ydf/cc/__init__.py
+Filename: ydf-0.4.1.dist-info/RECORD
 Comment: 
 
-Filename: ydf/cc/ydf.pyd
+Filename: ydf-0.4.1.dist-info/LICENSE
 Comment: 
 
-Filename: ydf/dataset/__init__.py
+Filename: ydf-0.4.1.dist-info/top_level.txt
 Comment: 
 
-Filename: ydf/dataset/dataset.py
+Filename: ydf-0.4.1.dist-info/METADATA
 Comment: 
 
-Filename: ydf/dataset/dataset_test.py
+Filename: ydf-0.4.1.dist-info/WHEEL
 Comment: 
 
-Filename: ydf/dataset/dataset_with_tf_test.py
+Filename: ydf/learner/
 Comment: 
 
-Filename: ydf/dataset/dataspec.py
+Filename: ydf/utils/
 Comment: 
 
-Filename: ydf/dataset/dataspec_test.py
+Filename: ydf/model/
 Comment: 
 
-Filename: ydf/dataset/io/__init__.py
+Filename: ydf/metric/
 Comment: 
 
-Filename: ydf/dataset/io/dataset_io.py
+Filename: ydf/dataset/
 Comment: 
 
-Filename: ydf/dataset/io/dataset_io_test.py
+Filename: ydf/proto/
 Comment: 
 
-Filename: ydf/dataset/io/dataset_io_types.py
+Filename: ydf/cc/
 Comment: 
 
-Filename: ydf/dataset/io/pandas_io.py
+Filename: ydf/__init__.py
 Comment: 
 
-Filename: ydf/dataset/io/pandas_io_test.py
+Filename: ydf/version.py
 Comment: 
 
-Filename: ydf/dataset/io/tensorflow_io.py
+Filename: ydf/api_test.py
 Comment: 
 
-Filename: ydf/learner/__init__.py
+Filename: ydf/learner/tuner_test.py
 Comment: 
 
-Filename: ydf/learner/custom_loss.py
+Filename: ydf/learner/hyperparameters.py
 Comment: 
 
-Filename: ydf/learner/custom_loss_test.py
+Filename: ydf/learner/generic_learner.py
 Comment: 
 
-Filename: ydf/learner/distributed_learner_test.py
+Filename: ydf/learner/specialized_learners_pre_generated.py
 Comment: 
 
-Filename: ydf/learner/generic_learner.py
+Filename: ydf/learner/learner_with_tf_test.py
 Comment: 
 
-Filename: ydf/learner/hyperparameters.py
+Filename: ydf/learner/tuner.py
 Comment: 
 
-Filename: ydf/learner/learner_test.py
+Filename: ydf/learner/worker.py
 Comment: 
 
-Filename: ydf/learner/learner_with_tf_test.py
+Filename: ydf/learner/custom_loss.py
+Comment: 
+
+Filename: ydf/learner/__init__.py
 Comment: 
 
 Filename: ydf/learner/specialized_learners.py
 Comment: 
 
-Filename: ydf/learner/specialized_learners_pre_generated.py
+Filename: ydf/learner/worker_main.py
 Comment: 
 
-Filename: ydf/learner/tuner.py
+Filename: ydf/learner/distributed_learner_test.py
 Comment: 
 
-Filename: ydf/learner/tuner_test.py
+Filename: ydf/learner/custom_loss_test.py
 Comment: 
 
-Filename: ydf/learner/worker.py
+Filename: ydf/learner/learner_test.py
 Comment: 
 
-Filename: ydf/learner/worker_main.py
+Filename: ydf/utils/test_utils.py
 Comment: 
 
-Filename: ydf/metric/__init__.py
+Filename: ydf/utils/test_utils_test.py
 Comment: 
 
-Filename: ydf/metric/display_metric.py
+Filename: ydf/utils/html.py
 Comment: 
 
-Filename: ydf/metric/metric.py
+Filename: ydf/utils/paths.py
 Comment: 
 
-Filename: ydf/metric/metric_test.py
+Filename: ydf/utils/string_lib_test.py
 Comment: 
 
-Filename: ydf/model/__init__.py
+Filename: ydf/utils/paths_test.py
 Comment: 
 
-Filename: ydf/model/analysis.py
+Filename: ydf/utils/documentation.py
 Comment: 
 
-Filename: ydf/model/export_cc_generator.py
+Filename: ydf/utils/html_test.py
 Comment: 
 
-Filename: ydf/model/export_tf.py
+Filename: ydf/utils/__init__.py
 Comment: 
 
-Filename: ydf/model/generic_model.py
+Filename: ydf/utils/string_lib.py
 Comment: 
 
-Filename: ydf/model/model_lib.py
+Filename: ydf/utils/log.py
 Comment: 
 
-Filename: ydf/model/model_metadata.py
+Filename: ydf/model/decision_forest_model/
 Comment: 
 
-Filename: ydf/model/model_test.py
+Filename: ydf/model/gradient_boosted_trees_model/
 Comment: 
 
-Filename: ydf/model/optimizer_logs.py
+Filename: ydf/model/tree/
+Comment: 
+
+Filename: ydf/model/random_forest_model/
 Comment: 
 
 Filename: ydf/model/optimizer_logs_test.py
 Comment: 
 
+Filename: ydf/model/model_test.py
+Comment: 
+
 Filename: ydf/model/template_cpp_export.py
 Comment: 
 
+Filename: ydf/model/model_lib.py
+Comment: 
+
+Filename: ydf/model/analysis.py
+Comment: 
+
+Filename: ydf/model/__init__.py
+Comment: 
+
+Filename: ydf/model/export_cc_generator.py
+Comment: 
+
+Filename: ydf/model/export_tf.py
+Comment: 
+
+Filename: ydf/model/model_metadata.py
+Comment: 
+
+Filename: ydf/model/generic_model.py
+Comment: 
+
 Filename: ydf/model/tf_model_test.py
 Comment: 
 
-Filename: ydf/model/decision_forest_model/__init__.py
+Filename: ydf/model/export_jax.py
+Comment: 
+
+Filename: ydf/model/jax_model_test.py
+Comment: 
+
+Filename: ydf/model/optimizer_logs.py
 Comment: 
 
 Filename: ydf/model/decision_forest_model/decision_forest_model.py
 Comment: 
 
+Filename: ydf/model/decision_forest_model/__init__.py
+Comment: 
+
 Filename: ydf/model/decision_forest_model/decision_forest_model_test.py
 Comment: 
 
-Filename: ydf/model/gradient_boosted_trees_model/__init__.py
+Filename: ydf/model/gradient_boosted_trees_model/gradient_boosted_trees_model_test.py
 Comment: 
 
 Filename: ydf/model/gradient_boosted_trees_model/gradient_boosted_trees_model.py
 Comment: 
 
-Filename: ydf/model/gradient_boosted_trees_model/gradient_boosted_trees_model_test.py
+Filename: ydf/model/gradient_boosted_trees_model/__init__.py
 Comment: 
 
-Filename: ydf/model/random_forest_model/__init__.py
+Filename: ydf/model/tree/plot_test.py
+Comment: 
+
+Filename: ydf/model/tree/condition.py
+Comment: 
+
+Filename: ydf/model/tree/condition_test.py
+Comment: 
+
+Filename: ydf/model/tree/node_test.py
+Comment: 
+
+Filename: ydf/model/tree/tree_test.py
+Comment: 
+
+Filename: ydf/model/tree/node.py
+Comment: 
+
+Filename: ydf/model/tree/value_test.py
+Comment: 
+
+Filename: ydf/model/tree/__init__.py
+Comment: 
+
+Filename: ydf/model/tree/value.py
+Comment: 
+
+Filename: ydf/model/tree/plot.py
+Comment: 
+
+Filename: ydf/model/tree/tree.py
 Comment: 
 
 Filename: ydf/model/random_forest_model/random_forest_model.py
 Comment: 
 
+Filename: ydf/model/random_forest_model/__init__.py
+Comment: 
+
 Filename: ydf/model/random_forest_model/random_forest_model_test.py
 Comment: 
 
-Filename: ydf/model/tree/__init__.py
+Filename: ydf/metric/__init__.py
 Comment: 
 
-Filename: ydf/model/tree/condition.py
+Filename: ydf/metric/metric.py
 Comment: 
 
-Filename: ydf/model/tree/condition_test.py
+Filename: ydf/metric/display_metric.py
 Comment: 
 
-Filename: ydf/model/tree/node.py
+Filename: ydf/metric/metric_test.py
 Comment: 
 
-Filename: ydf/model/tree/node_test.py
+Filename: ydf/dataset/io/
 Comment: 
 
-Filename: ydf/model/tree/plot.py
+Filename: ydf/dataset/dataspec.py
 Comment: 
 
-Filename: ydf/model/tree/plot_test.py
+Filename: ydf/dataset/dataset_test.py
 Comment: 
 
-Filename: ydf/model/tree/tree.py
+Filename: ydf/dataset/dataset.py
 Comment: 
 
-Filename: ydf/model/tree/tree_test.py
+Filename: ydf/dataset/__init__.py
 Comment: 
 
-Filename: ydf/model/tree/value.py
+Filename: ydf/dataset/dataset_with_tf_test.py
 Comment: 
 
-Filename: ydf/model/tree/value_test.py
+Filename: ydf/dataset/dataspec_test.py
 Comment: 
 
-Filename: ydf/utils/__init__.py
+Filename: ydf/dataset/io/pandas_io.py
 Comment: 
 
-Filename: ydf/utils/documentation.py
+Filename: ydf/dataset/io/dataset_io_types.py
 Comment: 
 
-Filename: ydf/utils/html.py
+Filename: ydf/dataset/io/__init__.py
 Comment: 
 
-Filename: ydf/utils/html_test.py
+Filename: ydf/dataset/io/dataset_io.py
 Comment: 
 
-Filename: ydf/utils/log.py
+Filename: ydf/dataset/io/pandas_io_test.py
 Comment: 
 
-Filename: ydf/utils/paths.py
+Filename: ydf/dataset/io/tensorflow_io.py
 Comment: 
 
-Filename: ydf/utils/paths_test.py
+Filename: ydf/dataset/io/dataset_io_test.py
 Comment: 
 
-Filename: ydf/utils/string_lib.py
+Filename: ydf/proto/learner/
 Comment: 
 
-Filename: ydf/utils/string_lib_test.py
+Filename: ydf/proto/utils/
 Comment: 
 
-Filename: ydf/utils/test_utils.py
+Filename: ydf/proto/model/
 Comment: 
 
-Filename: ydf/utils/test_utils_test.py
+Filename: ydf/proto/metric/
+Comment: 
+
+Filename: ydf/proto/dataset/
+Comment: 
+
+Filename: ydf/proto/__init__.py
+Comment: 
+
+Filename: ydf/proto/learner/hyperparameters_optimizer/
+Comment: 
+
+Filename: ydf/proto/learner/__init__.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/__init__.py
+Filename: ydf/proto/learner/abstract_learner_pb2.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/dataset/__init__.py
+Filename: ydf/proto/learner/hyperparameters_optimizer/optimizers/
 Comment: 
 
-Filename: yggdrasil_decision_forests/dataset/data_spec_pb2.py
+Filename: ydf/proto/learner/hyperparameters_optimizer/hyperparameters_optimizer_pb2.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/dataset/example_pb2.py
+Filename: ydf/proto/learner/hyperparameters_optimizer/__init__.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/dataset/weight_pb2.py
+Filename: ydf/proto/learner/hyperparameters_optimizer/optimizers/random_pb2.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/learner/__init__.py
+Filename: ydf/proto/learner/hyperparameters_optimizer/optimizers/__init__.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/learner/abstract_learner_pb2.py
+Filename: ydf/proto/utils/distribute/
 Comment: 
 
-Filename: yggdrasil_decision_forests/learner/hyperparameters_optimizer/__init__.py
+Filename: ydf/proto/utils/model_analysis_pb2.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/learner/hyperparameters_optimizer/hyperparameters_optimizer_pb2.py
+Filename: ydf/proto/utils/fold_generator_pb2.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/learner/hyperparameters_optimizer/optimizers/__init__.py
+Filename: ydf/proto/utils/__init__.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/learner/hyperparameters_optimizer/optimizers/random_pb2.py
+Filename: ydf/proto/utils/partial_dependence_plot_pb2.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/metric/__init__.py
+Filename: ydf/proto/utils/distribution_pb2.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/metric/metric_pb2.py
+Filename: ydf/proto/utils/distribute/implementations/
 Comment: 
 
-Filename: yggdrasil_decision_forests/model/__init__.py
+Filename: ydf/proto/utils/distribute/distribute_pb2.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/model/abstract_model_pb2.py
+Filename: ydf/proto/utils/distribute/__init__.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/model/hyperparameter_pb2.py
+Filename: ydf/proto/utils/distribute/implementations/grpc/
 Comment: 
 
-Filename: yggdrasil_decision_forests/model/prediction_pb2.py
+Filename: ydf/proto/utils/distribute/implementations/__init__.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/model/decision_tree/__init__.py
+Filename: ydf/proto/utils/distribute/implementations/grpc/grpc_pb2.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/model/decision_tree/decision_tree_pb2.py
+Filename: ydf/proto/utils/distribute/implementations/grpc/__init__.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/model/random_forest/__init__.py
+Filename: ydf/proto/model/decision_tree/
 Comment: 
 
-Filename: yggdrasil_decision_forests/model/random_forest/random_forest_pb2.py
+Filename: ydf/proto/model/random_forest/
 Comment: 
 
-Filename: yggdrasil_decision_forests/utils/__init__.py
+Filename: ydf/proto/model/hyperparameter_pb2.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/utils/distribution_pb2.py
+Filename: ydf/proto/model/prediction_pb2.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/utils/fold_generator_pb2.py
+Filename: ydf/proto/model/__init__.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/utils/model_analysis_pb2.py
+Filename: ydf/proto/model/abstract_model_pb2.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/utils/partial_dependence_plot_pb2.py
+Filename: ydf/proto/model/decision_tree/decision_tree_pb2.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/utils/distribute/__init__.py
+Filename: ydf/proto/model/decision_tree/__init__.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/utils/distribute/distribute_pb2.py
+Filename: ydf/proto/model/random_forest/__init__.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/utils/distribute/implementations/__init__.py
+Filename: ydf/proto/model/random_forest/random_forest_pb2.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/utils/distribute/implementations/grpc/__init__.py
+Filename: ydf/proto/metric/__init__.py
 Comment: 
 
-Filename: yggdrasil_decision_forests/utils/distribute/implementations/grpc/grpc_pb2.py
+Filename: ydf/proto/metric/metric_pb2.py
 Comment: 
 
-Filename: ydf-0.4.0.dist-info/LICENSE
+Filename: ydf/proto/dataset/weight_pb2.py
 Comment: 
 
-Filename: ydf-0.4.0.dist-info/METADATA
+Filename: ydf/proto/dataset/data_spec_pb2.py
 Comment: 
 
-Filename: ydf-0.4.0.dist-info/WHEEL
+Filename: ydf/proto/dataset/__init__.py
 Comment: 
 
-Filename: ydf-0.4.0.dist-info/top_level.txt
+Filename: ydf/proto/dataset/example_pb2.py
 Comment: 
 
-Filename: ydf-0.4.0.dist-info/RECORD
+Filename: ydf/cc/ydf.so
+Comment: 
+
+Filename: ydf/cc/__init__.py
 Comment: 
 
 Zip file comment:
```

## filetype from file(1)

```diff
@@ -1 +1 @@
-Zip archive data, at least v2.0 to extract, compression method=deflate
+Zip archive data, at least v2.0 to extract, compression method=store
```

## ydf/__init__.py

```diff
@@ -1,69 +1,73 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""(P)YDF - Yggdrasil Decision Forests in Python."""
-
-# pylint: disable=g-importing-member,g-import-not-at-top,g-bad-import-order,reimported
-
-# Version
-from ydf.version import version as __version__
-
-# Dataset
-from ydf.dataset.dataset import create_vertical_dataset
-from ydf.dataset.dataspec import Column
-from ydf.dataset.dataspec import Semantic
-# A feature is a column used as input of a model. In practice, users generally
-# use them interchangeably.
-from ydf.dataset.dataspec import Column as Feature
-
-# Model
-from ydf.model.model_lib import load_model
-from ydf.model.model_lib import from_tensorflow_decision_forests
-from ydf.model.generic_model import Task
-from ydf.model.generic_model import ModelIOOptions
-from ydf.model.generic_model import GenericModel
-from ydf.model.random_forest_model.random_forest_model import RandomForestModel
-from ydf.model.gradient_boosted_trees_model.gradient_boosted_trees_model import GradientBoostedTreesModel
-from ydf.model.model_metadata import ModelMetadata
-# A CART model is a Random Forest with a single tree
-CARTModel = RandomForestModel
-
-# Learner
-from ydf.learner.generic_learner import GenericLearner
-from ydf.learner.specialized_learners import CartLearner
-from ydf.learner.specialized_learners import RandomForestLearner
-from ydf.learner.specialized_learners import GradientBoostedTreesLearner
-from ydf.learner.specialized_learners import DistributedGradientBoostedTreesLearner
-
-# Custom Losses
-from ydf.learner.custom_loss import Activation
-from ydf.learner.custom_loss import RegressionLoss
-from ydf.learner.custom_loss import BinaryClassificationLoss
-from ydf.learner.custom_loss import MultiClassificationLoss
-
-# Worker
-from ydf.learner.worker import start_worker
-
-# Tuner
-from ydf.learner.tuner import RandomSearchTuner
-from ydf.learner.tuner import VizierTuner
-
-# Logs
-from ydf.utils.log import verbose
-from ydf.utils.log import strict
-
-# Tree inspector
-from ydf.model import tree
-
-# pylint: enable=g-importing-member,g-import-not-at-top,g-bad-import-order,reimported
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""(P)YDF - Yggdrasil Decision Forests in Python."""
+
+# pylint: disable=g-importing-member,g-import-not-at-top,g-bad-import-order,reimported
+
+# Version
+from ydf.version import version as __version__
+
+# Note: Keep this file organized in the same order as:
+# public/docs/py_api/index.md
+
+# Learner
+from ydf.learner.generic_learner import GenericLearner
+from ydf.learner.specialized_learners import CartLearner
+from ydf.learner.specialized_learners import RandomForestLearner
+from ydf.learner.specialized_learners import GradientBoostedTreesLearner
+from ydf.learner.specialized_learners import DistributedGradientBoostedTreesLearner
+
+# Model
+from ydf.model.generic_model import GenericModel
+from ydf.model.random_forest_model.random_forest_model import RandomForestModel
+from ydf.model.gradient_boosted_trees_model.gradient_boosted_trees_model import GradientBoostedTreesModel
+# A CART model is a Random Forest with a single tree
+CARTModel = RandomForestModel
+
+# Tuner
+from ydf.learner.tuner import RandomSearchTuner
+from ydf.learner.tuner import VizierTuner
+
+# Utilities
+from ydf.utils.log import verbose
+from ydf.model.model_lib import load_model
+# A feature is a column used as input of a model. In practice, users generally
+# use them interchangeably.
+from ydf.dataset.dataspec import Column as Feature
+from ydf.dataset.dataspec import Column
+from ydf.model.generic_model import Task
+from ydf.dataset.dataspec import Semantic
+from ydf.learner.worker import start_worker
+from ydf.utils.log import strict
+
+# Advanced Utilities
+from ydf.model.generic_model import ModelIOOptions
+from ydf.dataset.dataset import create_vertical_dataset
+from ydf.model.model_metadata import ModelMetadata
+from ydf.model.model_lib import from_tensorflow_decision_forests
+
+
+# Custom Loss
+from ydf.learner.custom_loss import RegressionLoss
+from ydf.learner.custom_loss import BinaryClassificationLoss
+from ydf.learner.custom_loss import MultiClassificationLoss
+from ydf.learner.custom_loss import Activation
+
+
+# Tree
+from ydf.model import tree
+
+
+# pylint: enable=g-importing-member,g-import-not-at-top,g-bad-import-order,reimported
```

## ydf/api_test.py

 * *Ordering differences only*

```diff
@@ -1,196 +1,196 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Test the API of YDF."""
-
-import math
-import os
-
-from absl import logging
-from absl.testing import absltest
-import pandas as pd
-
-import ydf  # In the world, use "import ydf"
-from ydf.utils import test_utils
-
-
-class ApiTest(absltest.TestCase):
-
-  def test_create_dataset(self):
-    pd_ds = pd.DataFrame({
-        "c1": [1.0, 1.1, math.nan],
-        "c2": [1, 2, 3],
-        "c3": [True, False, True],
-        "c4": ["x", "y", ""],
-    })
-    ds = ydf.create_vertical_dataset(pd_ds)
-    logging.info("Dataset:\n%s", ds)
-
-  def test_create_dataset_with_column(self):
-    pd_ds = pd.DataFrame({
-        "c1": [1.0, 1.1, math.nan],
-        "c2": [1, 2, 3],
-        "c3": [True, False, True],
-        "c4": ["x", "y", ""],
-    })
-    ds = ydf.create_vertical_dataset(
-        pd_ds,
-        columns=[
-            "c1",
-            ("c2", ydf.Semantic.NUMERICAL),
-            ydf.Column("c3"),
-            ydf.Column("c4", ydf.Semantic.CATEGORICAL),
-        ],
-    )
-    logging.info("Dataset:\n%s", ds)
-
-  def test_load_and_save_model(self):
-    model_path = os.path.join(
-        test_utils.ydf_test_data_path(), "model", "adult_binary_class_rf"
-    )
-    model = ydf.load_model(model_path)
-    logging.info(model)
-    tempdir = self.create_tempdir().full_path
-    model.save(tempdir, ydf.ModelIOOptions(file_prefix="model_prefix_"))
-    logging.info(os.listdir(tempdir))
-
-  def test_train_random_forest(self):
-    pd_ds = pd.DataFrame({
-        "c1": [1.0, 1.1, 2.0, 3.5, 4.2] + list(range(10)),
-        "label": ["a", "b", "b", "a", "a"] * 3,
-    })
-    model = ydf.RandomForestLearner(num_trees=3, label="label").train(pd_ds)
-    logging.info(model)
-
-  def test_train_gradient_boosted_tree(self):
-    pd_ds = pd.DataFrame({
-        "c1": [1.0, 1.1, 2.0, 3.5, 4.2] + list(range(10)),
-        "label": ["a", "b", "b", "a", "a"] * 3,
-    })
-    model = ydf.GradientBoostedTreesLearner(num_trees=10, label="label").train(
-        pd_ds
-    )
-    logging.info(model)
-
-  def test_train_cart(self):
-    pd_ds = pd.DataFrame({
-        "c1": [1.0, 1.1, 2.0, 3.5, 4.2] + list(range(10)),
-        "label": ["a", "b", "b", "a", "a"] * 3,
-    })
-    model = ydf.CartLearner(label="label").train(pd_ds)
-    logging.info(model)
-
-  def test_evaluate_model(self):
-    model_path = os.path.join(
-        test_utils.ydf_test_data_path(), "model", "adult_binary_class_gbdt"
-    )
-    model = ydf.load_model(model_path)
-    test_ds = pd.read_csv(
-        os.path.join(
-            test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
-        )
-    )
-    evaluation = model.evaluate(test_ds)
-    logging.info("Evaluation:\n%s", evaluation)
-    self.assertAlmostEqual(evaluation.accuracy, 0.87235, 3)
-
-    tempdir = self.create_tempdir().full_path
-    with open(os.path.join(tempdir, "evaluation.html"), "w") as f:
-      f.write(evaluation.html())
-
-  def test_analyze_model(self):
-    model_path = os.path.join(
-        test_utils.ydf_test_data_path(), "model", "adult_binary_class_gbdt"
-    )
-    model = ydf.load_model(model_path)
-    test_ds = pd.read_csv(
-        os.path.join(
-            test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
-        )
-    )
-    analysis = model.analyze(test_ds)
-    logging.info("Analysis:\n%s", analysis)
-
-    tempdir = self.create_tempdir().full_path
-    with open(os.path.join(tempdir, "analysis.html"), "w") as f:
-      f.write(analysis.html())
-
-  def test_cross_validation(self):
-    pd_ds = pd.DataFrame({
-        "c1": [1.0, 1.1, 2.0, 3.5, 4.2] + list(range(10)),
-        "label": ["a", "b", "b", "a", "a"] * 3,
-    })
-    learner = ydf.RandomForestLearner(num_trees=3, label="label")
-    evaluation = learner.cross_validation(pd_ds)
-    logging.info(evaluation)
-
-  def test_export_to_cc(self):
-    model_path = os.path.join(
-        test_utils.ydf_test_data_path(), "model", "adult_binary_class_gbdt"
-    )
-    model = ydf.load_model(model_path)
-    logging.info(
-        "Copy the following in a .h file to run the model in C++:\n%s",
-        model.to_cpp(),
-    )
-
-  def test_verbose_full(self):
-    save_verbose = ydf.verbose(2)
-    learner = ydf.RandomForestLearner(label="label")
-    _ = learner.train(pd.DataFrame({"feature": [0, 1], "label": [0, 1]}))
-    ydf.verbose(save_verbose)
-
-  def test_verbose_default(self):
-    learner = ydf.RandomForestLearner(label="label")
-    _ = learner.train(pd.DataFrame({"feature": [0, 1], "label": [0, 1]}))
-
-  def test_verbose_none(self):
-    save_verbose = ydf.verbose(0)
-    learner = ydf.RandomForestLearner(label="label")
-    _ = learner.train(pd.DataFrame({"feature": [0, 1], "label": [0, 1]}))
-    ydf.verbose(save_verbose)
-
-  def test_print_a_tree(self):
-    train_ds = pd.DataFrame({
-        "c1": [1.0, 1.1, 2.0, 3.5, 4.2] + list(range(10)),
-        "label": ["a", "b", "b", "a", "a"] * 3,
-    })
-    learner = ydf.CartLearner(label="label")
-    model = learner.train(train_ds)
-    assert isinstance(model, ydf.CARTModel)
-    model.print_tree(tree_idx=0)
-
-  def test_get_a_tree(self):
-    train_ds = pd.DataFrame({
-        "c1": [1.0, 1.1, 2.0, 3.5, 4.2] + list(range(10)),
-        "label": ["a", "b", "b", "a", "a"] * 3,
-    })
-    learner = ydf.RandomForestLearner(label="label")
-    model = learner.train(train_ds)
-    assert isinstance(model, ydf.RandomForestModel)
-    tree = model.get_tree(tree_idx=0)
-    logging.info("Found tree:\n%s", tree)
-
-  def test_list_input_features(self):
-    train_ds = pd.DataFrame({
-        "c1": [1.0, 1.1, 2.0, 3.5, 4.2] + list(range(10)),
-        "label": ["a", "b", "b", "a", "a"] * 3,
-    })
-    learner = ydf.RandomForestLearner(label="label")
-    model = learner.train(train_ds)
-    logging.info("Input features:\n%s", model.input_features())
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Test the API of YDF."""
+
+import math
+import os
+
+from absl import logging
+from absl.testing import absltest
+import pandas as pd
+
+import ydf  # In the world, use "import ydf"
+from ydf.utils import test_utils
+
+
+class ApiTest(absltest.TestCase):
+
+  def test_create_dataset(self):
+    pd_ds = pd.DataFrame({
+        "c1": [1.0, 1.1, math.nan],
+        "c2": [1, 2, 3],
+        "c3": [True, False, True],
+        "c4": ["x", "y", ""],
+    })
+    ds = ydf.create_vertical_dataset(pd_ds)
+    logging.info("Dataset:\n%s", ds)
+
+  def test_create_dataset_with_column(self):
+    pd_ds = pd.DataFrame({
+        "c1": [1.0, 1.1, math.nan],
+        "c2": [1, 2, 3],
+        "c3": [True, False, True],
+        "c4": ["x", "y", ""],
+    })
+    ds = ydf.create_vertical_dataset(
+        pd_ds,
+        columns=[
+            "c1",
+            ("c2", ydf.Semantic.NUMERICAL),
+            ydf.Column("c3"),
+            ydf.Column("c4", ydf.Semantic.CATEGORICAL),
+        ],
+    )
+    logging.info("Dataset:\n%s", ds)
+
+  def test_load_and_save_model(self):
+    model_path = os.path.join(
+        test_utils.ydf_test_data_path(), "model", "adult_binary_class_rf"
+    )
+    model = ydf.load_model(model_path)
+    logging.info(model)
+    tempdir = self.create_tempdir().full_path
+    model.save(tempdir, ydf.ModelIOOptions(file_prefix="model_prefix_"))
+    logging.info(os.listdir(tempdir))
+
+  def test_train_random_forest(self):
+    pd_ds = pd.DataFrame({
+        "c1": [1.0, 1.1, 2.0, 3.5, 4.2] + list(range(10)),
+        "label": ["a", "b", "b", "a", "a"] * 3,
+    })
+    model = ydf.RandomForestLearner(num_trees=3, label="label").train(pd_ds)
+    logging.info(model)
+
+  def test_train_gradient_boosted_tree(self):
+    pd_ds = pd.DataFrame({
+        "c1": [1.0, 1.1, 2.0, 3.5, 4.2] + list(range(10)),
+        "label": ["a", "b", "b", "a", "a"] * 3,
+    })
+    model = ydf.GradientBoostedTreesLearner(num_trees=10, label="label").train(
+        pd_ds
+    )
+    logging.info(model)
+
+  def test_train_cart(self):
+    pd_ds = pd.DataFrame({
+        "c1": [1.0, 1.1, 2.0, 3.5, 4.2] + list(range(10)),
+        "label": ["a", "b", "b", "a", "a"] * 3,
+    })
+    model = ydf.CartLearner(label="label").train(pd_ds)
+    logging.info(model)
+
+  def test_evaluate_model(self):
+    model_path = os.path.join(
+        test_utils.ydf_test_data_path(), "model", "adult_binary_class_gbdt"
+    )
+    model = ydf.load_model(model_path)
+    test_ds = pd.read_csv(
+        os.path.join(
+            test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
+        )
+    )
+    evaluation = model.evaluate(test_ds)
+    logging.info("Evaluation:\n%s", evaluation)
+    self.assertAlmostEqual(evaluation.accuracy, 0.87235, 3)
+
+    tempdir = self.create_tempdir().full_path
+    with open(os.path.join(tempdir, "evaluation.html"), "w") as f:
+      f.write(evaluation.html())
+
+  def test_analyze_model(self):
+    model_path = os.path.join(
+        test_utils.ydf_test_data_path(), "model", "adult_binary_class_gbdt"
+    )
+    model = ydf.load_model(model_path)
+    test_ds = pd.read_csv(
+        os.path.join(
+            test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
+        )
+    )
+    analysis = model.analyze(test_ds)
+    logging.info("Analysis:\n%s", analysis)
+
+    tempdir = self.create_tempdir().full_path
+    with open(os.path.join(tempdir, "analysis.html"), "w") as f:
+      f.write(analysis.html())
+
+  def test_cross_validation(self):
+    pd_ds = pd.DataFrame({
+        "c1": [1.0, 1.1, 2.0, 3.5, 4.2] + list(range(10)),
+        "label": ["a", "b", "b", "a", "a"] * 3,
+    })
+    learner = ydf.RandomForestLearner(num_trees=3, label="label")
+    evaluation = learner.cross_validation(pd_ds)
+    logging.info(evaluation)
+
+  def test_export_to_cc(self):
+    model_path = os.path.join(
+        test_utils.ydf_test_data_path(), "model", "adult_binary_class_gbdt"
+    )
+    model = ydf.load_model(model_path)
+    logging.info(
+        "Copy the following in a .h file to run the model in C++:\n%s",
+        model.to_cpp(),
+    )
+
+  def test_verbose_full(self):
+    save_verbose = ydf.verbose(2)
+    learner = ydf.RandomForestLearner(label="label")
+    _ = learner.train(pd.DataFrame({"feature": [0, 1], "label": [0, 1]}))
+    ydf.verbose(save_verbose)
+
+  def test_verbose_default(self):
+    learner = ydf.RandomForestLearner(label="label")
+    _ = learner.train(pd.DataFrame({"feature": [0, 1], "label": [0, 1]}))
+
+  def test_verbose_none(self):
+    save_verbose = ydf.verbose(0)
+    learner = ydf.RandomForestLearner(label="label")
+    _ = learner.train(pd.DataFrame({"feature": [0, 1], "label": [0, 1]}))
+    ydf.verbose(save_verbose)
+
+  def test_print_a_tree(self):
+    train_ds = pd.DataFrame({
+        "c1": [1.0, 1.1, 2.0, 3.5, 4.2] + list(range(10)),
+        "label": ["a", "b", "b", "a", "a"] * 3,
+    })
+    learner = ydf.CartLearner(label="label")
+    model = learner.train(train_ds)
+    assert isinstance(model, ydf.CARTModel)
+    model.print_tree(tree_idx=0)
+
+  def test_get_a_tree(self):
+    train_ds = pd.DataFrame({
+        "c1": [1.0, 1.1, 2.0, 3.5, 4.2] + list(range(10)),
+        "label": ["a", "b", "b", "a", "a"] * 3,
+    })
+    learner = ydf.RandomForestLearner(label="label")
+    model = learner.train(train_ds)
+    assert isinstance(model, ydf.RandomForestModel)
+    tree = model.get_tree(tree_idx=0)
+    logging.info("Found tree:\n%s", tree)
+
+  def test_list_input_features(self):
+    train_ds = pd.DataFrame({
+        "c1": [1.0, 1.1, 2.0, 3.5, 4.2] + list(range(10)),
+        "label": ["a", "b", "b", "a", "a"] * 3,
+    })
+    learner = ydf.RandomForestLearner(label="label")
+    model = learner.train(train_ds)
+    logging.info("Input features:\n%s", model.input_features())
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/version.py

```diff
@@ -1,15 +1,15 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-version = "0.4.0"
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+version = "0.4.1"
```

## ydf/cc/__init__.py

 * *Ordering differences only*

```diff
@@ -1,14 +1,14 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
```

## ydf/dataset/__init__.py

 * *Ordering differences only*

```diff
@@ -1,14 +1,14 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
```

## ydf/dataset/dataset.py

```diff
@@ -1,612 +1,612 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Dataset implementations of PYDF."""
-
-import copy
-from typing import Any, Dict, List, Optional, Sequence, Union
-
-import numpy as np
-
-from yggdrasil_decision_forests.dataset import data_spec_pb2
-from ydf.cc import ydf
-from ydf.dataset import dataspec
-from ydf.dataset.io import dataset_io
-from ydf.dataset.io import dataset_io_types
-from ydf.utils import log
-from ydf.utils import paths
-
-InputDataset = Union[dataset_io_types.IODataset, "VerticalDataset"]
-
-
-class VerticalDataset:
-  """Dataset for fast column-wise iteration."""
-
-  def __init__(self):
-    self._dataset = ydf.VerticalDataset()
-
-  def memory_usage(self) -> int:
-    """Memory usage of the dataset in bytes."""
-    return self._dataset.MemoryUsage()
-
-  def data_spec(self) -> data_spec_pb2.DataSpecification:
-    """Data spec of the dataset.
-
-    The dataspec is the protobuffer containing the dataset schema.
-
-    Returns:
-      A dataspec proto.
-    """
-    return self._dataset.data_spec()
-
-  def _add_column(
-      self,
-      column: dataspec.Column,
-      column_data: Any,
-      inference_args: Optional[dataspec.DataSpecInferenceArgs],
-      column_idx: Optional[int],
-  ):
-    """Adds a column to the dataset and computes the column statistics."""
-    assert (column_idx is None) != (inference_args is None)
-
-    original_column_data = column_data
-
-    if column.semantic == dataspec.Semantic.NUMERICAL:
-      if not isinstance(column_data, np.ndarray):
-        column_data = np.array(column_data, np.float32)
-      ydf_dtype = dataspec.np_dtype_to_ydf_dtype(column_data.dtype)
-
-      if column_data.dtype != np.float32:
-        log.warning(
-            "Column '%s' with NUMERICAL semantic has dtype %s. Casting value"
-            " to float32.",
-            column.name,
-            column_data.dtype.name,
-            message_id=log.WarningMessage.CAST_NUMERICAL_TO_FLOAT32,
-            is_strict=True,
-        )
-
-        try:
-          column_data = column_data.astype(np.float32)
-        except ValueError as e:
-          raise ValueError(
-              f"Cannot convert NUMERICAL column {column.name!r} of type"
-              f" {_type(column_data)} and with content={column_data!r} to"
-              " np.float32 values.\nNote: If the column is a label, make sure"
-              " the training task is compatible. For example, you cannot train"
-              " a regression model (task=ydf.Task.REGRESSION) on a string"
-              " column."
-          ) from e
-
-      self._dataset.PopulateColumnNumericalNPFloat32(
-          column.name,
-          column_data,
-          ydf_dtype=ydf_dtype,
-          column_idx=column_idx,  # `column_idx` may be None
-      )
-      return
-
-    elif column.semantic == dataspec.Semantic.BOOLEAN:
-      if not isinstance(column_data, np.ndarray):
-        column_data = np.array(column_data, np.bool_)
-      ydf_dtype = dataspec.np_dtype_to_ydf_dtype(column_data.dtype)
-
-      self._dataset.PopulateColumnBooleanNPBool(
-          column.name,
-          column_data,
-          ydf_dtype=ydf_dtype,
-          column_idx=column_idx,  # `column_idx` may be None
-      )
-      return
-
-    elif column.semantic == dataspec.Semantic.CATEGORICAL:
-
-      from_boolean = False
-      if not isinstance(column_data, np.ndarray):
-        column_data = np.array(column_data, dtype=np.bytes_)
-      ydf_dtype = dataspec.np_dtype_to_ydf_dtype(column_data.dtype)
-
-      if column_data.dtype.type in [np.bool_]:
-        bool_column_data = column_data
-        column_data = np.full_like(bool_column_data, b"false", "|S5")
-        column_data[bool_column_data] = b"true"
-        from_boolean = True
-      elif (
-          column_data.dtype.type
-          in [
-              np.object_,
-              np.string_,
-              np.str_,
-          ]
-          or column_data.dtype.type in dataspec.NP_SUPPORTED_INT_DTYPE
-      ):
-        column_data = column_data.astype(np.bytes_)
-      elif np.issubdtype(column_data.dtype, np.floating):
-        raise ValueError(
-            f"Cannot import column {column.name!r} with"
-            f" semantic={column.semantic} as it contains floating point values."
-            f" Got {original_column_data!r}.\nNote: If the column is a label,"
-            " make sure the correct task is selected. For example, you cannot"
-            " train a classification model (task=ydf.Task.CLASSIFICATION) with"
-            " floating point labels."
-        )
-
-      if column_data.dtype.type == np.bytes_:
-        if inference_args is not None:
-          guide = dataspec.categorical_column_guide(column, inference_args)
-          if from_boolean:
-            guide["dictionary"] = np.array(
-                [b"<OOV>", b"false", b"true"], dtype=np.bytes_
-            )
-          self._dataset.PopulateColumnCategoricalNPBytes(
-              column.name, column_data, **guide, ydf_dtype=ydf_dtype
-          )
-        else:
-          self._dataset.PopulateColumnCategoricalNPBytes(
-              column.name,
-              column_data,
-              column_idx=column_idx,
-              ydf_dtype=ydf_dtype,
-          )
-        return
-
-    elif column.semantic == dataspec.Semantic.CATEGORICAL_SET:
-      if (
-          not isinstance(column_data, list)
-          and column_data.dtype.type != np.object_
-      ):
-        raise ValueError("Categorical Set columns must be a list of lists.")
-      if len(column_data) > 0:
-        if column_data[0] and not isinstance(
-            column_data[0], (list, np.ndarray)
-        ):
-          raise ValueError("Categorical Set columns must be a list of lists.")
-      # TODO: b/313414785 - Consider speeding this up by moving logic to C++.
-      # np.unique also sorts the unique elements, which is expected by YDF.
-      column_data = [np.unique(row).astype(np.bytes_) for row in column_data]
-      boundaries = np.cumsum(
-          [0] + [len(row) for row in column_data[:-1]], dtype=np.int64
-      )
-      bank = np.concatenate(column_data)
-      ydf_dtype = dataspec.np_dtype_to_ydf_dtype(bank.dtype)
-
-      if inference_args is not None:
-        guide = dataspec.categorical_column_guide(column, inference_args)
-        self._dataset.PopulateColumnCategoricalSetNPBytes(
-            column.name,
-            bank,
-            boundaries,
-            ydf_dtype,
-            **guide,
-        )
-      else:
-        self._dataset.PopulateColumnCategoricalSetNPBytes(
-            column.name,
-            bank,
-            boundaries,
-            ydf_dtype,
-            column_idx=column_idx,
-        )
-      return
-
-    elif column.semantic == dataspec.Semantic.HASH:
-      if not isinstance(column_data, np.ndarray):
-        column_data = np.array(column_data, dtype=np.bytes_)
-      ydf_dtype = dataspec.np_dtype_to_ydf_dtype(column_data.dtype)
-
-      if column_data.dtype.type in [
-          np.object_,
-          np.string_,
-          np.bool_,
-      ] or np.issubdtype(column_data.dtype, np.integer):
-        column_data = column_data.astype(np.bytes_)
-      elif np.issubdtype(column_data.dtype, np.floating):
-        raise ValueError(
-            f"Cannot import column {column.name!r} with"
-            f" semantic={column.semantic} as it contains floating point values."
-            f" Got {original_column_data!r}."
-        )
-
-      if column_data.dtype.type == np.bytes_:
-        self._dataset.PopulateColumnHashNPBytes(
-            column.name,
-            column_data,
-            ydf_dtype=ydf_dtype,
-            column_idx=column_idx,
-        )
-        return
-
-    raise ValueError(
-        f"Cannot import column {column.name!r} with semantic={column.semantic},"
-        f" type={_type(original_column_data)} and"
-        f" content={original_column_data!r}.\nNote: If the column is a label,"
-        " the semantic was selected based on the task. For example,"
-        " task=ydf.Task.CLASSIFICATION requires a CATEGORICAL compatible label"
-        " column, and task=ydf.Task.REGRESSION requires a NUMERICAL compatible"
-        " label column."
-    )
-
-  def _initialize_from_data_spec(
-      self, data_spec: data_spec_pb2.DataSpecification
-  ):
-    self._dataset.CreateColumnsFromDataSpec(data_spec)
-
-  def _finalize(self, set_num_rows_in_data_spec: bool):
-    self._dataset.SetAndCheckNumRowsAndFillMissing(set_num_rows_in_data_spec)
-
-
-def create_vertical_dataset(
-    data: InputDataset,
-    columns: dataspec.ColumnDefs = None,
-    include_all_columns: bool = False,
-    max_vocab_count: int = 2000,
-    min_vocab_frequency: int = 5,
-    discretize_numerical_columns: bool = False,
-    num_discretized_numerical_bins: int = 255,
-    max_num_scanned_rows_to_infer_semantic: int = 10000,
-    max_num_scanned_rows_to_compute_statistics: int = 10000,
-    data_spec: Optional[data_spec_pb2.DataSpecification] = None,
-    required_columns: Optional[Sequence[str]] = None,
-    dont_unroll_columns: Optional[Sequence[str]] = None,
-) -> VerticalDataset:
-  """Creates a VerticalDataset from various sources of data.
-
-  The feature semantics are automatically determined and can be explicitly
-  set with the `columns` argument. The semantics of a dataset (or model) are
-  available its data_spec.
-
-  Note that the CATEGORICAL_SET semantic is not automatically inferred when
-  reading from file. When reading from CSV files, setting the CATEGORICAL_SET
-  semantic for a feature will have YDF tokenize the feature. When reading from
-  in-memory datasets (e.g. pandas), YDF only accepts lists of lists for
-  CATEGORICAL_SET features.
-
-  Usage example:
-
-  ```python
-  import pandas as pd
-  import ydf
-
-  df = pd.read_csv("my_dataset.csv")
-
-  # Loads all the columns
-  ds = ydf.create_vertical_dataset(df)
-
-  # Only load columns "a" and "b". Ensure "b" is interpreted as a categorical
-  # feature.
-  ds = ydf.create_vertical_dataset(df,
-    columns=[
-      "a",
-      ("b", ydf.semantic.categorical),
-    ])
-  ```
-
-  Args:
-    data: Source dataset. Supported formats: VerticalDataset, (typed) path,
-      Pandas Dataframe, dictionary of string to Numpy array or lists. If the
-      data is already a VerticalDataset, it is returned unchanged.
-    columns: If None, all columns are imported. The semantic of the columns is
-      determined automatically. Otherwise, if include_all_columns=False
-      (default) only the column listed in `columns` are imported. If
-      include_all_columns=True, all the columns are imported and only the
-      semantic of the columns NOT in `columns` is determined automatically. If
-      specified, "columns" defines the order of the columns - any non-listed
-      columns are appended in-order after the specified columns (if
-      include_all_columns=True).
-    include_all_columns: See `columns`.
-    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
-      CATEGORICAL_SET columns stored as strings. If more unique values exist,
-      only the most frequent values are kept, and the remaining values are
-      considered as out-of-vocabulary.  If max_vocab_count = -1, the number of
-      values in the column is not limited (not recommended).
-    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
-      and CATEGORICAL_SET columns. Value observed less than
-      `min_vocab_frequency` are considered as out-of-vocabulary.
-    discretize_numerical_columns: If true, discretize all the numerical columns
-      before training. Discretized numerical columns are faster to train with,
-      but they can have a negative impact on the model quality. Using
-      `discretize_numerical_columns=True` is equivalent as setting the column
-      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
-      definition of DISCRETIZED_NUMERICAL for more details.
-    num_discretized_numerical_bins: Number of bins used when disretizing
-      numerical columns.
-    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
-      inferring the column's semantic if it is not explicitly specified. Only
-      used when reading from file, in-memory datasets are always read in full.
-      Setting this to a lower number will speed up dataset reading, but might
-      result in incorrect column semantics. Set to -1 to scan the entire
-      dataset.
-    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
-      computing a column's statistics. Only used when reading from file,
-      in-memory datasets are always read in full. A column's statistics include
-      the dictionary for categorical features and the mean / min / max for
-      numerical features. Setting this to a lower number will speed up dataset
-      reading, but skew statistics in the dataspec, which can hurt model quality
-      (e.g. if an important category of a categorical feature is considered
-      OOV). Set to -1 to scan the entire dataset.
-    data_spec: Dataspec to be used for this dataset. If a data spec is given,
-      all other arguments except `data` and `required_columns` should not be
-      provided.
-    required_columns: List of columns required in the data. If None, all columns
-      mentioned in the data spec or `columns` are required.
-    dont_unroll_columns: List of columns that cannot be unrolled. If one such
-      column needs to be unrolled, raise an error.
-
-  Returns:
-    Dataset to be ingested by the learner algorithms.
-
-  Raises:
-    ValueError: If the dataset has an unsupported type.
-  """
-  if isinstance(data, VerticalDataset):
-    return data
-
-  if data_spec is not None:
-    if columns is not None:
-      raise ValueError(
-          "When passing a data spec at dataset creation, `columns` must be"
-          " None and all arguments to guide data spec inference are ignored."
-      )
-    return create_vertical_dataset_with_spec_or_args(
-        data,
-        required_columns,
-        data_spec=data_spec,
-        inference_args=None,
-        dont_unroll_columns=dont_unroll_columns,
-    )
-  else:
-    inference_args = dataspec.DataSpecInferenceArgs(
-        columns=dataspec.normalize_column_defs(columns),
-        include_all_columns=include_all_columns,
-        max_vocab_count=max_vocab_count,
-        min_vocab_frequency=min_vocab_frequency,
-        discretize_numerical_columns=discretize_numerical_columns,
-        num_discretized_numerical_bins=num_discretized_numerical_bins,
-        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
-        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
-    )
-    return create_vertical_dataset_with_spec_or_args(
-        data,
-        required_columns,
-        inference_args=inference_args,
-        data_spec=None,
-        dont_unroll_columns=dont_unroll_columns,
-    )
-
-
-def create_vertical_dataset_with_spec_or_args(
-    data: dataset_io_types.IODataset,
-    required_columns: Optional[Sequence[str]],
-    inference_args: Optional[dataspec.DataSpecInferenceArgs],
-    data_spec: Optional[data_spec_pb2.DataSpecification],
-    dont_unroll_columns: Optional[Sequence[str]] = None,
-) -> VerticalDataset:
-  """Returns a vertical dataset from inference args or data spec (not both!)."""
-  assert (data_spec is None) != (inference_args is None)
-  # If `data` is a path, try to import from the path directly from C++.
-  # Everything else we try to transform into a dictionary with Python.
-  if isinstance(data, str) or (
-      isinstance(data, Sequence)
-      and data
-      and all(isinstance(s, str) for s in data)
-  ):
-    return create_vertical_dataset_from_path(
-        data, required_columns, inference_args, data_spec
-    )
-  else:
-    # Convert the data to an in-memory dictionary of numpy array.
-    # Also unroll multi-dimensional features.
-    data_dict, unroll_feature_info = dataset_io.cast_input_dataset_to_dict(
-        data, dont_unroll_columns=dont_unroll_columns
-    )
-    return create_vertical_dataset_from_dict_of_values(
-        data_dict,
-        unroll_feature_info,
-        required_columns,
-        inference_args=inference_args,
-        data_spec=data_spec,
-    )
-
-
-def create_vertical_dataset_from_path(
-    path: Union[str, List[str]],
-    required_columns: Optional[Sequence[str]],
-    inference_args: Optional[dataspec.DataSpecInferenceArgs],
-    data_spec: Optional[data_spec_pb2.DataSpecification],
-) -> VerticalDataset:
-  """Returns a VerticalDataset from (list of) path using YDF dataset reading."""
-  assert (data_spec is None) != (inference_args is None)
-  if not isinstance(path, str):
-    path = paths.normalize_list_of_paths(path)
-  dataset = VerticalDataset()
-  if data_spec is not None:
-    dataset._dataset.CreateFromPathWithDataSpec(
-        path, data_spec, required_columns
-    )
-  if inference_args is not None:
-    dataset._dataset.CreateFromPathWithDataSpecGuide(
-        path, inference_args.to_proto_guide(), required_columns
-    )
-  return dataset
-
-
-def create_vertical_dataset_from_dict_of_values(
-    data: Dict[str, dataset_io_types.InputValues],
-    unroll_feature_info: dataset_io_types.UnrolledFeaturesInfo,
-    required_columns: Optional[Sequence[str]],
-    inference_args: Optional[dataspec.DataSpecInferenceArgs],
-    data_spec: Optional[data_spec_pb2.DataSpecification],
-) -> VerticalDataset:
-  """Specialization of create_vertical_dataset to dictionary of values.
-
-  The data spec is either inferred using inference_args or uses the given
-  data_spec
-
-  Args:
-    data: Data to copy to the Vertical Dataset.
-    unroll_feature_info: Information about feature unrolling.
-    required_columns: Names of columns that are required in the data.
-    inference_args: Arguments for data spec inference. Must be None if data_spec
-      is set.
-    data_spec: Data spec of the given data. Must be None if inference_args is
-      set.
-
-  Returns:
-    A Vertical dataset with the given properties.
-  """
-
-  def dataspec_to_normalized_columns(
-      data: Dict[str, dataset_io_types.InputValues],
-      columns,
-      required_columns: Sequence[str],
-  ):
-    normalized_columns = []
-    for column_spec in columns:
-      if column_spec.name not in data and column_spec.name in required_columns:
-        raise ValueError(
-            f"The data spec expects columns {column_spec.name} which was not"
-            f" found in the data. Available columns: {list(data)}. Required"
-            f" columns: {required_columns}"
-        )
-      if (
-          column_spec.type == data_spec_pb2.CATEGORICAL_SET
-          and column_spec.HasField("tokenizer")
-          and column_spec.tokenizer.splitter
-          != data_spec_pb2.Tokenizer.NO_SPLITTING
-      ):
-        log.warning(
-            f"The dataspec for columns {column_spec.name} specifies a"
-            " tokenizer, but it is ignored when reading in-memory datasets."
-        )
-      else:
-        normalized_columns.append(
-            dataspec.Column(
-                name=column_spec.name,
-                semantic=dataspec.Semantic.from_proto_type(column_spec.type),
-            )
-        )
-    return normalized_columns
-
-  assert (data_spec is None) != (inference_args is None)
-  dataset = VerticalDataset()
-  if data_spec is None:
-    # If `required_columns` is None, only check if the columns mentioned in the
-    # `inference_args` are required. This is checked by
-    # dataspec.get_all_columns()
-    normalized_columns, effective_unroll_feature_info = (
-        dataspec.get_all_columns(
-            available_columns=list(data.keys()),
-            inference_args=inference_args,
-            required_columns=required_columns,
-            unroll_feature_info=unroll_feature_info,
-        )
-    )
-  else:
-    effective_unroll_feature_info = None  # To please linter
-    dataset._initialize_from_data_spec(data_spec)  # pylint: disable=protected-access
-    required_columns = (
-        required_columns
-        if required_columns is not None
-        else [c.name for c in data_spec.columns]
-    )
-    normalized_columns = dataspec_to_normalized_columns(
-        data, data_spec.columns, required_columns
-    )
-
-  for column_idx, column in enumerate(normalized_columns):
-    effective_column = copy.deepcopy(column)
-    if column.name not in data:
-      # The column is missing, so no data is passed but the column will still be
-      # created.
-      column_data = []
-    else:
-      column_data = data[column.name]
-    if column.semantic is None:
-      infered_semantic = infer_semantic(column.name, column_data)
-      effective_column.semantic = infered_semantic
-
-    dataset._add_column(  # pylint: disable=protected-access
-        effective_column,
-        column_data,
-        inference_args=inference_args,  # Might be None
-        column_idx=column_idx if data_spec is not None else None,
-    )
-
-  if data_spec is None:
-    assert effective_unroll_feature_info is not None
-    dataset._dataset.SetMultiDimDataspec(effective_unroll_feature_info)  # pylint: disable=protected-access
-
-  dataset._finalize(set_num_rows_in_data_spec=(data_spec is None))  # pylint: disable=protected-access
-  return dataset
-
-
-def infer_semantic(name: str, data: Any) -> dataspec.Semantic:
-  """Infers the semantic of a column from its data."""
-
-  # If a column has no data, we assume it only contains missing values.
-  if len(data) == 0:
-    raise ValueError(
-        f"Cannot infer automatically the semantic of column {name!r} since no"
-        " data for this column was provided. Make sure this column exists in"
-        " the dataset, or exclude the column from the list of required"
-        " columns. If the dataset should contain missing values for the all"
-        " examples of this column, specify the semantic of the column manually"
-        f" using the `features` argument e.g. `features=[({name!r},"
-        " ydf.Semantic.NUMERICAL)]` if the feature is numerical."
-    )
-
-  if isinstance(data, np.ndarray):
-    # We finely control the supported types.
-    if (
-        data.dtype.type in dataspec.NP_SUPPORTED_INT_DTYPE
-        or data.dtype.type in dataspec.NP_SUPPORTED_FLOAT_DTYPE
-    ):
-      return dataspec.Semantic.NUMERICAL
-
-    if data.dtype.type in [np.string_, np.bytes_, np.str_]:
-      return dataspec.Semantic.CATEGORICAL
-
-    if data.dtype.type in [np.object_]:
-      # For performance reasons, only check the type on the first and last item
-      # of the column if it is tokenized.
-      if (
-          len(data) > 0
-          and (isinstance(data[0], list) or isinstance(data[0], np.ndarray))
-          and (isinstance(data[-1], list) or isinstance(data[-1], np.ndarray))
-      ):
-        return dataspec.Semantic.CATEGORICAL_SET
-      return dataspec.Semantic.CATEGORICAL
-
-    if data.dtype.type in [np.bool_]:
-      return dataspec.Semantic.BOOLEAN
-    type_str = f"numpy.array of {data.dtype}"
-  else:
-    type_str = str(type(data))
-
-  raise ValueError(
-      f"Cannot infer automatically the semantic of column {name!r} with"
-      f" type={type_str}, and content={data}. Convert the column to a supported"
-      " type, or specify the semantic of the column manually using the"
-      f" `features` argument e.g. `features=[({name!r},"
-      " ydf.Semantic.NUMERICAL)]` if the feature is numerical."
-  )
-
-
-def _type(value: Any) -> str:
-  """Returns a string representation of the type of value."""
-
-  if isinstance(value, np.ndarray):
-    return f"numpy's array of '{value.dtype.name}'"
-  else:
-    return str(type(value))
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Dataset implementations of PYDF."""
+
+import copy
+from typing import Any, Dict, List, Optional, Sequence, Union
+
+import numpy as np
+
+from ydf.proto.dataset import data_spec_pb2
+from ydf.cc import ydf
+from ydf.dataset import dataspec
+from ydf.dataset.io import dataset_io
+from ydf.dataset.io import dataset_io_types
+from ydf.utils import log
+from ydf.utils import paths
+
+InputDataset = Union[dataset_io_types.IODataset, "VerticalDataset"]
+
+
+class VerticalDataset:
+  """Dataset for fast column-wise iteration."""
+
+  def __init__(self):
+    self._dataset = ydf.VerticalDataset()
+
+  def memory_usage(self) -> int:
+    """Memory usage of the dataset in bytes."""
+    return self._dataset.MemoryUsage()
+
+  def data_spec(self) -> data_spec_pb2.DataSpecification:
+    """Data spec of the dataset.
+
+    The dataspec is the protobuffer containing the dataset schema.
+
+    Returns:
+      A dataspec proto.
+    """
+    return self._dataset.data_spec()
+
+  def _add_column(
+      self,
+      column: dataspec.Column,
+      column_data: Any,
+      inference_args: Optional[dataspec.DataSpecInferenceArgs],
+      column_idx: Optional[int],
+  ):
+    """Adds a column to the dataset and computes the column statistics."""
+    assert (column_idx is None) != (inference_args is None)
+
+    original_column_data = column_data
+
+    if column.semantic == dataspec.Semantic.NUMERICAL:
+      if not isinstance(column_data, np.ndarray):
+        column_data = np.array(column_data, np.float32)
+      ydf_dtype = dataspec.np_dtype_to_ydf_dtype(column_data.dtype)
+
+      if column_data.dtype != np.float32:
+        log.warning(
+            "Column '%s' with NUMERICAL semantic has dtype %s. Casting value"
+            " to float32.",
+            column.name,
+            column_data.dtype.name,
+            message_id=log.WarningMessage.CAST_NUMERICAL_TO_FLOAT32,
+            is_strict=True,
+        )
+
+        try:
+          column_data = column_data.astype(np.float32)
+        except ValueError as e:
+          raise ValueError(
+              f"Cannot convert NUMERICAL column {column.name!r} of type"
+              f" {_type(column_data)} and with content={column_data!r} to"
+              " np.float32 values.\nNote: If the column is a label, make sure"
+              " the training task is compatible. For example, you cannot train"
+              " a regression model (task=ydf.Task.REGRESSION) on a string"
+              " column."
+          ) from e
+
+      self._dataset.PopulateColumnNumericalNPFloat32(
+          column.name,
+          column_data,
+          ydf_dtype=ydf_dtype,
+          column_idx=column_idx,  # `column_idx` may be None
+      )
+      return
+
+    elif column.semantic == dataspec.Semantic.BOOLEAN:
+      if not isinstance(column_data, np.ndarray):
+        column_data = np.array(column_data, np.bool_)
+      ydf_dtype = dataspec.np_dtype_to_ydf_dtype(column_data.dtype)
+
+      self._dataset.PopulateColumnBooleanNPBool(
+          column.name,
+          column_data,
+          ydf_dtype=ydf_dtype,
+          column_idx=column_idx,  # `column_idx` may be None
+      )
+      return
+
+    elif column.semantic == dataspec.Semantic.CATEGORICAL:
+
+      from_boolean = False
+      if not isinstance(column_data, np.ndarray):
+        column_data = np.array(column_data, dtype=np.bytes_)
+      ydf_dtype = dataspec.np_dtype_to_ydf_dtype(column_data.dtype)
+
+      if column_data.dtype.type in [np.bool_]:
+        bool_column_data = column_data
+        column_data = np.full_like(bool_column_data, b"false", "|S5")
+        column_data[bool_column_data] = b"true"
+        from_boolean = True
+      elif (
+          column_data.dtype.type
+          in [
+              np.object_,
+              np.string_,
+              np.str_,
+          ]
+          or column_data.dtype.type in dataspec.NP_SUPPORTED_INT_DTYPE
+      ):
+        column_data = column_data.astype(np.bytes_)
+      elif np.issubdtype(column_data.dtype, np.floating):
+        raise ValueError(
+            f"Cannot import column {column.name!r} with"
+            f" semantic={column.semantic} as it contains floating point values."
+            f" Got {original_column_data!r}.\nNote: If the column is a label,"
+            " make sure the correct task is selected. For example, you cannot"
+            " train a classification model (task=ydf.Task.CLASSIFICATION) with"
+            " floating point labels."
+        )
+
+      if column_data.dtype.type == np.bytes_:
+        if inference_args is not None:
+          guide = dataspec.categorical_column_guide(column, inference_args)
+          if from_boolean:
+            guide["dictionary"] = np.array(
+                [b"<OOV>", b"false", b"true"], dtype=np.bytes_
+            )
+          self._dataset.PopulateColumnCategoricalNPBytes(
+              column.name, column_data, **guide, ydf_dtype=ydf_dtype
+          )
+        else:
+          self._dataset.PopulateColumnCategoricalNPBytes(
+              column.name,
+              column_data,
+              column_idx=column_idx,
+              ydf_dtype=ydf_dtype,
+          )
+        return
+
+    elif column.semantic == dataspec.Semantic.CATEGORICAL_SET:
+      if (
+          not isinstance(column_data, list)
+          and column_data.dtype.type != np.object_
+      ):
+        raise ValueError("Categorical Set columns must be a list of lists.")
+      if len(column_data) > 0:
+        if column_data[0] and not isinstance(
+            column_data[0], (list, np.ndarray)
+        ):
+          raise ValueError("Categorical Set columns must be a list of lists.")
+      # TODO: b/313414785 - Consider speeding this up by moving logic to C++.
+      # np.unique also sorts the unique elements, which is expected by YDF.
+      column_data = [np.unique(row).astype(np.bytes_) for row in column_data]
+      boundaries = np.cumsum(
+          [0] + [len(row) for row in column_data[:-1]], dtype=np.int64
+      )
+      bank = np.concatenate(column_data)
+      ydf_dtype = dataspec.np_dtype_to_ydf_dtype(bank.dtype)
+
+      if inference_args is not None:
+        guide = dataspec.categorical_column_guide(column, inference_args)
+        self._dataset.PopulateColumnCategoricalSetNPBytes(
+            column.name,
+            bank,
+            boundaries,
+            ydf_dtype,
+            **guide,
+        )
+      else:
+        self._dataset.PopulateColumnCategoricalSetNPBytes(
+            column.name,
+            bank,
+            boundaries,
+            ydf_dtype,
+            column_idx=column_idx,
+        )
+      return
+
+    elif column.semantic == dataspec.Semantic.HASH:
+      if not isinstance(column_data, np.ndarray):
+        column_data = np.array(column_data, dtype=np.bytes_)
+      ydf_dtype = dataspec.np_dtype_to_ydf_dtype(column_data.dtype)
+
+      if column_data.dtype.type in [
+          np.object_,
+          np.string_,
+          np.bool_,
+      ] or np.issubdtype(column_data.dtype, np.integer):
+        column_data = column_data.astype(np.bytes_)
+      elif np.issubdtype(column_data.dtype, np.floating):
+        raise ValueError(
+            f"Cannot import column {column.name!r} with"
+            f" semantic={column.semantic} as it contains floating point values."
+            f" Got {original_column_data!r}."
+        )
+
+      if column_data.dtype.type == np.bytes_:
+        self._dataset.PopulateColumnHashNPBytes(
+            column.name,
+            column_data,
+            ydf_dtype=ydf_dtype,
+            column_idx=column_idx,
+        )
+        return
+
+    raise ValueError(
+        f"Cannot import column {column.name!r} with semantic={column.semantic},"
+        f" type={_type(original_column_data)} and"
+        f" content={original_column_data!r}.\nNote: If the column is a label,"
+        " the semantic was selected based on the task. For example,"
+        " task=ydf.Task.CLASSIFICATION requires a CATEGORICAL compatible label"
+        " column, and task=ydf.Task.REGRESSION requires a NUMERICAL compatible"
+        " label column."
+    )
+
+  def _initialize_from_data_spec(
+      self, data_spec: data_spec_pb2.DataSpecification
+  ):
+    self._dataset.CreateColumnsFromDataSpec(data_spec)
+
+  def _finalize(self, set_num_rows_in_data_spec: bool):
+    self._dataset.SetAndCheckNumRowsAndFillMissing(set_num_rows_in_data_spec)
+
+
+def create_vertical_dataset(
+    data: InputDataset,
+    columns: dataspec.ColumnDefs = None,
+    include_all_columns: bool = False,
+    max_vocab_count: int = 2000,
+    min_vocab_frequency: int = 5,
+    discretize_numerical_columns: bool = False,
+    num_discretized_numerical_bins: int = 255,
+    max_num_scanned_rows_to_infer_semantic: int = 10000,
+    max_num_scanned_rows_to_compute_statistics: int = 10000,
+    data_spec: Optional[data_spec_pb2.DataSpecification] = None,
+    required_columns: Optional[Sequence[str]] = None,
+    dont_unroll_columns: Optional[Sequence[str]] = None,
+) -> VerticalDataset:
+  """Creates a VerticalDataset from various sources of data.
+
+  The feature semantics are automatically determined and can be explicitly
+  set with the `columns` argument. The semantics of a dataset (or model) are
+  available its data_spec.
+
+  Note that the CATEGORICAL_SET semantic is not automatically inferred when
+  reading from file. When reading from CSV files, setting the CATEGORICAL_SET
+  semantic for a feature will have YDF tokenize the feature. When reading from
+  in-memory datasets (e.g. pandas), YDF only accepts lists of lists for
+  CATEGORICAL_SET features.
+
+  Usage example:
+
+  ```python
+  import pandas as pd
+  import ydf
+
+  df = pd.read_csv("my_dataset.csv")
+
+  # Loads all the columns
+  ds = ydf.create_vertical_dataset(df)
+
+  # Only load columns "a" and "b". Ensure "b" is interpreted as a categorical
+  # feature.
+  ds = ydf.create_vertical_dataset(df,
+    columns=[
+      "a",
+      ("b", ydf.semantic.categorical),
+    ])
+  ```
+
+  Args:
+    data: Source dataset. Supported formats: VerticalDataset, (typed) path,
+      Pandas Dataframe, dictionary of string to Numpy array or lists. If the
+      data is already a VerticalDataset, it is returned unchanged.
+    columns: If None, all columns are imported. The semantic of the columns is
+      determined automatically. Otherwise, if include_all_columns=False
+      (default) only the column listed in `columns` are imported. If
+      include_all_columns=True, all the columns are imported and only the
+      semantic of the columns NOT in `columns` is determined automatically. If
+      specified, "columns" defines the order of the columns - any non-listed
+      columns are appended in-order after the specified columns (if
+      include_all_columns=True).
+    include_all_columns: See `columns`.
+    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
+      CATEGORICAL_SET columns stored as strings. If more unique values exist,
+      only the most frequent values are kept, and the remaining values are
+      considered as out-of-vocabulary.  If max_vocab_count = -1, the number of
+      values in the column is not limited (not recommended).
+    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
+      and CATEGORICAL_SET columns. Value observed less than
+      `min_vocab_frequency` are considered as out-of-vocabulary.
+    discretize_numerical_columns: If true, discretize all the numerical columns
+      before training. Discretized numerical columns are faster to train with,
+      but they can have a negative impact on the model quality. Using
+      `discretize_numerical_columns=True` is equivalent as setting the column
+      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
+      definition of DISCRETIZED_NUMERICAL for more details.
+    num_discretized_numerical_bins: Number of bins used when disretizing
+      numerical columns.
+    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
+      inferring the column's semantic if it is not explicitly specified. Only
+      used when reading from file, in-memory datasets are always read in full.
+      Setting this to a lower number will speed up dataset reading, but might
+      result in incorrect column semantics. Set to -1 to scan the entire
+      dataset.
+    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
+      computing a column's statistics. Only used when reading from file,
+      in-memory datasets are always read in full. A column's statistics include
+      the dictionary for categorical features and the mean / min / max for
+      numerical features. Setting this to a lower number will speed up dataset
+      reading, but skew statistics in the dataspec, which can hurt model quality
+      (e.g. if an important category of a categorical feature is considered
+      OOV). Set to -1 to scan the entire dataset.
+    data_spec: Dataspec to be used for this dataset. If a data spec is given,
+      all other arguments except `data` and `required_columns` should not be
+      provided.
+    required_columns: List of columns required in the data. If None, all columns
+      mentioned in the data spec or `columns` are required.
+    dont_unroll_columns: List of columns that cannot be unrolled. If one such
+      column needs to be unrolled, raise an error.
+
+  Returns:
+    Dataset to be ingested by the learner algorithms.
+
+  Raises:
+    ValueError: If the dataset has an unsupported type.
+  """
+  if isinstance(data, VerticalDataset):
+    return data
+
+  if data_spec is not None:
+    if columns is not None:
+      raise ValueError(
+          "When passing a data spec at dataset creation, `columns` must be"
+          " None and all arguments to guide data spec inference are ignored."
+      )
+    return create_vertical_dataset_with_spec_or_args(
+        data,
+        required_columns,
+        data_spec=data_spec,
+        inference_args=None,
+        dont_unroll_columns=dont_unroll_columns,
+    )
+  else:
+    inference_args = dataspec.DataSpecInferenceArgs(
+        columns=dataspec.normalize_column_defs(columns),
+        include_all_columns=include_all_columns,
+        max_vocab_count=max_vocab_count,
+        min_vocab_frequency=min_vocab_frequency,
+        discretize_numerical_columns=discretize_numerical_columns,
+        num_discretized_numerical_bins=num_discretized_numerical_bins,
+        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
+        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
+    )
+    return create_vertical_dataset_with_spec_or_args(
+        data,
+        required_columns,
+        inference_args=inference_args,
+        data_spec=None,
+        dont_unroll_columns=dont_unroll_columns,
+    )
+
+
+def create_vertical_dataset_with_spec_or_args(
+    data: dataset_io_types.IODataset,
+    required_columns: Optional[Sequence[str]],
+    inference_args: Optional[dataspec.DataSpecInferenceArgs],
+    data_spec: Optional[data_spec_pb2.DataSpecification],
+    dont_unroll_columns: Optional[Sequence[str]] = None,
+) -> VerticalDataset:
+  """Returns a vertical dataset from inference args or data spec (not both!)."""
+  assert (data_spec is None) != (inference_args is None)
+  # If `data` is a path, try to import from the path directly from C++.
+  # Everything else we try to transform into a dictionary with Python.
+  if isinstance(data, str) or (
+      isinstance(data, Sequence)
+      and data
+      and all(isinstance(s, str) for s in data)
+  ):
+    return create_vertical_dataset_from_path(
+        data, required_columns, inference_args, data_spec
+    )
+  else:
+    # Convert the data to an in-memory dictionary of numpy array.
+    # Also unroll multi-dimensional features.
+    data_dict, unroll_feature_info = dataset_io.cast_input_dataset_to_dict(
+        data, dont_unroll_columns=dont_unroll_columns
+    )
+    return create_vertical_dataset_from_dict_of_values(
+        data_dict,
+        unroll_feature_info,
+        required_columns,
+        inference_args=inference_args,
+        data_spec=data_spec,
+    )
+
+
+def create_vertical_dataset_from_path(
+    path: Union[str, List[str]],
+    required_columns: Optional[Sequence[str]],
+    inference_args: Optional[dataspec.DataSpecInferenceArgs],
+    data_spec: Optional[data_spec_pb2.DataSpecification],
+) -> VerticalDataset:
+  """Returns a VerticalDataset from (list of) path using YDF dataset reading."""
+  assert (data_spec is None) != (inference_args is None)
+  if not isinstance(path, str):
+    path = paths.normalize_list_of_paths(path)
+  dataset = VerticalDataset()
+  if data_spec is not None:
+    dataset._dataset.CreateFromPathWithDataSpec(
+        path, data_spec, required_columns
+    )
+  if inference_args is not None:
+    dataset._dataset.CreateFromPathWithDataSpecGuide(
+        path, inference_args.to_proto_guide(), required_columns
+    )
+  return dataset
+
+
+def create_vertical_dataset_from_dict_of_values(
+    data: Dict[str, dataset_io_types.InputValues],
+    unroll_feature_info: dataset_io_types.UnrolledFeaturesInfo,
+    required_columns: Optional[Sequence[str]],
+    inference_args: Optional[dataspec.DataSpecInferenceArgs],
+    data_spec: Optional[data_spec_pb2.DataSpecification],
+) -> VerticalDataset:
+  """Specialization of create_vertical_dataset to dictionary of values.
+
+  The data spec is either inferred using inference_args or uses the given
+  data_spec
+
+  Args:
+    data: Data to copy to the Vertical Dataset.
+    unroll_feature_info: Information about feature unrolling.
+    required_columns: Names of columns that are required in the data.
+    inference_args: Arguments for data spec inference. Must be None if data_spec
+      is set.
+    data_spec: Data spec of the given data. Must be None if inference_args is
+      set.
+
+  Returns:
+    A Vertical dataset with the given properties.
+  """
+
+  def dataspec_to_normalized_columns(
+      data: Dict[str, dataset_io_types.InputValues],
+      columns,
+      required_columns: Sequence[str],
+  ):
+    normalized_columns = []
+    for column_spec in columns:
+      if column_spec.name not in data and column_spec.name in required_columns:
+        raise ValueError(
+            f"The data spec expects columns {column_spec.name} which was not"
+            f" found in the data. Available columns: {list(data)}. Required"
+            f" columns: {required_columns}"
+        )
+      if (
+          column_spec.type == data_spec_pb2.CATEGORICAL_SET
+          and column_spec.HasField("tokenizer")
+          and column_spec.tokenizer.splitter
+          != data_spec_pb2.Tokenizer.NO_SPLITTING
+      ):
+        log.warning(
+            f"The dataspec for columns {column_spec.name} specifies a"
+            " tokenizer, but it is ignored when reading in-memory datasets."
+        )
+      else:
+        normalized_columns.append(
+            dataspec.Column(
+                name=column_spec.name,
+                semantic=dataspec.Semantic.from_proto_type(column_spec.type),
+            )
+        )
+    return normalized_columns
+
+  assert (data_spec is None) != (inference_args is None)
+  dataset = VerticalDataset()
+  if data_spec is None:
+    # If `required_columns` is None, only check if the columns mentioned in the
+    # `inference_args` are required. This is checked by
+    # dataspec.get_all_columns()
+    normalized_columns, effective_unroll_feature_info = (
+        dataspec.get_all_columns(
+            available_columns=list(data.keys()),
+            inference_args=inference_args,
+            required_columns=required_columns,
+            unroll_feature_info=unroll_feature_info,
+        )
+    )
+  else:
+    effective_unroll_feature_info = None  # To please linter
+    dataset._initialize_from_data_spec(data_spec)  # pylint: disable=protected-access
+    required_columns = (
+        required_columns
+        if required_columns is not None
+        else [c.name for c in data_spec.columns]
+    )
+    normalized_columns = dataspec_to_normalized_columns(
+        data, data_spec.columns, required_columns
+    )
+
+  for column_idx, column in enumerate(normalized_columns):
+    effective_column = copy.deepcopy(column)
+    if column.name not in data:
+      # The column is missing, so no data is passed but the column will still be
+      # created.
+      column_data = []
+    else:
+      column_data = data[column.name]
+    if column.semantic is None:
+      infered_semantic = infer_semantic(column.name, column_data)
+      effective_column.semantic = infered_semantic
+
+    dataset._add_column(  # pylint: disable=protected-access
+        effective_column,
+        column_data,
+        inference_args=inference_args,  # Might be None
+        column_idx=column_idx if data_spec is not None else None,
+    )
+
+  if data_spec is None:
+    assert effective_unroll_feature_info is not None
+    dataset._dataset.SetMultiDimDataspec(effective_unroll_feature_info)  # pylint: disable=protected-access
+
+  dataset._finalize(set_num_rows_in_data_spec=(data_spec is None))  # pylint: disable=protected-access
+  return dataset
+
+
+def infer_semantic(name: str, data: Any) -> dataspec.Semantic:
+  """Infers the semantic of a column from its data."""
+
+  # If a column has no data, we assume it only contains missing values.
+  if len(data) == 0:
+    raise ValueError(
+        f"Cannot infer automatically the semantic of column {name!r} since no"
+        " data for this column was provided. Make sure this column exists in"
+        " the dataset, or exclude the column from the list of required"
+        " columns. If the dataset should contain missing values for the all"
+        " examples of this column, specify the semantic of the column manually"
+        f" using the `features` argument e.g. `features=[({name!r},"
+        " ydf.Semantic.NUMERICAL)]` if the feature is numerical."
+    )
+
+  if isinstance(data, np.ndarray):
+    # We finely control the supported types.
+    if (
+        data.dtype.type in dataspec.NP_SUPPORTED_INT_DTYPE
+        or data.dtype.type in dataspec.NP_SUPPORTED_FLOAT_DTYPE
+    ):
+      return dataspec.Semantic.NUMERICAL
+
+    if data.dtype.type in [np.string_, np.bytes_, np.str_]:
+      return dataspec.Semantic.CATEGORICAL
+
+    if data.dtype.type in [np.object_]:
+      # For performance reasons, only check the type on the first and last item
+      # of the column if it is tokenized.
+      if (
+          len(data) > 0
+          and (isinstance(data[0], list) or isinstance(data[0], np.ndarray))
+          and (isinstance(data[-1], list) or isinstance(data[-1], np.ndarray))
+      ):
+        return dataspec.Semantic.CATEGORICAL_SET
+      return dataspec.Semantic.CATEGORICAL
+
+    if data.dtype.type in [np.bool_]:
+      return dataspec.Semantic.BOOLEAN
+    type_str = f"numpy.array of {data.dtype}"
+  else:
+    type_str = str(type(data))
+
+  raise ValueError(
+      f"Cannot infer automatically the semantic of column {name!r} with"
+      f" type={type_str}, and content={data}. Convert the column to a supported"
+      " type, or specify the semantic of the column manually using the"
+      f" `features` argument e.g. `features=[({name!r},"
+      " ydf.Semantic.NUMERICAL)]` if the feature is numerical."
+  )
+
+
+def _type(value: Any) -> str:
+  """Returns a string representation of the type of value."""
+
+  if isinstance(value, np.ndarray):
+    return f"numpy's array of '{value.dtype.name}'"
+  else:
+    return str(type(value))
```

## ydf/dataset/dataset_test.py

```diff
@@ -1,1829 +1,1829 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-import os
-import unittest
-
-from absl.testing import absltest
-from absl.testing import parameterized
-import numpy as np
-import pandas as pd
-
-from yggdrasil_decision_forests.dataset import data_spec_pb2 as ds_pb
-from ydf.dataset import dataset
-from ydf.dataset import dataspec
-from ydf.utils import test_utils
-
-# Make "assertEqual" print more details.
-unittest.util._MAX_LENGTH = 10000
-
-Semantic = dataspec.Semantic
-VocabValue = ds_pb.CategoricalSpec.VocabValue
-Column = dataspec.Column
-
-
-class GenericDatasetTest(parameterized.TestCase):
-
-  @parameterized.parameters(
-      (np.array([1], np.int8), Semantic.NUMERICAL),
-      (np.array([1], np.int16), Semantic.NUMERICAL),
-      (np.array([1], np.int32), Semantic.NUMERICAL),
-      (np.array([1], np.int64), Semantic.NUMERICAL),
-      (np.array([1], np.uint8), Semantic.NUMERICAL),
-      (np.array([1], np.uint16), Semantic.NUMERICAL),
-      (np.array([1], np.uint32), Semantic.NUMERICAL),
-      (np.array([1], np.uint64), Semantic.NUMERICAL),
-      (np.array([1], np.float32), Semantic.NUMERICAL),
-      (np.array([1], np.float64), Semantic.NUMERICAL),
-      (np.array([1], np.bool_), Semantic.BOOLEAN),
-      (np.array(["a"], np.bytes_), Semantic.CATEGORICAL),
-      (np.array(["a"], np.string_), Semantic.CATEGORICAL),
-      (np.array(["a", np.nan], np.object_), Semantic.CATEGORICAL),
-  )
-  def test_infer_semantic(self, value, expected_semantic):
-    self.assertEqual(dataset.infer_semantic("", value), expected_semantic)
-
-  @parameterized.parameters(
-      np.float16,
-      np.float32,
-      np.float64,
-      np.int8,
-      np.int16,
-      np.int32,
-      np.int64,
-  )
-  def test_create_vds_pd_numerical(self, dtype):
-    df = pd.DataFrame(
-        {
-            "col_pos": [1, 2, 3],
-            "col_neg": [-1, -2, -3],
-            "col_zero": [0, 0, 0],
-        },
-        dtype=dtype,
-    )
-    ds = dataset.create_vertical_dataset(df)
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=3,
-        columns=(
-            ds_pb.Column(
-                name="col_pos",
-                type=ds_pb.ColumnType.NUMERICAL,
-                dtype=dataspec.np_dtype_to_ydf_dtype(dtype),
-                count_nas=0,
-                numerical=ds_pb.NumericalSpec(
-                    mean=2,
-                    standard_deviation=0.8164965809277263,  # ~math.sqrt(2 / 3)
-                    min_value=1,
-                    max_value=3,
-                ),
-            ),
-            ds_pb.Column(
-                name="col_neg",
-                type=ds_pb.ColumnType.NUMERICAL,
-                dtype=dataspec.np_dtype_to_ydf_dtype(dtype),
-                count_nas=0,
-                numerical=ds_pb.NumericalSpec(
-                    mean=-2,
-                    standard_deviation=0.8164965809277263,  # ~math.sqrt(2 / 3)
-                    min_value=-3,
-                    max_value=-1,
-                ),
-            ),
-            ds_pb.Column(
-                name="col_zero",
-                type=ds_pb.ColumnType.NUMERICAL,
-                dtype=dataspec.np_dtype_to_ydf_dtype(dtype),
-                count_nas=0,
-                numerical=ds_pb.NumericalSpec(
-                    mean=0,
-                    standard_deviation=0,
-                    min_value=0,
-                    max_value=0,
-                ),
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  @parameterized.parameters(np.float16, np.float32, np.float64)
-  def test_create_vds_pd_numerical_with_nan(self, dtype):
-    df = pd.DataFrame(
-        {
-            "col_single_nan": [1, 2, np.NaN],
-            "col_nan_only": [np.NaN, np.NaN, np.NaN],
-        },
-        dtype=dtype,
-    )
-    ds = dataset.create_vertical_dataset(df)
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=3,
-        columns=(
-            ds_pb.Column(
-                name="col_single_nan",
-                type=ds_pb.ColumnType.NUMERICAL,
-                dtype=dataspec.np_dtype_to_ydf_dtype(dtype),
-                count_nas=1,
-                numerical=ds_pb.NumericalSpec(
-                    mean=1.5,
-                    standard_deviation=0.5,
-                    min_value=1,
-                    max_value=2,
-                ),
-            ),
-            ds_pb.Column(
-                name="col_nan_only",
-                type=ds_pb.ColumnType.NUMERICAL,
-                dtype=dataspec.np_dtype_to_ydf_dtype(dtype),
-                count_nas=3,
-                numerical=ds_pb.NumericalSpec(),
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_create_vds_pd_categorical_string(self):
-    df = pd.DataFrame({
-        "col1": ["A", "A", "B", "B", "C"],
-        "col2": ["", "A", "B", "C", "D"],
-    })
-
-    ds = dataset.create_vertical_dataset(
-        df,
-        min_vocab_frequency=2,
-        columns=[
-            Column(
-                "col2",
-                Semantic.CATEGORICAL,
-                min_vocab_frequency=1,
-                max_vocab_count=3,
-            )
-        ],
-        include_all_columns=True,
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=5,
-        columns=(
-            ds_pb.Column(
-                name="col2",
-                type=ds_pb.ColumnType.CATEGORICAL,
-                dtype=ds_pb.DType.DTYPE_BYTES,
-                count_nas=1,
-                categorical=ds_pb.CategoricalSpec(
-                    items={
-                        "<OOD>": VocabValue(index=0, count=1),
-                        "A": VocabValue(index=1, count=1),
-                        "B": VocabValue(index=2, count=1),
-                        "C": VocabValue(index=3, count=1),
-                    },
-                    number_of_unique_values=4,
-                ),
-            ),
-            ds_pb.Column(
-                name="col1",
-                type=ds_pb.ColumnType.CATEGORICAL,
-                dtype=ds_pb.DType.DTYPE_BYTES,
-                count_nas=0,
-                categorical=ds_pb.CategoricalSpec(
-                    items={
-                        "<OOD>": VocabValue(index=0, count=1),
-                        "A": VocabValue(index=1, count=2),
-                        "B": VocabValue(index=2, count=2),
-                    },
-                    number_of_unique_values=3,
-                ),
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_create_vds_pd_categorical_int(self):
-    df = pd.DataFrame({
-        "col1": [1, 1, 2, 2, 3],
-    })
-
-    ds = dataset.create_vertical_dataset(
-        df,
-        min_vocab_frequency=1,
-        columns=[
-            Column(
-                "col1",
-                Semantic.CATEGORICAL,
-            )
-        ],
-        include_all_columns=True,
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=5,
-        columns=(
-            ds_pb.Column(
-                name="col1",
-                type=ds_pb.ColumnType.CATEGORICAL,
-                dtype=ds_pb.DType.DTYPE_INT64,
-                count_nas=0,
-                categorical=ds_pb.CategoricalSpec(
-                    items={
-                        "<OOD>": VocabValue(index=0, count=0),
-                        "1": VocabValue(index=1, count=2),
-                        "2": VocabValue(index=2, count=2),
-                        "3": VocabValue(index=3, count=1),
-                    },
-                    number_of_unique_values=4,
-                ),
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_create_vds_pd_boolean(self):
-    df = pd.DataFrame(
-        {"col_boolean": [True, False, False]},
-    )
-
-    ds = dataset.create_vertical_dataset(df)
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=3,
-        columns=(
-            ds_pb.Column(
-                name="col_boolean",
-                type=ds_pb.ColumnType.BOOLEAN,
-                dtype=ds_pb.DType.DTYPE_BOOL,
-                count_nas=0,
-                boolean=ds_pb.BooleanSpec(
-                    count_true=1,
-                    count_false=2,
-                ),
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_create_vds_pd_hash(self):
-    df = pd.DataFrame(
-        {"col_hash": ["a", "b", "abc"]},
-    )
-
-    ds = dataset.create_vertical_dataset(
-        df, columns=[("col_hash", Semantic.HASH)]
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=3,
-        columns=(
-            ds_pb.Column(
-                name="col_hash",
-                type=ds_pb.ColumnType.HASH,
-                dtype=ds_pb.DType.DTYPE_BYTES,
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  @parameterized.parameters(
-      (["col_numerical"],),
-      ([Column("col_numerical")],),
-      ([("col_numerical", Semantic.NUMERICAL)],),
-  )
-  def test_create_vds_exclude_columns(self, column_definition):
-    df = pd.DataFrame({
-        "col_numerical": [1, 2, 3],
-        "col_str": ["A", "B", "C"],
-    })
-    ds = dataset.create_vertical_dataset(df, columns=column_definition)
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=3,
-        columns=(
-            ds_pb.Column(
-                name="col_numerical",
-                type=ds_pb.ColumnType.NUMERICAL,
-                dtype=ds_pb.DType.DTYPE_INT64,
-                count_nas=0,
-                numerical=ds_pb.NumericalSpec(
-                    mean=2,
-                    standard_deviation=0.8164965809277263,  # ~math.sqrt(2 / 3)
-                    min_value=1,
-                    max_value=3,
-                ),
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_create_vds_dict_of_values(self):
-    ds_dict = {
-        "a": np.array([1, 2, 3]),
-        "b": np.array([-1, -2, -3]),
-    }
-    ds = dataset.create_vertical_dataset(ds_dict)
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=3,
-        columns=(
-            ds_pb.Column(
-                name="a",
-                type=ds_pb.ColumnType.NUMERICAL,
-                dtype=ds_pb.DType.DTYPE_INT64,
-                count_nas=0,
-                numerical=ds_pb.NumericalSpec(
-                    mean=2,
-                    standard_deviation=0.8164965809277263,  # ~math.sqrt(2 / 3)
-                    min_value=1,
-                    max_value=3,
-                ),
-            ),
-            ds_pb.Column(
-                name="b",
-                type=ds_pb.ColumnType.NUMERICAL,
-                dtype=ds_pb.DType.DTYPE_INT64,
-                count_nas=0,
-                numerical=ds_pb.NumericalSpec(
-                    mean=-2,
-                    standard_deviation=0.8164965809277263,  # ~math.sqrt(2 / 3)
-                    min_value=-3,
-                    max_value=-1,
-                ),
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_memory_usage(self):
-    df = pd.DataFrame({
-        "col1": [1.1, 21.1, np.NaN],
-        "col2": [1, 2, 3],
-    })
-    ds = dataset.create_vertical_dataset(df)
-    # 2 columns * 3 rows * 4 bytes per values
-    self.assertEqual(ds.memory_usage(), 2 * 3 * 4)
-
-  def test_create_vds_pd_with_spec(self):
-    data_spec = ds_pb.DataSpecification(
-        created_num_rows=3,
-        columns=(
-            ds_pb.Column(
-                name="col_num",
-                type=ds_pb.ColumnType.NUMERICAL,
-                dtype=ds_pb.DType.DTYPE_INT64,
-                count_nas=0,
-                numerical=ds_pb.NumericalSpec(
-                    mean=2,
-                    standard_deviation=0.8164965809277263,  # ~math.sqrt(2 / 3)
-                    min_value=1,
-                    max_value=3,
-                ),
-            ),
-            ds_pb.Column(
-                name="col_cat",
-                type=ds_pb.ColumnType.CATEGORICAL,
-                dtype=ds_pb.DType.DTYPE_BYTES,
-                count_nas=0,
-                categorical=ds_pb.CategoricalSpec(
-                    items={
-                        "<OOD>": VocabValue(index=0, count=1),
-                        "A": VocabValue(index=1, count=2),
-                        "B": VocabValue(index=2, count=2),
-                    },
-                    number_of_unique_values=3,
-                ),
-            ),
-            ds_pb.Column(
-                name="col_bool",
-                type=ds_pb.ColumnType.BOOLEAN,
-                dtype=ds_pb.DType.DTYPE_BOOL,
-                count_nas=0,
-                boolean=ds_pb.BooleanSpec(
-                    count_true=55,
-                    count_false=123,
-                ),
-            ),
-        ),
-    )
-    df = pd.DataFrame({
-        "col_num": [1, 2, 3],
-        "col_cat": ["A", "B", "C"],
-        "col_bool": [True, True, False],
-    })
-    ds = dataset.create_vertical_dataset(df, data_spec=data_spec)
-    test_utils.assertProto2Equal(self, ds.data_spec(), data_spec)
-
-  def test_create_vds_dict_of_values_with_spec(self):
-    data_spec = ds_pb.DataSpecification(
-        created_num_rows=3,
-        columns=(
-            ds_pb.Column(
-                name="col_num",
-                type=ds_pb.ColumnType.NUMERICAL,
-                dtype=ds_pb.DType.DTYPE_INT64,
-                count_nas=0,
-                numerical=ds_pb.NumericalSpec(
-                    mean=2,
-                    standard_deviation=0.8164965809277263,  # ~math.sqrt(2 / 3)
-                    min_value=1,
-                    max_value=3,
-                ),
-            ),
-            ds_pb.Column(
-                name="col_cat",
-                type=ds_pb.ColumnType.CATEGORICAL,
-                dtype=ds_pb.DType.DTYPE_BYTES,
-                count_nas=0,
-                categorical=ds_pb.CategoricalSpec(
-                    items={
-                        "<OOD>": VocabValue(index=0, count=1),
-                        "A": VocabValue(index=1, count=2),
-                        "B": VocabValue(index=2, count=2),
-                    },
-                    number_of_unique_values=3,
-                ),
-            ),
-            ds_pb.Column(
-                name="col_bool",
-                type=ds_pb.ColumnType.BOOLEAN,
-                dtype=ds_pb.DType.DTYPE_BOOL,
-                count_nas=0,
-                boolean=ds_pb.BooleanSpec(
-                    count_true=55,
-                    count_false=123,
-                ),
-            ),
-        ),
-    )
-    ds_dict = {
-        "col_num": [1, 2, 3],
-        "col_cat": ["A", "B", "C"],
-        "col_bool": [True, False, True],
-    }
-    ds = dataset.create_vertical_dataset(ds_dict, data_spec=data_spec)
-    test_utils.assertProto2Equal(self, ds.data_spec(), data_spec)
-    # 2 columns * 3 rows * 4 bytes per value + 1 col * 3 rows * 1 byte p.v.
-    self.assertEqual(ds.memory_usage(), 2 * 3 * 4 + 1 * 3 * 1)
-
-  def test_create_vds_pd_check_contents(self):
-    df = pd.DataFrame({
-        "col_str": ["A", "string", "column with", "four entries"],
-        "col_int": [5, 6, 7, 8],
-        "col_int_cat": [1, 2, 3, 4],
-        "col_float": [1.1, 2.2, 3.3, 4.4],
-        "col_bool": [True, True, False, False],
-    })
-    feature_definitions = [
-        Column("col_str", Semantic.CATEGORICAL, min_vocab_frequency=1),
-        Column("col_int_cat", Semantic.CATEGORICAL, min_vocab_frequency=1),
-    ]
-    ds = dataset.create_vertical_dataset(
-        df, columns=feature_definitions, include_all_columns=True
-    )
-    expected_dataset_content = """col_str,col_int_cat,col_int,col_float,col_bool
-A,1,5,1.1,1
-string,2,6,2.2,1
-column with,3,7,3.3,0
-four entries,4,8,4.4,0
-"""
-    self.assertEqual(expected_dataset_content, ds._dataset.DebugString())
-
-  def test_create_vds_pd_check_categorical(self):
-    df = pd.DataFrame({"col_str": ["A", "B"] * 5})
-    ds = dataset.create_vertical_dataset(df)
-    expected_dataset_content = "col_str\n" + "A\nB\n" * 5
-    self.assertEqual(expected_dataset_content, ds._dataset.DebugString())
-
-  def test_max_vocab_count_minus_1(self):
-    df = pd.DataFrame({
-        "col1": ["A", "B", "C", "D", "D"],
-    })
-    ds = dataset.create_vertical_dataset(
-        df,
-        columns=[
-            Column(
-                "col1",
-                Semantic.CATEGORICAL,
-                min_vocab_frequency=1,
-                max_vocab_count=-1,
-            )
-        ],
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=5,
-        columns=(
-            ds_pb.Column(
-                name="col1",
-                type=ds_pb.ColumnType.CATEGORICAL,
-                dtype=ds_pb.DType.DTYPE_BYTES,
-                count_nas=0,
-                categorical=ds_pb.CategoricalSpec(
-                    items={
-                        "<OOD>": VocabValue(index=0, count=0),
-                        "A": VocabValue(index=2, count=1),
-                        "B": VocabValue(index=3, count=1),
-                        "C": VocabValue(index=4, count=1),
-                        "D": VocabValue(index=1, count=2),
-                    },
-                    number_of_unique_values=5,
-                ),
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_invalid_max_vocab_count(self):
-    df = pd.DataFrame({
-        "col1": ["A", "B", "C", "D", "D"],
-    })
-    with self.assertRaisesRegex(
-        test_utils.AbslInvalidArgumentError, "max_vocab_count"
-    ):
-      dataset.create_vertical_dataset(
-          df,
-          columns=[
-              Column(
-                  "col1",
-                  Semantic.CATEGORICAL,
-                  max_vocab_count=-2,
-              )
-          ],
-      )
-
-  @parameterized.parameters(
-      ([True, True, True], (0, 0, 3), 0),
-      ([False, False, False], (0, 3, 0), 0),
-      ([True, False, False], (0, 2, 1), 0),
-  )
-  def test_order_boolean(self, values, expected_counts, count_nas):
-    ds = dataset.create_vertical_dataset(
-        {"col": np.array(values)},
-        columns=[Column("col", dataspec.Semantic.CATEGORICAL)],
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=3,
-        columns=(
-            ds_pb.Column(
-                name="col",
-                type=ds_pb.ColumnType.CATEGORICAL,
-                dtype=ds_pb.DType.DTYPE_BOOL,
-                count_nas=count_nas,
-                categorical=ds_pb.CategoricalSpec(
-                    items={
-                        "<OOV>": VocabValue(index=0, count=expected_counts[0]),
-                        "false": VocabValue(index=1, count=expected_counts[1]),
-                        "true": VocabValue(index=2, count=expected_counts[2]),
-                    },
-                    number_of_unique_values=3,
-                ),
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  @parameterized.parameters(
-      ([True, True, True], (0, 0, 3), 0),
-      ([False, False, False], (0, 3, 0), 0),
-      ([True, False, False], (0, 2, 1), 0),
-  )
-  def test_order_boolean(self, values, expected_counts, count_nas):
-    ds = dataset.create_vertical_dataset(
-        {"col": np.array(values)},
-        columns=[Column("col", dataspec.Semantic.CATEGORICAL)],
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=3,
-        columns=(
-            ds_pb.Column(
-                name="col",
-                type=ds_pb.ColumnType.CATEGORICAL,
-                dtype=ds_pb.DType.DTYPE_BOOL,
-                count_nas=count_nas,
-                categorical=ds_pb.CategoricalSpec(
-                    items={
-                        "<OOV>": VocabValue(index=0, count=expected_counts[0]),
-                        "false": VocabValue(index=1, count=expected_counts[1]),
-                        "true": VocabValue(index=2, count=expected_counts[2]),
-                    },
-                    number_of_unique_values=3,
-                ),
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  @parameterized.parameters("", "csv:")
-  def test_read_csv(self, path_prefix):
-    tmp_dir = self.create_tempdir()
-    csv_file = self.create_tempfile(
-        content="""col_cat,col_num
-A,1
-B,2
-B,3""",
-        file_path=os.path.join(tmp_dir.full_path, "file.csv"),
-    )
-    ds = dataset.create_vertical_dataset(
-        path_prefix + csv_file.full_path, min_vocab_frequency=1
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=3,
-        columns=(
-            ds_pb.Column(
-                name="col_cat",
-                type=ds_pb.ColumnType.CATEGORICAL,
-                is_manual_type=False,
-                categorical=ds_pb.CategoricalSpec(
-                    items={
-                        "<OOD>": VocabValue(index=0, count=0),
-                        "A": VocabValue(index=2, count=1),
-                        "B": VocabValue(index=1, count=2),
-                    },
-                    number_of_unique_values=3,
-                    most_frequent_value=1,
-                    min_value_count=1,
-                    max_number_of_unique_values=2000,
-                    is_already_integerized=False,
-                ),
-            ),
-            ds_pb.Column(
-                name="col_num",
-                type=ds_pb.ColumnType.NUMERICAL,
-                is_manual_type=False,
-                numerical=ds_pb.NumericalSpec(
-                    mean=2,
-                    standard_deviation=0.8164965809277263,  # ~math.sqrt(2 / 3)
-                    min_value=1,
-                    max_value=3,
-                ),
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_max_num_scanned_rows_to_compute_statistics(self):
-    tmp_dir = self.create_tempdir()
-    csv_file = self.create_tempfile(
-        content="""col_cat,col_num
-A,1
-A,2
-B,3""",
-        file_path=os.path.join(tmp_dir.full_path, "file.csv"),
-    )
-    ds = dataset.create_vertical_dataset(
-        "csv:" + csv_file.full_path,
-        min_vocab_frequency=1,
-        max_num_scanned_rows_to_compute_statistics=2,
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=2,
-        columns=(
-            ds_pb.Column(
-                name="col_cat",
-                type=ds_pb.ColumnType.CATEGORICAL,
-                is_manual_type=False,
-                categorical=ds_pb.CategoricalSpec(
-                    items={
-                        "<OOD>": VocabValue(index=0, count=0),
-                        "A": VocabValue(index=1, count=2),
-                    },
-                    number_of_unique_values=2,
-                    most_frequent_value=1,
-                    min_value_count=1,
-                    max_number_of_unique_values=2000,
-                    is_already_integerized=False,
-                ),
-            ),
-            ds_pb.Column(
-                name="col_num",
-                type=ds_pb.ColumnType.NUMERICAL,
-                is_manual_type=False,
-                numerical=ds_pb.NumericalSpec(
-                    mean=1.5,
-                    standard_deviation=0.5,
-                    min_value=1,
-                    max_value=2,
-                ),
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_max_num_scanned_rows_to_infer_semantic(self):
-    tmp_dir = self.create_tempdir()
-    csv_file = self.create_tempfile(
-        content="""col_cat,col_num
-A,1
-B,2
-B,3""",
-        file_path=os.path.join(tmp_dir.full_path, "file.csv"),
-    )
-    ds = dataset.create_vertical_dataset(
-        "csv:" + csv_file.full_path,
-        min_vocab_frequency=1,
-        max_num_scanned_rows_to_infer_semantic=1,
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=3,
-        columns=(
-            ds_pb.Column(
-                name="col_cat",
-                type=ds_pb.ColumnType.CATEGORICAL,
-                is_manual_type=False,
-                categorical=ds_pb.CategoricalSpec(
-                    number_of_unique_values=3,
-                    most_frequent_value=1,
-                    min_value_count=1,
-                    max_number_of_unique_values=2000,
-                    is_already_integerized=False,
-                    items={
-                        "<OOD>": VocabValue(index=0, count=0),
-                        "A": VocabValue(index=2, count=1),
-                        "B": VocabValue(index=1, count=2),
-                    },
-                ),
-            ),
-            ds_pb.Column(
-                name="col_num",
-                type=ds_pb.ColumnType.BOOLEAN,
-                is_manual_type=False,
-                boolean=ds_pb.BooleanSpec(count_true=3),
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_read_from_path(self):
-    csv_file = self.create_tempfile(content="""col_cat,col_num
-A,1
-B,2
-B,3""")
-    ds = dataset.create_vertical_dataset(
-        "csv:" + csv_file.full_path, min_vocab_frequency=1
-    )
-    df_pd = pd.read_csv(csv_file)
-    ds_pd = dataset.create_vertical_dataset(df_pd, data_spec=ds.data_spec())
-    self.assertEqual(ds._dataset.DebugString(), ds_pd._dataset.DebugString())
-
-  @unittest.skip("Requires building YDF with tensorflow io")
-  def test_read_from_sharded_tfe(self):
-    sharded_path = "tfrecord+tfe:" + os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "toy.tfe-tfrecord@2"
-    )
-    ds = dataset.create_vertical_dataset(
-        sharded_path,
-        min_vocab_frequency=1,
-        columns=["Bool_1", "Cat_2", "Num_1"],
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        columns=(
-            ds_pb.Column(
-                name="Bool_1",
-                type=ds_pb.BOOLEAN,
-                dtype=ds_pb.DType.DTYPE_INT64,
-                is_manual_type=False,
-                boolean=ds_pb.BooleanSpec(count_true=2, count_false=2),
-            ),
-            ds_pb.Column(
-                name="Cat_2",
-                type=ds_pb.CATEGORICAL,
-                dtype=ds_pb.DType.DTYPE_BYTES,
-                is_manual_type=False,
-                categorical=ds_pb.CategoricalSpec(
-                    number_of_unique_values=3,
-                    most_frequent_value=1,
-                    min_value_count=1,
-                    max_number_of_unique_values=2000,
-                    is_already_integerized=False,
-                    items={
-                        "<OOD>": VocabValue(index=0, count=0),
-                        "A": VocabValue(index=2, count=1),
-                        "B": VocabValue(index=1, count=1),
-                    },
-                ),
-                count_nas=2,
-            ),
-            ds_pb.Column(
-                name="Num_1",
-                type=ds_pb.NUMERICAL,
-                dtype=ds_pb.DType.DTYPE_FLOAT32,
-                is_manual_type=False,
-                numerical=ds_pb.NumericalSpec(
-                    mean=2.5,
-                    min_value=1.0,
-                    max_value=4.0,
-                    standard_deviation=1.118033988749895,
-                ),
-            ),
-        ),
-        created_num_rows=4,
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_read_from_sharded_tfe_nocompress(self):
-    sharded_path = "tfrecordv2+tfe:" + os.path.join(
-        test_utils.ydf_test_data_path(),
-        "dataset",
-        "toy.nocompress-tfe-tfrecord@2",
-    )
-    ds = dataset.create_vertical_dataset(
-        sharded_path,
-        min_vocab_frequency=1,
-        columns=["Bool_1", "Cat_2", "Num_1"],
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        columns=(
-            ds_pb.Column(
-                name="Bool_1",
-                type=ds_pb.BOOLEAN,
-                dtype=ds_pb.DType.DTYPE_INT64,
-                is_manual_type=False,
-                boolean=ds_pb.BooleanSpec(count_true=2, count_false=2),
-            ),
-            ds_pb.Column(
-                name="Cat_2",
-                type=ds_pb.CATEGORICAL,
-                dtype=ds_pb.DType.DTYPE_BYTES,
-                is_manual_type=False,
-                categorical=ds_pb.CategoricalSpec(
-                    number_of_unique_values=3,
-                    most_frequent_value=1,
-                    min_value_count=1,
-                    max_number_of_unique_values=2000,
-                    is_already_integerized=False,
-                    items={
-                        "<OOD>": VocabValue(index=0, count=0),
-                        "A": VocabValue(index=2, count=1),
-                        "B": VocabValue(index=1, count=1),
-                    },
-                ),
-                count_nas=2,
-            ),
-            ds_pb.Column(
-                name="Num_1",
-                type=ds_pb.NUMERICAL,
-                dtype=ds_pb.DType.DTYPE_FLOAT32,
-                is_manual_type=False,
-                numerical=ds_pb.NumericalSpec(
-                    mean=2.5,
-                    min_value=1.0,
-                    max_value=4.0,
-                    standard_deviation=1.118033988749895,
-                ),
-            ),
-        ),
-        created_num_rows=4,
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_multidimensional_input(self):
-    ds = dataset.create_vertical_dataset(
-        {"feature": np.array([[0, 1, 2], [4, 5, 6]])}
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=2,
-        columns=(
-            ds_pb.Column(
-                name="feature.0_of_3",
-                type=ds_pb.ColumnType.NUMERICAL,
-                dtype=ds_pb.DType.DTYPE_INT64,
-                count_nas=0,
-                numerical=ds_pb.NumericalSpec(
-                    mean=2,
-                    standard_deviation=2,
-                    min_value=0,
-                    max_value=4,
-                ),
-                is_unstacked=True,
-            ),
-            ds_pb.Column(
-                name="feature.1_of_3",
-                type=ds_pb.ColumnType.NUMERICAL,
-                dtype=ds_pb.DType.DTYPE_INT64,
-                count_nas=0,
-                numerical=ds_pb.NumericalSpec(
-                    mean=3,
-                    standard_deviation=2,
-                    min_value=1,
-                    max_value=5,
-                ),
-                is_unstacked=True,
-            ),
-            ds_pb.Column(
-                name="feature.2_of_3",
-                type=ds_pb.ColumnType.NUMERICAL,
-                dtype=ds_pb.DType.DTYPE_INT64,
-                count_nas=0,
-                numerical=ds_pb.NumericalSpec(
-                    mean=4,
-                    standard_deviation=2,
-                    min_value=2,
-                    max_value=6,
-                ),
-                is_unstacked=True,
-            ),
-        ),
-        unstackeds=(
-            ds_pb.Unstacked(
-                original_name="feature",
-                begin_column_idx=0,
-                size=3,
-                type=ds_pb.ColumnType.NUMERICAL,
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  @parameterized.parameters(
-      (1, "feature.0_of_1"),
-      (9, "feature.0_of_9"),
-      (10, "feature.00_of_10"),
-      (9999, "feature.0000_of_9999"),
-      (10000, "feature.00000_of_10000"),
-  )
-  def test_multidimensional_feature_name(
-      self, num_dims: int, expected_first_feature: str
-  ):
-    ds = dataset.create_vertical_dataset({"feature": np.zeros((3, num_dims))})
-    self.assertLen(ds.data_spec().columns, num_dims)
-    self.assertEqual(ds.data_spec().columns[0].name, expected_first_feature)
-
-  def test_multi_dimensions_error_too_many_dims(self):
-    with self.assertRaisesRegex(
-        ValueError, "Input features can only be one or two dimensional"
-    ):
-      _ = dataset.create_vertical_dataset({"feature": np.zeros((3, 3, 3))})
-
-  def test_list_of_csv_datasets(self):
-    df = pd.DataFrame({
-        "col_str": ["A", "string", "column with", "four entries"],
-        "col_int": [5, 6, 7, 8],
-        "col_int_cat": [1, 2, 3, 4],
-        "col_float": [1.1, 2.2, 3.3, 4.4],
-    })
-    feature_definitions = [
-        Column("col_str", Semantic.CATEGORICAL, min_vocab_frequency=1),
-        Column("col_int_cat", Semantic.CATEGORICAL, min_vocab_frequency=1),
-    ]
-    dataset_directory = self.create_tempdir()
-    path1 = os.path.join(dataset_directory.full_path, "ds1")
-    path2 = os.path.join(dataset_directory.full_path, "ds2")
-    df.head(3).to_csv(path1, index=False)
-    df.tail(1).to_csv(path2, index=False)
-
-    ds = dataset.create_vertical_dataset(
-        ["csv:" + path1, "csv:" + path2],
-        columns=feature_definitions,
-        include_all_columns=True,
-    )
-
-    expected_dataset_content = """\
-col_str,col_int,col_int_cat,col_float
-A,5,1,1.1
-string,6,2,2.2
-column with,7,3,3.3
-four entries,8,4,4.4
-"""
-    self.assertEqual(expected_dataset_content, ds._dataset.DebugString())
-
-  def test_singledimensional_strided_float32(self):
-    data = np.array([[0, 1], [4, 5]], np.float32)
-    feature = data[:, 0]  # "feature" shares the same memory as "data".
-    self.assertEqual(data.strides, (8, 4))
-    self.assertEqual(feature.strides, (8,))
-
-    ds = dataset.create_vertical_dataset({"feature": feature})
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=2,
-        columns=(
-            ds_pb.Column(
-                name="feature",
-                type=ds_pb.ColumnType.NUMERICAL,
-                dtype=ds_pb.DType.DTYPE_FLOAT32,
-                count_nas=0,
-                numerical=ds_pb.NumericalSpec(
-                    mean=2,
-                    standard_deviation=2,
-                    min_value=0,
-                    max_value=4,
-                ),
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-    self.assertEqual("feature\n0\n4\n", ds._dataset.DebugString())
-
-  def test_singledimensional_strided_boolean(self):
-    data = np.array([[True, False], [False, True]])
-    feature = data[:, 0]  # "feature" shares the same memory as "data".
-    self.assertEqual(data.strides, (2, 1))
-    self.assertEqual(feature.strides, (2,))
-
-    ds = dataset.create_vertical_dataset({"feature": feature})
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=2,
-        columns=(
-            ds_pb.Column(
-                name="feature",
-                type=ds_pb.ColumnType.BOOLEAN,
-                dtype=ds_pb.DType.DTYPE_BOOL,
-                count_nas=0,
-                boolean=ds_pb.BooleanSpec(
-                    count_true=1,
-                    count_false=1,
-                ),
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-    self.assertEqual("feature\n1\n0\n", ds._dataset.DebugString())
-
-  def test_multidimensional_strided(self):
-    # Note: multidimensional features are unrolled into singledimensional
-    # strided features.
-    ds = dataset.create_vertical_dataset(
-        {"feature": np.array([[0, 1, 2], [4, 5, 6]], np.float32)}
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=2,
-        columns=(
-            ds_pb.Column(
-                name="feature.0_of_3",
-                type=ds_pb.ColumnType.NUMERICAL,
-                dtype=ds_pb.DType.DTYPE_FLOAT32,
-                count_nas=0,
-                numerical=ds_pb.NumericalSpec(
-                    mean=2,
-                    standard_deviation=2,
-                    min_value=0,
-                    max_value=4,
-                ),
-                is_unstacked=True,
-            ),
-            ds_pb.Column(
-                name="feature.1_of_3",
-                type=ds_pb.ColumnType.NUMERICAL,
-                dtype=ds_pb.DType.DTYPE_FLOAT32,
-                count_nas=0,
-                numerical=ds_pb.NumericalSpec(
-                    mean=3,
-                    standard_deviation=2,
-                    min_value=1,
-                    max_value=5,
-                ),
-                is_unstacked=True,
-            ),
-            ds_pb.Column(
-                name="feature.2_of_3",
-                type=ds_pb.ColumnType.NUMERICAL,
-                dtype=ds_pb.DType.DTYPE_FLOAT32,
-                count_nas=0,
-                numerical=ds_pb.NumericalSpec(
-                    mean=4,
-                    standard_deviation=2,
-                    min_value=2,
-                    max_value=6,
-                ),
-                is_unstacked=True,
-            ),
-        ),
-        unstackeds=(
-            ds_pb.Unstacked(
-                original_name="feature",
-                begin_column_idx=0,
-                size=3,
-                type=ds_pb.ColumnType.NUMERICAL,
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-    expected_dataset_content = """\
-feature.0_of_3,feature.1_of_3,feature.2_of_3
-0,1,2
-4,5,6
-"""
-    self.assertEqual(expected_dataset_content, ds._dataset.DebugString())
-
-
-class CategoricalSetTest(absltest.TestCase):
-
-  def create_toy_csv(self) -> str:
-    """Returns the path to a small csv file with sentences."""
-    tmp_dir = self.create_tempdir()
-    csv_file = self.create_tempfile(
-        content="""\
-col_cat_set
-first sentence foo bar foo bar
-second sentence foo bar foo foo foo""",
-        file_path=os.path.join(tmp_dir.full_path, "file.csv"),
-    )
-    return csv_file.full_path
-
-  def toy_csv_dataspec_categorical(self) -> ds_pb.DataSpecification:
-    """Returns a dataspec for the toy CSV example with a categorical feature."""
-    return ds_pb.DataSpecification(
-        created_num_rows=2,
-        columns=(
-            ds_pb.Column(
-                name="col_cat_set",
-                type=ds_pb.ColumnType.CATEGORICAL,
-                is_manual_type=False,
-                categorical=ds_pb.CategoricalSpec(
-                    items={
-                        "<OOD>": VocabValue(index=0, count=0),
-                        "first sentence foo bar foo bar": VocabValue(
-                            index=2, count=1
-                        ),
-                        "second sentence foo bar foo foo foo": VocabValue(
-                            index=1, count=1
-                        ),
-                    },
-                    number_of_unique_values=3,
-                    most_frequent_value=1,
-                    min_value_count=1,
-                    max_number_of_unique_values=2000,
-                    is_already_integerized=False,
-                ),
-            ),
-        ),
-    )
-
-  def toy_csv_dataspec_catset(self) -> ds_pb.DataSpecification:
-    """Returns a dataspec for the toy CSV example with a catset feature."""
-    return ds_pb.DataSpecification(
-        created_num_rows=2,
-        columns=(
-            ds_pb.Column(
-                name="col_cat_set",
-                type=ds_pb.ColumnType.CATEGORICAL_SET,
-                is_manual_type=True,
-                categorical=ds_pb.CategoricalSpec(
-                    items={
-                        "<OOD>": VocabValue(index=0, count=0),
-                        "foo": VocabValue(index=1, count=6),
-                        "bar": VocabValue(index=2, count=3),
-                        "sentence": VocabValue(index=3, count=2),
-                        "second": VocabValue(index=4, count=1),
-                        "first": VocabValue(index=5, count=1),
-                    },
-                    number_of_unique_values=6,
-                    most_frequent_value=1,
-                    min_value_count=1,
-                    max_number_of_unique_values=2000,
-                    is_already_integerized=False,
-                ),
-            ),
-        ),
-    )
-
-  def test_csv_file_no_automatic_tokenization(self):
-    path_to_csv = self.create_toy_csv()
-    ds = dataset.create_vertical_dataset(
-        "csv:" + path_to_csv, min_vocab_frequency=1
-    )
-    expected_data_spec = self.toy_csv_dataspec_categorical()
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_csv_tokenization_when_semantic_specified(self):
-    path_to_csv = self.create_toy_csv()
-    ds = dataset.create_vertical_dataset(
-        "csv:" + path_to_csv,
-        min_vocab_frequency=1,
-        columns=[("col_cat_set", Semantic.CATEGORICAL_SET)],
-    )
-    expected_data_spec = self.toy_csv_dataspec_catset()
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_csv_file_reading_respects_data_spec_categorical(self):
-    path_to_csv = self.create_toy_csv()
-    data_spec = self.toy_csv_dataspec_categorical()
-    ds = dataset.create_vertical_dataset(
-        "csv:" + path_to_csv, data_spec=data_spec
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), data_spec)
-    self.assertEqual(
-        ds._dataset.DebugString(),
-        """\
-col_cat_set
-first sentence foo bar foo bar
-second sentence foo bar foo foo foo
-""",
-    )
-
-  def test_csv_file_reading_respects_data_spec_catset(self):
-    path_to_csv = self.create_toy_csv()
-    data_spec = self.toy_csv_dataspec_catset()
-    ds = dataset.create_vertical_dataset(
-        "csv:" + path_to_csv, data_spec=data_spec
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), data_spec)
-    self.assertEqual(
-        ds._dataset.DebugString(),
-        """\
-col_cat_set
-foo, bar, sentence, first
-foo, bar, sentence, second
-""",
-    )
-
-  def test_pd_list_of_list(self):
-    df = pd.DataFrame({
-        "feature": [
-            ["single item"],
-            ["two", "words"],
-            ["three", "simple", "words", "words"],
-            [""],
-        ]
-    })
-    ds = dataset.create_vertical_dataset(
-        df,
-        min_vocab_frequency=1,
-        columns=[("feature", Semantic.CATEGORICAL_SET)],
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=4,
-        columns=(
-            ds_pb.Column(
-                name="feature",
-                type=ds_pb.ColumnType.CATEGORICAL_SET,
-                dtype=ds_pb.DType.DTYPE_BYTES,
-                categorical=ds_pb.CategoricalSpec(
-                    items={
-                        "<OOD>": VocabValue(index=0, count=0),
-                        "words": VocabValue(index=1, count=2),
-                        "simple": VocabValue(index=2, count=1),
-                        "single item": VocabValue(index=3, count=1),
-                        "three": VocabValue(index=4, count=1),
-                        "two": VocabValue(index=5, count=1),
-                    },
-                    number_of_unique_values=6,
-                ),
-                count_nas=1,
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_pd_invalid_type_text(self):
-    df = pd.DataFrame({"feature": ["a", "b c", "d e f g h"]})
-    with self.assertRaisesRegex(
-        ValueError, "Categorical Set columns must be a list of lists."
-    ):
-      _ = dataset.create_vertical_dataset(
-          df,
-          min_vocab_frequency=1,
-          columns=[("feature", Semantic.CATEGORICAL_SET)],
-      )
-
-  def test_pd_np_bytes(self):
-    df = pd.DataFrame({
-        "feature": [
-            np.array(["single item"], np.bytes_),
-            np.array(["two", "words"], np.bytes_),
-            np.array(["three", "simple", "words", "words"], np.bytes_),
-            np.array([""], np.bytes_),
-        ]
-    })
-    ds = dataset.create_vertical_dataset(
-        df,
-        min_vocab_frequency=1,
-        columns=[("feature", Semantic.CATEGORICAL_SET)],
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=4,
-        columns=(
-            ds_pb.Column(
-                name="feature",
-                type=ds_pb.ColumnType.CATEGORICAL_SET,
-                dtype=ds_pb.DType.DTYPE_BYTES,
-                categorical=ds_pb.CategoricalSpec(
-                    items={
-                        "<OOD>": VocabValue(index=0, count=0),
-                        "words": VocabValue(index=1, count=2),
-                        "simple": VocabValue(index=2, count=1),
-                        "single item": VocabValue(index=3, count=1),
-                        "three": VocabValue(index=4, count=1),
-                        "two": VocabValue(index=5, count=1),
-                    },
-                    number_of_unique_values=6,
-                ),
-                count_nas=1,
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_pd_with_na(self):
-    df = pd.DataFrame({
-        "feature": [
-            pd.NA,
-            ["single item"],
-            ["two", "words"],
-            ["three", "simple", "words", "words"],
-        ]
-    })
-    ds = dataset.create_vertical_dataset(
-        df,
-        min_vocab_frequency=1,
-        columns=[("feature", Semantic.CATEGORICAL_SET)],
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=4,
-        columns=(
-            ds_pb.Column(
-                name="feature",
-                type=ds_pb.ColumnType.CATEGORICAL_SET,
-                dtype=ds_pb.DType.DTYPE_BYTES,
-                categorical=ds_pb.CategoricalSpec(
-                    items={
-                        "<OOD>": VocabValue(index=0, count=0),
-                        "words": VocabValue(index=1, count=2),
-                        "simple": VocabValue(index=2, count=1),
-                        "single item": VocabValue(index=3, count=1),
-                        "three": VocabValue(index=4, count=1),
-                        "two": VocabValue(index=5, count=1),
-                    },
-                    number_of_unique_values=6,
-                ),
-                count_nas=1,
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_pd_with_empty_list(self):
-    df = pd.DataFrame({
-        "feature": [
-            [],
-            ["single item"],
-            ["two", "words"],
-            ["three", "simple", "words", "words"],
-        ]
-    })
-    ds = dataset.create_vertical_dataset(
-        df,
-        min_vocab_frequency=1,
-        columns=[("feature", Semantic.CATEGORICAL_SET)],
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=4,
-        columns=(
-            ds_pb.Column(
-                name="feature",
-                type=ds_pb.ColumnType.CATEGORICAL_SET,
-                dtype=ds_pb.DType.DTYPE_BYTES,
-                categorical=ds_pb.CategoricalSpec(
-                    items={
-                        "<OOD>": VocabValue(index=0, count=0),
-                        "words": VocabValue(index=1, count=2),
-                        "simple": VocabValue(index=2, count=1),
-                        "single item": VocabValue(index=3, count=1),
-                        "three": VocabValue(index=4, count=1),
-                        "two": VocabValue(index=5, count=1),
-                    },
-                    number_of_unique_values=6,
-                ),
-                count_nas=0,
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_pd_type_inference_lists(self):
-    df = pd.DataFrame(
-        {
-            "feature": [
-                ["single item"],
-                ["two", "words"],
-            ]
-        }
-    )
-    ds = dataset.create_vertical_dataset(
-        df,
-        min_vocab_frequency=1,
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=2,
-        columns=(
-            ds_pb.Column(
-                name="feature",
-                type=ds_pb.ColumnType.CATEGORICAL_SET,
-                dtype=ds_pb.DType.DTYPE_BYTES,
-                categorical=ds_pb.CategoricalSpec(
-                    items={
-                        "<OOD>": VocabValue(index=0, count=0),
-                        "single item": VocabValue(index=1, count=1),
-                        "two": VocabValue(index=2, count=1),
-                        "words": VocabValue(index=3, count=1),
-                    },
-                    number_of_unique_values=4,
-                ),
-                count_nas=0,
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-  def test_pd_type_inference_nparrays(self):
-    df = pd.DataFrame(
-        {
-            "feature": [
-                np.array(["single item"]),
-                np.array(["two", "words"]),
-            ]
-        }
-    )
-    ds = dataset.create_vertical_dataset(
-        df,
-        min_vocab_frequency=1,
-    )
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=2,
-        columns=(
-            ds_pb.Column(
-                name="feature",
-                type=ds_pb.ColumnType.CATEGORICAL_SET,
-                dtype=ds_pb.DType.DTYPE_BYTES,
-                categorical=ds_pb.CategoricalSpec(
-                    items={
-                        "<OOD>": VocabValue(index=0, count=0),
-                        "single item": VocabValue(index=1, count=1),
-                        "two": VocabValue(index=2, count=1),
-                        "words": VocabValue(index=3, count=1),
-                    },
-                    number_of_unique_values=4,
-                ),
-                count_nas=0,
-            ),
-        ),
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
-
-
-@parameterized.parameters(
-    ds_pb.ColumnType.NUMERICAL, ds_pb.ColumnType.CATEGORICAL
-)
-class MissingColumnsTest(parameterized.TestCase):
-
-  def create_data_spec(self, column_type: ds_pb.ColumnType):
-    if column_type == ds_pb.ColumnType.NUMERICAL:
-      return ds_pb.DataSpecification(
-          created_num_rows=3,
-          columns=(
-              ds_pb.Column(name="f1", type=ds_pb.ColumnType.NUMERICAL),
-              ds_pb.Column(name="f2", type=ds_pb.ColumnType.NUMERICAL),
-          ),
-      )
-    if column_type == ds_pb.ColumnType.CATEGORICAL:
-      return ds_pb.DataSpecification(
-          created_num_rows=3,
-          columns=(
-              ds_pb.Column(
-                  name="f1",
-                  type=ds_pb.ColumnType.CATEGORICAL,
-                  dtype=ds_pb.DType.DTYPE_BYTES,
-                  categorical=ds_pb.CategoricalSpec(
-                      items={
-                          "<OOD>": VocabValue(index=0, count=0),
-                          "1": VocabValue(index=1, count=1),
-                          "2": VocabValue(index=2, count=1),
-                          "3": VocabValue(index=3, count=1),
-                      },
-                      number_of_unique_values=4,
-                      min_value_count=1,
-                  ),
-              ),
-              ds_pb.Column(
-                  name="f2",
-                  type=ds_pb.ColumnType.CATEGORICAL,
-                  dtype=ds_pb.DType.DTYPE_BYTES,
-                  categorical=ds_pb.CategoricalSpec(
-                      items={
-                          "<OOD>": VocabValue(index=0, count=0),
-                          "1": VocabValue(index=1, count=1),
-                          "2": VocabValue(index=2, count=1),
-                          "3": VocabValue(index=3, count=1),
-                      },
-                      number_of_unique_values=4,
-                      min_value_count=1,
-                  ),
-              ),
-          ),
-      )
-    raise NotImplementedError(f"Errors for type {column_type} not implemented")
-
-  def get_inferred_dataspec_pd_f1only(self, column_type: ds_pb.ColumnType):
-    if column_type == ds_pb.ColumnType.NUMERICAL:
-      return ds_pb.DataSpecification(
-          created_num_rows=3,
-          columns=(
-              ds_pb.Column(
-                  name="f1",
-                  type=ds_pb.ColumnType.NUMERICAL,
-                  dtype=ds_pb.DType.DTYPE_INT64,
-                  numerical=ds_pb.NumericalSpec(
-                      mean=2.0,
-                      min_value=1.0,
-                      max_value=3.0,
-                      standard_deviation=0.8164965809277263,
-                  ),
-                  count_nas=0,
-              ),
-          ),
-      )
-    if column_type == ds_pb.ColumnType.CATEGORICAL:
-      return ds_pb.DataSpecification(
-          created_num_rows=3,
-          columns=(
-              ds_pb.Column(
-                  name="f1",
-                  type=ds_pb.ColumnType.CATEGORICAL,
-                  dtype=ds_pb.DType.DTYPE_INT64,
-                  categorical=ds_pb.CategoricalSpec(
-                      items={
-                          "<OOD>": VocabValue(index=0, count=0),
-                          "1": VocabValue(index=1, count=1),
-                          "2": VocabValue(index=2, count=1),
-                          "3": VocabValue(index=3, count=1),
-                      },
-                      number_of_unique_values=4,
-                  ),
-                  count_nas=0,
-              ),
-          ),
-      )
-    raise NotImplementedError(f"Errors for type {column_type} not implemented")
-
-  def get_inferred_dataspec_file_f1only(self, column_type: ds_pb.ColumnType):
-    if column_type == ds_pb.ColumnType.NUMERICAL:
-      return ds_pb.DataSpecification(
-          created_num_rows=3,
-          columns=(
-              ds_pb.Column(
-                  name="f1",
-                  is_manual_type=True,
-                  type=ds_pb.ColumnType.NUMERICAL,
-                  numerical=ds_pb.NumericalSpec(
-                      mean=2.0,
-                      min_value=1.0,
-                      max_value=3.0,
-                      standard_deviation=0.8164965809277263,
-                  ),
-              ),
-          ),
-      )
-    if column_type == ds_pb.ColumnType.CATEGORICAL:
-      return ds_pb.DataSpecification(
-          created_num_rows=3,
-          columns=(
-              ds_pb.Column(
-                  name="f1",
-                  is_manual_type=True,
-                  type=ds_pb.ColumnType.CATEGORICAL,
-                  categorical=ds_pb.CategoricalSpec(
-                      items={
-                          "<OOD>": VocabValue(index=0, count=0),
-                          "1": VocabValue(index=3, count=1),
-                          "2": VocabValue(index=2, count=1),
-                          "3": VocabValue(index=1, count=1),
-                      },
-                      number_of_unique_values=4,
-                      most_frequent_value=1,
-                      min_value_count=1,
-                      is_already_integerized=False,
-                      max_number_of_unique_values=2000,
-                  ),
-              ),
-          ),
-      )
-    raise NotImplementedError(f"Errors for type {column_type} not implemented")
-
-  def create_csv(self) -> str:
-    tmp_dir = self.create_tempdir()
-    csv_file = self.create_tempfile(
-        content="""f1
-1
-2
-3""",
-        file_path=os.path.join(tmp_dir.full_path, "file.csv"),
-    )
-    return csv_file.full_path
-
-  def get_debug_string(self, column_type: ds_pb.ColumnType):
-    if column_type == ds_pb.ColumnType.NUMERICAL:
-      return "f1,f2\n1,nan\n2,nan\n3,nan\n"
-    if column_type == ds_pb.ColumnType.CATEGORICAL:
-      return "f1,f2\n1,NA\n2,NA\n3,NA\n"
-
-    raise NotImplementedError(f"Errors for type {column_type} not implemented")
-
-  def test_required_columns_pd_data_spec_none(
-      self, column_type: ds_pb.ColumnType
-  ):
-    data_spec = self.create_data_spec(column_type)
-    df = pd.DataFrame({"f1": [1, 2, 3]})
-    with self.assertRaises(ValueError):
-      _ = dataset.create_vertical_dataset(df, data_spec=data_spec)
-
-  def test_required_columns_pd_data_spec_empty(
-      self, column_type: ds_pb.ColumnType
-  ):
-    data_spec = self.create_data_spec(column_type)
-    df = pd.DataFrame({"f1": [1, 2, 3]})
-    ds = dataset.create_vertical_dataset(
-        df, data_spec=data_spec, required_columns=[]
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), data_spec)
-    self.assertEqual(
-        ds._dataset.DebugString(), self.get_debug_string(column_type)
-    )
-
-  def test_required_columns_pd_data_spec_explicit_success(
-      self, column_type: ds_pb.ColumnType
-  ):
-    data_spec = self.create_data_spec(column_type)
-    df = pd.DataFrame({"f1": [1, 2, 3]})
-    ds = dataset.create_vertical_dataset(
-        df, data_spec=data_spec, required_columns=["f1"]
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), data_spec)
-    self.assertEqual(
-        ds._dataset.DebugString(), self.get_debug_string(column_type)
-    )
-
-  def test_required_columns_pd_data_spec_explicit_failure(
-      self, column_type: ds_pb.ColumnType
-  ):
-    data_spec = self.create_data_spec(column_type)
-    df = pd.DataFrame({"f1": [1, 2, 3]})
-    with self.assertRaises(ValueError):
-      _ = dataset.create_vertical_dataset(
-          df, data_spec=data_spec, required_columns=["f2"]
-      )
-
-  def test_required_columns_pd_inference_args_none(
-      self, column_type: ds_pb.ColumnType
-  ):
-    df = pd.DataFrame({"f1": [1, 2, 3]})
-    column_semantic = dataspec.Semantic.from_proto_type(column_type)
-    with self.assertRaises(ValueError):
-      _ = dataset.create_vertical_dataset(
-          df, columns=[("f1", column_semantic), ("f2", column_semantic)]
-      )
-
-  def test_required_columns_pd_inference_args_empty(
-      self, column_type: ds_pb.ColumnType
-  ):
-    df = pd.DataFrame({"f1": [1, 2, 3]})
-    column_semantic = dataspec.Semantic.from_proto_type(column_type)
-    ds = dataset.create_vertical_dataset(
-        df,
-        columns=[("f1", column_semantic), ("f2", column_semantic)],
-        required_columns=[],
-        min_vocab_frequency=1,
-    )
-    # Note that the dataspec does not contain column f2 since it was not found
-    # in the data.
-    test_utils.assertProto2Equal(
-        self,
-        ds._dataset.data_spec(),
-        self.get_inferred_dataspec_pd_f1only(column_type),
-    )
-    self.assertEqual(ds._dataset.DebugString(), "f1\n1\n2\n3\n")
-
-  def test_required_columns_pd_inference_args_explicit_failure(
-      self, column_type: ds_pb.ColumnType
-  ):
-    df = pd.DataFrame({"f1": [1, 2, 3]})
-    column_semantic = dataspec.Semantic.from_proto_type(column_type)
-    with self.assertRaises(ValueError):
-      _ = dataset.create_vertical_dataset(df, columns=[("f2", column_semantic)])
-
-  def test_required_columns_pd_inference_args_explicit_success(
-      self, column_type: ds_pb.ColumnType
-  ):
-    df = pd.DataFrame({"f1": [1, 2, 3]})
-    column_semantic = dataspec.Semantic.from_proto_type(column_type)
-    ds = dataset.create_vertical_dataset(
-        df,
-        columns=[("f1", column_semantic), ("f2", column_semantic)],
-        min_vocab_frequency=1,
-        required_columns=["f1"],
-    )
-    # Note that the dataspec does not contain column f2 since it was not found
-    # in the data.
-    test_utils.assertProto2Equal(
-        self,
-        ds._dataset.data_spec(),
-        self.get_inferred_dataspec_pd_f1only(column_type),
-    )
-    self.assertEqual(ds._dataset.DebugString(), "f1\n1\n2\n3\n")
-
-  def test_required_columns_file_data_spec_none(
-      self, column_type: ds_pb.ColumnType
-  ):
-    data_spec = self.create_data_spec(column_type)
-    file_path = self.create_csv()
-    with self.assertRaises(ValueError):
-      _ = dataset.create_vertical_dataset(file_path, data_spec=data_spec)
-
-  def test_required_columns_file_data_spec_empty(
-      self, column_type: ds_pb.ColumnType
-  ):
-    data_spec = self.create_data_spec(column_type)
-    file_path = self.create_csv()
-    ds = dataset.create_vertical_dataset(
-        file_path, data_spec=data_spec, required_columns=[]
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), data_spec)
-    self.assertEqual(
-        ds._dataset.DebugString(), self.get_debug_string(column_type)
-    )
-
-  def test_required_columns_file_data_spec_explicit_success(
-      self, column_type: ds_pb.ColumnType
-  ):
-    data_spec = self.create_data_spec(column_type)
-    file_path = self.create_csv()
-    ds = dataset.create_vertical_dataset(
-        file_path, data_spec=data_spec, required_columns=["f1"]
-    )
-    test_utils.assertProto2Equal(self, ds.data_spec(), data_spec)
-    self.assertEqual(
-        ds._dataset.DebugString(), self.get_debug_string(column_type)
-    )
-
-  def test_required_columns_file_data_spec_explicit_failure(
-      self, column_type: ds_pb.ColumnType
-  ):
-    data_spec = self.create_data_spec(column_type)
-    file_path = self.create_csv()
-    with self.assertRaises(ValueError):
-      _ = dataset.create_vertical_dataset(
-          file_path, data_spec=data_spec, required_columns=["f2"]
-      )
-
-  def test_required_columns_file_inference_args_none(
-      self, column_type: ds_pb.ColumnType
-  ):
-    file_path = self.create_csv()
-    column_semantic = dataspec.Semantic.from_proto_type(column_type)
-    with self.assertRaises(ValueError):
-      _ = dataset.create_vertical_dataset(
-          file_path, columns=[("f1", column_semantic), ("f2", column_semantic)]
-      )
-
-  def test_required_columns_file_inference_args_empty(
-      self, column_type: ds_pb.ColumnType
-  ):
-    file_path = self.create_csv()
-    column_semantic = dataspec.Semantic.from_proto_type(column_type)
-    ds = dataset.create_vertical_dataset(
-        file_path,
-        columns=[("f1", column_semantic), ("f2", column_semantic)],
-        required_columns=[],
-        min_vocab_frequency=1,
-    )
-    # Note that the dataspec does not contain column f2 since it was not found
-    # in the data.
-    test_utils.assertProto2Equal(
-        self,
-        ds._dataset.data_spec(),
-        self.get_inferred_dataspec_file_f1only(column_type),
-    )
-    self.assertEqual(ds._dataset.DebugString(), "f1\n1\n2\n3\n")
-
-  def test_required_columns_file_inference_args_explicit_failure(
-      self, column_type: ds_pb.ColumnType
-  ):
-    file_path = self.create_csv()
-    column_semantic = dataspec.Semantic.from_proto_type(column_type)
-    with self.assertRaises(ValueError):
-      _ = dataset.create_vertical_dataset(
-          file_path, columns=[("f2", column_semantic)]
-      )
-
-  def test_required_columns_file_inference_args_explicit_success(
-      self, column_type: ds_pb.ColumnType
-  ):
-    file_path = self.create_csv()
-    column_semantic = dataspec.Semantic.from_proto_type(column_type)
-    ds = dataset.create_vertical_dataset(
-        file_path,
-        columns=[("f1", column_semantic), ("f2", column_semantic)],
-        required_columns=["f1"],
-        min_vocab_frequency=1,
-    )
-    # Note that the dataspec does not contain column f2 since it was not found
-    # in the data.
-    test_utils.assertProto2Equal(
-        self,
-        ds._dataset.data_spec(),
-        self.get_inferred_dataspec_file_f1only(column_type),
-    )
-    self.assertEqual(ds._dataset.DebugString(), "f1\n1\n2\n3\n")
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import os
+import unittest
+
+from absl.testing import absltest
+from absl.testing import parameterized
+import numpy as np
+import pandas as pd
+
+from ydf.proto.dataset import data_spec_pb2 as ds_pb
+from ydf.dataset import dataset
+from ydf.dataset import dataspec
+from ydf.utils import test_utils
+
+# Make "assertEqual" print more details.
+unittest.util._MAX_LENGTH = 10000
+
+Semantic = dataspec.Semantic
+VocabValue = ds_pb.CategoricalSpec.VocabValue
+Column = dataspec.Column
+
+
+class GenericDatasetTest(parameterized.TestCase):
+
+  @parameterized.parameters(
+      (np.array([1], np.int8), Semantic.NUMERICAL),
+      (np.array([1], np.int16), Semantic.NUMERICAL),
+      (np.array([1], np.int32), Semantic.NUMERICAL),
+      (np.array([1], np.int64), Semantic.NUMERICAL),
+      (np.array([1], np.uint8), Semantic.NUMERICAL),
+      (np.array([1], np.uint16), Semantic.NUMERICAL),
+      (np.array([1], np.uint32), Semantic.NUMERICAL),
+      (np.array([1], np.uint64), Semantic.NUMERICAL),
+      (np.array([1], np.float32), Semantic.NUMERICAL),
+      (np.array([1], np.float64), Semantic.NUMERICAL),
+      (np.array([1], np.bool_), Semantic.BOOLEAN),
+      (np.array(["a"], np.bytes_), Semantic.CATEGORICAL),
+      (np.array(["a"], np.string_), Semantic.CATEGORICAL),
+      (np.array(["a", np.nan], np.object_), Semantic.CATEGORICAL),
+  )
+  def test_infer_semantic(self, value, expected_semantic):
+    self.assertEqual(dataset.infer_semantic("", value), expected_semantic)
+
+  @parameterized.parameters(
+      np.float16,
+      np.float32,
+      np.float64,
+      np.int8,
+      np.int16,
+      np.int32,
+      np.int64,
+  )
+  def test_create_vds_pd_numerical(self, dtype):
+    df = pd.DataFrame(
+        {
+            "col_pos": [1, 2, 3],
+            "col_neg": [-1, -2, -3],
+            "col_zero": [0, 0, 0],
+        },
+        dtype=dtype,
+    )
+    ds = dataset.create_vertical_dataset(df)
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=3,
+        columns=(
+            ds_pb.Column(
+                name="col_pos",
+                type=ds_pb.ColumnType.NUMERICAL,
+                dtype=dataspec.np_dtype_to_ydf_dtype(dtype),
+                count_nas=0,
+                numerical=ds_pb.NumericalSpec(
+                    mean=2,
+                    standard_deviation=0.8164965809277263,  # ~math.sqrt(2 / 3)
+                    min_value=1,
+                    max_value=3,
+                ),
+            ),
+            ds_pb.Column(
+                name="col_neg",
+                type=ds_pb.ColumnType.NUMERICAL,
+                dtype=dataspec.np_dtype_to_ydf_dtype(dtype),
+                count_nas=0,
+                numerical=ds_pb.NumericalSpec(
+                    mean=-2,
+                    standard_deviation=0.8164965809277263,  # ~math.sqrt(2 / 3)
+                    min_value=-3,
+                    max_value=-1,
+                ),
+            ),
+            ds_pb.Column(
+                name="col_zero",
+                type=ds_pb.ColumnType.NUMERICAL,
+                dtype=dataspec.np_dtype_to_ydf_dtype(dtype),
+                count_nas=0,
+                numerical=ds_pb.NumericalSpec(
+                    mean=0,
+                    standard_deviation=0,
+                    min_value=0,
+                    max_value=0,
+                ),
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  @parameterized.parameters(np.float16, np.float32, np.float64)
+  def test_create_vds_pd_numerical_with_nan(self, dtype):
+    df = pd.DataFrame(
+        {
+            "col_single_nan": [1, 2, np.NaN],
+            "col_nan_only": [np.NaN, np.NaN, np.NaN],
+        },
+        dtype=dtype,
+    )
+    ds = dataset.create_vertical_dataset(df)
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=3,
+        columns=(
+            ds_pb.Column(
+                name="col_single_nan",
+                type=ds_pb.ColumnType.NUMERICAL,
+                dtype=dataspec.np_dtype_to_ydf_dtype(dtype),
+                count_nas=1,
+                numerical=ds_pb.NumericalSpec(
+                    mean=1.5,
+                    standard_deviation=0.5,
+                    min_value=1,
+                    max_value=2,
+                ),
+            ),
+            ds_pb.Column(
+                name="col_nan_only",
+                type=ds_pb.ColumnType.NUMERICAL,
+                dtype=dataspec.np_dtype_to_ydf_dtype(dtype),
+                count_nas=3,
+                numerical=ds_pb.NumericalSpec(),
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_create_vds_pd_categorical_string(self):
+    df = pd.DataFrame({
+        "col1": ["A", "A", "B", "B", "C"],
+        "col2": ["", "A", "B", "C", "D"],
+    })
+
+    ds = dataset.create_vertical_dataset(
+        df,
+        min_vocab_frequency=2,
+        columns=[
+            Column(
+                "col2",
+                Semantic.CATEGORICAL,
+                min_vocab_frequency=1,
+                max_vocab_count=3,
+            )
+        ],
+        include_all_columns=True,
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=5,
+        columns=(
+            ds_pb.Column(
+                name="col2",
+                type=ds_pb.ColumnType.CATEGORICAL,
+                dtype=ds_pb.DType.DTYPE_BYTES,
+                count_nas=1,
+                categorical=ds_pb.CategoricalSpec(
+                    items={
+                        "<OOD>": VocabValue(index=0, count=1),
+                        "A": VocabValue(index=1, count=1),
+                        "B": VocabValue(index=2, count=1),
+                        "C": VocabValue(index=3, count=1),
+                    },
+                    number_of_unique_values=4,
+                ),
+            ),
+            ds_pb.Column(
+                name="col1",
+                type=ds_pb.ColumnType.CATEGORICAL,
+                dtype=ds_pb.DType.DTYPE_BYTES,
+                count_nas=0,
+                categorical=ds_pb.CategoricalSpec(
+                    items={
+                        "<OOD>": VocabValue(index=0, count=1),
+                        "A": VocabValue(index=1, count=2),
+                        "B": VocabValue(index=2, count=2),
+                    },
+                    number_of_unique_values=3,
+                ),
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_create_vds_pd_categorical_int(self):
+    df = pd.DataFrame({
+        "col1": [1, 1, 2, 2, 3],
+    })
+
+    ds = dataset.create_vertical_dataset(
+        df,
+        min_vocab_frequency=1,
+        columns=[
+            Column(
+                "col1",
+                Semantic.CATEGORICAL,
+            )
+        ],
+        include_all_columns=True,
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=5,
+        columns=(
+            ds_pb.Column(
+                name="col1",
+                type=ds_pb.ColumnType.CATEGORICAL,
+                dtype=ds_pb.DType.DTYPE_INT64,
+                count_nas=0,
+                categorical=ds_pb.CategoricalSpec(
+                    items={
+                        "<OOD>": VocabValue(index=0, count=0),
+                        "1": VocabValue(index=1, count=2),
+                        "2": VocabValue(index=2, count=2),
+                        "3": VocabValue(index=3, count=1),
+                    },
+                    number_of_unique_values=4,
+                ),
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_create_vds_pd_boolean(self):
+    df = pd.DataFrame(
+        {"col_boolean": [True, False, False]},
+    )
+
+    ds = dataset.create_vertical_dataset(df)
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=3,
+        columns=(
+            ds_pb.Column(
+                name="col_boolean",
+                type=ds_pb.ColumnType.BOOLEAN,
+                dtype=ds_pb.DType.DTYPE_BOOL,
+                count_nas=0,
+                boolean=ds_pb.BooleanSpec(
+                    count_true=1,
+                    count_false=2,
+                ),
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_create_vds_pd_hash(self):
+    df = pd.DataFrame(
+        {"col_hash": ["a", "b", "abc"]},
+    )
+
+    ds = dataset.create_vertical_dataset(
+        df, columns=[("col_hash", Semantic.HASH)]
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=3,
+        columns=(
+            ds_pb.Column(
+                name="col_hash",
+                type=ds_pb.ColumnType.HASH,
+                dtype=ds_pb.DType.DTYPE_BYTES,
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  @parameterized.parameters(
+      (["col_numerical"],),
+      ([Column("col_numerical")],),
+      ([("col_numerical", Semantic.NUMERICAL)],),
+  )
+  def test_create_vds_exclude_columns(self, column_definition):
+    df = pd.DataFrame({
+        "col_numerical": [1, 2, 3],
+        "col_str": ["A", "B", "C"],
+    })
+    ds = dataset.create_vertical_dataset(df, columns=column_definition)
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=3,
+        columns=(
+            ds_pb.Column(
+                name="col_numerical",
+                type=ds_pb.ColumnType.NUMERICAL,
+                dtype=ds_pb.DType.DTYPE_INT64,
+                count_nas=0,
+                numerical=ds_pb.NumericalSpec(
+                    mean=2,
+                    standard_deviation=0.8164965809277263,  # ~math.sqrt(2 / 3)
+                    min_value=1,
+                    max_value=3,
+                ),
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_create_vds_dict_of_values(self):
+    ds_dict = {
+        "a": np.array([1, 2, 3]),
+        "b": np.array([-1, -2, -3]),
+    }
+    ds = dataset.create_vertical_dataset(ds_dict)
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=3,
+        columns=(
+            ds_pb.Column(
+                name="a",
+                type=ds_pb.ColumnType.NUMERICAL,
+                dtype=ds_pb.DType.DTYPE_INT64,
+                count_nas=0,
+                numerical=ds_pb.NumericalSpec(
+                    mean=2,
+                    standard_deviation=0.8164965809277263,  # ~math.sqrt(2 / 3)
+                    min_value=1,
+                    max_value=3,
+                ),
+            ),
+            ds_pb.Column(
+                name="b",
+                type=ds_pb.ColumnType.NUMERICAL,
+                dtype=ds_pb.DType.DTYPE_INT64,
+                count_nas=0,
+                numerical=ds_pb.NumericalSpec(
+                    mean=-2,
+                    standard_deviation=0.8164965809277263,  # ~math.sqrt(2 / 3)
+                    min_value=-3,
+                    max_value=-1,
+                ),
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_memory_usage(self):
+    df = pd.DataFrame({
+        "col1": [1.1, 21.1, np.NaN],
+        "col2": [1, 2, 3],
+    })
+    ds = dataset.create_vertical_dataset(df)
+    # 2 columns * 3 rows * 4 bytes per values
+    self.assertEqual(ds.memory_usage(), 2 * 3 * 4)
+
+  def test_create_vds_pd_with_spec(self):
+    data_spec = ds_pb.DataSpecification(
+        created_num_rows=3,
+        columns=(
+            ds_pb.Column(
+                name="col_num",
+                type=ds_pb.ColumnType.NUMERICAL,
+                dtype=ds_pb.DType.DTYPE_INT64,
+                count_nas=0,
+                numerical=ds_pb.NumericalSpec(
+                    mean=2,
+                    standard_deviation=0.8164965809277263,  # ~math.sqrt(2 / 3)
+                    min_value=1,
+                    max_value=3,
+                ),
+            ),
+            ds_pb.Column(
+                name="col_cat",
+                type=ds_pb.ColumnType.CATEGORICAL,
+                dtype=ds_pb.DType.DTYPE_BYTES,
+                count_nas=0,
+                categorical=ds_pb.CategoricalSpec(
+                    items={
+                        "<OOD>": VocabValue(index=0, count=1),
+                        "A": VocabValue(index=1, count=2),
+                        "B": VocabValue(index=2, count=2),
+                    },
+                    number_of_unique_values=3,
+                ),
+            ),
+            ds_pb.Column(
+                name="col_bool",
+                type=ds_pb.ColumnType.BOOLEAN,
+                dtype=ds_pb.DType.DTYPE_BOOL,
+                count_nas=0,
+                boolean=ds_pb.BooleanSpec(
+                    count_true=55,
+                    count_false=123,
+                ),
+            ),
+        ),
+    )
+    df = pd.DataFrame({
+        "col_num": [1, 2, 3],
+        "col_cat": ["A", "B", "C"],
+        "col_bool": [True, True, False],
+    })
+    ds = dataset.create_vertical_dataset(df, data_spec=data_spec)
+    test_utils.assertProto2Equal(self, ds.data_spec(), data_spec)
+
+  def test_create_vds_dict_of_values_with_spec(self):
+    data_spec = ds_pb.DataSpecification(
+        created_num_rows=3,
+        columns=(
+            ds_pb.Column(
+                name="col_num",
+                type=ds_pb.ColumnType.NUMERICAL,
+                dtype=ds_pb.DType.DTYPE_INT64,
+                count_nas=0,
+                numerical=ds_pb.NumericalSpec(
+                    mean=2,
+                    standard_deviation=0.8164965809277263,  # ~math.sqrt(2 / 3)
+                    min_value=1,
+                    max_value=3,
+                ),
+            ),
+            ds_pb.Column(
+                name="col_cat",
+                type=ds_pb.ColumnType.CATEGORICAL,
+                dtype=ds_pb.DType.DTYPE_BYTES,
+                count_nas=0,
+                categorical=ds_pb.CategoricalSpec(
+                    items={
+                        "<OOD>": VocabValue(index=0, count=1),
+                        "A": VocabValue(index=1, count=2),
+                        "B": VocabValue(index=2, count=2),
+                    },
+                    number_of_unique_values=3,
+                ),
+            ),
+            ds_pb.Column(
+                name="col_bool",
+                type=ds_pb.ColumnType.BOOLEAN,
+                dtype=ds_pb.DType.DTYPE_BOOL,
+                count_nas=0,
+                boolean=ds_pb.BooleanSpec(
+                    count_true=55,
+                    count_false=123,
+                ),
+            ),
+        ),
+    )
+    ds_dict = {
+        "col_num": [1, 2, 3],
+        "col_cat": ["A", "B", "C"],
+        "col_bool": [True, False, True],
+    }
+    ds = dataset.create_vertical_dataset(ds_dict, data_spec=data_spec)
+    test_utils.assertProto2Equal(self, ds.data_spec(), data_spec)
+    # 2 columns * 3 rows * 4 bytes per value + 1 col * 3 rows * 1 byte p.v.
+    self.assertEqual(ds.memory_usage(), 2 * 3 * 4 + 1 * 3 * 1)
+
+  def test_create_vds_pd_check_contents(self):
+    df = pd.DataFrame({
+        "col_str": ["A", "string", "column with", "four entries"],
+        "col_int": [5, 6, 7, 8],
+        "col_int_cat": [1, 2, 3, 4],
+        "col_float": [1.1, 2.2, 3.3, 4.4],
+        "col_bool": [True, True, False, False],
+    })
+    feature_definitions = [
+        Column("col_str", Semantic.CATEGORICAL, min_vocab_frequency=1),
+        Column("col_int_cat", Semantic.CATEGORICAL, min_vocab_frequency=1),
+    ]
+    ds = dataset.create_vertical_dataset(
+        df, columns=feature_definitions, include_all_columns=True
+    )
+    expected_dataset_content = """col_str,col_int_cat,col_int,col_float,col_bool
+A,1,5,1.1,1
+string,2,6,2.2,1
+column with,3,7,3.3,0
+four entries,4,8,4.4,0
+"""
+    self.assertEqual(expected_dataset_content, ds._dataset.DebugString())
+
+  def test_create_vds_pd_check_categorical(self):
+    df = pd.DataFrame({"col_str": ["A", "B"] * 5})
+    ds = dataset.create_vertical_dataset(df)
+    expected_dataset_content = "col_str\n" + "A\nB\n" * 5
+    self.assertEqual(expected_dataset_content, ds._dataset.DebugString())
+
+  def test_max_vocab_count_minus_1(self):
+    df = pd.DataFrame({
+        "col1": ["A", "B", "C", "D", "D"],
+    })
+    ds = dataset.create_vertical_dataset(
+        df,
+        columns=[
+            Column(
+                "col1",
+                Semantic.CATEGORICAL,
+                min_vocab_frequency=1,
+                max_vocab_count=-1,
+            )
+        ],
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=5,
+        columns=(
+            ds_pb.Column(
+                name="col1",
+                type=ds_pb.ColumnType.CATEGORICAL,
+                dtype=ds_pb.DType.DTYPE_BYTES,
+                count_nas=0,
+                categorical=ds_pb.CategoricalSpec(
+                    items={
+                        "<OOD>": VocabValue(index=0, count=0),
+                        "A": VocabValue(index=2, count=1),
+                        "B": VocabValue(index=3, count=1),
+                        "C": VocabValue(index=4, count=1),
+                        "D": VocabValue(index=1, count=2),
+                    },
+                    number_of_unique_values=5,
+                ),
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_invalid_max_vocab_count(self):
+    df = pd.DataFrame({
+        "col1": ["A", "B", "C", "D", "D"],
+    })
+    with self.assertRaisesRegex(
+        test_utils.AbslInvalidArgumentError, "max_vocab_count"
+    ):
+      dataset.create_vertical_dataset(
+          df,
+          columns=[
+              Column(
+                  "col1",
+                  Semantic.CATEGORICAL,
+                  max_vocab_count=-2,
+              )
+          ],
+      )
+
+  @parameterized.parameters(
+      ([True, True, True], (0, 0, 3), 0),
+      ([False, False, False], (0, 3, 0), 0),
+      ([True, False, False], (0, 2, 1), 0),
+  )
+  def test_order_boolean(self, values, expected_counts, count_nas):
+    ds = dataset.create_vertical_dataset(
+        {"col": np.array(values)},
+        columns=[Column("col", dataspec.Semantic.CATEGORICAL)],
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=3,
+        columns=(
+            ds_pb.Column(
+                name="col",
+                type=ds_pb.ColumnType.CATEGORICAL,
+                dtype=ds_pb.DType.DTYPE_BOOL,
+                count_nas=count_nas,
+                categorical=ds_pb.CategoricalSpec(
+                    items={
+                        "<OOV>": VocabValue(index=0, count=expected_counts[0]),
+                        "false": VocabValue(index=1, count=expected_counts[1]),
+                        "true": VocabValue(index=2, count=expected_counts[2]),
+                    },
+                    number_of_unique_values=3,
+                ),
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  @parameterized.parameters(
+      ([True, True, True], (0, 0, 3), 0),
+      ([False, False, False], (0, 3, 0), 0),
+      ([True, False, False], (0, 2, 1), 0),
+  )
+  def test_order_boolean(self, values, expected_counts, count_nas):
+    ds = dataset.create_vertical_dataset(
+        {"col": np.array(values)},
+        columns=[Column("col", dataspec.Semantic.CATEGORICAL)],
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=3,
+        columns=(
+            ds_pb.Column(
+                name="col",
+                type=ds_pb.ColumnType.CATEGORICAL,
+                dtype=ds_pb.DType.DTYPE_BOOL,
+                count_nas=count_nas,
+                categorical=ds_pb.CategoricalSpec(
+                    items={
+                        "<OOV>": VocabValue(index=0, count=expected_counts[0]),
+                        "false": VocabValue(index=1, count=expected_counts[1]),
+                        "true": VocabValue(index=2, count=expected_counts[2]),
+                    },
+                    number_of_unique_values=3,
+                ),
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  @parameterized.parameters("", "csv:")
+  def test_read_csv(self, path_prefix):
+    tmp_dir = self.create_tempdir()
+    csv_file = self.create_tempfile(
+        content="""col_cat,col_num
+A,1
+B,2
+B,3""",
+        file_path=os.path.join(tmp_dir.full_path, "file.csv"),
+    )
+    ds = dataset.create_vertical_dataset(
+        path_prefix + csv_file.full_path, min_vocab_frequency=1
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=3,
+        columns=(
+            ds_pb.Column(
+                name="col_cat",
+                type=ds_pb.ColumnType.CATEGORICAL,
+                is_manual_type=False,
+                categorical=ds_pb.CategoricalSpec(
+                    items={
+                        "<OOD>": VocabValue(index=0, count=0),
+                        "A": VocabValue(index=2, count=1),
+                        "B": VocabValue(index=1, count=2),
+                    },
+                    number_of_unique_values=3,
+                    most_frequent_value=1,
+                    min_value_count=1,
+                    max_number_of_unique_values=2000,
+                    is_already_integerized=False,
+                ),
+            ),
+            ds_pb.Column(
+                name="col_num",
+                type=ds_pb.ColumnType.NUMERICAL,
+                is_manual_type=False,
+                numerical=ds_pb.NumericalSpec(
+                    mean=2,
+                    standard_deviation=0.8164965809277263,  # ~math.sqrt(2 / 3)
+                    min_value=1,
+                    max_value=3,
+                ),
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_max_num_scanned_rows_to_compute_statistics(self):
+    tmp_dir = self.create_tempdir()
+    csv_file = self.create_tempfile(
+        content="""col_cat,col_num
+A,1
+A,2
+B,3""",
+        file_path=os.path.join(tmp_dir.full_path, "file.csv"),
+    )
+    ds = dataset.create_vertical_dataset(
+        "csv:" + csv_file.full_path,
+        min_vocab_frequency=1,
+        max_num_scanned_rows_to_compute_statistics=2,
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=2,
+        columns=(
+            ds_pb.Column(
+                name="col_cat",
+                type=ds_pb.ColumnType.CATEGORICAL,
+                is_manual_type=False,
+                categorical=ds_pb.CategoricalSpec(
+                    items={
+                        "<OOD>": VocabValue(index=0, count=0),
+                        "A": VocabValue(index=1, count=2),
+                    },
+                    number_of_unique_values=2,
+                    most_frequent_value=1,
+                    min_value_count=1,
+                    max_number_of_unique_values=2000,
+                    is_already_integerized=False,
+                ),
+            ),
+            ds_pb.Column(
+                name="col_num",
+                type=ds_pb.ColumnType.NUMERICAL,
+                is_manual_type=False,
+                numerical=ds_pb.NumericalSpec(
+                    mean=1.5,
+                    standard_deviation=0.5,
+                    min_value=1,
+                    max_value=2,
+                ),
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_max_num_scanned_rows_to_infer_semantic(self):
+    tmp_dir = self.create_tempdir()
+    csv_file = self.create_tempfile(
+        content="""col_cat,col_num
+A,1
+B,2
+B,3""",
+        file_path=os.path.join(tmp_dir.full_path, "file.csv"),
+    )
+    ds = dataset.create_vertical_dataset(
+        "csv:" + csv_file.full_path,
+        min_vocab_frequency=1,
+        max_num_scanned_rows_to_infer_semantic=1,
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=3,
+        columns=(
+            ds_pb.Column(
+                name="col_cat",
+                type=ds_pb.ColumnType.CATEGORICAL,
+                is_manual_type=False,
+                categorical=ds_pb.CategoricalSpec(
+                    number_of_unique_values=3,
+                    most_frequent_value=1,
+                    min_value_count=1,
+                    max_number_of_unique_values=2000,
+                    is_already_integerized=False,
+                    items={
+                        "<OOD>": VocabValue(index=0, count=0),
+                        "A": VocabValue(index=2, count=1),
+                        "B": VocabValue(index=1, count=2),
+                    },
+                ),
+            ),
+            ds_pb.Column(
+                name="col_num",
+                type=ds_pb.ColumnType.BOOLEAN,
+                is_manual_type=False,
+                boolean=ds_pb.BooleanSpec(count_true=3),
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_read_from_path(self):
+    csv_file = self.create_tempfile(content="""col_cat,col_num
+A,1
+B,2
+B,3""")
+    ds = dataset.create_vertical_dataset(
+        "csv:" + csv_file.full_path, min_vocab_frequency=1
+    )
+    df_pd = pd.read_csv(csv_file)
+    ds_pd = dataset.create_vertical_dataset(df_pd, data_spec=ds.data_spec())
+    self.assertEqual(ds._dataset.DebugString(), ds_pd._dataset.DebugString())
+
+  @unittest.skip("Requires building YDF with tensorflow io")
+  def test_read_from_sharded_tfe(self):
+    sharded_path = "tfrecord+tfe:" + os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "toy.tfe-tfrecord@2"
+    )
+    ds = dataset.create_vertical_dataset(
+        sharded_path,
+        min_vocab_frequency=1,
+        columns=["Bool_1", "Cat_2", "Num_1"],
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        columns=(
+            ds_pb.Column(
+                name="Bool_1",
+                type=ds_pb.BOOLEAN,
+                dtype=ds_pb.DType.DTYPE_INT64,
+                is_manual_type=False,
+                boolean=ds_pb.BooleanSpec(count_true=2, count_false=2),
+            ),
+            ds_pb.Column(
+                name="Cat_2",
+                type=ds_pb.CATEGORICAL,
+                dtype=ds_pb.DType.DTYPE_BYTES,
+                is_manual_type=False,
+                categorical=ds_pb.CategoricalSpec(
+                    number_of_unique_values=3,
+                    most_frequent_value=1,
+                    min_value_count=1,
+                    max_number_of_unique_values=2000,
+                    is_already_integerized=False,
+                    items={
+                        "<OOD>": VocabValue(index=0, count=0),
+                        "A": VocabValue(index=2, count=1),
+                        "B": VocabValue(index=1, count=1),
+                    },
+                ),
+                count_nas=2,
+            ),
+            ds_pb.Column(
+                name="Num_1",
+                type=ds_pb.NUMERICAL,
+                dtype=ds_pb.DType.DTYPE_FLOAT32,
+                is_manual_type=False,
+                numerical=ds_pb.NumericalSpec(
+                    mean=2.5,
+                    min_value=1.0,
+                    max_value=4.0,
+                    standard_deviation=1.118033988749895,
+                ),
+            ),
+        ),
+        created_num_rows=4,
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_read_from_sharded_tfe_nocompress(self):
+    sharded_path = "tfrecordv2+tfe:" + os.path.join(
+        test_utils.ydf_test_data_path(),
+        "dataset",
+        "toy.nocompress-tfe-tfrecord@2",
+    )
+    ds = dataset.create_vertical_dataset(
+        sharded_path,
+        min_vocab_frequency=1,
+        columns=["Bool_1", "Cat_2", "Num_1"],
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        columns=(
+            ds_pb.Column(
+                name="Bool_1",
+                type=ds_pb.BOOLEAN,
+                dtype=ds_pb.DType.DTYPE_INT64,
+                is_manual_type=False,
+                boolean=ds_pb.BooleanSpec(count_true=2, count_false=2),
+            ),
+            ds_pb.Column(
+                name="Cat_2",
+                type=ds_pb.CATEGORICAL,
+                dtype=ds_pb.DType.DTYPE_BYTES,
+                is_manual_type=False,
+                categorical=ds_pb.CategoricalSpec(
+                    number_of_unique_values=3,
+                    most_frequent_value=1,
+                    min_value_count=1,
+                    max_number_of_unique_values=2000,
+                    is_already_integerized=False,
+                    items={
+                        "<OOD>": VocabValue(index=0, count=0),
+                        "A": VocabValue(index=2, count=1),
+                        "B": VocabValue(index=1, count=1),
+                    },
+                ),
+                count_nas=2,
+            ),
+            ds_pb.Column(
+                name="Num_1",
+                type=ds_pb.NUMERICAL,
+                dtype=ds_pb.DType.DTYPE_FLOAT32,
+                is_manual_type=False,
+                numerical=ds_pb.NumericalSpec(
+                    mean=2.5,
+                    min_value=1.0,
+                    max_value=4.0,
+                    standard_deviation=1.118033988749895,
+                ),
+            ),
+        ),
+        created_num_rows=4,
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_multidimensional_input(self):
+    ds = dataset.create_vertical_dataset(
+        {"feature": np.array([[0, 1, 2], [4, 5, 6]])}
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=2,
+        columns=(
+            ds_pb.Column(
+                name="feature.0_of_3",
+                type=ds_pb.ColumnType.NUMERICAL,
+                dtype=ds_pb.DType.DTYPE_INT64,
+                count_nas=0,
+                numerical=ds_pb.NumericalSpec(
+                    mean=2,
+                    standard_deviation=2,
+                    min_value=0,
+                    max_value=4,
+                ),
+                is_unstacked=True,
+            ),
+            ds_pb.Column(
+                name="feature.1_of_3",
+                type=ds_pb.ColumnType.NUMERICAL,
+                dtype=ds_pb.DType.DTYPE_INT64,
+                count_nas=0,
+                numerical=ds_pb.NumericalSpec(
+                    mean=3,
+                    standard_deviation=2,
+                    min_value=1,
+                    max_value=5,
+                ),
+                is_unstacked=True,
+            ),
+            ds_pb.Column(
+                name="feature.2_of_3",
+                type=ds_pb.ColumnType.NUMERICAL,
+                dtype=ds_pb.DType.DTYPE_INT64,
+                count_nas=0,
+                numerical=ds_pb.NumericalSpec(
+                    mean=4,
+                    standard_deviation=2,
+                    min_value=2,
+                    max_value=6,
+                ),
+                is_unstacked=True,
+            ),
+        ),
+        unstackeds=(
+            ds_pb.Unstacked(
+                original_name="feature",
+                begin_column_idx=0,
+                size=3,
+                type=ds_pb.ColumnType.NUMERICAL,
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  @parameterized.parameters(
+      (1, "feature.0_of_1"),
+      (9, "feature.0_of_9"),
+      (10, "feature.00_of_10"),
+      (9999, "feature.0000_of_9999"),
+      (10000, "feature.00000_of_10000"),
+  )
+  def test_multidimensional_feature_name(
+      self, num_dims: int, expected_first_feature: str
+  ):
+    ds = dataset.create_vertical_dataset({"feature": np.zeros((3, num_dims))})
+    self.assertLen(ds.data_spec().columns, num_dims)
+    self.assertEqual(ds.data_spec().columns[0].name, expected_first_feature)
+
+  def test_multi_dimensions_error_too_many_dims(self):
+    with self.assertRaisesRegex(
+        ValueError, "Input features can only be one or two dimensional"
+    ):
+      _ = dataset.create_vertical_dataset({"feature": np.zeros((3, 3, 3))})
+
+  def test_list_of_csv_datasets(self):
+    df = pd.DataFrame({
+        "col_str": ["A", "string", "column with", "four entries"],
+        "col_int": [5, 6, 7, 8],
+        "col_int_cat": [1, 2, 3, 4],
+        "col_float": [1.1, 2.2, 3.3, 4.4],
+    })
+    feature_definitions = [
+        Column("col_str", Semantic.CATEGORICAL, min_vocab_frequency=1),
+        Column("col_int_cat", Semantic.CATEGORICAL, min_vocab_frequency=1),
+    ]
+    dataset_directory = self.create_tempdir()
+    path1 = os.path.join(dataset_directory.full_path, "ds1")
+    path2 = os.path.join(dataset_directory.full_path, "ds2")
+    df.head(3).to_csv(path1, index=False)
+    df.tail(1).to_csv(path2, index=False)
+
+    ds = dataset.create_vertical_dataset(
+        ["csv:" + path1, "csv:" + path2],
+        columns=feature_definitions,
+        include_all_columns=True,
+    )
+
+    expected_dataset_content = """\
+col_str,col_int,col_int_cat,col_float
+A,5,1,1.1
+string,6,2,2.2
+column with,7,3,3.3
+four entries,8,4,4.4
+"""
+    self.assertEqual(expected_dataset_content, ds._dataset.DebugString())
+
+  def test_singledimensional_strided_float32(self):
+    data = np.array([[0, 1], [4, 5]], np.float32)
+    feature = data[:, 0]  # "feature" shares the same memory as "data".
+    self.assertEqual(data.strides, (8, 4))
+    self.assertEqual(feature.strides, (8,))
+
+    ds = dataset.create_vertical_dataset({"feature": feature})
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=2,
+        columns=(
+            ds_pb.Column(
+                name="feature",
+                type=ds_pb.ColumnType.NUMERICAL,
+                dtype=ds_pb.DType.DTYPE_FLOAT32,
+                count_nas=0,
+                numerical=ds_pb.NumericalSpec(
+                    mean=2,
+                    standard_deviation=2,
+                    min_value=0,
+                    max_value=4,
+                ),
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+    self.assertEqual("feature\n0\n4\n", ds._dataset.DebugString())
+
+  def test_singledimensional_strided_boolean(self):
+    data = np.array([[True, False], [False, True]])
+    feature = data[:, 0]  # "feature" shares the same memory as "data".
+    self.assertEqual(data.strides, (2, 1))
+    self.assertEqual(feature.strides, (2,))
+
+    ds = dataset.create_vertical_dataset({"feature": feature})
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=2,
+        columns=(
+            ds_pb.Column(
+                name="feature",
+                type=ds_pb.ColumnType.BOOLEAN,
+                dtype=ds_pb.DType.DTYPE_BOOL,
+                count_nas=0,
+                boolean=ds_pb.BooleanSpec(
+                    count_true=1,
+                    count_false=1,
+                ),
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+    self.assertEqual("feature\n1\n0\n", ds._dataset.DebugString())
+
+  def test_multidimensional_strided(self):
+    # Note: multidimensional features are unrolled into singledimensional
+    # strided features.
+    ds = dataset.create_vertical_dataset(
+        {"feature": np.array([[0, 1, 2], [4, 5, 6]], np.float32)}
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=2,
+        columns=(
+            ds_pb.Column(
+                name="feature.0_of_3",
+                type=ds_pb.ColumnType.NUMERICAL,
+                dtype=ds_pb.DType.DTYPE_FLOAT32,
+                count_nas=0,
+                numerical=ds_pb.NumericalSpec(
+                    mean=2,
+                    standard_deviation=2,
+                    min_value=0,
+                    max_value=4,
+                ),
+                is_unstacked=True,
+            ),
+            ds_pb.Column(
+                name="feature.1_of_3",
+                type=ds_pb.ColumnType.NUMERICAL,
+                dtype=ds_pb.DType.DTYPE_FLOAT32,
+                count_nas=0,
+                numerical=ds_pb.NumericalSpec(
+                    mean=3,
+                    standard_deviation=2,
+                    min_value=1,
+                    max_value=5,
+                ),
+                is_unstacked=True,
+            ),
+            ds_pb.Column(
+                name="feature.2_of_3",
+                type=ds_pb.ColumnType.NUMERICAL,
+                dtype=ds_pb.DType.DTYPE_FLOAT32,
+                count_nas=0,
+                numerical=ds_pb.NumericalSpec(
+                    mean=4,
+                    standard_deviation=2,
+                    min_value=2,
+                    max_value=6,
+                ),
+                is_unstacked=True,
+            ),
+        ),
+        unstackeds=(
+            ds_pb.Unstacked(
+                original_name="feature",
+                begin_column_idx=0,
+                size=3,
+                type=ds_pb.ColumnType.NUMERICAL,
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+    expected_dataset_content = """\
+feature.0_of_3,feature.1_of_3,feature.2_of_3
+0,1,2
+4,5,6
+"""
+    self.assertEqual(expected_dataset_content, ds._dataset.DebugString())
+
+
+class CategoricalSetTest(absltest.TestCase):
+
+  def create_toy_csv(self) -> str:
+    """Returns the path to a small csv file with sentences."""
+    tmp_dir = self.create_tempdir()
+    csv_file = self.create_tempfile(
+        content="""\
+col_cat_set
+first sentence foo bar foo bar
+second sentence foo bar foo foo foo""",
+        file_path=os.path.join(tmp_dir.full_path, "file.csv"),
+    )
+    return csv_file.full_path
+
+  def toy_csv_dataspec_categorical(self) -> ds_pb.DataSpecification:
+    """Returns a dataspec for the toy CSV example with a categorical feature."""
+    return ds_pb.DataSpecification(
+        created_num_rows=2,
+        columns=(
+            ds_pb.Column(
+                name="col_cat_set",
+                type=ds_pb.ColumnType.CATEGORICAL,
+                is_manual_type=False,
+                categorical=ds_pb.CategoricalSpec(
+                    items={
+                        "<OOD>": VocabValue(index=0, count=0),
+                        "first sentence foo bar foo bar": VocabValue(
+                            index=2, count=1
+                        ),
+                        "second sentence foo bar foo foo foo": VocabValue(
+                            index=1, count=1
+                        ),
+                    },
+                    number_of_unique_values=3,
+                    most_frequent_value=1,
+                    min_value_count=1,
+                    max_number_of_unique_values=2000,
+                    is_already_integerized=False,
+                ),
+            ),
+        ),
+    )
+
+  def toy_csv_dataspec_catset(self) -> ds_pb.DataSpecification:
+    """Returns a dataspec for the toy CSV example with a catset feature."""
+    return ds_pb.DataSpecification(
+        created_num_rows=2,
+        columns=(
+            ds_pb.Column(
+                name="col_cat_set",
+                type=ds_pb.ColumnType.CATEGORICAL_SET,
+                is_manual_type=True,
+                categorical=ds_pb.CategoricalSpec(
+                    items={
+                        "<OOD>": VocabValue(index=0, count=0),
+                        "foo": VocabValue(index=1, count=6),
+                        "bar": VocabValue(index=2, count=3),
+                        "sentence": VocabValue(index=3, count=2),
+                        "second": VocabValue(index=4, count=1),
+                        "first": VocabValue(index=5, count=1),
+                    },
+                    number_of_unique_values=6,
+                    most_frequent_value=1,
+                    min_value_count=1,
+                    max_number_of_unique_values=2000,
+                    is_already_integerized=False,
+                ),
+            ),
+        ),
+    )
+
+  def test_csv_file_no_automatic_tokenization(self):
+    path_to_csv = self.create_toy_csv()
+    ds = dataset.create_vertical_dataset(
+        "csv:" + path_to_csv, min_vocab_frequency=1
+    )
+    expected_data_spec = self.toy_csv_dataspec_categorical()
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_csv_tokenization_when_semantic_specified(self):
+    path_to_csv = self.create_toy_csv()
+    ds = dataset.create_vertical_dataset(
+        "csv:" + path_to_csv,
+        min_vocab_frequency=1,
+        columns=[("col_cat_set", Semantic.CATEGORICAL_SET)],
+    )
+    expected_data_spec = self.toy_csv_dataspec_catset()
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_csv_file_reading_respects_data_spec_categorical(self):
+    path_to_csv = self.create_toy_csv()
+    data_spec = self.toy_csv_dataspec_categorical()
+    ds = dataset.create_vertical_dataset(
+        "csv:" + path_to_csv, data_spec=data_spec
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), data_spec)
+    self.assertEqual(
+        ds._dataset.DebugString(),
+        """\
+col_cat_set
+first sentence foo bar foo bar
+second sentence foo bar foo foo foo
+""",
+    )
+
+  def test_csv_file_reading_respects_data_spec_catset(self):
+    path_to_csv = self.create_toy_csv()
+    data_spec = self.toy_csv_dataspec_catset()
+    ds = dataset.create_vertical_dataset(
+        "csv:" + path_to_csv, data_spec=data_spec
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), data_spec)
+    self.assertEqual(
+        ds._dataset.DebugString(),
+        """\
+col_cat_set
+foo, bar, sentence, first
+foo, bar, sentence, second
+""",
+    )
+
+  def test_pd_list_of_list(self):
+    df = pd.DataFrame({
+        "feature": [
+            ["single item"],
+            ["two", "words"],
+            ["three", "simple", "words", "words"],
+            [""],
+        ]
+    })
+    ds = dataset.create_vertical_dataset(
+        df,
+        min_vocab_frequency=1,
+        columns=[("feature", Semantic.CATEGORICAL_SET)],
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=4,
+        columns=(
+            ds_pb.Column(
+                name="feature",
+                type=ds_pb.ColumnType.CATEGORICAL_SET,
+                dtype=ds_pb.DType.DTYPE_BYTES,
+                categorical=ds_pb.CategoricalSpec(
+                    items={
+                        "<OOD>": VocabValue(index=0, count=0),
+                        "words": VocabValue(index=1, count=2),
+                        "simple": VocabValue(index=2, count=1),
+                        "single item": VocabValue(index=3, count=1),
+                        "three": VocabValue(index=4, count=1),
+                        "two": VocabValue(index=5, count=1),
+                    },
+                    number_of_unique_values=6,
+                ),
+                count_nas=1,
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_pd_invalid_type_text(self):
+    df = pd.DataFrame({"feature": ["a", "b c", "d e f g h"]})
+    with self.assertRaisesRegex(
+        ValueError, "Categorical Set columns must be a list of lists."
+    ):
+      _ = dataset.create_vertical_dataset(
+          df,
+          min_vocab_frequency=1,
+          columns=[("feature", Semantic.CATEGORICAL_SET)],
+      )
+
+  def test_pd_np_bytes(self):
+    df = pd.DataFrame({
+        "feature": [
+            np.array(["single item"], np.bytes_),
+            np.array(["two", "words"], np.bytes_),
+            np.array(["three", "simple", "words", "words"], np.bytes_),
+            np.array([""], np.bytes_),
+        ]
+    })
+    ds = dataset.create_vertical_dataset(
+        df,
+        min_vocab_frequency=1,
+        columns=[("feature", Semantic.CATEGORICAL_SET)],
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=4,
+        columns=(
+            ds_pb.Column(
+                name="feature",
+                type=ds_pb.ColumnType.CATEGORICAL_SET,
+                dtype=ds_pb.DType.DTYPE_BYTES,
+                categorical=ds_pb.CategoricalSpec(
+                    items={
+                        "<OOD>": VocabValue(index=0, count=0),
+                        "words": VocabValue(index=1, count=2),
+                        "simple": VocabValue(index=2, count=1),
+                        "single item": VocabValue(index=3, count=1),
+                        "three": VocabValue(index=4, count=1),
+                        "two": VocabValue(index=5, count=1),
+                    },
+                    number_of_unique_values=6,
+                ),
+                count_nas=1,
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_pd_with_na(self):
+    df = pd.DataFrame({
+        "feature": [
+            pd.NA,
+            ["single item"],
+            ["two", "words"],
+            ["three", "simple", "words", "words"],
+        ]
+    })
+    ds = dataset.create_vertical_dataset(
+        df,
+        min_vocab_frequency=1,
+        columns=[("feature", Semantic.CATEGORICAL_SET)],
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=4,
+        columns=(
+            ds_pb.Column(
+                name="feature",
+                type=ds_pb.ColumnType.CATEGORICAL_SET,
+                dtype=ds_pb.DType.DTYPE_BYTES,
+                categorical=ds_pb.CategoricalSpec(
+                    items={
+                        "<OOD>": VocabValue(index=0, count=0),
+                        "words": VocabValue(index=1, count=2),
+                        "simple": VocabValue(index=2, count=1),
+                        "single item": VocabValue(index=3, count=1),
+                        "three": VocabValue(index=4, count=1),
+                        "two": VocabValue(index=5, count=1),
+                    },
+                    number_of_unique_values=6,
+                ),
+                count_nas=1,
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_pd_with_empty_list(self):
+    df = pd.DataFrame({
+        "feature": [
+            [],
+            ["single item"],
+            ["two", "words"],
+            ["three", "simple", "words", "words"],
+        ]
+    })
+    ds = dataset.create_vertical_dataset(
+        df,
+        min_vocab_frequency=1,
+        columns=[("feature", Semantic.CATEGORICAL_SET)],
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=4,
+        columns=(
+            ds_pb.Column(
+                name="feature",
+                type=ds_pb.ColumnType.CATEGORICAL_SET,
+                dtype=ds_pb.DType.DTYPE_BYTES,
+                categorical=ds_pb.CategoricalSpec(
+                    items={
+                        "<OOD>": VocabValue(index=0, count=0),
+                        "words": VocabValue(index=1, count=2),
+                        "simple": VocabValue(index=2, count=1),
+                        "single item": VocabValue(index=3, count=1),
+                        "three": VocabValue(index=4, count=1),
+                        "two": VocabValue(index=5, count=1),
+                    },
+                    number_of_unique_values=6,
+                ),
+                count_nas=0,
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_pd_type_inference_lists(self):
+    df = pd.DataFrame(
+        {
+            "feature": [
+                ["single item"],
+                ["two", "words"],
+            ]
+        }
+    )
+    ds = dataset.create_vertical_dataset(
+        df,
+        min_vocab_frequency=1,
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=2,
+        columns=(
+            ds_pb.Column(
+                name="feature",
+                type=ds_pb.ColumnType.CATEGORICAL_SET,
+                dtype=ds_pb.DType.DTYPE_BYTES,
+                categorical=ds_pb.CategoricalSpec(
+                    items={
+                        "<OOD>": VocabValue(index=0, count=0),
+                        "single item": VocabValue(index=1, count=1),
+                        "two": VocabValue(index=2, count=1),
+                        "words": VocabValue(index=3, count=1),
+                    },
+                    number_of_unique_values=4,
+                ),
+                count_nas=0,
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+  def test_pd_type_inference_nparrays(self):
+    df = pd.DataFrame(
+        {
+            "feature": [
+                np.array(["single item"]),
+                np.array(["two", "words"]),
+            ]
+        }
+    )
+    ds = dataset.create_vertical_dataset(
+        df,
+        min_vocab_frequency=1,
+    )
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=2,
+        columns=(
+            ds_pb.Column(
+                name="feature",
+                type=ds_pb.ColumnType.CATEGORICAL_SET,
+                dtype=ds_pb.DType.DTYPE_BYTES,
+                categorical=ds_pb.CategoricalSpec(
+                    items={
+                        "<OOD>": VocabValue(index=0, count=0),
+                        "single item": VocabValue(index=1, count=1),
+                        "two": VocabValue(index=2, count=1),
+                        "words": VocabValue(index=3, count=1),
+                    },
+                    number_of_unique_values=4,
+                ),
+                count_nas=0,
+            ),
+        ),
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), expected_data_spec)
+
+
+@parameterized.parameters(
+    ds_pb.ColumnType.NUMERICAL, ds_pb.ColumnType.CATEGORICAL
+)
+class MissingColumnsTest(parameterized.TestCase):
+
+  def create_data_spec(self, column_type: ds_pb.ColumnType):
+    if column_type == ds_pb.ColumnType.NUMERICAL:
+      return ds_pb.DataSpecification(
+          created_num_rows=3,
+          columns=(
+              ds_pb.Column(name="f1", type=ds_pb.ColumnType.NUMERICAL),
+              ds_pb.Column(name="f2", type=ds_pb.ColumnType.NUMERICAL),
+          ),
+      )
+    if column_type == ds_pb.ColumnType.CATEGORICAL:
+      return ds_pb.DataSpecification(
+          created_num_rows=3,
+          columns=(
+              ds_pb.Column(
+                  name="f1",
+                  type=ds_pb.ColumnType.CATEGORICAL,
+                  dtype=ds_pb.DType.DTYPE_BYTES,
+                  categorical=ds_pb.CategoricalSpec(
+                      items={
+                          "<OOD>": VocabValue(index=0, count=0),
+                          "1": VocabValue(index=1, count=1),
+                          "2": VocabValue(index=2, count=1),
+                          "3": VocabValue(index=3, count=1),
+                      },
+                      number_of_unique_values=4,
+                      min_value_count=1,
+                  ),
+              ),
+              ds_pb.Column(
+                  name="f2",
+                  type=ds_pb.ColumnType.CATEGORICAL,
+                  dtype=ds_pb.DType.DTYPE_BYTES,
+                  categorical=ds_pb.CategoricalSpec(
+                      items={
+                          "<OOD>": VocabValue(index=0, count=0),
+                          "1": VocabValue(index=1, count=1),
+                          "2": VocabValue(index=2, count=1),
+                          "3": VocabValue(index=3, count=1),
+                      },
+                      number_of_unique_values=4,
+                      min_value_count=1,
+                  ),
+              ),
+          ),
+      )
+    raise NotImplementedError(f"Errors for type {column_type} not implemented")
+
+  def get_inferred_dataspec_pd_f1only(self, column_type: ds_pb.ColumnType):
+    if column_type == ds_pb.ColumnType.NUMERICAL:
+      return ds_pb.DataSpecification(
+          created_num_rows=3,
+          columns=(
+              ds_pb.Column(
+                  name="f1",
+                  type=ds_pb.ColumnType.NUMERICAL,
+                  dtype=ds_pb.DType.DTYPE_INT64,
+                  numerical=ds_pb.NumericalSpec(
+                      mean=2.0,
+                      min_value=1.0,
+                      max_value=3.0,
+                      standard_deviation=0.8164965809277263,
+                  ),
+                  count_nas=0,
+              ),
+          ),
+      )
+    if column_type == ds_pb.ColumnType.CATEGORICAL:
+      return ds_pb.DataSpecification(
+          created_num_rows=3,
+          columns=(
+              ds_pb.Column(
+                  name="f1",
+                  type=ds_pb.ColumnType.CATEGORICAL,
+                  dtype=ds_pb.DType.DTYPE_INT64,
+                  categorical=ds_pb.CategoricalSpec(
+                      items={
+                          "<OOD>": VocabValue(index=0, count=0),
+                          "1": VocabValue(index=1, count=1),
+                          "2": VocabValue(index=2, count=1),
+                          "3": VocabValue(index=3, count=1),
+                      },
+                      number_of_unique_values=4,
+                  ),
+                  count_nas=0,
+              ),
+          ),
+      )
+    raise NotImplementedError(f"Errors for type {column_type} not implemented")
+
+  def get_inferred_dataspec_file_f1only(self, column_type: ds_pb.ColumnType):
+    if column_type == ds_pb.ColumnType.NUMERICAL:
+      return ds_pb.DataSpecification(
+          created_num_rows=3,
+          columns=(
+              ds_pb.Column(
+                  name="f1",
+                  is_manual_type=True,
+                  type=ds_pb.ColumnType.NUMERICAL,
+                  numerical=ds_pb.NumericalSpec(
+                      mean=2.0,
+                      min_value=1.0,
+                      max_value=3.0,
+                      standard_deviation=0.8164965809277263,
+                  ),
+              ),
+          ),
+      )
+    if column_type == ds_pb.ColumnType.CATEGORICAL:
+      return ds_pb.DataSpecification(
+          created_num_rows=3,
+          columns=(
+              ds_pb.Column(
+                  name="f1",
+                  is_manual_type=True,
+                  type=ds_pb.ColumnType.CATEGORICAL,
+                  categorical=ds_pb.CategoricalSpec(
+                      items={
+                          "<OOD>": VocabValue(index=0, count=0),
+                          "1": VocabValue(index=3, count=1),
+                          "2": VocabValue(index=2, count=1),
+                          "3": VocabValue(index=1, count=1),
+                      },
+                      number_of_unique_values=4,
+                      most_frequent_value=1,
+                      min_value_count=1,
+                      is_already_integerized=False,
+                      max_number_of_unique_values=2000,
+                  ),
+              ),
+          ),
+      )
+    raise NotImplementedError(f"Errors for type {column_type} not implemented")
+
+  def create_csv(self) -> str:
+    tmp_dir = self.create_tempdir()
+    csv_file = self.create_tempfile(
+        content="""f1
+1
+2
+3""",
+        file_path=os.path.join(tmp_dir.full_path, "file.csv"),
+    )
+    return csv_file.full_path
+
+  def get_debug_string(self, column_type: ds_pb.ColumnType):
+    if column_type == ds_pb.ColumnType.NUMERICAL:
+      return "f1,f2\n1,nan\n2,nan\n3,nan\n"
+    if column_type == ds_pb.ColumnType.CATEGORICAL:
+      return "f1,f2\n1,NA\n2,NA\n3,NA\n"
+
+    raise NotImplementedError(f"Errors for type {column_type} not implemented")
+
+  def test_required_columns_pd_data_spec_none(
+      self, column_type: ds_pb.ColumnType
+  ):
+    data_spec = self.create_data_spec(column_type)
+    df = pd.DataFrame({"f1": [1, 2, 3]})
+    with self.assertRaises(ValueError):
+      _ = dataset.create_vertical_dataset(df, data_spec=data_spec)
+
+  def test_required_columns_pd_data_spec_empty(
+      self, column_type: ds_pb.ColumnType
+  ):
+    data_spec = self.create_data_spec(column_type)
+    df = pd.DataFrame({"f1": [1, 2, 3]})
+    ds = dataset.create_vertical_dataset(
+        df, data_spec=data_spec, required_columns=[]
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), data_spec)
+    self.assertEqual(
+        ds._dataset.DebugString(), self.get_debug_string(column_type)
+    )
+
+  def test_required_columns_pd_data_spec_explicit_success(
+      self, column_type: ds_pb.ColumnType
+  ):
+    data_spec = self.create_data_spec(column_type)
+    df = pd.DataFrame({"f1": [1, 2, 3]})
+    ds = dataset.create_vertical_dataset(
+        df, data_spec=data_spec, required_columns=["f1"]
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), data_spec)
+    self.assertEqual(
+        ds._dataset.DebugString(), self.get_debug_string(column_type)
+    )
+
+  def test_required_columns_pd_data_spec_explicit_failure(
+      self, column_type: ds_pb.ColumnType
+  ):
+    data_spec = self.create_data_spec(column_type)
+    df = pd.DataFrame({"f1": [1, 2, 3]})
+    with self.assertRaises(ValueError):
+      _ = dataset.create_vertical_dataset(
+          df, data_spec=data_spec, required_columns=["f2"]
+      )
+
+  def test_required_columns_pd_inference_args_none(
+      self, column_type: ds_pb.ColumnType
+  ):
+    df = pd.DataFrame({"f1": [1, 2, 3]})
+    column_semantic = dataspec.Semantic.from_proto_type(column_type)
+    with self.assertRaises(ValueError):
+      _ = dataset.create_vertical_dataset(
+          df, columns=[("f1", column_semantic), ("f2", column_semantic)]
+      )
+
+  def test_required_columns_pd_inference_args_empty(
+      self, column_type: ds_pb.ColumnType
+  ):
+    df = pd.DataFrame({"f1": [1, 2, 3]})
+    column_semantic = dataspec.Semantic.from_proto_type(column_type)
+    ds = dataset.create_vertical_dataset(
+        df,
+        columns=[("f1", column_semantic), ("f2", column_semantic)],
+        required_columns=[],
+        min_vocab_frequency=1,
+    )
+    # Note that the dataspec does not contain column f2 since it was not found
+    # in the data.
+    test_utils.assertProto2Equal(
+        self,
+        ds._dataset.data_spec(),
+        self.get_inferred_dataspec_pd_f1only(column_type),
+    )
+    self.assertEqual(ds._dataset.DebugString(), "f1\n1\n2\n3\n")
+
+  def test_required_columns_pd_inference_args_explicit_failure(
+      self, column_type: ds_pb.ColumnType
+  ):
+    df = pd.DataFrame({"f1": [1, 2, 3]})
+    column_semantic = dataspec.Semantic.from_proto_type(column_type)
+    with self.assertRaises(ValueError):
+      _ = dataset.create_vertical_dataset(df, columns=[("f2", column_semantic)])
+
+  def test_required_columns_pd_inference_args_explicit_success(
+      self, column_type: ds_pb.ColumnType
+  ):
+    df = pd.DataFrame({"f1": [1, 2, 3]})
+    column_semantic = dataspec.Semantic.from_proto_type(column_type)
+    ds = dataset.create_vertical_dataset(
+        df,
+        columns=[("f1", column_semantic), ("f2", column_semantic)],
+        min_vocab_frequency=1,
+        required_columns=["f1"],
+    )
+    # Note that the dataspec does not contain column f2 since it was not found
+    # in the data.
+    test_utils.assertProto2Equal(
+        self,
+        ds._dataset.data_spec(),
+        self.get_inferred_dataspec_pd_f1only(column_type),
+    )
+    self.assertEqual(ds._dataset.DebugString(), "f1\n1\n2\n3\n")
+
+  def test_required_columns_file_data_spec_none(
+      self, column_type: ds_pb.ColumnType
+  ):
+    data_spec = self.create_data_spec(column_type)
+    file_path = self.create_csv()
+    with self.assertRaises(ValueError):
+      _ = dataset.create_vertical_dataset(file_path, data_spec=data_spec)
+
+  def test_required_columns_file_data_spec_empty(
+      self, column_type: ds_pb.ColumnType
+  ):
+    data_spec = self.create_data_spec(column_type)
+    file_path = self.create_csv()
+    ds = dataset.create_vertical_dataset(
+        file_path, data_spec=data_spec, required_columns=[]
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), data_spec)
+    self.assertEqual(
+        ds._dataset.DebugString(), self.get_debug_string(column_type)
+    )
+
+  def test_required_columns_file_data_spec_explicit_success(
+      self, column_type: ds_pb.ColumnType
+  ):
+    data_spec = self.create_data_spec(column_type)
+    file_path = self.create_csv()
+    ds = dataset.create_vertical_dataset(
+        file_path, data_spec=data_spec, required_columns=["f1"]
+    )
+    test_utils.assertProto2Equal(self, ds.data_spec(), data_spec)
+    self.assertEqual(
+        ds._dataset.DebugString(), self.get_debug_string(column_type)
+    )
+
+  def test_required_columns_file_data_spec_explicit_failure(
+      self, column_type: ds_pb.ColumnType
+  ):
+    data_spec = self.create_data_spec(column_type)
+    file_path = self.create_csv()
+    with self.assertRaises(ValueError):
+      _ = dataset.create_vertical_dataset(
+          file_path, data_spec=data_spec, required_columns=["f2"]
+      )
+
+  def test_required_columns_file_inference_args_none(
+      self, column_type: ds_pb.ColumnType
+  ):
+    file_path = self.create_csv()
+    column_semantic = dataspec.Semantic.from_proto_type(column_type)
+    with self.assertRaises(ValueError):
+      _ = dataset.create_vertical_dataset(
+          file_path, columns=[("f1", column_semantic), ("f2", column_semantic)]
+      )
+
+  def test_required_columns_file_inference_args_empty(
+      self, column_type: ds_pb.ColumnType
+  ):
+    file_path = self.create_csv()
+    column_semantic = dataspec.Semantic.from_proto_type(column_type)
+    ds = dataset.create_vertical_dataset(
+        file_path,
+        columns=[("f1", column_semantic), ("f2", column_semantic)],
+        required_columns=[],
+        min_vocab_frequency=1,
+    )
+    # Note that the dataspec does not contain column f2 since it was not found
+    # in the data.
+    test_utils.assertProto2Equal(
+        self,
+        ds._dataset.data_spec(),
+        self.get_inferred_dataspec_file_f1only(column_type),
+    )
+    self.assertEqual(ds._dataset.DebugString(), "f1\n1\n2\n3\n")
+
+  def test_required_columns_file_inference_args_explicit_failure(
+      self, column_type: ds_pb.ColumnType
+  ):
+    file_path = self.create_csv()
+    column_semantic = dataspec.Semantic.from_proto_type(column_type)
+    with self.assertRaises(ValueError):
+      _ = dataset.create_vertical_dataset(
+          file_path, columns=[("f2", column_semantic)]
+      )
+
+  def test_required_columns_file_inference_args_explicit_success(
+      self, column_type: ds_pb.ColumnType
+  ):
+    file_path = self.create_csv()
+    column_semantic = dataspec.Semantic.from_proto_type(column_type)
+    ds = dataset.create_vertical_dataset(
+        file_path,
+        columns=[("f1", column_semantic), ("f2", column_semantic)],
+        required_columns=["f1"],
+        min_vocab_frequency=1,
+    )
+    # Note that the dataspec does not contain column f2 since it was not found
+    # in the data.
+    test_utils.assertProto2Equal(
+        self,
+        ds._dataset.data_spec(),
+        self.get_inferred_dataspec_file_f1only(column_type),
+    )
+    self.assertEqual(ds._dataset.DebugString(), "f1\n1\n2\n3\n")
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/dataset/dataset_with_tf_test.py

```diff
@@ -1,53 +1,53 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Tests for the dataset with TensorFlow dependency."""
-
-from absl.testing import absltest
-import numpy as np
-import tensorflow as tf
-
-from yggdrasil_decision_forests.dataset import data_spec_pb2 as ds_pb
-from ydf.dataset import dataset
-
-
-class DatasetWithTfTest(absltest.TestCase):
-
-  def test_create_tensorflow_batched_dataset(self):
-    ds_tf = tf.data.Dataset.from_tensor_slices({
-        "a": np.array([1, 2, 3]),
-    }).batch(1)
-    ds = dataset.create_vertical_dataset(ds_tf)
-    expected_data_spec = ds_pb.DataSpecification(
-        created_num_rows=3,
-        columns=(
-            ds_pb.Column(
-                name="a",
-                type=ds_pb.ColumnType.NUMERICAL,
-                dtype=ds_pb.DType.DTYPE_INT64,
-                count_nas=0,
-                numerical=ds_pb.NumericalSpec(
-                    mean=2,
-                    standard_deviation=0.8164965809277263,  # ~math.sqrt(2 / 3)
-                    min_value=1,
-                    max_value=3,
-                ),
-            ),
-        ),
-    )
-    self.assertEqual(ds.data_spec(), expected_data_spec)
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Tests for the dataset with TensorFlow dependency."""
+
+from absl.testing import absltest
+import numpy as np
+import tensorflow as tf
+
+from ydf.proto.dataset import data_spec_pb2 as ds_pb
+from ydf.dataset import dataset
+
+
+class DatasetWithTfTest(absltest.TestCase):
+
+  def test_create_tensorflow_batched_dataset(self):
+    ds_tf = tf.data.Dataset.from_tensor_slices({
+        "a": np.array([1, 2, 3]),
+    }).batch(1)
+    ds = dataset.create_vertical_dataset(ds_tf)
+    expected_data_spec = ds_pb.DataSpecification(
+        created_num_rows=3,
+        columns=(
+            ds_pb.Column(
+                name="a",
+                type=ds_pb.ColumnType.NUMERICAL,
+                dtype=ds_pb.DType.DTYPE_INT64,
+                count_nas=0,
+                numerical=ds_pb.NumericalSpec(
+                    mean=2,
+                    standard_deviation=0.8164965809277263,  # ~math.sqrt(2 / 3)
+                    min_value=1,
+                    max_value=3,
+                ),
+            ),
+        ),
+    )
+    self.assertEqual(ds.data_spec(), expected_data_spec)
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/dataset/dataspec.py

```diff
@@ -1,645 +1,645 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Dataspec utilities."""
-
-import dataclasses
-import enum
-import logging
-from typing import Any, Dict, List, Literal, Optional, Sequence, Tuple, Union
-
-import numpy as np
-
-from yggdrasil_decision_forests.dataset import data_spec_pb2 as ds_pb
-
-
-# The different ways a user can specify the columns of VerticalDataset.
-ColumnDef = Union["Column", str, Tuple[str, "Semantic"]]
-ColumnDefs = Optional[List[ColumnDef]]
-
-# Normalized version of "ColumnDefs".
-NormalizedColumnDefs = Optional[List["Column"]]
-
-# The key of the "out of vocabulary" (OOV) used by YDF.
-# Must match kOutOfDictionaryItemKey in
-# yggdrasil_decision_forests/dataset/data_spec.h
-YDF_OOD = "<OOD>"
-
-# Mapping between Numpy dtypes and YDF dtypes.
-_NP_DTYPE_TO_YDF_DTYPE = {
-    np.int8: ds_pb.DType.DTYPE_INT8,
-    np.int16: ds_pb.DType.DTYPE_INT16,
-    np.int32: ds_pb.DType.DTYPE_INT32,
-    np.int64: ds_pb.DType.DTYPE_INT64,
-    np.uint8: ds_pb.DType.DTYPE_UINT8,
-    np.uint16: ds_pb.DType.DTYPE_UINT16,
-    np.uint32: ds_pb.DType.DTYPE_UINT32,
-    np.uint64: ds_pb.DType.DTYPE_UINT64,
-    np.float16: ds_pb.DType.DTYPE_FLOAT16,
-    np.float32: ds_pb.DType.DTYPE_FLOAT32,
-    np.float64: ds_pb.DType.DTYPE_FLOAT64,
-    np.bool_: ds_pb.DType.DTYPE_BOOL,
-    np.string_: ds_pb.DType.DTYPE_BYTES,
-    np.str_: ds_pb.DType.DTYPE_BYTES,
-    np.bytes_: ds_pb.DType.DTYPE_BYTES,
-    np.object_: ds_pb.DType.DTYPE_BYTES,
-}
-
-NP_SUPPORTED_INT_DTYPE = [
-    np.int8,
-    np.int16,
-    np.int32,
-    np.int64,
-    np.uint8,
-    np.uint16,
-    np.uint32,
-    np.uint64,
-]
-
-NP_SUPPORTED_FLOAT_DTYPE = [
-    np.float16,
-    np.float32,
-    np.float64,
-]
-
-
-def np_dtype_to_ydf_dtype(np_dtype: np.dtype) -> Optional["ds_pb.DType"]:
-  """Converts a Numpy dtype to a YDF dtype.
-
-  If the numpy dtype has no matching ydf dtype, returns None.
-
-  Args:
-    np_dtype: Numpy dtype.
-
-  Returns:
-    YDF dtype.
-  """
-
-  if hasattr(np_dtype, "type"):
-    ydf_dtype = _NP_DTYPE_TO_YDF_DTYPE.get(np_dtype.type)
-  else:
-    ydf_dtype = _NP_DTYPE_TO_YDF_DTYPE.get(np_dtype)
-
-  if ydf_dtype is None:
-    raise ValueError(f"ydf_dtype: {ydf_dtype!r} {np_dtype!r}")
-
-  return ydf_dtype
-
-
-class Semantic(enum.Enum):
-  """Semantic (e.g. numerical, categorical) of a column.
-
-  Determines how a column is interpreted by the model.
-  Similar to the "ColumnType" of YDF's DataSpecification.
-
-  Attributes:
-    NUMERICAL: Numerical value. Generally for quantities or counts with full
-      ordering. For example, the age of a person, or the number of items in a
-      bag. Can be a float or an integer.  Missing values are represented by
-      math.nan.
-    CATEGORICAL: A categorical value. Generally for a type/class in finite set
-      of possible values without ordering. For example, the color RED in the set
-      {RED, BLUE, GREEN}. Can be a string or an integer.  Missing values are
-      represented by "" (empty sting) or value -2. An out-of-vocabulary value
-      (i.e. a value that was never seen in training) is represented by any new
-      string value or the value -1. Integer categorical values: (1) The training
-      logic and model representation is optimized with the assumption that
-      values are dense. (2) Internally, the value is stored as int32. The values
-      should be <~2B. (3) The number of possible values is computed
-      automatically from the training dataset. During inference, integer values
-      greater than any value seen during training will be treated as
-      out-of-vocabulary. (4) Minimum frequency and maximum vocabulary size
-      constraints do not apply.
-    HASH: The hash of a string value. Used when only the equality between values
-      is important (not the value itself). Currently, only used for groups in
-      ranking problems e.g. the query in a query/document problem. The hashing
-      is computed with Google's farmhash and stored as an uint64.
-    CATEGORICAL_SET: Set of categorical values. Great to represent tokenized
-      texts. Can be a string. Unlike CATEGORICAL, the number of items in a
-      CATEGORICAL_SET can change between examples. The order of values inside a
-      feature values does not matter.
-    BOOLEAN: Boolean value. Can be a float or an integer. Missing values are
-      represented by math.nan.  If a numerical tensor contains multiple values,
-      its size should be constant, and each dimension isthreaded independently
-      (and each dimension should always have the same "meaning").
-    DISCRETIZED_NUMERICAL: Numerical values automatically discretized into bins.
-      Discretized numerical columns are faster to train than (non-discretized)
-      numerical columns. If the number of unique values of these columns is
-      lower than the number of bins, the discretization is lossless from the
-      point of view of the model. If the number of unique values of this columns
-      is greater than the number of bins, the discretization is lossy from the
-      point of view of the model. Lossy discretization can reduce and sometime
-      increase (due to regularization) the quality of the model.
-  """
-
-  NUMERICAL = 1
-  CATEGORICAL = 2
-  HASH = 3
-  CATEGORICAL_SET = 4
-  BOOLEAN = 5
-  DISCRETIZED_NUMERICAL = 6
-
-  def to_proto_type(self) -> ds_pb.ColumnType:
-    if self in SEMANTIC_TO_PROTO:
-      return SEMANTIC_TO_PROTO[self]
-    else:
-      raise NotImplementedError(f"Unsupported semantic {self}")
-
-  @classmethod
-  def from_proto_type(cls, column_type: ds_pb.ColumnType):
-    if column_type in PROTO_TO_SEMANTIC:
-      return PROTO_TO_SEMANTIC[column_type]
-    else:
-      raise NotImplementedError(f"Unsupported semantic {column_type}")
-
-
-# Mappings between semantic enum in python and in protobuffer and vice versa.
-SEMANTIC_TO_PROTO = {
-    Semantic.NUMERICAL: ds_pb.NUMERICAL,
-    Semantic.CATEGORICAL: ds_pb.CATEGORICAL,
-    Semantic.HASH: ds_pb.HASH,
-    Semantic.CATEGORICAL_SET: ds_pb.CATEGORICAL_SET,
-    Semantic.BOOLEAN: ds_pb.BOOLEAN,
-    Semantic.DISCRETIZED_NUMERICAL: ds_pb.DISCRETIZED_NUMERICAL,
-}
-PROTO_TO_SEMANTIC = {v: k for k, v in SEMANTIC_TO_PROTO.items()}
-
-
-class Monotonic(enum.Enum):
-  """Monotonic constraint between a feature and the model output."""
-
-  INCREASING = 1
-  DECREASING = 2
-
-
-# Map between integer monotonic constraints (as commonly used by decision
-# forests libraries) and Monotonic enum value.
-_INTEGER_MONOTONIC_TUPLES = (
-    (0, None),
-    (1, Monotonic.INCREASING),
-    (-1, Monotonic.DECREASING),
-)
-
-
-def _build_integer_monotonic_map() -> Dict[int, Optional[Monotonic]]:
-  """Returns a mapping between integer monotonic constraints and enum value.
-
-  The returned value is always the same. So, when possible, create and reuse
-  the result instead of calling "_build_integer_monotonic_map" multiple times.
-  """
-
-  return {key: value for key, value in _INTEGER_MONOTONIC_TUPLES}
-
-
-# Various ways for a user to specify a monotonic constraint.
-MonotonicConstraint = Optional[Union[Monotonic, Literal[-1, 0, +1]]]
-
-
-def _normalize_monotonic_constraint(
-    constraint: MonotonicConstraint,
-) -> Optional[Monotonic]:
-  """Normalizes monotonic constraints provided by the user.
-
-  Args:
-    constraint: User monotonic constraints.
-
-  Returns:
-    Normalized monotonic constraint.
-
-  Raises:
-    ValueError: If the user input is not a valid monotonic constraint.
-  """
-
-  if isinstance(constraint, int):
-    monotonic_map = _build_integer_monotonic_map()
-    if constraint not in monotonic_map:
-      raise ValueError(
-          "monotonic argument provided as integer should be one of"
-          f" {list(monotonic_map)!r}. Got {constraint!r} instead"
-      )
-    constraint = monotonic_map[constraint]
-
-  if constraint is None or isinstance(constraint, Monotonic):
-    return constraint
-
-  raise ValueError(
-      "Unexpected monotonic value. monotonic value can be 0, +1, -1, None,"
-      " Monotonic.INCREASING, or Monotonic.DECREASING. Got"
-      f" {constraint!r} instead"
-  )
-
-
-@dataclasses.dataclass
-class Column(object):
-  """Semantic and parameters for a single column.
-
-  This class allows to:
-    1. Limit the input features of the model.
-    2. Manually specify the semantic of a feature.
-    3. Specify feature specific hyper-parameters.
-
-  Attributes:
-    name: The name of the column or feature.
-    semantic: Semantic of the column. If None, the semantic is automatically
-      determined. The semantic controls how a column is interpreted by a model.
-      Using the wrong semantic (e.g. numerical instead of categorical) will hurt
-      your model"s quality.
-    max_vocab_count: For CATEGORICAL and CATEGORICAL_SET columns only. Number of
-      unique categorical values stored as string. If more categorical values are
-      present, the least frequent values are grouped into a Out-of-vocabulary
-      item. Reducing the value can improve or hurt the model. If max_vocab_count
-      = -1, the number of values in the column is not limited.
-    min_vocab_frequency: For CATEGORICAL and CATEGORICAL_SET columns only.
-      Minimum number of occurrence of a categorical value. Values present less
-      than "min_vocab_frequency" times in the training dataset are treated as
-      "Out-of-vocabulary".
-    num_discretized_numerical_bins: For DISCRETIZED_NUMERICAL columns only.
-      Number of bins used to discretize DISCRETIZED_NUMERICAL columns.
-    monotonic: Monotonic constraints between the feature and the model output.
-      Use `None` (default; or 0) for an unconstrained feature. Use
-      `Monotonic.INCREASING` (or +1) to ensure the model is monotonically
-      increasing with the features. Use `Monotonic.DECREASING` (or -1) to ensure
-      the model is monotonically decreasing with the features.
-  """
-
-  name: str
-  semantic: Optional[Semantic] = None
-  max_vocab_count: Optional[int] = None
-  min_vocab_frequency: Optional[int] = None
-  num_discretized_numerical_bins: Optional[int] = None
-  monotonic: MonotonicConstraint = None
-
-  def __post_init__(self):
-    # Check matching between hyper-parameters and semantic.
-    if self.semantic != Semantic.DISCRETIZED_NUMERICAL:
-      if self.num_discretized_numerical_bins is not None:
-        raise ValueError(
-            "Argument num_discretized_numerical_bins requires"
-            " semantic=DISCRETIZED_NUMERICAL."
-        )
-
-    if self.semantic not in [
-        Semantic.CATEGORICAL,
-        Semantic.CATEGORICAL_SET,
-    ]:
-      if self.max_vocab_count is not None:
-        raise ValueError(
-            "Argment max_vocab_count requires semantic=CATEGORICAL "
-            " or semantic=CATEGORICAL_SET."
-        )
-      if self.min_vocab_frequency is not None:
-        raise ValueError(
-            "Argment min_vocab_frequency requires semantic=CATEGORICAL "
-            " or semantic=CATEGORICAL_SET."
-        )
-
-    self._normalized_monotonic = _normalize_monotonic_constraint(self.monotonic)
-
-    if self.monotonic and self.semantic and self.semantic != Semantic.NUMERICAL:
-      raise ValueError(
-          f"Feature {self.name!r} with monotonic constraint is expected to have"
-          " semantic=NUMERICAL or semantic=None (default). Got"
-          f" semantic={self.semantic!r} instead."
-      )
-
-  @property
-  def normalized_monotonic(self) -> Optional[Monotonic]:
-    """Returns the normalized version of the "monotonic" attribute."""
-
-    return self._normalized_monotonic
-
-  def to_proto_column_guide(self) -> ds_pb.ColumnGuide:
-    """Creates a proto ColumnGuide from the given specification."""
-    guide = ds_pb.ColumnGuide(
-        # Only match the exact name
-        column_name_pattern=f"^{self.name}$",
-        categorial=ds_pb.CategoricalGuide(
-            max_vocab_count=self.max_vocab_count,
-            min_vocab_frequency=self.min_vocab_frequency,
-        ),
-        discretized_numerical=ds_pb.DiscretizedNumericalGuide(
-            maximum_num_bins=self.num_discretized_numerical_bins
-        ),
-    )
-    column_semantic = self.semantic
-    if column_semantic is not None:
-      guide.type = column_semantic.to_proto_type()
-    return guide
-
-  @classmethod
-  def from_column_def(cls, column_def: ColumnDef):
-    """Converts a ColumnDef to a Column."""
-    if isinstance(column_def, cls):
-      return column_def
-    if isinstance(column_def, str):
-      return Column(name=column_def)
-    if isinstance(column_def, tuple):
-      if (
-          len(column_def) == 2
-          and isinstance(column_def[0], str)
-          and isinstance(column_def[1], Semantic)
-      ):
-        return Column(name=column_def[0], semantic=column_def[1])
-    raise ValueError(
-        f"Unsupported column definition: {column_def}. Supported definitions:"
-        f" {ColumnDefs}"
-    )
-
-
-def normalize_column_defs(
-    columns: ColumnDefs,
-) -> NormalizedColumnDefs:
-  """Converts a user column set into a normalized list of columns.
-
-  Args:
-    columns: Columns as defined by the user.
-
-  Returns:
-    Normalized column definitions.
-  """
-
-  if columns is None:
-    return None
-
-  elif isinstance(columns, list):
-    return [_normalized_column(column) for column in columns]
-
-  else:
-    raise ValueError(
-        f"Unsupported column definition with type: {type(columns)}"
-    )
-
-
-def _normalized_column(
-    column: Union[Column, str, Tuple[str, Semantic]],
-) -> Column:
-  """Normalizes a single column."""
-
-  if isinstance(column, str):
-    # Raw column names
-    return Column(column)
-  elif isinstance(column, tuple):
-    # Tuples of column names and semantics
-    if (
-        len(column) != 2
-        or not isinstance(column[0], str)
-        or not isinstance(column[1], Semantic)
-    ):
-      raise ValueError(
-          "Column definition tuple should be a (name:str,"
-          f" semantic:Semantic). Instead, got {column}"
-      )
-    return Column(column[0], column[1])
-  elif isinstance(column, Column):
-    # An already normalized column
-    return column
-  else:
-    raise ValueError(f"Unsupported column item with type: {type(column)}")
-
-
-@dataclasses.dataclass
-class DataSpecInferenceArgs:
-  """Arguments for the construction of a dataset."""
-
-  columns: NormalizedColumnDefs
-  include_all_columns: bool
-  max_vocab_count: int
-  min_vocab_frequency: int
-  discretize_numerical_columns: bool
-  num_discretized_numerical_bins: int
-  max_num_scanned_rows_to_infer_semantic: int
-  max_num_scanned_rows_to_compute_statistics: int
-
-  def to_proto_guide(self) -> ds_pb.DataSpecificationGuide:
-    """Creates a proto DataSpecGuide for these arguments.
-
-    For consistency with in-memory datasets, the proto guide deactivates YDF's
-    tokenizer.
-
-    Returns:
-      A guide to be used for dataspec inference by C++ YDF.
-    """
-    ignore_columns_without_guides = (
-        False if self.columns is None else not self.include_all_columns
-    )
-    guide = ds_pb.DataSpecificationGuide(
-        ignore_columns_without_guides=ignore_columns_without_guides,
-        detect_numerical_as_discretized_numerical=self.discretize_numerical_columns,
-        default_column_guide=ds_pb.ColumnGuide(
-            categorial=ds_pb.CategoricalGuide(
-                max_vocab_count=self.max_vocab_count,
-                min_vocab_frequency=self.min_vocab_frequency,
-            ),
-            discretized_numerical=ds_pb.DiscretizedNumericalGuide(
-                maximum_num_bins=self.num_discretized_numerical_bins
-            ),
-        ),
-        allow_tokenization_for_inference_as_categorical_set=False,
-    )
-    if self.columns is not None:
-      for column_def in self.columns:
-        column = Column.from_column_def(column_def)
-        guide.column_guides.append(column.to_proto_column_guide())
-
-    guide.max_num_scanned_rows_to_guess_type = (
-        self.max_num_scanned_rows_to_infer_semantic
-    )
-    guide.max_num_scanned_rows_to_accumulate_statistics = (
-        self.max_num_scanned_rows_to_compute_statistics
-    )
-    return guide
-
-
-def categorical_column_dictionary_to_list(
-    column_spec: ds_pb.Column,
-) -> List[str]:
-  """Returns a list of string representation of dictionary items in a column.
-
-  If the categorical column is integerized (i.e., it does not contain a
-  dictionary), returns the string representation of the indices e.g. ["0", "1",
-  "2"].
-
-  Args:
-    column_spec: Dataspec column.
-  """
-
-  if column_spec.categorical.is_already_integerized:
-    return [
-        str(i) for i in range(column_spec.categorical.number_of_unique_values)
-    ]
-
-  items = [None] * column_spec.categorical.number_of_unique_values
-
-  for key, value in column_spec.categorical.items.items():
-    if items[value.index] is not None:
-      raise ValueError(
-          f"Invalid dictionary. Duplicated index {value.index} in dictionary"
-      )
-    items[value.index] = key
-
-  for index, value in enumerate(items):
-    if value is None:
-      raise ValueError(
-          f"Invalid dictionary. No value for index {index} "
-          f"in column {column_spec}"
-      )
-
-  return items  # pytype: disable=bad-return-type
-
-
-def get_all_columns(
-    available_columns: Sequence[str],
-    inference_args: DataSpecInferenceArgs,
-    required_columns: Optional[Sequence[str]],
-    unroll_feature_info: Dict[str, List[str]] = {},
-) -> Tuple[Sequence[Column], Dict[str, List[str]]]:
-  """Gets all the columns to use by the model / learner.
-
-  Args:
-    available_columns: All the available column names in the dataset.
-    inference_args: User configurations for the consumption of columns.
-    required_columns: List of columns that must be used by the model. If None,
-      defaults the columns specified in `inference_args`.  The order of the
-      columns is important. First should come the columns in
-      `inference_args.columns` in unchanged order, then any remaining columns in
-      the order they appear at in the data.
-    unroll_feature_info: Information about feature unrolling.
-
-  Returns:
-    The list of model input columns (This includes the required columns plus the
-    specified columns in the inference_args that are available), and the
-    actually used unrolled features.
-
-  Raises:
-    ValueError: One of the required columns is not available
-  """
-  available_columns_as_set = frozenset(available_columns)
-  required_columns_as_set = frozenset(required_columns or [])
-
-  if not required_columns_as_set.issubset(available_columns_as_set):
-    raise ValueError(
-        "One of the required columns was not found in the data. Required"
-        f" columns: {required_columns}, available columns: {available_columns}"
-    )
-
-  used_unroll_feature_info = {}
-
-  if inference_args.columns is None:
-    # The user did not filter on the column names, so we use all the columns.
-    specified_columns = [Column(col) for col in available_columns]
-    used_unroll_feature_info.update(unroll_feature_info)
-  else:
-    specified_columns = []
-    # Add the specified columns in order, but ignore those that do not exist.
-    for col in inference_args.columns:
-      if col.name in available_columns_as_set:
-        if col.name in unroll_feature_info:
-          raise ValueError(
-              f"Column {col.name!r} is available both natively and unrolled."
-          )
-        specified_columns.append(col)
-      elif col.name in unroll_feature_info:
-        if col.name in unroll_feature_info:
-          used_unroll_feature_info[col.name] = unroll_feature_info[col.name]
-        specified_columns.extend(
-            [Column(col) for col in unroll_feature_info[col.name]]
-        )
-      else:
-        if required_columns is None or col.name in required_columns_as_set:
-          raise ValueError(
-              f"Column {col.name!r} is required but was not found in the data."
-              f" Available columns: {available_columns}"
-          )
-
-  specified_columns_names = set(col.name for col in specified_columns)
-  if inference_args.include_all_columns and inference_args.columns is not None:
-    # The user specified the type of some of the columns, but asked for all the
-    # columns to be used.
-    used_unroll_feature_info.update(unroll_feature_info)
-    for col_name in available_columns:
-      if col_name not in specified_columns_names:
-        specified_columns.append(Column(col_name))
-        specified_columns_names.add(col_name)
-
-  # Add remaining available columns.
-  for col_name in available_columns:
-    if (
-        col_name in required_columns_as_set
-        and col_name not in specified_columns_names
-    ):
-      # Note: Required columns that are not already included, are not unrolled.
-      specified_columns.append(Column(col_name))
-      specified_columns_names.add(col_name)
-  return specified_columns, used_unroll_feature_info
-
-
-def priority(a: Any, b: Any) -> Any:
-  """Merge arguments with priority.
-
-  If "a" is not None, return "a".
-  If "a" is None, return "b".
-
-  Args:
-    a: High priority argument.
-    b: Low priority argument.
-
-  Returns:
-    Selected argument.
-  """
-  return a if a is not None else b
-
-
-def categorical_column_guide(
-    column: Column, inference_args: DataSpecInferenceArgs
-) -> Dict[str, Any]:
-  return {
-      "max_vocab_count": priority(
-          column.max_vocab_count, inference_args.max_vocab_count
-      ),
-      "min_vocab_frequency": priority(
-          column.min_vocab_frequency, inference_args.min_vocab_frequency
-      ),
-  }
-
-
-def column_defs_contains_column(column_name: str, columns: ColumnDefs) -> bool:
-  """Checks if the given ColumnDefs contain a column of the given name."""
-  if columns is None:
-    return False
-
-  elif isinstance(columns, list):
-    for item in columns:
-      if isinstance(item, str):
-        if item == column_name:
-          return True
-      elif isinstance(item, tuple):
-        if (
-            len(item) != 2
-            or not isinstance(item[0], str)
-            or not isinstance(item[1], Semantic)
-        ):
-          raise ValueError(
-              "Column definition tuple should be a (name:str,"
-              f" semantic:Semantic). Instead, got {item}"
-          )
-        if item[0] == column_name:
-          return True
-      elif isinstance(item, Column):
-        if item.name == column_name:
-          return True
-    return False
-  else:
-    raise ValueError(
-        f"Unsupported column definition with type: {type(columns)}"
-    )
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Dataspec utilities."""
+
+import dataclasses
+import enum
+import logging
+from typing import Any, Dict, List, Literal, Optional, Sequence, Tuple, Union
+
+import numpy as np
+
+from ydf.proto.dataset import data_spec_pb2 as ds_pb
+
+
+# The different ways a user can specify the columns of VerticalDataset.
+ColumnDef = Union["Column", str, Tuple[str, "Semantic"]]
+ColumnDefs = Optional[List[ColumnDef]]
+
+# Normalized version of "ColumnDefs".
+NormalizedColumnDefs = Optional[List["Column"]]
+
+# The key of the "out of vocabulary" (OOV) used by YDF.
+# Must match kOutOfDictionaryItemKey in
+# yggdrasil_decision_forests/dataset/data_spec.h
+YDF_OOD = "<OOD>"
+
+# Mapping between Numpy dtypes and YDF dtypes.
+_NP_DTYPE_TO_YDF_DTYPE = {
+    np.int8: ds_pb.DType.DTYPE_INT8,
+    np.int16: ds_pb.DType.DTYPE_INT16,
+    np.int32: ds_pb.DType.DTYPE_INT32,
+    np.int64: ds_pb.DType.DTYPE_INT64,
+    np.uint8: ds_pb.DType.DTYPE_UINT8,
+    np.uint16: ds_pb.DType.DTYPE_UINT16,
+    np.uint32: ds_pb.DType.DTYPE_UINT32,
+    np.uint64: ds_pb.DType.DTYPE_UINT64,
+    np.float16: ds_pb.DType.DTYPE_FLOAT16,
+    np.float32: ds_pb.DType.DTYPE_FLOAT32,
+    np.float64: ds_pb.DType.DTYPE_FLOAT64,
+    np.bool_: ds_pb.DType.DTYPE_BOOL,
+    np.string_: ds_pb.DType.DTYPE_BYTES,
+    np.str_: ds_pb.DType.DTYPE_BYTES,
+    np.bytes_: ds_pb.DType.DTYPE_BYTES,
+    np.object_: ds_pb.DType.DTYPE_BYTES,
+}
+
+NP_SUPPORTED_INT_DTYPE = [
+    np.int8,
+    np.int16,
+    np.int32,
+    np.int64,
+    np.uint8,
+    np.uint16,
+    np.uint32,
+    np.uint64,
+]
+
+NP_SUPPORTED_FLOAT_DTYPE = [
+    np.float16,
+    np.float32,
+    np.float64,
+]
+
+
+def np_dtype_to_ydf_dtype(np_dtype: np.dtype) -> Optional["ds_pb.DType"]:
+  """Converts a Numpy dtype to a YDF dtype.
+
+  If the numpy dtype has no matching ydf dtype, returns None.
+
+  Args:
+    np_dtype: Numpy dtype.
+
+  Returns:
+    YDF dtype.
+  """
+
+  if hasattr(np_dtype, "type"):
+    ydf_dtype = _NP_DTYPE_TO_YDF_DTYPE.get(np_dtype.type)
+  else:
+    ydf_dtype = _NP_DTYPE_TO_YDF_DTYPE.get(np_dtype)
+
+  if ydf_dtype is None:
+    raise ValueError(f"ydf_dtype: {ydf_dtype!r} {np_dtype!r}")
+
+  return ydf_dtype
+
+
+class Semantic(enum.Enum):
+  """Semantic (e.g. numerical, categorical) of a column.
+
+  Determines how a column is interpreted by the model.
+  Similar to the "ColumnType" of YDF's DataSpecification.
+
+  Attributes:
+    NUMERICAL: Numerical value. Generally for quantities or counts with full
+      ordering. For example, the age of a person, or the number of items in a
+      bag. Can be a float or an integer.  Missing values are represented by
+      math.nan.
+    CATEGORICAL: A categorical value. Generally for a type/class in finite set
+      of possible values without ordering. For example, the color RED in the set
+      {RED, BLUE, GREEN}. Can be a string or an integer.  Missing values are
+      represented by "" (empty sting) or value -2. An out-of-vocabulary value
+      (i.e. a value that was never seen in training) is represented by any new
+      string value or the value -1. Integer categorical values: (1) The training
+      logic and model representation is optimized with the assumption that
+      values are dense. (2) Internally, the value is stored as int32. The values
+      should be <~2B. (3) The number of possible values is computed
+      automatically from the training dataset. During inference, integer values
+      greater than any value seen during training will be treated as
+      out-of-vocabulary. (4) Minimum frequency and maximum vocabulary size
+      constraints do not apply.
+    HASH: The hash of a string value. Used when only the equality between values
+      is important (not the value itself). Currently, only used for groups in
+      ranking problems e.g. the query in a query/document problem. The hashing
+      is computed with Google's farmhash and stored as an uint64.
+    CATEGORICAL_SET: Set of categorical values. Great to represent tokenized
+      texts. Can be a string. Unlike CATEGORICAL, the number of items in a
+      CATEGORICAL_SET can change between examples. The order of values inside a
+      feature values does not matter.
+    BOOLEAN: Boolean value. Can be a float or an integer. Missing values are
+      represented by math.nan.  If a numerical tensor contains multiple values,
+      its size should be constant, and each dimension isthreaded independently
+      (and each dimension should always have the same "meaning").
+    DISCRETIZED_NUMERICAL: Numerical values automatically discretized into bins.
+      Discretized numerical columns are faster to train than (non-discretized)
+      numerical columns. If the number of unique values of these columns is
+      lower than the number of bins, the discretization is lossless from the
+      point of view of the model. If the number of unique values of this columns
+      is greater than the number of bins, the discretization is lossy from the
+      point of view of the model. Lossy discretization can reduce and sometime
+      increase (due to regularization) the quality of the model.
+  """
+
+  NUMERICAL = 1
+  CATEGORICAL = 2
+  HASH = 3
+  CATEGORICAL_SET = 4
+  BOOLEAN = 5
+  DISCRETIZED_NUMERICAL = 6
+
+  def to_proto_type(self) -> ds_pb.ColumnType:
+    if self in SEMANTIC_TO_PROTO:
+      return SEMANTIC_TO_PROTO[self]
+    else:
+      raise NotImplementedError(f"Unsupported semantic {self}")
+
+  @classmethod
+  def from_proto_type(cls, column_type: ds_pb.ColumnType):
+    if column_type in PROTO_TO_SEMANTIC:
+      return PROTO_TO_SEMANTIC[column_type]
+    else:
+      raise NotImplementedError(f"Unsupported semantic {column_type}")
+
+
+# Mappings between semantic enum in python and in protobuffer and vice versa.
+SEMANTIC_TO_PROTO = {
+    Semantic.NUMERICAL: ds_pb.NUMERICAL,
+    Semantic.CATEGORICAL: ds_pb.CATEGORICAL,
+    Semantic.HASH: ds_pb.HASH,
+    Semantic.CATEGORICAL_SET: ds_pb.CATEGORICAL_SET,
+    Semantic.BOOLEAN: ds_pb.BOOLEAN,
+    Semantic.DISCRETIZED_NUMERICAL: ds_pb.DISCRETIZED_NUMERICAL,
+}
+PROTO_TO_SEMANTIC = {v: k for k, v in SEMANTIC_TO_PROTO.items()}
+
+
+class Monotonic(enum.Enum):
+  """Monotonic constraint between a feature and the model output."""
+
+  INCREASING = 1
+  DECREASING = 2
+
+
+# Map between integer monotonic constraints (as commonly used by decision
+# forests libraries) and Monotonic enum value.
+_INTEGER_MONOTONIC_TUPLES = (
+    (0, None),
+    (1, Monotonic.INCREASING),
+    (-1, Monotonic.DECREASING),
+)
+
+
+def _build_integer_monotonic_map() -> Dict[int, Optional[Monotonic]]:
+  """Returns a mapping between integer monotonic constraints and enum value.
+
+  The returned value is always the same. So, when possible, create and reuse
+  the result instead of calling "_build_integer_monotonic_map" multiple times.
+  """
+
+  return {key: value for key, value in _INTEGER_MONOTONIC_TUPLES}
+
+
+# Various ways for a user to specify a monotonic constraint.
+MonotonicConstraint = Optional[Union[Monotonic, Literal[-1, 0, +1]]]
+
+
+def _normalize_monotonic_constraint(
+    constraint: MonotonicConstraint,
+) -> Optional[Monotonic]:
+  """Normalizes monotonic constraints provided by the user.
+
+  Args:
+    constraint: User monotonic constraints.
+
+  Returns:
+    Normalized monotonic constraint.
+
+  Raises:
+    ValueError: If the user input is not a valid monotonic constraint.
+  """
+
+  if isinstance(constraint, int):
+    monotonic_map = _build_integer_monotonic_map()
+    if constraint not in monotonic_map:
+      raise ValueError(
+          "monotonic argument provided as integer should be one of"
+          f" {list(monotonic_map)!r}. Got {constraint!r} instead"
+      )
+    constraint = monotonic_map[constraint]
+
+  if constraint is None or isinstance(constraint, Monotonic):
+    return constraint
+
+  raise ValueError(
+      "Unexpected monotonic value. monotonic value can be 0, +1, -1, None,"
+      " Monotonic.INCREASING, or Monotonic.DECREASING. Got"
+      f" {constraint!r} instead"
+  )
+
+
+@dataclasses.dataclass
+class Column(object):
+  """Semantic and parameters for a single column.
+
+  This class allows to:
+    1. Limit the input features of the model.
+    2. Manually specify the semantic of a feature.
+    3. Specify feature specific hyper-parameters.
+
+  Attributes:
+    name: The name of the column or feature.
+    semantic: Semantic of the column. If None, the semantic is automatically
+      determined. The semantic controls how a column is interpreted by a model.
+      Using the wrong semantic (e.g. numerical instead of categorical) will hurt
+      your model"s quality.
+    max_vocab_count: For CATEGORICAL and CATEGORICAL_SET columns only. Number of
+      unique categorical values stored as string. If more categorical values are
+      present, the least frequent values are grouped into a Out-of-vocabulary
+      item. Reducing the value can improve or hurt the model. If max_vocab_count
+      = -1, the number of values in the column is not limited.
+    min_vocab_frequency: For CATEGORICAL and CATEGORICAL_SET columns only.
+      Minimum number of occurrence of a categorical value. Values present less
+      than "min_vocab_frequency" times in the training dataset are treated as
+      "Out-of-vocabulary".
+    num_discretized_numerical_bins: For DISCRETIZED_NUMERICAL columns only.
+      Number of bins used to discretize DISCRETIZED_NUMERICAL columns.
+    monotonic: Monotonic constraints between the feature and the model output.
+      Use `None` (default; or 0) for an unconstrained feature. Use
+      `Monotonic.INCREASING` (or +1) to ensure the model is monotonically
+      increasing with the features. Use `Monotonic.DECREASING` (or -1) to ensure
+      the model is monotonically decreasing with the features.
+  """
+
+  name: str
+  semantic: Optional[Semantic] = None
+  max_vocab_count: Optional[int] = None
+  min_vocab_frequency: Optional[int] = None
+  num_discretized_numerical_bins: Optional[int] = None
+  monotonic: MonotonicConstraint = None
+
+  def __post_init__(self):
+    # Check matching between hyper-parameters and semantic.
+    if self.semantic != Semantic.DISCRETIZED_NUMERICAL:
+      if self.num_discretized_numerical_bins is not None:
+        raise ValueError(
+            "Argument num_discretized_numerical_bins requires"
+            " semantic=DISCRETIZED_NUMERICAL."
+        )
+
+    if self.semantic not in [
+        Semantic.CATEGORICAL,
+        Semantic.CATEGORICAL_SET,
+    ]:
+      if self.max_vocab_count is not None:
+        raise ValueError(
+            "Argment max_vocab_count requires semantic=CATEGORICAL "
+            " or semantic=CATEGORICAL_SET."
+        )
+      if self.min_vocab_frequency is not None:
+        raise ValueError(
+            "Argment min_vocab_frequency requires semantic=CATEGORICAL "
+            " or semantic=CATEGORICAL_SET."
+        )
+
+    self._normalized_monotonic = _normalize_monotonic_constraint(self.monotonic)
+
+    if self.monotonic and self.semantic and self.semantic != Semantic.NUMERICAL:
+      raise ValueError(
+          f"Feature {self.name!r} with monotonic constraint is expected to have"
+          " semantic=NUMERICAL or semantic=None (default). Got"
+          f" semantic={self.semantic!r} instead."
+      )
+
+  @property
+  def normalized_monotonic(self) -> Optional[Monotonic]:
+    """Returns the normalized version of the "monotonic" attribute."""
+
+    return self._normalized_monotonic
+
+  def to_proto_column_guide(self) -> ds_pb.ColumnGuide:
+    """Creates a proto ColumnGuide from the given specification."""
+    guide = ds_pb.ColumnGuide(
+        # Only match the exact name
+        column_name_pattern=f"^{self.name}$",
+        categorial=ds_pb.CategoricalGuide(
+            max_vocab_count=self.max_vocab_count,
+            min_vocab_frequency=self.min_vocab_frequency,
+        ),
+        discretized_numerical=ds_pb.DiscretizedNumericalGuide(
+            maximum_num_bins=self.num_discretized_numerical_bins
+        ),
+    )
+    column_semantic = self.semantic
+    if column_semantic is not None:
+      guide.type = column_semantic.to_proto_type()
+    return guide
+
+  @classmethod
+  def from_column_def(cls, column_def: ColumnDef):
+    """Converts a ColumnDef to a Column."""
+    if isinstance(column_def, cls):
+      return column_def
+    if isinstance(column_def, str):
+      return Column(name=column_def)
+    if isinstance(column_def, tuple):
+      if (
+          len(column_def) == 2
+          and isinstance(column_def[0], str)
+          and isinstance(column_def[1], Semantic)
+      ):
+        return Column(name=column_def[0], semantic=column_def[1])
+    raise ValueError(
+        f"Unsupported column definition: {column_def}. Supported definitions:"
+        f" {ColumnDefs}"
+    )
+
+
+def normalize_column_defs(
+    columns: ColumnDefs,
+) -> NormalizedColumnDefs:
+  """Converts a user column set into a normalized list of columns.
+
+  Args:
+    columns: Columns as defined by the user.
+
+  Returns:
+    Normalized column definitions.
+  """
+
+  if columns is None:
+    return None
+
+  elif isinstance(columns, list):
+    return [_normalized_column(column) for column in columns]
+
+  else:
+    raise ValueError(
+        f"Unsupported column definition with type: {type(columns)}"
+    )
+
+
+def _normalized_column(
+    column: Union[Column, str, Tuple[str, Semantic]],
+) -> Column:
+  """Normalizes a single column."""
+
+  if isinstance(column, str):
+    # Raw column names
+    return Column(column)
+  elif isinstance(column, tuple):
+    # Tuples of column names and semantics
+    if (
+        len(column) != 2
+        or not isinstance(column[0], str)
+        or not isinstance(column[1], Semantic)
+    ):
+      raise ValueError(
+          "Column definition tuple should be a (name:str,"
+          f" semantic:Semantic). Instead, got {column}"
+      )
+    return Column(column[0], column[1])
+  elif isinstance(column, Column):
+    # An already normalized column
+    return column
+  else:
+    raise ValueError(f"Unsupported column item with type: {type(column)}")
+
+
+@dataclasses.dataclass
+class DataSpecInferenceArgs:
+  """Arguments for the construction of a dataset."""
+
+  columns: NormalizedColumnDefs
+  include_all_columns: bool
+  max_vocab_count: int
+  min_vocab_frequency: int
+  discretize_numerical_columns: bool
+  num_discretized_numerical_bins: int
+  max_num_scanned_rows_to_infer_semantic: int
+  max_num_scanned_rows_to_compute_statistics: int
+
+  def to_proto_guide(self) -> ds_pb.DataSpecificationGuide:
+    """Creates a proto DataSpecGuide for these arguments.
+
+    For consistency with in-memory datasets, the proto guide deactivates YDF's
+    tokenizer.
+
+    Returns:
+      A guide to be used for dataspec inference by C++ YDF.
+    """
+    ignore_columns_without_guides = (
+        False if self.columns is None else not self.include_all_columns
+    )
+    guide = ds_pb.DataSpecificationGuide(
+        ignore_columns_without_guides=ignore_columns_without_guides,
+        detect_numerical_as_discretized_numerical=self.discretize_numerical_columns,
+        default_column_guide=ds_pb.ColumnGuide(
+            categorial=ds_pb.CategoricalGuide(
+                max_vocab_count=self.max_vocab_count,
+                min_vocab_frequency=self.min_vocab_frequency,
+            ),
+            discretized_numerical=ds_pb.DiscretizedNumericalGuide(
+                maximum_num_bins=self.num_discretized_numerical_bins
+            ),
+        ),
+        allow_tokenization_for_inference_as_categorical_set=False,
+    )
+    if self.columns is not None:
+      for column_def in self.columns:
+        column = Column.from_column_def(column_def)
+        guide.column_guides.append(column.to_proto_column_guide())
+
+    guide.max_num_scanned_rows_to_guess_type = (
+        self.max_num_scanned_rows_to_infer_semantic
+    )
+    guide.max_num_scanned_rows_to_accumulate_statistics = (
+        self.max_num_scanned_rows_to_compute_statistics
+    )
+    return guide
+
+
+def categorical_column_dictionary_to_list(
+    column_spec: ds_pb.Column,
+) -> List[str]:
+  """Returns a list of string representation of dictionary items in a column.
+
+  If the categorical column is integerized (i.e., it does not contain a
+  dictionary), returns the string representation of the indices e.g. ["0", "1",
+  "2"].
+
+  Args:
+    column_spec: Dataspec column.
+  """
+
+  if column_spec.categorical.is_already_integerized:
+    return [
+        str(i) for i in range(column_spec.categorical.number_of_unique_values)
+    ]
+
+  items = [None] * column_spec.categorical.number_of_unique_values
+
+  for key, value in column_spec.categorical.items.items():
+    if items[value.index] is not None:
+      raise ValueError(
+          f"Invalid dictionary. Duplicated index {value.index} in dictionary"
+      )
+    items[value.index] = key
+
+  for index, value in enumerate(items):
+    if value is None:
+      raise ValueError(
+          f"Invalid dictionary. No value for index {index} "
+          f"in column {column_spec}"
+      )
+
+  return items  # pytype: disable=bad-return-type
+
+
+def get_all_columns(
+    available_columns: Sequence[str],
+    inference_args: DataSpecInferenceArgs,
+    required_columns: Optional[Sequence[str]],
+    unroll_feature_info: Dict[str, List[str]] = {},
+) -> Tuple[Sequence[Column], Dict[str, List[str]]]:
+  """Gets all the columns to use by the model / learner.
+
+  Args:
+    available_columns: All the available column names in the dataset.
+    inference_args: User configurations for the consumption of columns.
+    required_columns: List of columns that must be used by the model. If None,
+      defaults the columns specified in `inference_args`.  The order of the
+      columns is important. First should come the columns in
+      `inference_args.columns` in unchanged order, then any remaining columns in
+      the order they appear at in the data.
+    unroll_feature_info: Information about feature unrolling.
+
+  Returns:
+    The list of model input columns (This includes the required columns plus the
+    specified columns in the inference_args that are available), and the
+    actually used unrolled features.
+
+  Raises:
+    ValueError: One of the required columns is not available
+  """
+  available_columns_as_set = frozenset(available_columns)
+  required_columns_as_set = frozenset(required_columns or [])
+
+  if not required_columns_as_set.issubset(available_columns_as_set):
+    raise ValueError(
+        "One of the required columns was not found in the data. Required"
+        f" columns: {required_columns}, available columns: {available_columns}"
+    )
+
+  used_unroll_feature_info = {}
+
+  if inference_args.columns is None:
+    # The user did not filter on the column names, so we use all the columns.
+    specified_columns = [Column(col) for col in available_columns]
+    used_unroll_feature_info.update(unroll_feature_info)
+  else:
+    specified_columns = []
+    # Add the specified columns in order, but ignore those that do not exist.
+    for col in inference_args.columns:
+      if col.name in available_columns_as_set:
+        if col.name in unroll_feature_info:
+          raise ValueError(
+              f"Column {col.name!r} is available both natively and unrolled."
+          )
+        specified_columns.append(col)
+      elif col.name in unroll_feature_info:
+        if col.name in unroll_feature_info:
+          used_unroll_feature_info[col.name] = unroll_feature_info[col.name]
+        specified_columns.extend(
+            [Column(col) for col in unroll_feature_info[col.name]]
+        )
+      else:
+        if required_columns is None or col.name in required_columns_as_set:
+          raise ValueError(
+              f"Column {col.name!r} is required but was not found in the data."
+              f" Available columns: {available_columns}"
+          )
+
+  specified_columns_names = set(col.name for col in specified_columns)
+  if inference_args.include_all_columns and inference_args.columns is not None:
+    # The user specified the type of some of the columns, but asked for all the
+    # columns to be used.
+    used_unroll_feature_info.update(unroll_feature_info)
+    for col_name in available_columns:
+      if col_name not in specified_columns_names:
+        specified_columns.append(Column(col_name))
+        specified_columns_names.add(col_name)
+
+  # Add remaining available columns.
+  for col_name in available_columns:
+    if (
+        col_name in required_columns_as_set
+        and col_name not in specified_columns_names
+    ):
+      # Note: Required columns that are not already included, are not unrolled.
+      specified_columns.append(Column(col_name))
+      specified_columns_names.add(col_name)
+  return specified_columns, used_unroll_feature_info
+
+
+def priority(a: Any, b: Any) -> Any:
+  """Merge arguments with priority.
+
+  If "a" is not None, return "a".
+  If "a" is None, return "b".
+
+  Args:
+    a: High priority argument.
+    b: Low priority argument.
+
+  Returns:
+    Selected argument.
+  """
+  return a if a is not None else b
+
+
+def categorical_column_guide(
+    column: Column, inference_args: DataSpecInferenceArgs
+) -> Dict[str, Any]:
+  return {
+      "max_vocab_count": priority(
+          column.max_vocab_count, inference_args.max_vocab_count
+      ),
+      "min_vocab_frequency": priority(
+          column.min_vocab_frequency, inference_args.min_vocab_frequency
+      ),
+  }
+
+
+def column_defs_contains_column(column_name: str, columns: ColumnDefs) -> bool:
+  """Checks if the given ColumnDefs contain a column of the given name."""
+  if columns is None:
+    return False
+
+  elif isinstance(columns, list):
+    for item in columns:
+      if isinstance(item, str):
+        if item == column_name:
+          return True
+      elif isinstance(item, tuple):
+        if (
+            len(item) != 2
+            or not isinstance(item[0], str)
+            or not isinstance(item[1], Semantic)
+        ):
+          raise ValueError(
+              "Column definition tuple should be a (name:str,"
+              f" semantic:Semantic). Instead, got {item}"
+          )
+        if item[0] == column_name:
+          return True
+      elif isinstance(item, Column):
+        if item.name == column_name:
+          return True
+    return False
+  else:
+    raise ValueError(
+        f"Unsupported column definition with type: {type(columns)}"
+    )
```

## ydf/dataset/dataspec_test.py

```diff
@@ -1,391 +1,391 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Test dataspec utilities."""
-
-from absl.testing import absltest
-from absl.testing import parameterized
-
-from yggdrasil_decision_forests.dataset import data_spec_pb2 as ds_pb
-from ydf.dataset import dataspec as dataspec_lib
-
-Semantic = dataspec_lib.Semantic
-VocabValue = ds_pb.CategoricalSpec.VocabValue
-Column = dataspec_lib.Column
-Monotonic = dataspec_lib.Monotonic
-DataSpecInferenceArgs = dataspec_lib.DataSpecInferenceArgs
-
-
-def toy_dataspec():
-  return ds_pb.DataSpecification(
-      columns=[
-          ds_pb.Column(
-              name="f0",
-              type=ds_pb.ColumnType.NUMERICAL,
-          ),
-          ds_pb.Column(
-              name="f1",
-              type=ds_pb.ColumnType.CATEGORICAL,
-              categorical=ds_pb.CategoricalSpec(
-                  number_of_unique_values=3,
-                  items={
-                      "<OOD>": ds_pb.CategoricalSpec.VocabValue(index=0),
-                      "x": ds_pb.CategoricalSpec.VocabValue(index=1),
-                      "y": ds_pb.CategoricalSpec.VocabValue(index=2),
-                  },
-              ),
-          ),
-          ds_pb.Column(
-              name="f2",
-              type=ds_pb.ColumnType.CATEGORICAL,
-              categorical=ds_pb.CategoricalSpec(
-                  number_of_unique_values=3,
-                  is_already_integerized=True,
-              ),
-          ),
-          ds_pb.Column(
-              name="f3",
-              type=ds_pb.ColumnType.DISCRETIZED_NUMERICAL,
-              discretized_numerical=ds_pb.DiscretizedNumericalSpec(
-                  boundaries=[0, 1, 2],
-              ),
-          ),
-          ds_pb.Column(
-              name="f4_invalid",
-              type=ds_pb.ColumnType.CATEGORICAL,
-              categorical=ds_pb.CategoricalSpec(
-                  number_of_unique_values=3,
-                  items={
-                      "<OOD>": ds_pb.CategoricalSpec.VocabValue(index=0),
-                      "x": ds_pb.CategoricalSpec.VocabValue(index=1),
-                      "y": ds_pb.CategoricalSpec.VocabValue(index=1),
-                  },
-              ),
-          ),
-          ds_pb.Column(
-              name="f5_invalid",
-              type=ds_pb.ColumnType.CATEGORICAL,
-              categorical=ds_pb.CategoricalSpec(
-                  number_of_unique_values=3,
-                  items={
-                      "<OOD>": ds_pb.CategoricalSpec.VocabValue(index=0),
-                      "y": ds_pb.CategoricalSpec.VocabValue(index=2),
-                  },
-              ),
-          ),
-      ]
-  )
-
-
-class DataspecTest(absltest.TestCase):
-
-  def test_categorical_column_dictionary_to_list(self):
-    dataspec = toy_dataspec()
-
-    self.assertEqual(
-        dataspec_lib.categorical_column_dictionary_to_list(dataspec.columns[1]),
-        ["<OOD>", "x", "y"],
-    )
-
-    self.assertEqual(
-        dataspec_lib.categorical_column_dictionary_to_list(dataspec.columns[2]),
-        ["0", "1", "2"],
-    )
-
-  def test_categorical_column_dictionary_to_list_issues(self):
-    dataspec = toy_dataspec()
-    with self.assertRaisesRegex(ValueError, "Duplicated index"):
-      dataspec_lib.categorical_column_dictionary_to_list(dataspec.columns[4])
-    with self.assertRaisesRegex(ValueError, "No value for index"):
-      dataspec_lib.categorical_column_dictionary_to_list(dataspec.columns[5])
-
-  def test_column_defs_contains_column(self):
-    column_name = "target"
-    self.assertFalse(
-        dataspec_lib.column_defs_contains_column(column_name, None)
-    )
-    str_defs_positive = ["foo", "target", "bar", "", "*"]
-    self.assertTrue(
-        dataspec_lib.column_defs_contains_column(column_name, str_defs_positive)
-    )
-    str_defs_negative = ["foo", "tar", "bar", "", "*"]
-    self.assertFalse(
-        dataspec_lib.column_defs_contains_column(column_name, str_defs_negative)
-    )
-    tuple_defs_positive = [
-        ("foo", Semantic.NUMERICAL),
-        ("target", Semantic.CATEGORICAL),
-    ]
-    self.assertTrue(
-        dataspec_lib.column_defs_contains_column(
-            column_name, tuple_defs_positive
-        )
-    )
-    tuple_defs_negative = [
-        ("foo", Semantic.NUMERICAL),
-        ("tar", Semantic.CATEGORICAL),
-    ]
-    self.assertFalse(
-        dataspec_lib.column_defs_contains_column(
-            column_name, tuple_defs_negative
-        )
-    )
-    column_defs_positive = [Column("foo"), Column("target")]
-    self.assertTrue(
-        dataspec_lib.column_defs_contains_column(
-            column_name, column_defs_positive
-        )
-    )
-    column_defs_negative = [Column("foo"), Column("tar")]
-    self.assertFalse(
-        dataspec_lib.column_defs_contains_column(
-            column_name, column_defs_negative
-        )
-    )
-
-  def test_categorical_column_guide(self):
-    self.assertEqual(
-        dataspec_lib.categorical_column_guide(
-            Column("a", Semantic.CATEGORICAL, max_vocab_count=3),
-            DataSpecInferenceArgs(
-                columns=[],
-                include_all_columns=False,
-                max_vocab_count=1,
-                min_vocab_frequency=2,
-                discretize_numerical_columns=False,
-                num_discretized_numerical_bins=1,
-                max_num_scanned_rows_to_infer_semantic=10000,
-                max_num_scanned_rows_to_compute_statistics=10000,
-            ),
-        ),
-        {"max_vocab_count": 3, "min_vocab_frequency": 2},
-    )
-
-  def test_priority(self):
-    self.assertEqual(dataspec_lib.priority(1, 2), 1)
-    self.assertEqual(dataspec_lib.priority(None, 2), 2)
-    self.assertIsNone(dataspec_lib.priority(None, None), None)
-
-  def test_get_all_columns(self):
-    self.assertEqual(
-        dataspec_lib.get_all_columns(
-            ["a", "b", "c", "d"],
-            DataSpecInferenceArgs(
-                columns=[
-                    Column("a"),
-                    Column("b", Semantic.NUMERICAL),
-                    Column("c", Semantic.CATEGORICAL),
-                ],
-                include_all_columns=False,
-                max_vocab_count=1,
-                min_vocab_frequency=1,
-                discretize_numerical_columns=False,
-                num_discretized_numerical_bins=1,
-                max_num_scanned_rows_to_infer_semantic=10000,
-                max_num_scanned_rows_to_compute_statistics=10000,
-            ),
-            required_columns=None,
-        )[0],
-        [
-            Column("a"),
-            Column("b", Semantic.NUMERICAL),
-            Column("c", Semantic.CATEGORICAL),
-        ],
-    )
-
-  def test_get_all_columns_include_all_columns(self):
-    self.assertEqual(
-        dataspec_lib.get_all_columns(
-            ["a", "b"],
-            DataSpecInferenceArgs(
-                columns=[Column("a")],
-                include_all_columns=True,
-                max_vocab_count=1,
-                min_vocab_frequency=1,
-                discretize_numerical_columns=False,
-                num_discretized_numerical_bins=1,
-                max_num_scanned_rows_to_infer_semantic=10000,
-                max_num_scanned_rows_to_compute_statistics=10000,
-            ),
-            required_columns=None,
-        )[0],
-        [
-            Column("a"),
-            Column("b"),
-        ],
-    )
-
-  def test_get_all_columns_missing(self):
-    with self.assertRaisesRegex(
-        ValueError, "Column 'b' is required but was not found in the data."
-    ):
-      dataspec_lib.get_all_columns(
-          ["a"],
-          DataSpecInferenceArgs(
-              columns=[Column("b")],
-              include_all_columns=True,
-              max_vocab_count=1,
-              min_vocab_frequency=1,
-              discretize_numerical_columns=False,
-              num_discretized_numerical_bins=1,
-              max_num_scanned_rows_to_infer_semantic=10000,
-              max_num_scanned_rows_to_compute_statistics=10000,
-          ),
-          required_columns=None,
-      )
-
-  def test_get_all_columns_required_missing(self):
-    with self.assertRaisesRegex(
-        ValueError, "One of the required columns was not found in the data."
-    ):
-      dataspec_lib.get_all_columns(
-          ["a"],
-          DataSpecInferenceArgs(
-              columns=[Column("a")],
-              include_all_columns=True,
-              max_vocab_count=1,
-              min_vocab_frequency=1,
-              discretize_numerical_columns=False,
-              num_discretized_numerical_bins=1,
-              max_num_scanned_rows_to_infer_semantic=10000,
-              max_num_scanned_rows_to_compute_statistics=10000,
-          ),
-          required_columns=["b"],
-      )[0]
-
-  def test_get_all_columns_does_not_require_all_specified(self):
-    self.assertEqual(
-        dataspec_lib.get_all_columns(
-            ["a"],
-            DataSpecInferenceArgs(
-                columns=[Column("b")],
-                include_all_columns=True,
-                max_vocab_count=1,
-                min_vocab_frequency=1,
-                discretize_numerical_columns=False,
-                num_discretized_numerical_bins=1,
-                max_num_scanned_rows_to_infer_semantic=10000,
-                max_num_scanned_rows_to_compute_statistics=10000,
-            ),
-            required_columns=[],
-        )[0],
-        [
-            Column("a"),
-        ],
-    )
-
-  def test_get_all_columns_specified_and_available_always_included(self):
-    self.assertEqual(
-        dataspec_lib.get_all_columns(
-            ["a"],
-            DataSpecInferenceArgs(
-                columns=[Column("a")],
-                include_all_columns=False,
-                max_vocab_count=1,
-                min_vocab_frequency=1,
-                discretize_numerical_columns=False,
-                num_discretized_numerical_bins=1,
-                max_num_scanned_rows_to_infer_semantic=10000,
-                max_num_scanned_rows_to_compute_statistics=10000,
-            ),
-            required_columns=[],
-        )[0],
-        [
-            Column("a"),
-        ],
-    )
-
-  def test_get_all_columns_with_unrolled_features(self):
-    columns, unroll_info = dataspec_lib.get_all_columns(
-        ["a.0", "a.1", "a.2", "b.0", "b.1", "b.2"],
-        DataSpecInferenceArgs(
-            columns=[Column("a")],
-            include_all_columns=False,
-            max_vocab_count=1,
-            min_vocab_frequency=1,
-            discretize_numerical_columns=False,
-            num_discretized_numerical_bins=1,
-            max_num_scanned_rows_to_infer_semantic=10000,
-            max_num_scanned_rows_to_compute_statistics=10000,
-        ),
-        required_columns=[],
-        unroll_feature_info={
-            "a": ["a.0", "a.1", "a.2"],
-            "b": ["b.0", "b.1", "b.2"],
-        },
-    )
-    self.assertEqual(
-        columns,
-        [Column("a.0"), Column("a.1"), Column("a.2")],
-    )
-    self.assertEqual(unroll_info, {"a": ["a.0", "a.1", "a.2"]})
-
-  def test_normalize_column_defs(self):
-    self.assertEqual(
-        dataspec_lib.normalize_column_defs([
-            "a",
-            ("b", Semantic.NUMERICAL),
-            Column("c"),
-            Column("d", Semantic.CATEGORICAL),
-        ]),
-        [
-            Column("a"),
-            Column("b", Semantic.NUMERICAL),
-            Column("c"),
-            Column("d", Semantic.CATEGORICAL),
-        ],
-    )
-
-  def test_normalize_column_defs_none(self):
-    self.assertIsNone(dataspec_lib.normalize_column_defs(None))
-
-
-class MonotonicTest(parameterized.TestCase):
-
-  @parameterized.parameters(
-      Monotonic.INCREASING,
-      Monotonic.DECREASING,
-  )
-  def test_already_normalized_value(self, value):
-    self.assertEqual(Column("f", monotonic=value).normalized_monotonic, value)
-
-  def test_already_normalized_value_none(self):
-    self.assertIsNone(Column("f", monotonic=None).normalized_monotonic)
-
-  @parameterized.parameters(
-      (+1, Monotonic.INCREASING),
-      (-1, Monotonic.DECREASING),
-  )
-  def test_normalize_value(self, non_normalized_value, normalized_value):
-    self.assertEqual(
-        Column("f", monotonic=non_normalized_value).normalized_monotonic,
-        normalized_value,
-    )
-
-  def test_normalize_value_none(self):
-    self.assertIsNone(Column("f", monotonic=0).normalized_monotonic)
-
-  def test_good_semantic(self):
-    _ = Column("f", monotonic=+1)
-    _ = Column("f", semantic=Semantic.NUMERICAL, monotonic=+1)
-
-  def test_bad_semantic(self):
-    with self.assertRaisesRegex(
-        ValueError, "with monotonic constraint is expected to have"
-    ):
-      _ = Column("feature", semantic=Semantic.CATEGORICAL, monotonic=+1)
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Test dataspec utilities."""
+
+from absl.testing import absltest
+from absl.testing import parameterized
+
+from ydf.proto.dataset import data_spec_pb2 as ds_pb
+from ydf.dataset import dataspec as dataspec_lib
+
+Semantic = dataspec_lib.Semantic
+VocabValue = ds_pb.CategoricalSpec.VocabValue
+Column = dataspec_lib.Column
+Monotonic = dataspec_lib.Monotonic
+DataSpecInferenceArgs = dataspec_lib.DataSpecInferenceArgs
+
+
+def toy_dataspec():
+  return ds_pb.DataSpecification(
+      columns=[
+          ds_pb.Column(
+              name="f0",
+              type=ds_pb.ColumnType.NUMERICAL,
+          ),
+          ds_pb.Column(
+              name="f1",
+              type=ds_pb.ColumnType.CATEGORICAL,
+              categorical=ds_pb.CategoricalSpec(
+                  number_of_unique_values=3,
+                  items={
+                      "<OOD>": ds_pb.CategoricalSpec.VocabValue(index=0),
+                      "x": ds_pb.CategoricalSpec.VocabValue(index=1),
+                      "y": ds_pb.CategoricalSpec.VocabValue(index=2),
+                  },
+              ),
+          ),
+          ds_pb.Column(
+              name="f2",
+              type=ds_pb.ColumnType.CATEGORICAL,
+              categorical=ds_pb.CategoricalSpec(
+                  number_of_unique_values=3,
+                  is_already_integerized=True,
+              ),
+          ),
+          ds_pb.Column(
+              name="f3",
+              type=ds_pb.ColumnType.DISCRETIZED_NUMERICAL,
+              discretized_numerical=ds_pb.DiscretizedNumericalSpec(
+                  boundaries=[0, 1, 2],
+              ),
+          ),
+          ds_pb.Column(
+              name="f4_invalid",
+              type=ds_pb.ColumnType.CATEGORICAL,
+              categorical=ds_pb.CategoricalSpec(
+                  number_of_unique_values=3,
+                  items={
+                      "<OOD>": ds_pb.CategoricalSpec.VocabValue(index=0),
+                      "x": ds_pb.CategoricalSpec.VocabValue(index=1),
+                      "y": ds_pb.CategoricalSpec.VocabValue(index=1),
+                  },
+              ),
+          ),
+          ds_pb.Column(
+              name="f5_invalid",
+              type=ds_pb.ColumnType.CATEGORICAL,
+              categorical=ds_pb.CategoricalSpec(
+                  number_of_unique_values=3,
+                  items={
+                      "<OOD>": ds_pb.CategoricalSpec.VocabValue(index=0),
+                      "y": ds_pb.CategoricalSpec.VocabValue(index=2),
+                  },
+              ),
+          ),
+      ]
+  )
+
+
+class DataspecTest(absltest.TestCase):
+
+  def test_categorical_column_dictionary_to_list(self):
+    dataspec = toy_dataspec()
+
+    self.assertEqual(
+        dataspec_lib.categorical_column_dictionary_to_list(dataspec.columns[1]),
+        ["<OOD>", "x", "y"],
+    )
+
+    self.assertEqual(
+        dataspec_lib.categorical_column_dictionary_to_list(dataspec.columns[2]),
+        ["0", "1", "2"],
+    )
+
+  def test_categorical_column_dictionary_to_list_issues(self):
+    dataspec = toy_dataspec()
+    with self.assertRaisesRegex(ValueError, "Duplicated index"):
+      dataspec_lib.categorical_column_dictionary_to_list(dataspec.columns[4])
+    with self.assertRaisesRegex(ValueError, "No value for index"):
+      dataspec_lib.categorical_column_dictionary_to_list(dataspec.columns[5])
+
+  def test_column_defs_contains_column(self):
+    column_name = "target"
+    self.assertFalse(
+        dataspec_lib.column_defs_contains_column(column_name, None)
+    )
+    str_defs_positive = ["foo", "target", "bar", "", "*"]
+    self.assertTrue(
+        dataspec_lib.column_defs_contains_column(column_name, str_defs_positive)
+    )
+    str_defs_negative = ["foo", "tar", "bar", "", "*"]
+    self.assertFalse(
+        dataspec_lib.column_defs_contains_column(column_name, str_defs_negative)
+    )
+    tuple_defs_positive = [
+        ("foo", Semantic.NUMERICAL),
+        ("target", Semantic.CATEGORICAL),
+    ]
+    self.assertTrue(
+        dataspec_lib.column_defs_contains_column(
+            column_name, tuple_defs_positive
+        )
+    )
+    tuple_defs_negative = [
+        ("foo", Semantic.NUMERICAL),
+        ("tar", Semantic.CATEGORICAL),
+    ]
+    self.assertFalse(
+        dataspec_lib.column_defs_contains_column(
+            column_name, tuple_defs_negative
+        )
+    )
+    column_defs_positive = [Column("foo"), Column("target")]
+    self.assertTrue(
+        dataspec_lib.column_defs_contains_column(
+            column_name, column_defs_positive
+        )
+    )
+    column_defs_negative = [Column("foo"), Column("tar")]
+    self.assertFalse(
+        dataspec_lib.column_defs_contains_column(
+            column_name, column_defs_negative
+        )
+    )
+
+  def test_categorical_column_guide(self):
+    self.assertEqual(
+        dataspec_lib.categorical_column_guide(
+            Column("a", Semantic.CATEGORICAL, max_vocab_count=3),
+            DataSpecInferenceArgs(
+                columns=[],
+                include_all_columns=False,
+                max_vocab_count=1,
+                min_vocab_frequency=2,
+                discretize_numerical_columns=False,
+                num_discretized_numerical_bins=1,
+                max_num_scanned_rows_to_infer_semantic=10000,
+                max_num_scanned_rows_to_compute_statistics=10000,
+            ),
+        ),
+        {"max_vocab_count": 3, "min_vocab_frequency": 2},
+    )
+
+  def test_priority(self):
+    self.assertEqual(dataspec_lib.priority(1, 2), 1)
+    self.assertEqual(dataspec_lib.priority(None, 2), 2)
+    self.assertIsNone(dataspec_lib.priority(None, None), None)
+
+  def test_get_all_columns(self):
+    self.assertEqual(
+        dataspec_lib.get_all_columns(
+            ["a", "b", "c", "d"],
+            DataSpecInferenceArgs(
+                columns=[
+                    Column("a"),
+                    Column("b", Semantic.NUMERICAL),
+                    Column("c", Semantic.CATEGORICAL),
+                ],
+                include_all_columns=False,
+                max_vocab_count=1,
+                min_vocab_frequency=1,
+                discretize_numerical_columns=False,
+                num_discretized_numerical_bins=1,
+                max_num_scanned_rows_to_infer_semantic=10000,
+                max_num_scanned_rows_to_compute_statistics=10000,
+            ),
+            required_columns=None,
+        )[0],
+        [
+            Column("a"),
+            Column("b", Semantic.NUMERICAL),
+            Column("c", Semantic.CATEGORICAL),
+        ],
+    )
+
+  def test_get_all_columns_include_all_columns(self):
+    self.assertEqual(
+        dataspec_lib.get_all_columns(
+            ["a", "b"],
+            DataSpecInferenceArgs(
+                columns=[Column("a")],
+                include_all_columns=True,
+                max_vocab_count=1,
+                min_vocab_frequency=1,
+                discretize_numerical_columns=False,
+                num_discretized_numerical_bins=1,
+                max_num_scanned_rows_to_infer_semantic=10000,
+                max_num_scanned_rows_to_compute_statistics=10000,
+            ),
+            required_columns=None,
+        )[0],
+        [
+            Column("a"),
+            Column("b"),
+        ],
+    )
+
+  def test_get_all_columns_missing(self):
+    with self.assertRaisesRegex(
+        ValueError, "Column 'b' is required but was not found in the data."
+    ):
+      dataspec_lib.get_all_columns(
+          ["a"],
+          DataSpecInferenceArgs(
+              columns=[Column("b")],
+              include_all_columns=True,
+              max_vocab_count=1,
+              min_vocab_frequency=1,
+              discretize_numerical_columns=False,
+              num_discretized_numerical_bins=1,
+              max_num_scanned_rows_to_infer_semantic=10000,
+              max_num_scanned_rows_to_compute_statistics=10000,
+          ),
+          required_columns=None,
+      )
+
+  def test_get_all_columns_required_missing(self):
+    with self.assertRaisesRegex(
+        ValueError, "One of the required columns was not found in the data."
+    ):
+      dataspec_lib.get_all_columns(
+          ["a"],
+          DataSpecInferenceArgs(
+              columns=[Column("a")],
+              include_all_columns=True,
+              max_vocab_count=1,
+              min_vocab_frequency=1,
+              discretize_numerical_columns=False,
+              num_discretized_numerical_bins=1,
+              max_num_scanned_rows_to_infer_semantic=10000,
+              max_num_scanned_rows_to_compute_statistics=10000,
+          ),
+          required_columns=["b"],
+      )[0]
+
+  def test_get_all_columns_does_not_require_all_specified(self):
+    self.assertEqual(
+        dataspec_lib.get_all_columns(
+            ["a"],
+            DataSpecInferenceArgs(
+                columns=[Column("b")],
+                include_all_columns=True,
+                max_vocab_count=1,
+                min_vocab_frequency=1,
+                discretize_numerical_columns=False,
+                num_discretized_numerical_bins=1,
+                max_num_scanned_rows_to_infer_semantic=10000,
+                max_num_scanned_rows_to_compute_statistics=10000,
+            ),
+            required_columns=[],
+        )[0],
+        [
+            Column("a"),
+        ],
+    )
+
+  def test_get_all_columns_specified_and_available_always_included(self):
+    self.assertEqual(
+        dataspec_lib.get_all_columns(
+            ["a"],
+            DataSpecInferenceArgs(
+                columns=[Column("a")],
+                include_all_columns=False,
+                max_vocab_count=1,
+                min_vocab_frequency=1,
+                discretize_numerical_columns=False,
+                num_discretized_numerical_bins=1,
+                max_num_scanned_rows_to_infer_semantic=10000,
+                max_num_scanned_rows_to_compute_statistics=10000,
+            ),
+            required_columns=[],
+        )[0],
+        [
+            Column("a"),
+        ],
+    )
+
+  def test_get_all_columns_with_unrolled_features(self):
+    columns, unroll_info = dataspec_lib.get_all_columns(
+        ["a.0", "a.1", "a.2", "b.0", "b.1", "b.2"],
+        DataSpecInferenceArgs(
+            columns=[Column("a")],
+            include_all_columns=False,
+            max_vocab_count=1,
+            min_vocab_frequency=1,
+            discretize_numerical_columns=False,
+            num_discretized_numerical_bins=1,
+            max_num_scanned_rows_to_infer_semantic=10000,
+            max_num_scanned_rows_to_compute_statistics=10000,
+        ),
+        required_columns=[],
+        unroll_feature_info={
+            "a": ["a.0", "a.1", "a.2"],
+            "b": ["b.0", "b.1", "b.2"],
+        },
+    )
+    self.assertEqual(
+        columns,
+        [Column("a.0"), Column("a.1"), Column("a.2")],
+    )
+    self.assertEqual(unroll_info, {"a": ["a.0", "a.1", "a.2"]})
+
+  def test_normalize_column_defs(self):
+    self.assertEqual(
+        dataspec_lib.normalize_column_defs([
+            "a",
+            ("b", Semantic.NUMERICAL),
+            Column("c"),
+            Column("d", Semantic.CATEGORICAL),
+        ]),
+        [
+            Column("a"),
+            Column("b", Semantic.NUMERICAL),
+            Column("c"),
+            Column("d", Semantic.CATEGORICAL),
+        ],
+    )
+
+  def test_normalize_column_defs_none(self):
+    self.assertIsNone(dataspec_lib.normalize_column_defs(None))
+
+
+class MonotonicTest(parameterized.TestCase):
+
+  @parameterized.parameters(
+      Monotonic.INCREASING,
+      Monotonic.DECREASING,
+  )
+  def test_already_normalized_value(self, value):
+    self.assertEqual(Column("f", monotonic=value).normalized_monotonic, value)
+
+  def test_already_normalized_value_none(self):
+    self.assertIsNone(Column("f", monotonic=None).normalized_monotonic)
+
+  @parameterized.parameters(
+      (+1, Monotonic.INCREASING),
+      (-1, Monotonic.DECREASING),
+  )
+  def test_normalize_value(self, non_normalized_value, normalized_value):
+    self.assertEqual(
+        Column("f", monotonic=non_normalized_value).normalized_monotonic,
+        normalized_value,
+    )
+
+  def test_normalize_value_none(self):
+    self.assertIsNone(Column("f", monotonic=0).normalized_monotonic)
+
+  def test_good_semantic(self):
+    _ = Column("f", monotonic=+1)
+    _ = Column("f", semantic=Semantic.NUMERICAL, monotonic=+1)
+
+  def test_bad_semantic(self):
+    with self.assertRaisesRegex(
+        ValueError, "with monotonic constraint is expected to have"
+    ):
+      _ = Column("feature", semantic=Semantic.CATEGORICAL, monotonic=+1)
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/dataset/io/__init__.py

 * *Ordering differences only*

```diff
@@ -1,14 +1,14 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
```

## ydf/dataset/io/dataset_io.py

 * *Ordering differences only*

```diff
@@ -1,172 +1,172 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Common functionality for all dataset I/O connectors."""
-
-import math
-from typing import Dict, Iterator, List, Optional, Sequence, Tuple
-
-import numpy as np
-
-from ydf.dataset.io import dataset_io_types
-from ydf.dataset.io import pandas_io
-from ydf.dataset.io import tensorflow_io
-
-
-def unrolled_feature_names(name: str, num_dims: int) -> Sequence[str]:
-  """Returns the names of an unrolled feature."""
-
-  if num_dims <= 0:
-    raise ValueError("num_dims should be strictly positive.")
-
-  # For example:
-  #   num_features=1 => num_leading_zeroes = 1
-  #   num_features=9 => num_leading_zeroes = 1
-  #   num_features=10 => num_leading_zeroes = 2
-  num_leading_zeroes = int(math.log10(num_dims)) + 1
-
-  postfix = f"_of_{num_dims:0{num_leading_zeroes}}"
-
-  return [
-      f"{name}.{dim_idx:0{num_leading_zeroes}}{postfix}"
-      for dim_idx in range(num_dims)
-  ]
-
-
-def _unroll_column(
-    name: str, src: dataset_io_types.InputValues, allow_unroll: bool
-) -> Iterator[Tuple[str, dataset_io_types.InputValues, bool]]:
-  """Unrolls a possibly multi-dim. column into multiple single-dim columns.
-
-  Yield the results. If the "src" column is not multi-dimensional, yields "src"
-  directly. Fails if "src" contains more than two dimensions.
-
-  Args:
-    name: Name of the source column.
-    src: Single-dimensional or multi-dimensional value.
-    allow_unroll: If false, fails if the column should be unrolled.
-
-  Yields:
-    Tuple of key and values of single-dimentional features, and boolean
-    indicating if the feature is unrolled.
-  """
-
-  # Numpy is currently the only way to pass multi-dim features.
-  if not isinstance(src, np.ndarray) or src.ndim <= 1:
-    yield name, src, False
-    return
-
-  if not allow_unroll:
-    raise ValueError(
-        f"The column {name!r} is multi-dimensional (shape={src.shape}) while"
-        " the model requires this column to be single-dimensional (e.g."
-        " shape=[num_examples])."
-    )
-
-  if src.ndim > 2:
-    raise ValueError(
-        "Input features can only be one or two dimensional. Feature"
-        f" {name!r} has {src.ndim} dimensions."
-    )
-
-  num_features = src.shape[1]
-  if num_features == 0:
-    raise ValueError(f"Multi-dimention feature {name!r} has no features.")
-
-  sub_names = unrolled_feature_names(name, num_features)
-  for dim_idx, sub_name in enumerate(sub_names):
-    yield sub_name, src[:, dim_idx], True
-
-
-def _unroll_dict(
-    src: dataset_io_types.DictInputValues,
-    dont_unroll_columns: Optional[Sequence[str]] = None,
-) -> Tuple[
-    dataset_io_types.DictInputValues, dataset_io_types.UnrolledFeaturesInfo
-]:
-  """Unrolls multi-dim. columns into multiple single-dim. columns.
-
-  Args:
-    src: Dictionary of single and multi-dim values.
-    dont_unroll_columns: List of columns that cannot be unrolled. If one such
-      column needs to be unrolled, raise an error.
-
-  Returns:
-    Dictionary containing only single-dimensional values.
-  """
-
-  # Index the columns for fast query.
-  dont_unroll_columns_set = (
-      set(dont_unroll_columns) if dont_unroll_columns else set()
-  )
-
-  unrolled_features_info = {}
-
-  # Note: We only create a one dictionary independently of the number of
-  # features.
-  dst = {}
-  for name, value in src.items():
-
-    any_is_unrolling = False
-    sub_dst = {}
-    for sub_name, sub_value, is_unrolling in _unroll_column(
-        name, value, allow_unroll=name not in dont_unroll_columns_set
-    ):
-      sub_dst[sub_name] = sub_value
-      any_is_unrolling |= is_unrolling
-
-    dst.update(sub_dst)
-
-    if any_is_unrolling:
-      unrolled_features_info[name] = list(sub_dst.keys())
-
-  return dst, unrolled_features_info
-
-
-def cast_input_dataset_to_dict(
-    data: dataset_io_types.IODataset,
-    dont_unroll_columns: Optional[Sequence[str]] = None,
-) -> Tuple[
-    dataset_io_types.DictInputValues, dataset_io_types.UnrolledFeaturesInfo
-]:
-  """Normalizes the input dataset into a dictionary of values.
-
-    Also unrolls the multi-dimentional features.
-
-  Args:
-    data: Input data.
-    dont_unroll_columns: Column in "dont_unroll_columns" will not be unrolled.
-
-  Returns:
-    The normalized features, and information about unrolled features.
-  """
-
-  unroll_dict_kwargs = {
-      "dont_unroll_columns": dont_unroll_columns,
-  }
-
-  if pandas_io.is_pandas_dataframe(data):
-    return _unroll_dict(pandas_io.to_dict(data), **unroll_dict_kwargs)
-  elif tensorflow_io.is_tensorflow_dataset(data):
-    return _unroll_dict(tensorflow_io.to_dict(data), **unroll_dict_kwargs)
-
-  elif isinstance(data, dict):
-    # Dictionary of values
-    return _unroll_dict(data, **unroll_dict_kwargs)
-
-  # TODO: Maybe this error should be raised at a layer above this one?
-  raise ValueError(
-      "Cannot import dataset from"
-      f" {type(data)}.\n{dataset_io_types.SUPPORTED_INPUT_DATA_DESCRIPTION}"
-  )
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Common functionality for all dataset I/O connectors."""
+
+import math
+from typing import Dict, Iterator, List, Optional, Sequence, Tuple
+
+import numpy as np
+
+from ydf.dataset.io import dataset_io_types
+from ydf.dataset.io import pandas_io
+from ydf.dataset.io import tensorflow_io
+
+
+def unrolled_feature_names(name: str, num_dims: int) -> Sequence[str]:
+  """Returns the names of an unrolled feature."""
+
+  if num_dims <= 0:
+    raise ValueError("num_dims should be strictly positive.")
+
+  # For example:
+  #   num_features=1 => num_leading_zeroes = 1
+  #   num_features=9 => num_leading_zeroes = 1
+  #   num_features=10 => num_leading_zeroes = 2
+  num_leading_zeroes = int(math.log10(num_dims)) + 1
+
+  postfix = f"_of_{num_dims:0{num_leading_zeroes}}"
+
+  return [
+      f"{name}.{dim_idx:0{num_leading_zeroes}}{postfix}"
+      for dim_idx in range(num_dims)
+  ]
+
+
+def _unroll_column(
+    name: str, src: dataset_io_types.InputValues, allow_unroll: bool
+) -> Iterator[Tuple[str, dataset_io_types.InputValues, bool]]:
+  """Unrolls a possibly multi-dim. column into multiple single-dim columns.
+
+  Yield the results. If the "src" column is not multi-dimensional, yields "src"
+  directly. Fails if "src" contains more than two dimensions.
+
+  Args:
+    name: Name of the source column.
+    src: Single-dimensional or multi-dimensional value.
+    allow_unroll: If false, fails if the column should be unrolled.
+
+  Yields:
+    Tuple of key and values of single-dimentional features, and boolean
+    indicating if the feature is unrolled.
+  """
+
+  # Numpy is currently the only way to pass multi-dim features.
+  if not isinstance(src, np.ndarray) or src.ndim <= 1:
+    yield name, src, False
+    return
+
+  if not allow_unroll:
+    raise ValueError(
+        f"The column {name!r} is multi-dimensional (shape={src.shape}) while"
+        " the model requires this column to be single-dimensional (e.g."
+        " shape=[num_examples])."
+    )
+
+  if src.ndim > 2:
+    raise ValueError(
+        "Input features can only be one or two dimensional. Feature"
+        f" {name!r} has {src.ndim} dimensions."
+    )
+
+  num_features = src.shape[1]
+  if num_features == 0:
+    raise ValueError(f"Multi-dimention feature {name!r} has no features.")
+
+  sub_names = unrolled_feature_names(name, num_features)
+  for dim_idx, sub_name in enumerate(sub_names):
+    yield sub_name, src[:, dim_idx], True
+
+
+def _unroll_dict(
+    src: dataset_io_types.DictInputValues,
+    dont_unroll_columns: Optional[Sequence[str]] = None,
+) -> Tuple[
+    dataset_io_types.DictInputValues, dataset_io_types.UnrolledFeaturesInfo
+]:
+  """Unrolls multi-dim. columns into multiple single-dim. columns.
+
+  Args:
+    src: Dictionary of single and multi-dim values.
+    dont_unroll_columns: List of columns that cannot be unrolled. If one such
+      column needs to be unrolled, raise an error.
+
+  Returns:
+    Dictionary containing only single-dimensional values.
+  """
+
+  # Index the columns for fast query.
+  dont_unroll_columns_set = (
+      set(dont_unroll_columns) if dont_unroll_columns else set()
+  )
+
+  unrolled_features_info = {}
+
+  # Note: We only create a one dictionary independently of the number of
+  # features.
+  dst = {}
+  for name, value in src.items():
+
+    any_is_unrolling = False
+    sub_dst = {}
+    for sub_name, sub_value, is_unrolling in _unroll_column(
+        name, value, allow_unroll=name not in dont_unroll_columns_set
+    ):
+      sub_dst[sub_name] = sub_value
+      any_is_unrolling |= is_unrolling
+
+    dst.update(sub_dst)
+
+    if any_is_unrolling:
+      unrolled_features_info[name] = list(sub_dst.keys())
+
+  return dst, unrolled_features_info
+
+
+def cast_input_dataset_to_dict(
+    data: dataset_io_types.IODataset,
+    dont_unroll_columns: Optional[Sequence[str]] = None,
+) -> Tuple[
+    dataset_io_types.DictInputValues, dataset_io_types.UnrolledFeaturesInfo
+]:
+  """Normalizes the input dataset into a dictionary of values.
+
+    Also unrolls the multi-dimentional features.
+
+  Args:
+    data: Input data.
+    dont_unroll_columns: Column in "dont_unroll_columns" will not be unrolled.
+
+  Returns:
+    The normalized features, and information about unrolled features.
+  """
+
+  unroll_dict_kwargs = {
+      "dont_unroll_columns": dont_unroll_columns,
+  }
+
+  if pandas_io.is_pandas_dataframe(data):
+    return _unroll_dict(pandas_io.to_dict(data), **unroll_dict_kwargs)
+  elif tensorflow_io.is_tensorflow_dataset(data):
+    return _unroll_dict(tensorflow_io.to_dict(data), **unroll_dict_kwargs)
+
+  elif isinstance(data, dict):
+    # Dictionary of values
+    return _unroll_dict(data, **unroll_dict_kwargs)
+
+  # TODO: Maybe this error should be raised at a layer above this one?
+  raise ValueError(
+      "Cannot import dataset from"
+      f" {type(data)}.\n{dataset_io_types.SUPPORTED_INPUT_DATA_DESCRIPTION}"
+  )
```

## ydf/dataset/io/dataset_io_test.py

 * *Ordering differences only*

```diff
@@ -1,39 +1,39 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from absl.testing import absltest
-from ydf.dataset.io import dataset_io
-
-
-class DatasetIoTest(absltest.TestCase):
-
-  def test_unrolled_feature_names(self):
-    self.assertEqual(
-        dataset_io.unrolled_feature_names("a", 5),
-        [
-            "a.0_of_5",
-            "a.1_of_5",
-            "a.2_of_5",
-            "a.3_of_5",
-            "a.4_of_5",
-        ],
-    )
-
-  def test_unrolled_feature_names_with_zero_dim(self):
-    with self.assertRaisesRegex(ValueError, "should be strictly positive"):
-      dataset_io.unrolled_feature_names("a", 0)
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from absl.testing import absltest
+from ydf.dataset.io import dataset_io
+
+
+class DatasetIoTest(absltest.TestCase):
+
+  def test_unrolled_feature_names(self):
+    self.assertEqual(
+        dataset_io.unrolled_feature_names("a", 5),
+        [
+            "a.0_of_5",
+            "a.1_of_5",
+            "a.2_of_5",
+            "a.3_of_5",
+            "a.4_of_5",
+        ],
+    )
+
+  def test_unrolled_feature_names_with_zero_dim(self):
+    with self.assertRaisesRegex(ValueError, "should be strictly positive"):
+      dataset_io.unrolled_feature_names("a", 0)
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/dataset/io/dataset_io_types.py

 * *Ordering differences only*

```diff
@@ -1,49 +1,49 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Typing annotations for dataset I/O connectors."""
-
-import typing
-from typing import Any, Dict, List, Sequence, Union
-
-import numpy as np
-
-if typing.TYPE_CHECKING:
-  import pandas as pd  # pylint: disable=unused-import,g-bad-import-order
-
-# Supported type of column input values.
-InputValues = Union[np.ndarray, List[Any]]
-
-DictInputValues = Dict[str, InputValues]
-
-
-# Information about unrolled features
-# Maps an original feature name to a list of unrolled features.
-# e.g. {"f" : ["f.0", "f.1", "f.2"]}
-UnrolledFeaturesInfo = Dict[str, List[str]]
-
-
-# Supported types of datasets.
-IODataset = Union[Dict[str, InputValues], "pd.DataFrame", str, Sequence[str]]
-
-
-SUPPORTED_INPUT_DATA_DESCRIPTION = """\
-A dataset can be one of the following:
-- A Pandas DataFrame.
-- A dictionary of column names (str) to values. Values can be lists of int, float, bool, str or bytes. Values can also be Numpy arrays.
-- A YDF VerticalDataset
-- A TensorFlow Batched Dataset.
-- A typed (possibly sharded) path to a CSV file (e.g. csv:mydata).
-- A list of typed paths (e.g. ["csv:mydata1", "csv:mydata2"]).
-"""
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Typing annotations for dataset I/O connectors."""
+
+import typing
+from typing import Any, Dict, List, Sequence, Union
+
+import numpy as np
+
+if typing.TYPE_CHECKING:
+  import pandas as pd  # pylint: disable=unused-import,g-bad-import-order
+
+# Supported type of column input values.
+InputValues = Union[np.ndarray, List[Any]]
+
+DictInputValues = Dict[str, InputValues]
+
+
+# Information about unrolled features
+# Maps an original feature name to a list of unrolled features.
+# e.g. {"f" : ["f.0", "f.1", "f.2"]}
+UnrolledFeaturesInfo = Dict[str, List[str]]
+
+
+# Supported types of datasets.
+IODataset = Union[Dict[str, InputValues], "pd.DataFrame", str, Sequence[str]]
+
+
+SUPPORTED_INPUT_DATA_DESCRIPTION = """\
+A dataset can be one of the following:
+- A Pandas DataFrame.
+- A dictionary of column names (str) to values. Values can be lists of int, float, bool, str or bytes. Values can also be Numpy arrays.
+- A YDF VerticalDataset
+- A TensorFlow Batched Dataset.
+- A typed (possibly sharded) path to a CSV file (e.g. csv:mydata).
+- A list of typed paths (e.g. ["csv:mydata1", "csv:mydata2"]).
+"""
```

## ydf/dataset/io/pandas_io.py

 * *Ordering differences only*

```diff
@@ -1,64 +1,64 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Connectors for loading data from Pandas dataframes."""
-
-import sys
-from typing import Dict
-
-from absl import logging
-
-from ydf.dataset.io import dataset_io_types
-
-
-def import_pd():
-  try:
-    import pandas as pd  # pylint: disable=g-import-not-at-top
-
-    return pd
-  except ImportError:
-    logging.warning(
-        "Importing data from pandas dataframes requires pandas to be installed."
-        " Install Pandas with pip using `pip install ydf[pandas]` or"
-        " `pip install pandas`."
-    )
-    raise
-
-
-def is_pandas_dataframe(data: dataset_io_types.IODataset) -> bool:
-  if "pandas" in sys.modules:
-    return isinstance(data, sys.modules["pandas"].DataFrame)
-  return False
-
-
-def to_dict(
-    data: dataset_io_types.IODataset,
-) -> Dict[str, dataset_io_types.InputValues]:
-  """Converts a Pandas dataframe to a dict of numpy arrays."""
-  pd = import_pd()
-
-  assert isinstance(data, pd.DataFrame)
-  if data.ndim != 2:
-    raise ValueError("The pandas DataFrame must be two-dimensional.")
-  data_dict = data.to_dict("series")
-
-  def clean(values):
-    if values.dtype == "object":
-      return values.to_numpy(copy=False, na_value="")
-    else:
-      return values.to_numpy(copy=False)
-
-  data_dict = {k: clean(v) for k, v in data_dict.items()}
-
-  return data_dict
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Connectors for loading data from Pandas dataframes."""
+
+import sys
+from typing import Dict
+
+from absl import logging
+
+from ydf.dataset.io import dataset_io_types
+
+
+def import_pd():
+  try:
+    import pandas as pd  # pylint: disable=g-import-not-at-top
+
+    return pd
+  except ImportError:
+    logging.warning(
+        "Importing data from pandas dataframes requires pandas to be installed."
+        " Install Pandas with pip using `pip install ydf[pandas]` or"
+        " `pip install pandas`."
+    )
+    raise
+
+
+def is_pandas_dataframe(data: dataset_io_types.IODataset) -> bool:
+  if "pandas" in sys.modules:
+    return isinstance(data, sys.modules["pandas"].DataFrame)
+  return False
+
+
+def to_dict(
+    data: dataset_io_types.IODataset,
+) -> Dict[str, dataset_io_types.InputValues]:
+  """Converts a Pandas dataframe to a dict of numpy arrays."""
+  pd = import_pd()
+
+  assert isinstance(data, pd.DataFrame)
+  if data.ndim != 2:
+    raise ValueError("The pandas DataFrame must be two-dimensional.")
+  data_dict = data.to_dict("series")
+
+  def clean(values):
+    if values.dtype == "object":
+      return values.to_numpy(copy=False, na_value="")
+    else:
+      return values.to_numpy(copy=False)
+
+  data_dict = {k: clean(v) for k, v in data_dict.items()}
+
+  return data_dict
```

## ydf/dataset/io/pandas_io_test.py

 * *Ordering differences only*

```diff
@@ -1,31 +1,31 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Test dataspec utilities for pandas."""
-
-from absl.testing import absltest
-import pandas as pd
-
-from ydf.dataset.io import pandas_io
-
-
-class PandasIOTest(absltest.TestCase):
-
-  def test_is_pandas(self):
-    self.assertTrue(pandas_io.is_pandas_dataframe(pd.DataFrame()))
-    self.assertFalse(pandas_io.is_pandas_dataframe({}))
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Test dataspec utilities for pandas."""
+
+from absl.testing import absltest
+import pandas as pd
+
+from ydf.dataset.io import pandas_io
+
+
+class PandasIOTest(absltest.TestCase):
+
+  def test_is_pandas(self):
+    self.assertTrue(pandas_io.is_pandas_dataframe(pd.DataFrame()))
+    self.assertFalse(pandas_io.is_pandas_dataframe({}))
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/dataset/io/tensorflow_io.py

```diff
@@ -1,41 +1,55 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Connectors for loading data from Pandas dataframes."""
-
-import sys
-from typing import Dict
-
-from ydf.dataset.io import dataset_io_types
-
-
-def is_tensorflow_dataset(data: dataset_io_types.IODataset) -> bool:
-  # Note: We only test if the dataset is a TensorFlow dataset if the object name
-  # look like a TensorFlow object. This way, we avoid importing TF is not
-  # necessary.
-  return (
-      "tensorflow" in str(type(data))
-      and data.__class__.__name__
-      in ("_BatchDataset", "_MapDataset", "DatasetV1Adapter")
-      and hasattr(data, "rebatch")
-  )
-
-
-def to_dict(
-    data: dataset_io_types.IODataset,
-) -> Dict[str, dataset_io_types.InputValues]:
-  """Converts a Tensorflow dataset to a dict of numpy arrays."""
-  assert hasattr(data, "rebatch")
-  full_batch = next(iter(data.rebatch(sys.maxsize)))
-  return {k: v.numpy() for k, v in full_batch.items()}
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Connectors for loading data from Pandas dataframes."""
+
+import logging
+import sys
+from typing import Dict
+from ydf.dataset.io import dataset_io_types
+
+
+def is_tensorflow_dataset(data: dataset_io_types.IODataset) -> bool:
+  # Note: We only test if the dataset is a TensorFlow dataset if the object name
+  # look like a TensorFlow object. This way, we avoid importing TF is not
+  # necessary.
+  str_class = str(type(data))
+  if "tensorflow" in str_class and hasattr(data, "rebatch"):
+
+    if data.__class__.__name__ in (
+        "_BatchDataset",
+        "_MapDataset",
+        "DatasetV1Adapter",
+        "CacheDataset",
+    ):
+      return True
+
+    if "data.ops" in str_class:
+      logging.warning(
+          "The dataset %s object is not listed as a YDF compatible TensorFlow"
+          " Dataset, but it looks like one",
+          str_class,
+      )
+      return True
+
+  return False
+
+
+def to_dict(
+    data: dataset_io_types.IODataset,
+) -> Dict[str, dataset_io_types.InputValues]:
+  """Converts a Tensorflow dataset to a dict of numpy arrays."""
+  assert hasattr(data, "rebatch")
+  full_batch = next(iter(data.rebatch(sys.maxsize)))
+  return {k: v.numpy() for k, v in full_batch.items()}
```

## ydf/learner/__init__.py

 * *Ordering differences only*

```diff
@@ -1,14 +1,14 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
```

## ydf/learner/custom_loss.py

```diff
@@ -1,331 +1,331 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Implementations for custom loss containers."""
-
-import dataclasses
-import enum
-from typing import Any, Callable, Tuple
-
-import numpy as np
-import numpy.typing as npt
-
-from yggdrasil_decision_forests.model import abstract_model_pb2
-from ydf.cc import ydf
-
-
-class Activation(enum.Enum):
-  """Activation functions for custom losses.
-
-  Not all activation functions are supported for all custom losses. Activation
-  function IDENTITY (i.e., no activation function applied) is always supported.
-  """
-
-  IDENTITY = "IDENTITY"
-  SIGMOID = "SIGMOID"
-  SOFTMAX = "SOFTMAX"
-
-
-@dataclasses.dataclass(frozen=True)
-class AbstractCustomLoss:
-  """Abstract Base class for custom losses."""
-
-  activation: Any
-
-  initial_predictions: Callable[
-      [npt.NDArray[Any], npt.NDArray[np.float32]],
-      Any,
-  ]
-  loss: Callable[
-      [
-          npt.NDArray[Any],
-          npt.NDArray[np.float32],
-          npt.NDArray[np.float32],
-      ],
-      np.float32,
-  ]
-  gradient_and_hessian: Callable[
-      [npt.NDArray[Any], npt.NDArray[np.float32]],
-      Tuple[npt.NDArray[np.float32], npt.NDArray[np.float32]],
-  ]
-
-  may_trigger_gc: bool = True
-
-  def check_is_compatible_task(self, task: abstract_model_pb2.Task) -> None:
-    raise NotImplementedError("Not implemented")
-
-  def _to_cc(self):
-    raise NotImplementedError("Not implemented")
-
-
-@dataclasses.dataclass(frozen=True)
-class RegressionLoss(AbstractCustomLoss):
-  """A user-provided loss function for regression problems.
-
-  Loss functions may never reference their arguments outside after returning:
-  Bad:
-  ```
-  mylabels = None
-  def initial_predictions(labels, weights):
-    nonlocal mylabels
-    mylabels = labels  # labels is now referenced outside the function
-  ```
-  Good:
-  ```
-  mylabels = None
-  def initial_predictions(labels, weights):
-    nonlocal mylabels
-    mylabels = np.copy(labels)  # mylabels is a copy, not a reference.
-  ```
-
-  initial_predictions: The bias / initial predictions of the GBT model. Receives
-    the label values and the weights, outputs the initial prediction as a float.
-  loss: The loss function controls the early stopping. The loss function
-    receives the labels, the current predictions and the current weights and
-    must output the loss as a float. Note that the predictions provided to the
-    loss functions have not yet had an activation function applied to them.
-  gradient_and_hessian: Gradient and hessian of the current predictions. Note
-    that only the diagonal of the hessian must be provided. Receives as input
-    the labels and the current predictions (without activation) and returns a
-    tuple of the gradient and the hessian.
-  activation: Activation function to be applied to the model. Regression models
-    are expected to return a value in the same space as the labels after
-    applying the activation function.
-  may_trigger_gc: If True (default), YDF may trigger Python's garbage collection
-    to determine if a Numpy array that is backed by YDF-internal data is used
-    after its lifetime has ended. If False, checks for illegal memory accesses
-    are disabled. This can be useful when training many small models or if the
-    observed impact of triggering GC is large. If `may_trigger_gc=False`, it is
-    very important that the user validate manuallythat no memory leakage occurs.
-  """
-
-  initial_predictions: Callable[
-      [npt.NDArray[np.float32], npt.NDArray[np.float32]],
-      np.float32,
-  ]
-  loss: Callable[
-      [
-          npt.NDArray[np.float32],
-          npt.NDArray[np.float32],
-          npt.NDArray[np.float32],
-      ],
-      np.float32,
-  ]
-  gradient_and_hessian: Callable[
-      [npt.NDArray[np.float32], npt.NDArray[np.float32]],
-      Tuple[npt.NDArray[np.float32], npt.NDArray[np.float32]],
-  ]
-
-  activation: Activation
-
-  def __post_init__(self):
-    if self.activation != Activation.IDENTITY:
-      raise ValueError(
-          "Only activation function IDENTITY is supported for RegressionLoss."
-      )
-
-  def check_is_compatible_task(self, task: abstract_model_pb2.Task) -> None:
-    """Raises an error if the given task is incompatible with this loss type."""
-    if task != abstract_model_pb2.REGRESSION:
-      raise ValueError(
-          "A RegressionLoss is only compatible with REGRESSION"
-          f" tasks. Received task {abstract_model_pb2.Task.Name(task)}"
-      )
-
-  def _to_cc(self):
-    return ydf.CCRegressionLoss(
-        self.initial_predictions,
-        self.loss,
-        self.gradient_and_hessian,
-        self.may_trigger_gc,
-    )
-
-
-@dataclasses.dataclass(frozen=True)
-class BinaryClassificationLoss(AbstractCustomLoss):
-  """A user-provided loss function for binary classification problems.
-
-  Note that the labels are binary but 1-based, i.e. the positive class is 2, the
-  negative class is 1.
-
-  Loss functions may never reference their arguments outside after returning:
-  Bad:
-  ```
-  mylabels = None
-  def initial_predictions(labels, weights):
-    nonlocal mylabels
-    mylabels = labels  # labels is now referenced outside the function
-  ```
-  Good:
-  ```
-  mylabels = None
-  def initial_predictions(labels, weights):
-    nonlocal mylabels
-    mylabels = np.copy(labels)  # mylabels is a copy, not a reference.
-  ```
-
-  initial_predictions: The bias / initial predictions of the GBT model. Receives
-    the label values and the weights, outputs the initial prediction as a float.
-  loss: The loss function controls the early stopping. The loss function
-    receives the labels, the current predictions and the current weights and
-    must output the loss as a float. Note that the predictions provided to the
-    loss functions have not yet had an activation function applied to them.
-  gradient_and_hessian: Gradient and hessian of the current predictions. Note
-    that only the diagonal of the hessian must be provided. Receives as input
-    the labels and the current predictions (without activation). Returns a
-    tuple of the gradient and the hessian.
-  activation: Activation function to be applied to the model. Binary
-    classification models are expected to return a probability after applying
-    the activation function.
-  may_trigger_gc: If True (default), YDF may trigger Python's garbage collection
-    to determine if an Numpy array that is backed by YDF-internal data is used
-    after its lifetime has ended. If False, checks for illegal memory accesses
-    are disabled. Setting this parameter to False is dangerous, since illegal
-    memory accesses will no longer be detected.
-  """
-
-  initial_predictions: Callable[
-      [npt.NDArray[np.int32], npt.NDArray[np.float32]],
-      np.float32,
-  ]
-  loss: Callable[
-      [
-          npt.NDArray[np.int32],
-          npt.NDArray[np.float32],
-          npt.NDArray[np.float32],
-      ],
-      np.float32,
-  ]
-  gradient_and_hessian: Callable[
-      [npt.NDArray[np.int32], npt.NDArray[np.float32]],
-      Tuple[npt.NDArray[np.float32], npt.NDArray[np.float32]],
-  ]
-
-  activation: Activation
-
-  def __post_init__(self):
-    if self.activation not in [Activation.IDENTITY, Activation.SIGMOID]:
-      raise ValueError(
-          "Only activation functions IDENTITY and SIGMOID are supported for"
-          " BinaryClassificationLoss."
-      )
-
-  def check_is_compatible_task(self, task: abstract_model_pb2.Task) -> None:
-    """Raises an error if the given task is incompatible with this loss type."""
-    if task != abstract_model_pb2.CLASSIFICATION:
-      raise ValueError(
-          "A BinaryClassificationLoss is only compatible with CLASSIFICATION"
-          f" tasks. Received task {abstract_model_pb2.Task.Name(task)}"
-      )
-
-  def _to_cc(self):
-    return ydf.CCBinaryClassificationLoss(
-        self.initial_predictions,
-        self.loss,
-        self.gradient_and_hessian,
-        self.may_trigger_gc,
-    )
-
-
-@dataclasses.dataclass(frozen=True)
-class MultiClassificationLoss(AbstractCustomLoss):
-  """A user-provided loss function for multi-class problems.
-
-  Note that the labels are but 1-based. Predictions are given in an 2D
-  array with one row per example. Initial predictions, gradient and
-  hessian are expected for each class, e.g. for a 3-class classification
-  problem, output 3 gradients and hessians per class.
-
-  Loss functions may never reference their arguments outside after returning:
-  Bad:
-  ```
-  mylabels = None
-  def initial_predictions(labels, weights):
-    nonlocal mylabels
-    mylabels = labels  # labels is now referenced outside the function
-  ```
-  Good:
-  ```
-  mylabels = None
-  def initial_predictions(labels, weights):
-    nonlocal mylabels
-    mylabels = np.copy(labels)  # mylabels is a copy, not a reference.
-  ```
-
-  initial_predictions: The bias / initial predictions of the GBT model. Receives
-  the label values and the weights, outputs the initial prediction as an array
-  of floats (one initial prediction per class).
-  loss: The loss function controls the early stopping. The loss function
-    receives the labels, the current predictions and the current weights and
-    must output the loss as a float. Note that the predictions provided to the
-    loss functions have not yet had an activation function applied to them.
-  gradient_and_hessian: Gradient and hessian of the current predictions with
-    respect to each class. Note that only the diagonal of the hessian must be
-    provided. Receives as input the labels and the current predictions (without
-    activation). Returns a tuple of the gradient and the hessian. Both gradient
-    and hessian must be arrays of shape (num_classes, num_examples).
-  activation: Activation function to be applied to the model. Multi-class
-    classification models are expected to return a probability distribution over
-    the classes after applying the activation function.
-  may_trigger_gc: If True (default), YDF may trigger Python's garbage collection
-    to determine if an Numpy array that is backed by YDF-internal data is used
-    after its lifetime has ended. If False, checks for illegal memory accesses
-    are disabled. Setting this parameter to False is dangerous, since illegal
-    memory accesses will no longer be detected.
-  """
-
-  initial_predictions: Callable[
-      [npt.NDArray[np.int32], npt.NDArray[np.float32]],
-      npt.NDArray[np.float32],
-  ]
-  loss: Callable[
-      [
-          npt.NDArray[np.int32],
-          npt.NDArray[np.float32],
-          npt.NDArray[np.float32],
-      ],
-      np.float32,
-  ]
-  gradient_and_hessian: Callable[
-      [npt.NDArray[np.int32], npt.NDArray[np.float32]],
-      Tuple[npt.NDArray[np.float32], npt.NDArray[np.float32]],
-  ]
-
-  activation: Activation
-
-  def __post_init__(self):
-    if (
-        self.activation != Activation.IDENTITY
-        and self.activation != Activation.SOFTMAX
-    ):
-      raise ValueError(
-          "Only activation functions IDENTITY and SOFTMAX are supported for"
-          " MultiClassificationLoss."
-      )
-
-  def check_is_compatible_task(self, task: abstract_model_pb2.Task) -> None:
-    """Raises an error if the given task is incompatible with this loss type."""
-    if task != abstract_model_pb2.CLASSIFICATION:
-      raise ValueError(
-          "A MultiClassificationLoss is only compatible with CLASSIFICATION"
-          f" tasks. Received task {abstract_model_pb2.Task.Name(task)}"
-      )
-
-  def _to_cc(self):
-    return ydf.CCMultiClassificationLoss(
-        self.initial_predictions,
-        self.loss,
-        self.gradient_and_hessian,
-        self.may_trigger_gc,
-    )
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Implementations for custom loss containers."""
+
+import dataclasses
+import enum
+from typing import Any, Callable, Tuple
+
+import numpy as np
+import numpy.typing as npt
+
+from ydf.proto.model import abstract_model_pb2
+from ydf.cc import ydf
+
+
+class Activation(enum.Enum):
+  """Activation functions for custom losses.
+
+  Not all activation functions are supported for all custom losses. Activation
+  function IDENTITY (i.e., no activation function applied) is always supported.
+  """
+
+  IDENTITY = "IDENTITY"
+  SIGMOID = "SIGMOID"
+  SOFTMAX = "SOFTMAX"
+
+
+@dataclasses.dataclass(frozen=True)
+class AbstractCustomLoss:
+  """Abstract Base class for custom losses."""
+
+  activation: Any
+
+  initial_predictions: Callable[
+      [npt.NDArray[Any], npt.NDArray[np.float32]],
+      Any,
+  ]
+  loss: Callable[
+      [
+          npt.NDArray[Any],
+          npt.NDArray[np.float32],
+          npt.NDArray[np.float32],
+      ],
+      np.float32,
+  ]
+  gradient_and_hessian: Callable[
+      [npt.NDArray[Any], npt.NDArray[np.float32]],
+      Tuple[npt.NDArray[np.float32], npt.NDArray[np.float32]],
+  ]
+
+  may_trigger_gc: bool = True
+
+  def check_is_compatible_task(self, task: abstract_model_pb2.Task) -> None:
+    raise NotImplementedError("Not implemented")
+
+  def _to_cc(self):
+    raise NotImplementedError("Not implemented")
+
+
+@dataclasses.dataclass(frozen=True)
+class RegressionLoss(AbstractCustomLoss):
+  """A user-provided loss function for regression problems.
+
+  Loss functions may never reference their arguments outside after returning:
+  Bad:
+  ```
+  mylabels = None
+  def initial_predictions(labels, weights):
+    nonlocal mylabels
+    mylabels = labels  # labels is now referenced outside the function
+  ```
+  Good:
+  ```
+  mylabels = None
+  def initial_predictions(labels, weights):
+    nonlocal mylabels
+    mylabels = np.copy(labels)  # mylabels is a copy, not a reference.
+  ```
+
+  initial_predictions: The bias / initial predictions of the GBT model. Receives
+    the label values and the weights, outputs the initial prediction as a float.
+  loss: The loss function controls the early stopping. The loss function
+    receives the labels, the current predictions and the current weights and
+    must output the loss as a float. Note that the predictions provided to the
+    loss functions have not yet had an activation function applied to them.
+  gradient_and_hessian: Gradient and hessian of the current predictions. Note
+    that only the diagonal of the hessian must be provided. Receives as input
+    the labels and the current predictions (without activation) and returns a
+    tuple of the gradient and the hessian.
+  activation: Activation function to be applied to the model. Regression models
+    are expected to return a value in the same space as the labels after
+    applying the activation function.
+  may_trigger_gc: If True (default), YDF may trigger Python's garbage collection
+    to determine if a Numpy array that is backed by YDF-internal data is used
+    after its lifetime has ended. If False, checks for illegal memory accesses
+    are disabled. This can be useful when training many small models or if the
+    observed impact of triggering GC is large. If `may_trigger_gc=False`, it is
+    very important that the user validate manuallythat no memory leakage occurs.
+  """
+
+  initial_predictions: Callable[
+      [npt.NDArray[np.float32], npt.NDArray[np.float32]],
+      np.float32,
+  ]
+  loss: Callable[
+      [
+          npt.NDArray[np.float32],
+          npt.NDArray[np.float32],
+          npt.NDArray[np.float32],
+      ],
+      np.float32,
+  ]
+  gradient_and_hessian: Callable[
+      [npt.NDArray[np.float32], npt.NDArray[np.float32]],
+      Tuple[npt.NDArray[np.float32], npt.NDArray[np.float32]],
+  ]
+
+  activation: Activation
+
+  def __post_init__(self):
+    if self.activation != Activation.IDENTITY:
+      raise ValueError(
+          "Only activation function IDENTITY is supported for RegressionLoss."
+      )
+
+  def check_is_compatible_task(self, task: abstract_model_pb2.Task) -> None:
+    """Raises an error if the given task is incompatible with this loss type."""
+    if task != abstract_model_pb2.REGRESSION:
+      raise ValueError(
+          "A RegressionLoss is only compatible with REGRESSION"
+          f" tasks. Received task {abstract_model_pb2.Task.Name(task)}"
+      )
+
+  def _to_cc(self):
+    return ydf.CCRegressionLoss(
+        self.initial_predictions,
+        self.loss,
+        self.gradient_and_hessian,
+        self.may_trigger_gc,
+    )
+
+
+@dataclasses.dataclass(frozen=True)
+class BinaryClassificationLoss(AbstractCustomLoss):
+  """A user-provided loss function for binary classification problems.
+
+  Note that the labels are binary but 1-based, i.e. the positive class is 2, the
+  negative class is 1.
+
+  Loss functions may never reference their arguments outside after returning:
+  Bad:
+  ```
+  mylabels = None
+  def initial_predictions(labels, weights):
+    nonlocal mylabels
+    mylabels = labels  # labels is now referenced outside the function
+  ```
+  Good:
+  ```
+  mylabels = None
+  def initial_predictions(labels, weights):
+    nonlocal mylabels
+    mylabels = np.copy(labels)  # mylabels is a copy, not a reference.
+  ```
+
+  initial_predictions: The bias / initial predictions of the GBT model. Receives
+    the label values and the weights, outputs the initial prediction as a float.
+  loss: The loss function controls the early stopping. The loss function
+    receives the labels, the current predictions and the current weights and
+    must output the loss as a float. Note that the predictions provided to the
+    loss functions have not yet had an activation function applied to them.
+  gradient_and_hessian: Gradient and hessian of the current predictions. Note
+    that only the diagonal of the hessian must be provided. Receives as input
+    the labels and the current predictions (without activation). Returns a
+    tuple of the gradient and the hessian.
+  activation: Activation function to be applied to the model. Binary
+    classification models are expected to return a probability after applying
+    the activation function.
+  may_trigger_gc: If True (default), YDF may trigger Python's garbage collection
+    to determine if an Numpy array that is backed by YDF-internal data is used
+    after its lifetime has ended. If False, checks for illegal memory accesses
+    are disabled. Setting this parameter to False is dangerous, since illegal
+    memory accesses will no longer be detected.
+  """
+
+  initial_predictions: Callable[
+      [npt.NDArray[np.int32], npt.NDArray[np.float32]],
+      np.float32,
+  ]
+  loss: Callable[
+      [
+          npt.NDArray[np.int32],
+          npt.NDArray[np.float32],
+          npt.NDArray[np.float32],
+      ],
+      np.float32,
+  ]
+  gradient_and_hessian: Callable[
+      [npt.NDArray[np.int32], npt.NDArray[np.float32]],
+      Tuple[npt.NDArray[np.float32], npt.NDArray[np.float32]],
+  ]
+
+  activation: Activation
+
+  def __post_init__(self):
+    if self.activation not in [Activation.IDENTITY, Activation.SIGMOID]:
+      raise ValueError(
+          "Only activation functions IDENTITY and SIGMOID are supported for"
+          " BinaryClassificationLoss."
+      )
+
+  def check_is_compatible_task(self, task: abstract_model_pb2.Task) -> None:
+    """Raises an error if the given task is incompatible with this loss type."""
+    if task != abstract_model_pb2.CLASSIFICATION:
+      raise ValueError(
+          "A BinaryClassificationLoss is only compatible with CLASSIFICATION"
+          f" tasks. Received task {abstract_model_pb2.Task.Name(task)}"
+      )
+
+  def _to_cc(self):
+    return ydf.CCBinaryClassificationLoss(
+        self.initial_predictions,
+        self.loss,
+        self.gradient_and_hessian,
+        self.may_trigger_gc,
+    )
+
+
+@dataclasses.dataclass(frozen=True)
+class MultiClassificationLoss(AbstractCustomLoss):
+  """A user-provided loss function for multi-class problems.
+
+  Note that the labels are but 1-based. Predictions are given in an 2D
+  array with one row per example. Initial predictions, gradient and
+  hessian are expected for each class, e.g. for a 3-class classification
+  problem, output 3 gradients and hessians per class.
+
+  Loss functions may never reference their arguments outside after returning:
+  Bad:
+  ```
+  mylabels = None
+  def initial_predictions(labels, weights):
+    nonlocal mylabels
+    mylabels = labels  # labels is now referenced outside the function
+  ```
+  Good:
+  ```
+  mylabels = None
+  def initial_predictions(labels, weights):
+    nonlocal mylabels
+    mylabels = np.copy(labels)  # mylabels is a copy, not a reference.
+  ```
+
+  initial_predictions: The bias / initial predictions of the GBT model. Receives
+  the label values and the weights, outputs the initial prediction as an array
+  of floats (one initial prediction per class).
+  loss: The loss function controls the early stopping. The loss function
+    receives the labels, the current predictions and the current weights and
+    must output the loss as a float. Note that the predictions provided to the
+    loss functions have not yet had an activation function applied to them.
+  gradient_and_hessian: Gradient and hessian of the current predictions with
+    respect to each class. Note that only the diagonal of the hessian must be
+    provided. Receives as input the labels and the current predictions (without
+    activation). Returns a tuple of the gradient and the hessian. Both gradient
+    and hessian must be arrays of shape (num_classes, num_examples).
+  activation: Activation function to be applied to the model. Multi-class
+    classification models are expected to return a probability distribution over
+    the classes after applying the activation function.
+  may_trigger_gc: If True (default), YDF may trigger Python's garbage collection
+    to determine if an Numpy array that is backed by YDF-internal data is used
+    after its lifetime has ended. If False, checks for illegal memory accesses
+    are disabled. Setting this parameter to False is dangerous, since illegal
+    memory accesses will no longer be detected.
+  """
+
+  initial_predictions: Callable[
+      [npt.NDArray[np.int32], npt.NDArray[np.float32]],
+      npt.NDArray[np.float32],
+  ]
+  loss: Callable[
+      [
+          npt.NDArray[np.int32],
+          npt.NDArray[np.float32],
+          npt.NDArray[np.float32],
+      ],
+      np.float32,
+  ]
+  gradient_and_hessian: Callable[
+      [npt.NDArray[np.int32], npt.NDArray[np.float32]],
+      Tuple[npt.NDArray[np.float32], npt.NDArray[np.float32]],
+  ]
+
+  activation: Activation
+
+  def __post_init__(self):
+    if (
+        self.activation != Activation.IDENTITY
+        and self.activation != Activation.SOFTMAX
+    ):
+      raise ValueError(
+          "Only activation functions IDENTITY and SOFTMAX are supported for"
+          " MultiClassificationLoss."
+      )
+
+  def check_is_compatible_task(self, task: abstract_model_pb2.Task) -> None:
+    """Raises an error if the given task is incompatible with this loss type."""
+    if task != abstract_model_pb2.CLASSIFICATION:
+      raise ValueError(
+          "A MultiClassificationLoss is only compatible with CLASSIFICATION"
+          f" tasks. Received task {abstract_model_pb2.Task.Name(task)}"
+      )
+
+  def _to_cc(self):
+    return ydf.CCMultiClassificationLoss(
+        self.initial_predictions,
+        self.loss,
+        self.gradient_and_hessian,
+        self.may_trigger_gc,
+    )
```

## ydf/learner/custom_loss_test.py

 * *Ordering differences only*

```diff
@@ -1,713 +1,713 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Tests for the custom losses."""
-
-import os
-from typing import Tuple
-
-from absl.testing import absltest
-from absl.testing import parameterized
-import jax
-import jax.numpy as jnp
-import numpy as np
-import numpy.testing as npt
-import numpy.typing as npty
-import pandas as pd
-
-from ydf.dataset import dataspec
-from ydf.learner import custom_loss
-from ydf.learner import generic_learner
-from ydf.learner import specialized_learners
-from ydf.model import generic_model
-from ydf.model.gradient_boosted_trees_model import gradient_boosted_trees_model
-from ydf.utils import test_utils
-
-Column = dataspec.Column
-
-
-class CustomLossTest(parameterized.TestCase):
-
-  def setUp(self):
-    super().setUp()
-    self.dataset_directory = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset"
-    )
-
-    self.adult = test_utils.load_datasets("adult")
-    self.two_center_regression = test_utils.load_datasets(
-        "two_center_regression"
-    )
-
-  @parameterized.parameters(
-      (
-          custom_loss.RegressionLoss,
-          generic_learner.Task.REGRESSION,
-          "col_float",
-      ),
-      (
-          custom_loss.BinaryClassificationLoss,
-          generic_learner.Task.CLASSIFICATION,
-          "binary_int_label",
-      ),
-      (
-          custom_loss.MultiClassificationLoss,
-          generic_learner.Task.CLASSIFICATION,
-          "col_three_string",
-      ),
-  )
-  def test_loss_raises_exception(self, loss_type, task, label_col):
-
-    def faulty_initial_prediction(*_):
-      raise NotImplementedError("Faulty initial prediction")
-
-    faulty_custom_loss = loss_type(
-        initial_predictions=faulty_initial_prediction,
-        gradient_and_hessian=lambda x, y: (np.ones(len(x)), np.ones(len(x))),
-        loss=lambda x, y, z: np.float32(0),
-        activation=custom_loss.Activation.IDENTITY,
-    )
-    ds = test_utils.toy_dataset()
-    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
-        label=label_col,
-        loss=faulty_custom_loss,
-        task=task,
-    )
-    with self.assertRaisesRegex(
-        RuntimeError, ".*NotImplementedError: Faulty initial prediction"
-    ):
-      _ = learner_custom_loss.train(ds)
-
-  def test_avoid_memory_corruption(self):
-    ref_to_labels = None
-
-    def faulty_gradient_and_hessian(labels, predictions):
-      nonlocal ref_to_labels
-      ref_to_labels = labels
-      return (np.ones(len(labels)), np.ones(len(predictions)))
-
-    faulty_custom_loss = custom_loss.RegressionLoss(
-        initial_predictions=lambda x, y: np.float32(0),
-        gradient_and_hessian=faulty_gradient_and_hessian,
-        loss=lambda x, y, z: np.float32(0),
-        activation=custom_loss.Activation.IDENTITY,
-    )
-    ds = test_utils.toy_dataset()
-    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
-        label="col_float",
-        loss=faulty_custom_loss,
-        task=generic_learner.Task.REGRESSION,
-        num_trees=5,
-    )
-    with self.assertRaisesRegex(
-        RuntimeError,
-        'Cannot hold a reference to "labels" outside of a custom loss'
-        " function.*",
-    ):
-      _ = learner_custom_loss.train(ds)
-
-  def test_honor_trigger_gc(self):
-    ref_to_labels = None
-
-    def faulty_gradient_and_hessian(labels, predictions):
-      nonlocal ref_to_labels
-      ref_to_labels = labels
-      return (np.ones(len(labels)), np.ones(len(predictions)))
-
-    faulty_custom_loss = custom_loss.RegressionLoss(
-        initial_predictions=lambda x, y: np.float32(0),
-        gradient_and_hessian=faulty_gradient_and_hessian,
-        loss=lambda x, y, z: np.float32(0),
-        activation=custom_loss.Activation.IDENTITY,
-        may_trigger_gc=False,
-    )
-    ds = test_utils.toy_dataset()
-    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
-        label="col_float",
-        loss=faulty_custom_loss,
-        task=generic_learner.Task.REGRESSION,
-        num_trees=5,
-    )
-    model = learner_custom_loss.train(ds)
-    self.assertEqual(model.num_trees(), 5)
-
-  def test_readonly_args(self):
-    def faulty_initial_prediction(
-        labels: npty.NDArray[np.float32], _: npty.NDArray[np.float32]
-    ) -> np.float32:
-      labels[0] = 5
-      return np.float32(0)
-
-    faulty_custom_loss = custom_loss.RegressionLoss(
-        initial_predictions=faulty_initial_prediction,
-        gradient_and_hessian=lambda x, y: (np.ones(len(x)), np.ones(len(x))),
-        loss=lambda x, y, z: np.float32(0),
-        activation=custom_loss.Activation.IDENTITY,
-    )
-    ds = test_utils.toy_dataset()
-    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
-        label="col_float",
-        loss=faulty_custom_loss,
-        task=generic_learner.Task.REGRESSION,
-    )
-    with self.assertRaisesRegex(RuntimeError, ".*read-only.*"):
-      _ = learner_custom_loss.train(ds)
-
-  @parameterized.parameters(
-      (
-          custom_loss.RegressionLoss,
-          generic_learner.Task.CLASSIFICATION,
-          "binary_int_label",
-      ),
-      (
-          custom_loss.BinaryClassificationLoss,
-          generic_learner.Task.REGRESSION,
-          "col_float",
-      ),
-      (
-          custom_loss.MultiClassificationLoss,
-          generic_learner.Task.REGRESSION,
-          "col_float",
-      ),
-  )
-  def test_invalid_tasks(self, loss_type, task_type, label):
-    custom_loss_container = loss_type(
-        initial_predictions=lambda x, y: np.float32(0),
-        gradient_and_hessian=lambda x, y: np.ones([6, len(x)]),
-        loss=lambda x, y, z: np.float32(0),
-        activation=custom_loss.Activation.IDENTITY,
-    )
-    ds = test_utils.toy_dataset()
-    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
-        label=label,
-        loss=custom_loss_container,
-        task=task_type,
-        num_trees=1,
-    )
-    with self.assertRaisesRegex(
-        ValueError, "A .* is only compatible with .* tasks.*"
-    ):
-      _ = learner_custom_loss.train(ds)
-
-  def test_mse_custom_equal_to_builtin(self):
-    def mse_initial_predictions(
-        labels: npty.NDArray[np.float32], weights: npty.NDArray[np.float32]
-    ) -> np.float32:
-      return np.average(labels, weights=weights)
-
-    def mse_gradient(
-        labels: npty.NDArray[np.float32], predictions: npty.NDArray[np.float32]
-    ) -> Tuple[npty.NDArray[np.float32], npty.NDArray[np.float32]]:
-      return (predictions - labels, -np.ones(labels.shape))
-
-    def mse_loss(
-        labels: npty.NDArray[np.float32],
-        predictions: npty.NDArray[np.float32],
-        weights: npty.NDArray[np.float32],
-    ) -> np.float32:
-      numerator = np.sum(np.multiply(weights, np.square(labels - predictions)))
-      denominator = np.sum(weights)
-      return np.sqrt(numerator / denominator)
-
-    mse_custom_loss = custom_loss.RegressionLoss(
-        initial_predictions=mse_initial_predictions,
-        gradient_and_hessian=mse_gradient,
-        loss=mse_loss,
-        activation=custom_loss.Activation.IDENTITY,
-    )
-
-    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
-        label="target",
-        loss=mse_custom_loss,
-        task=generic_learner.Task.REGRESSION,
-        num_trees=30,
-        early_stopping="NONE",
-        validation_ratio=0.0,
-    )
-    model_custom_loss: generic_model.GenericModel = learner_custom_loss.train(
-        self.two_center_regression.train
-    )
-
-    learner_builtin_loss = specialized_learners.GradientBoostedTreesLearner(
-        label="target",
-        task=generic_learner.Task.REGRESSION,
-        num_trees=30,
-        early_stopping="NONE",
-        validation_ratio=0.0,
-    )
-    model_builtin_loss: generic_model.GenericModel = learner_builtin_loss.train(
-        self.two_center_regression.train
-    )
-    npt.assert_allclose(
-        model_custom_loss.predict(self.two_center_regression.test),
-        model_builtin_loss.predict(self.two_center_regression.test),
-        rtol=1e-5,  # Without activation function, the predictions can be large.
-        atol=1e-6,
-    )
-
-  @parameterized.parameters(
-      custom_loss.Activation.IDENTITY,
-      custom_loss.Activation.SIGMOID,
-  )
-  def test_binomial_custom_equal_to_builtin(self, activation):
-    def binomial_initial_predictions(
-        labels: npty.NDArray[np.int32], weights: npty.NDArray[np.float32]
-    ) -> np.float32:
-      sum_weights = np.sum(weights)
-      sum_weights_positive = np.sum((labels == 2) * weights)
-      ratio_positive = sum_weights_positive / sum_weights
-      if ratio_positive == 0.0:
-        return -np.iinfo(np.float32).max
-      elif ratio_positive == 1.0:
-        return np.iinfo(np.float32).max
-      return np.log(ratio_positive / (1 - ratio_positive))
-
-    def binomial_gradient(
-        labels: npty.NDArray[np.int32], predictions: npty.NDArray[np.float32]
-    ) -> Tuple[npty.NDArray[np.float32], npty.NDArray[np.float32]]:
-      pred_probability = 1.0 / (1.0 + np.exp(-predictions))
-      binary_labels = labels == 2
-      return (
-          pred_probability - binary_labels,
-          pred_probability * (pred_probability - 1),
-      )
-
-    def binomial_loss(
-        labels: npty.NDArray[np.int32],
-        predictions: npty.NDArray[np.float32],
-        weights: npty.NDArray[np.float32],
-    ) -> np.float32:
-      binary_labels = labels == 2
-      return (
-          -2.0
-          * np.sum(
-              weights
-              * (
-                  binary_labels * predictions
-                  - np.log(1.0 + np.exp(predictions))
-              )
-          )
-          / np.sum(weights)
-      )
-
-    binomial_custom_loss = custom_loss.BinaryClassificationLoss(
-        initial_predictions=binomial_initial_predictions,
-        gradient_and_hessian=binomial_gradient,
-        loss=binomial_loss,
-        activation=activation,
-    )
-
-    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
-        label="income",
-        loss=binomial_custom_loss,
-        task=generic_learner.Task.CLASSIFICATION,
-        early_stopping="NONE",
-        validation_ratio=0.0,
-        num_trees=30,
-    )
-    model_custom_loss: generic_model.GenericModel = learner_custom_loss.train(
-        self.adult.train
-    )
-
-    learner_builtin_loss = specialized_learners.GradientBoostedTreesLearner(
-        label="income",
-        task=generic_learner.Task.CLASSIFICATION,
-        apply_link_function=(activation == custom_loss.Activation.SIGMOID),
-        early_stopping="NONE",
-        validation_ratio=0.0,
-        num_trees=30,
-    )
-    model_builtin_loss: generic_model.GenericModel = learner_builtin_loss.train(
-        self.adult.train
-    )
-    npt.assert_allclose(
-        model_custom_loss.predict(self.adult.test),
-        model_builtin_loss.predict(self.adult.test),
-        rtol=1e-5,  # Without activation function, the predictions can be large.
-        atol=1e-6,
-    )
-
-  @parameterized.parameters(
-      custom_loss.Activation.IDENTITY,
-      custom_loss.Activation.SOFTMAX,
-  )
-  def test_multinomial_custom_equal_to_builtin(self, activation):
-    def multinomial_initial_predictions(
-        labels: npty.NDArray[np.int32], _: npty.NDArray[np.float32]
-    ) -> npty.NDArray[np.float32]:
-      dimension = np.max(labels)
-      return np.zeros(dimension, dtype=np.float32)
-
-    def multinomial_gradient(
-        labels: npty.NDArray[np.int32], predictions: npty.NDArray[np.float32]
-    ) -> Tuple[npty.NDArray[np.float32], npty.NDArray[np.float32]]:
-      dimension = np.max(labels)
-      normalization = 1.0 / np.sum(np.exp(predictions), axis=1)
-      normalized_predictions = np.exp(predictions) * normalization[:, None]
-      label_indicator = (
-          (labels - 1)[:, np.newaxis] == np.arange(dimension)
-      ).astype(int)
-      gradient = normalized_predictions - label_indicator
-      hessian = np.abs(gradient) * (np.abs(gradient) - 1)
-      return (np.transpose(gradient), np.transpose(hessian))
-
-    def multinomial_loss(
-        labels: npty.NDArray[np.int32],
-        predictions: npty.NDArray[np.float32],
-        weights: npty.NDArray[np.float32],
-    ) -> np.float32:
-      sum_weights = np.sum(weights)
-      dimension = np.max(labels)
-      sum_exp_pred = np.sum(np.exp(predictions), axis=1)
-      indicator_matrix = (
-          (labels - 1)[:, np.newaxis] == np.arange(dimension)
-      ).astype(int)
-      label_exp_pred = np.exp(np.sum(predictions * indicator_matrix, axis=1))
-      return (
-          -np.sum(weights * np.log(label_exp_pred / sum_exp_pred)) / sum_weights
-      )
-
-    # Path to dataset.
-    dataset_directory = os.path.join(test_utils.ydf_test_data_path(), "dataset")
-    train_path = os.path.join(dataset_directory, "dna.csv")
-    all_ds = pd.read_csv(train_path)
-    # Randomly split the dataset into a training (90%) and testing (10%) dataset
-    all_ds = all_ds.sample(frac=1)
-    split_idx = len(all_ds) * 9 // 10
-    train_ds = all_ds.iloc[:split_idx]
-    test_ds = all_ds.iloc[split_idx:]
-
-    multinomial_custom_loss = custom_loss.MultiClassificationLoss(
-        initial_predictions=multinomial_initial_predictions,
-        gradient_and_hessian=multinomial_gradient,
-        loss=multinomial_loss,
-        activation=activation,
-    )
-
-    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
-        label="LABEL",
-        loss=multinomial_custom_loss,
-        task=generic_learner.Task.CLASSIFICATION,
-        early_stopping="NONE",
-        validation_ratio=0.0,
-        num_trees=30,
-    )
-    model_custom_loss: generic_model.GenericModel = learner_custom_loss.train(
-        train_ds
-    )
-
-    learner_builtin_loss = specialized_learners.GradientBoostedTreesLearner(
-        label="LABEL",
-        task=generic_learner.Task.CLASSIFICATION,
-        apply_link_function=(activation == custom_loss.Activation.SOFTMAX),
-        early_stopping="NONE",
-        validation_ratio=0.0,
-        num_trees=30,
-    )
-    model_builtin_loss: generic_model.GenericModel = learner_builtin_loss.train(
-        train_ds
-    )
-    npt.assert_allclose(
-        model_custom_loss.predict(test_ds),
-        model_builtin_loss.predict(test_ds),
-        rtol=1e-5,  # Without activation function, the predictions can be large.
-        atol=1e-6,
-    )
-
-  def test_multiclass_initial_prediction(self):
-    def multiclass_initial_prediction(
-        labels: npty.NDArray[np.int32], _: npty.NDArray[np.float32]
-    ) -> npty.NDArray[np.float32]:
-      dimension = np.max(labels)
-      return np.arange(1, dimension + 1)
-
-    multiclass_custom_loss = custom_loss.MultiClassificationLoss(
-        initial_predictions=multiclass_initial_prediction,
-        gradient_and_hessian=lambda x, y: (
-            np.ones([3, len(x)]),
-            np.ones([3, len(x)]),
-        ),
-        loss=lambda x, y, z: np.float32(0),
-        activation=custom_loss.Activation.IDENTITY,
-    )
-    ds = test_utils.toy_dataset()
-    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
-        label="col_three_string",
-        loss=multiclass_custom_loss,
-        task=generic_learner.Task.CLASSIFICATION,
-        num_trees=1,
-    )
-    model: gradient_boosted_trees_model.GradientBoostedTreesModel = (
-        learner_custom_loss.train(ds)
-    )
-    npt.assert_equal(model.initial_predictions(), [1, 2, 3])
-
-  def test_multiclass_wrong_initial_prediction_dimensions(self):
-    def multiclass_initial_prediction(
-        labels: npty.NDArray[np.int32], _: npty.NDArray[np.float32]
-    ) -> npty.NDArray[np.float32]:
-      dimension = np.max(labels)
-      return np.arange(1, dimension)
-
-    multiclass_custom_loss = custom_loss.MultiClassificationLoss(
-        initial_predictions=multiclass_initial_prediction,
-        gradient_and_hessian=lambda x, y: (
-            np.ones([3, len(x)]),
-            np.ones([3, len(x)]),
-        ),
-        loss=lambda x, y, z: np.float32(0),
-        activation=custom_loss.Activation.IDENTITY,
-    )
-    ds = test_utils.toy_dataset()
-    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
-        label="col_three_string",
-        loss=multiclass_custom_loss,
-        task=generic_learner.Task.CLASSIFICATION,
-        num_trees=1,
-    )
-    with self.assertRaisesRegex(
-        ValueError,
-        "The initial_predictions must be a one-dimensional Numpy array of 3"
-        " elements.*",
-    ):
-      _ = learner_custom_loss.train(ds)
-
-  @parameterized.parameters(
-      (
-          custom_loss.RegressionLoss,
-          generic_learner.Task.REGRESSION,
-          "col_float",
-      ),
-      (
-          custom_loss.BinaryClassificationLoss,
-          generic_learner.Task.CLASSIFICATION,
-          "binary_int_label",
-      ),
-  )
-  def test_wrong_gradient_dimensions(self, loss_type, task_type, label):
-    faulty_loss = loss_type(
-        initial_predictions=lambda x, y: np.float32(0),
-        gradient_and_hessian=lambda x, y: (
-            np.ones(len(x) - 1),
-            np.ones(len(x)),
-        ),
-        loss=lambda x, y, z: np.float32(0),
-        activation=custom_loss.Activation.IDENTITY,
-    )
-    ds = test_utils.toy_dataset()
-    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
-        label=label,
-        loss=faulty_loss,
-        task=task_type,
-        num_trees=1,
-    )
-    with self.assertRaisesRegex(
-        ValueError,
-        "The gradient must be a one-dimensional Numpy array of 5 elements.*",
-    ):
-      _ = learner_custom_loss.train(ds)
-
-  @parameterized.parameters(
-      (
-          custom_loss.RegressionLoss,
-          generic_learner.Task.REGRESSION,
-          "col_float",
-      ),
-      (
-          custom_loss.BinaryClassificationLoss,
-          generic_learner.Task.CLASSIFICATION,
-          "binary_int_label",
-      ),
-  )
-  def test_wrong_hessian_dimensions(self, loss_type, task_type, label):
-    faulty_loss = loss_type(
-        initial_predictions=lambda x, y: np.float32(0),
-        gradient_and_hessian=lambda x, y: (
-            np.ones(len(x)),
-            np.ones(len(x) - 1),
-        ),
-        loss=lambda x, y, z: np.float32(0),
-        activation=custom_loss.Activation.IDENTITY,
-    )
-    ds = test_utils.toy_dataset()
-    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
-        label=label,
-        loss=faulty_loss,
-        task=task_type,
-        num_trees=1,
-    )
-    with self.assertRaisesRegex(
-        ValueError,
-        "The hessian must be a one-dimensional Numpy array of 5 elements.*",
-    ):
-      _ = learner_custom_loss.train(ds)
-
-  @parameterized.parameters(
-      (
-          custom_loss.RegressionLoss,
-          generic_learner.Task.REGRESSION,
-          "col_float",
-      ),
-      (
-          custom_loss.BinaryClassificationLoss,
-          generic_learner.Task.CLASSIFICATION,
-          "binary_int_label",
-      ),
-  )
-  def test_stacked_gradient_dimensions(self, loss_type, task_type, label):
-    faulty_loss = loss_type(
-        initial_predictions=lambda x, y: np.float32(0),
-        gradient_and_hessian=lambda x, y: 3 * np.ones([2, 5]),
-        loss=lambda x, y, z: np.float32(0),
-        activation=custom_loss.Activation.IDENTITY,
-    )
-    ds = test_utils.toy_dataset()
-    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
-        label=label,
-        loss=faulty_loss,
-        task=task_type,
-        num_trees=1,
-    )
-    with self.assertRaisesRegex(
-        ValueError,
-        ".*gradient_and_hessian function returned a numpy array, expected a"
-        " Sequence of two numpy arrays.*",
-    ):
-      _ = learner_custom_loss.train(ds)
-
-  def test_loss_with_jax_nojit(self):
-    def mse_loss(labels, predictions):
-      numerator = jnp.sum(jnp.square(jnp.subtract(labels, predictions)))
-      denominator = jnp.size(labels)
-      res = jax.block_until_ready(jnp.divide(numerator, denominator))
-      return res
-
-    def weighted_mse_loss(labels, predictions, _):
-      return mse_loss(labels, predictions)
-
-    mse_grad = jax.grad(mse_loss, argnums=1)
-    mse_hessian = jax.jacfwd(jax.jacrev(mse_loss), argnums=1)
-
-    def mse_gradient_and_hessian(labels, predictions):
-      res = (
-          mse_grad(labels, predictions).block_until_ready(),
-          jnp.diagonal(mse_hessian(labels, predictions)).block_until_ready(),
-      )
-      return res
-
-    def mse_initial_predictions(labels, weights):
-      res = jax.block_until_ready(jnp.average(labels, weights=weights))
-      return res
-
-    mse_custom_loss = custom_loss.RegressionLoss(
-        initial_predictions=mse_initial_predictions,
-        gradient_and_hessian=mse_gradient_and_hessian,
-        loss=weighted_mse_loss,
-        activation=custom_loss.Activation.IDENTITY,
-    )
-
-    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
-        label="target",
-        loss=mse_custom_loss,
-        task=generic_learner.Task.REGRESSION,
-        num_trees=30,
-        early_stopping="NONE",
-        validation_ratio=0.0,
-    )
-    model_custom_loss: generic_model.GenericModel = learner_custom_loss.train(
-        self.two_center_regression.train
-    )
-
-    learner_builtin_loss = specialized_learners.GradientBoostedTreesLearner(
-        label="target",
-        task=generic_learner.Task.REGRESSION,
-        num_trees=30,
-        early_stopping="NONE",
-        validation_ratio=0.0,
-    )
-    model_builtin_loss: generic_model.GenericModel = learner_builtin_loss.train(
-        self.two_center_regression.train
-    )
-    npt.assert_allclose(
-        model_custom_loss.predict(self.two_center_regression.test),
-        model_builtin_loss.predict(self.two_center_regression.test),
-        rtol=1e-5,  # Without activation function, the predictions can be large.
-        atol=1e-6,
-    )
-
-  def test_loss_with_jax_jit(self):
-    @jax.jit
-    def mse_loss(labels, predictions):
-      numerator = jnp.sum(jnp.square(jnp.subtract(labels, predictions)))
-      denominator = jnp.size(labels)
-      res = jax.block_until_ready(jnp.divide(numerator, denominator))
-      return res
-
-    @jax.jit
-    def weighted_mse_loss(labels, predictions, _):
-      return mse_loss(labels, predictions)
-
-    mse_grad = jax.jit(jax.grad(mse_loss, argnums=1))
-    mse_hessian = jax.jit(jax.jacfwd(jax.jacrev(mse_loss), argnums=1))
-
-    def mse_gradient_and_hessian(labels, predictions):
-      return (
-          mse_grad(labels, predictions).block_until_ready(),
-          jnp.diagonal(mse_hessian(labels, predictions)).block_until_ready(),
-      )
-
-    @jax.jit
-    def mse_initial_predictions(labels, weights):
-      res = jax.block_until_ready(jnp.average(labels, weights=weights))
-      return res
-
-    mse_custom_loss = custom_loss.RegressionLoss(
-        initial_predictions=mse_initial_predictions,
-        gradient_and_hessian=mse_gradient_and_hessian,
-        loss=weighted_mse_loss,
-        activation=custom_loss.Activation.IDENTITY,
-    )
-
-    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
-        label="target",
-        loss=mse_custom_loss,
-        task=generic_learner.Task.REGRESSION,
-        num_trees=30,
-        early_stopping="NONE",
-        validation_ratio=0.0,
-    )
-    model_custom_loss: generic_model.GenericModel = learner_custom_loss.train(
-        self.two_center_regression.train
-    )
-
-    learner_builtin_loss = specialized_learners.GradientBoostedTreesLearner(
-        label="target",
-        task=generic_learner.Task.REGRESSION,
-        num_trees=30,
-        early_stopping="NONE",
-        validation_ratio=0.0,
-    )
-    model_builtin_loss: generic_model.GenericModel = learner_builtin_loss.train(
-        self.two_center_regression.train
-    )
-    npt.assert_allclose(
-        model_custom_loss.predict(self.two_center_regression.test),
-        model_builtin_loss.predict(self.two_center_regression.test),
-        rtol=1e-5,  # Without activation function, the predictions can be large.
-        atol=1e-6,
-    )
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Tests for the custom losses."""
+
+import os
+from typing import Tuple
+
+from absl.testing import absltest
+from absl.testing import parameterized
+import jax
+import jax.numpy as jnp
+import numpy as np
+import numpy.testing as npt
+import numpy.typing as npty
+import pandas as pd
+
+from ydf.dataset import dataspec
+from ydf.learner import custom_loss
+from ydf.learner import generic_learner
+from ydf.learner import specialized_learners
+from ydf.model import generic_model
+from ydf.model.gradient_boosted_trees_model import gradient_boosted_trees_model
+from ydf.utils import test_utils
+
+Column = dataspec.Column
+
+
+class CustomLossTest(parameterized.TestCase):
+
+  def setUp(self):
+    super().setUp()
+    self.dataset_directory = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset"
+    )
+
+    self.adult = test_utils.load_datasets("adult")
+    self.two_center_regression = test_utils.load_datasets(
+        "two_center_regression"
+    )
+
+  @parameterized.parameters(
+      (
+          custom_loss.RegressionLoss,
+          generic_learner.Task.REGRESSION,
+          "col_float",
+      ),
+      (
+          custom_loss.BinaryClassificationLoss,
+          generic_learner.Task.CLASSIFICATION,
+          "binary_int_label",
+      ),
+      (
+          custom_loss.MultiClassificationLoss,
+          generic_learner.Task.CLASSIFICATION,
+          "col_three_string",
+      ),
+  )
+  def test_loss_raises_exception(self, loss_type, task, label_col):
+
+    def faulty_initial_prediction(*_):
+      raise NotImplementedError("Faulty initial prediction")
+
+    faulty_custom_loss = loss_type(
+        initial_predictions=faulty_initial_prediction,
+        gradient_and_hessian=lambda x, y: (np.ones(len(x)), np.ones(len(x))),
+        loss=lambda x, y, z: np.float32(0),
+        activation=custom_loss.Activation.IDENTITY,
+    )
+    ds = test_utils.toy_dataset()
+    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
+        label=label_col,
+        loss=faulty_custom_loss,
+        task=task,
+    )
+    with self.assertRaisesRegex(
+        RuntimeError, ".*NotImplementedError: Faulty initial prediction"
+    ):
+      _ = learner_custom_loss.train(ds)
+
+  def test_avoid_memory_corruption(self):
+    ref_to_labels = None
+
+    def faulty_gradient_and_hessian(labels, predictions):
+      nonlocal ref_to_labels
+      ref_to_labels = labels
+      return (np.ones(len(labels)), np.ones(len(predictions)))
+
+    faulty_custom_loss = custom_loss.RegressionLoss(
+        initial_predictions=lambda x, y: np.float32(0),
+        gradient_and_hessian=faulty_gradient_and_hessian,
+        loss=lambda x, y, z: np.float32(0),
+        activation=custom_loss.Activation.IDENTITY,
+    )
+    ds = test_utils.toy_dataset()
+    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
+        label="col_float",
+        loss=faulty_custom_loss,
+        task=generic_learner.Task.REGRESSION,
+        num_trees=5,
+    )
+    with self.assertRaisesRegex(
+        RuntimeError,
+        'Cannot hold a reference to "labels" outside of a custom loss'
+        " function.*",
+    ):
+      _ = learner_custom_loss.train(ds)
+
+  def test_honor_trigger_gc(self):
+    ref_to_labels = None
+
+    def faulty_gradient_and_hessian(labels, predictions):
+      nonlocal ref_to_labels
+      ref_to_labels = labels
+      return (np.ones(len(labels)), np.ones(len(predictions)))
+
+    faulty_custom_loss = custom_loss.RegressionLoss(
+        initial_predictions=lambda x, y: np.float32(0),
+        gradient_and_hessian=faulty_gradient_and_hessian,
+        loss=lambda x, y, z: np.float32(0),
+        activation=custom_loss.Activation.IDENTITY,
+        may_trigger_gc=False,
+    )
+    ds = test_utils.toy_dataset()
+    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
+        label="col_float",
+        loss=faulty_custom_loss,
+        task=generic_learner.Task.REGRESSION,
+        num_trees=5,
+    )
+    model = learner_custom_loss.train(ds)
+    self.assertEqual(model.num_trees(), 5)
+
+  def test_readonly_args(self):
+    def faulty_initial_prediction(
+        labels: npty.NDArray[np.float32], _: npty.NDArray[np.float32]
+    ) -> np.float32:
+      labels[0] = 5
+      return np.float32(0)
+
+    faulty_custom_loss = custom_loss.RegressionLoss(
+        initial_predictions=faulty_initial_prediction,
+        gradient_and_hessian=lambda x, y: (np.ones(len(x)), np.ones(len(x))),
+        loss=lambda x, y, z: np.float32(0),
+        activation=custom_loss.Activation.IDENTITY,
+    )
+    ds = test_utils.toy_dataset()
+    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
+        label="col_float",
+        loss=faulty_custom_loss,
+        task=generic_learner.Task.REGRESSION,
+    )
+    with self.assertRaisesRegex(RuntimeError, ".*read-only.*"):
+      _ = learner_custom_loss.train(ds)
+
+  @parameterized.parameters(
+      (
+          custom_loss.RegressionLoss,
+          generic_learner.Task.CLASSIFICATION,
+          "binary_int_label",
+      ),
+      (
+          custom_loss.BinaryClassificationLoss,
+          generic_learner.Task.REGRESSION,
+          "col_float",
+      ),
+      (
+          custom_loss.MultiClassificationLoss,
+          generic_learner.Task.REGRESSION,
+          "col_float",
+      ),
+  )
+  def test_invalid_tasks(self, loss_type, task_type, label):
+    custom_loss_container = loss_type(
+        initial_predictions=lambda x, y: np.float32(0),
+        gradient_and_hessian=lambda x, y: np.ones([6, len(x)]),
+        loss=lambda x, y, z: np.float32(0),
+        activation=custom_loss.Activation.IDENTITY,
+    )
+    ds = test_utils.toy_dataset()
+    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
+        label=label,
+        loss=custom_loss_container,
+        task=task_type,
+        num_trees=1,
+    )
+    with self.assertRaisesRegex(
+        ValueError, "A .* is only compatible with .* tasks.*"
+    ):
+      _ = learner_custom_loss.train(ds)
+
+  def test_mse_custom_equal_to_builtin(self):
+    def mse_initial_predictions(
+        labels: npty.NDArray[np.float32], weights: npty.NDArray[np.float32]
+    ) -> np.float32:
+      return np.average(labels, weights=weights)
+
+    def mse_gradient(
+        labels: npty.NDArray[np.float32], predictions: npty.NDArray[np.float32]
+    ) -> Tuple[npty.NDArray[np.float32], npty.NDArray[np.float32]]:
+      return (predictions - labels, -np.ones(labels.shape))
+
+    def mse_loss(
+        labels: npty.NDArray[np.float32],
+        predictions: npty.NDArray[np.float32],
+        weights: npty.NDArray[np.float32],
+    ) -> np.float32:
+      numerator = np.sum(np.multiply(weights, np.square(labels - predictions)))
+      denominator = np.sum(weights)
+      return np.sqrt(numerator / denominator)
+
+    mse_custom_loss = custom_loss.RegressionLoss(
+        initial_predictions=mse_initial_predictions,
+        gradient_and_hessian=mse_gradient,
+        loss=mse_loss,
+        activation=custom_loss.Activation.IDENTITY,
+    )
+
+    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
+        label="target",
+        loss=mse_custom_loss,
+        task=generic_learner.Task.REGRESSION,
+        num_trees=30,
+        early_stopping="NONE",
+        validation_ratio=0.0,
+    )
+    model_custom_loss: generic_model.GenericModel = learner_custom_loss.train(
+        self.two_center_regression.train
+    )
+
+    learner_builtin_loss = specialized_learners.GradientBoostedTreesLearner(
+        label="target",
+        task=generic_learner.Task.REGRESSION,
+        num_trees=30,
+        early_stopping="NONE",
+        validation_ratio=0.0,
+    )
+    model_builtin_loss: generic_model.GenericModel = learner_builtin_loss.train(
+        self.two_center_regression.train
+    )
+    npt.assert_allclose(
+        model_custom_loss.predict(self.two_center_regression.test),
+        model_builtin_loss.predict(self.two_center_regression.test),
+        rtol=1e-5,  # Without activation function, the predictions can be large.
+        atol=1e-6,
+    )
+
+  @parameterized.parameters(
+      custom_loss.Activation.IDENTITY,
+      custom_loss.Activation.SIGMOID,
+  )
+  def test_binomial_custom_equal_to_builtin(self, activation):
+    def binomial_initial_predictions(
+        labels: npty.NDArray[np.int32], weights: npty.NDArray[np.float32]
+    ) -> np.float32:
+      sum_weights = np.sum(weights)
+      sum_weights_positive = np.sum((labels == 2) * weights)
+      ratio_positive = sum_weights_positive / sum_weights
+      if ratio_positive == 0.0:
+        return -np.iinfo(np.float32).max
+      elif ratio_positive == 1.0:
+        return np.iinfo(np.float32).max
+      return np.log(ratio_positive / (1 - ratio_positive))
+
+    def binomial_gradient(
+        labels: npty.NDArray[np.int32], predictions: npty.NDArray[np.float32]
+    ) -> Tuple[npty.NDArray[np.float32], npty.NDArray[np.float32]]:
+      pred_probability = 1.0 / (1.0 + np.exp(-predictions))
+      binary_labels = labels == 2
+      return (
+          pred_probability - binary_labels,
+          pred_probability * (pred_probability - 1),
+      )
+
+    def binomial_loss(
+        labels: npty.NDArray[np.int32],
+        predictions: npty.NDArray[np.float32],
+        weights: npty.NDArray[np.float32],
+    ) -> np.float32:
+      binary_labels = labels == 2
+      return (
+          -2.0
+          * np.sum(
+              weights
+              * (
+                  binary_labels * predictions
+                  - np.log(1.0 + np.exp(predictions))
+              )
+          )
+          / np.sum(weights)
+      )
+
+    binomial_custom_loss = custom_loss.BinaryClassificationLoss(
+        initial_predictions=binomial_initial_predictions,
+        gradient_and_hessian=binomial_gradient,
+        loss=binomial_loss,
+        activation=activation,
+    )
+
+    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
+        label="income",
+        loss=binomial_custom_loss,
+        task=generic_learner.Task.CLASSIFICATION,
+        early_stopping="NONE",
+        validation_ratio=0.0,
+        num_trees=30,
+    )
+    model_custom_loss: generic_model.GenericModel = learner_custom_loss.train(
+        self.adult.train
+    )
+
+    learner_builtin_loss = specialized_learners.GradientBoostedTreesLearner(
+        label="income",
+        task=generic_learner.Task.CLASSIFICATION,
+        apply_link_function=(activation == custom_loss.Activation.SIGMOID),
+        early_stopping="NONE",
+        validation_ratio=0.0,
+        num_trees=30,
+    )
+    model_builtin_loss: generic_model.GenericModel = learner_builtin_loss.train(
+        self.adult.train
+    )
+    npt.assert_allclose(
+        model_custom_loss.predict(self.adult.test),
+        model_builtin_loss.predict(self.adult.test),
+        rtol=1e-5,  # Without activation function, the predictions can be large.
+        atol=1e-6,
+    )
+
+  @parameterized.parameters(
+      custom_loss.Activation.IDENTITY,
+      custom_loss.Activation.SOFTMAX,
+  )
+  def test_multinomial_custom_equal_to_builtin(self, activation):
+    def multinomial_initial_predictions(
+        labels: npty.NDArray[np.int32], _: npty.NDArray[np.float32]
+    ) -> npty.NDArray[np.float32]:
+      dimension = np.max(labels)
+      return np.zeros(dimension, dtype=np.float32)
+
+    def multinomial_gradient(
+        labels: npty.NDArray[np.int32], predictions: npty.NDArray[np.float32]
+    ) -> Tuple[npty.NDArray[np.float32], npty.NDArray[np.float32]]:
+      dimension = np.max(labels)
+      normalization = 1.0 / np.sum(np.exp(predictions), axis=1)
+      normalized_predictions = np.exp(predictions) * normalization[:, None]
+      label_indicator = (
+          (labels - 1)[:, np.newaxis] == np.arange(dimension)
+      ).astype(int)
+      gradient = normalized_predictions - label_indicator
+      hessian = np.abs(gradient) * (np.abs(gradient) - 1)
+      return (np.transpose(gradient), np.transpose(hessian))
+
+    def multinomial_loss(
+        labels: npty.NDArray[np.int32],
+        predictions: npty.NDArray[np.float32],
+        weights: npty.NDArray[np.float32],
+    ) -> np.float32:
+      sum_weights = np.sum(weights)
+      dimension = np.max(labels)
+      sum_exp_pred = np.sum(np.exp(predictions), axis=1)
+      indicator_matrix = (
+          (labels - 1)[:, np.newaxis] == np.arange(dimension)
+      ).astype(int)
+      label_exp_pred = np.exp(np.sum(predictions * indicator_matrix, axis=1))
+      return (
+          -np.sum(weights * np.log(label_exp_pred / sum_exp_pred)) / sum_weights
+      )
+
+    # Path to dataset.
+    dataset_directory = os.path.join(test_utils.ydf_test_data_path(), "dataset")
+    train_path = os.path.join(dataset_directory, "dna.csv")
+    all_ds = pd.read_csv(train_path)
+    # Randomly split the dataset into a training (90%) and testing (10%) dataset
+    all_ds = all_ds.sample(frac=1)
+    split_idx = len(all_ds) * 9 // 10
+    train_ds = all_ds.iloc[:split_idx]
+    test_ds = all_ds.iloc[split_idx:]
+
+    multinomial_custom_loss = custom_loss.MultiClassificationLoss(
+        initial_predictions=multinomial_initial_predictions,
+        gradient_and_hessian=multinomial_gradient,
+        loss=multinomial_loss,
+        activation=activation,
+    )
+
+    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
+        label="LABEL",
+        loss=multinomial_custom_loss,
+        task=generic_learner.Task.CLASSIFICATION,
+        early_stopping="NONE",
+        validation_ratio=0.0,
+        num_trees=30,
+    )
+    model_custom_loss: generic_model.GenericModel = learner_custom_loss.train(
+        train_ds
+    )
+
+    learner_builtin_loss = specialized_learners.GradientBoostedTreesLearner(
+        label="LABEL",
+        task=generic_learner.Task.CLASSIFICATION,
+        apply_link_function=(activation == custom_loss.Activation.SOFTMAX),
+        early_stopping="NONE",
+        validation_ratio=0.0,
+        num_trees=30,
+    )
+    model_builtin_loss: generic_model.GenericModel = learner_builtin_loss.train(
+        train_ds
+    )
+    npt.assert_allclose(
+        model_custom_loss.predict(test_ds),
+        model_builtin_loss.predict(test_ds),
+        rtol=1e-5,  # Without activation function, the predictions can be large.
+        atol=1e-6,
+    )
+
+  def test_multiclass_initial_prediction(self):
+    def multiclass_initial_prediction(
+        labels: npty.NDArray[np.int32], _: npty.NDArray[np.float32]
+    ) -> npty.NDArray[np.float32]:
+      dimension = np.max(labels)
+      return np.arange(1, dimension + 1)
+
+    multiclass_custom_loss = custom_loss.MultiClassificationLoss(
+        initial_predictions=multiclass_initial_prediction,
+        gradient_and_hessian=lambda x, y: (
+            np.ones([3, len(x)]),
+            np.ones([3, len(x)]),
+        ),
+        loss=lambda x, y, z: np.float32(0),
+        activation=custom_loss.Activation.IDENTITY,
+    )
+    ds = test_utils.toy_dataset()
+    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
+        label="col_three_string",
+        loss=multiclass_custom_loss,
+        task=generic_learner.Task.CLASSIFICATION,
+        num_trees=1,
+    )
+    model: gradient_boosted_trees_model.GradientBoostedTreesModel = (
+        learner_custom_loss.train(ds)
+    )
+    npt.assert_equal(model.initial_predictions(), [1, 2, 3])
+
+  def test_multiclass_wrong_initial_prediction_dimensions(self):
+    def multiclass_initial_prediction(
+        labels: npty.NDArray[np.int32], _: npty.NDArray[np.float32]
+    ) -> npty.NDArray[np.float32]:
+      dimension = np.max(labels)
+      return np.arange(1, dimension)
+
+    multiclass_custom_loss = custom_loss.MultiClassificationLoss(
+        initial_predictions=multiclass_initial_prediction,
+        gradient_and_hessian=lambda x, y: (
+            np.ones([3, len(x)]),
+            np.ones([3, len(x)]),
+        ),
+        loss=lambda x, y, z: np.float32(0),
+        activation=custom_loss.Activation.IDENTITY,
+    )
+    ds = test_utils.toy_dataset()
+    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
+        label="col_three_string",
+        loss=multiclass_custom_loss,
+        task=generic_learner.Task.CLASSIFICATION,
+        num_trees=1,
+    )
+    with self.assertRaisesRegex(
+        ValueError,
+        "The initial_predictions must be a one-dimensional Numpy array of 3"
+        " elements.*",
+    ):
+      _ = learner_custom_loss.train(ds)
+
+  @parameterized.parameters(
+      (
+          custom_loss.RegressionLoss,
+          generic_learner.Task.REGRESSION,
+          "col_float",
+      ),
+      (
+          custom_loss.BinaryClassificationLoss,
+          generic_learner.Task.CLASSIFICATION,
+          "binary_int_label",
+      ),
+  )
+  def test_wrong_gradient_dimensions(self, loss_type, task_type, label):
+    faulty_loss = loss_type(
+        initial_predictions=lambda x, y: np.float32(0),
+        gradient_and_hessian=lambda x, y: (
+            np.ones(len(x) - 1),
+            np.ones(len(x)),
+        ),
+        loss=lambda x, y, z: np.float32(0),
+        activation=custom_loss.Activation.IDENTITY,
+    )
+    ds = test_utils.toy_dataset()
+    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
+        label=label,
+        loss=faulty_loss,
+        task=task_type,
+        num_trees=1,
+    )
+    with self.assertRaisesRegex(
+        ValueError,
+        "The gradient must be a one-dimensional Numpy array of 5 elements.*",
+    ):
+      _ = learner_custom_loss.train(ds)
+
+  @parameterized.parameters(
+      (
+          custom_loss.RegressionLoss,
+          generic_learner.Task.REGRESSION,
+          "col_float",
+      ),
+      (
+          custom_loss.BinaryClassificationLoss,
+          generic_learner.Task.CLASSIFICATION,
+          "binary_int_label",
+      ),
+  )
+  def test_wrong_hessian_dimensions(self, loss_type, task_type, label):
+    faulty_loss = loss_type(
+        initial_predictions=lambda x, y: np.float32(0),
+        gradient_and_hessian=lambda x, y: (
+            np.ones(len(x)),
+            np.ones(len(x) - 1),
+        ),
+        loss=lambda x, y, z: np.float32(0),
+        activation=custom_loss.Activation.IDENTITY,
+    )
+    ds = test_utils.toy_dataset()
+    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
+        label=label,
+        loss=faulty_loss,
+        task=task_type,
+        num_trees=1,
+    )
+    with self.assertRaisesRegex(
+        ValueError,
+        "The hessian must be a one-dimensional Numpy array of 5 elements.*",
+    ):
+      _ = learner_custom_loss.train(ds)
+
+  @parameterized.parameters(
+      (
+          custom_loss.RegressionLoss,
+          generic_learner.Task.REGRESSION,
+          "col_float",
+      ),
+      (
+          custom_loss.BinaryClassificationLoss,
+          generic_learner.Task.CLASSIFICATION,
+          "binary_int_label",
+      ),
+  )
+  def test_stacked_gradient_dimensions(self, loss_type, task_type, label):
+    faulty_loss = loss_type(
+        initial_predictions=lambda x, y: np.float32(0),
+        gradient_and_hessian=lambda x, y: 3 * np.ones([2, 5]),
+        loss=lambda x, y, z: np.float32(0),
+        activation=custom_loss.Activation.IDENTITY,
+    )
+    ds = test_utils.toy_dataset()
+    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
+        label=label,
+        loss=faulty_loss,
+        task=task_type,
+        num_trees=1,
+    )
+    with self.assertRaisesRegex(
+        ValueError,
+        ".*gradient_and_hessian function returned a numpy array, expected a"
+        " Sequence of two numpy arrays.*",
+    ):
+      _ = learner_custom_loss.train(ds)
+
+  def test_loss_with_jax_nojit(self):
+    def mse_loss(labels, predictions):
+      numerator = jnp.sum(jnp.square(jnp.subtract(labels, predictions)))
+      denominator = jnp.size(labels)
+      res = jax.block_until_ready(jnp.divide(numerator, denominator))
+      return res
+
+    def weighted_mse_loss(labels, predictions, _):
+      return mse_loss(labels, predictions)
+
+    mse_grad = jax.grad(mse_loss, argnums=1)
+    mse_hessian = jax.jacfwd(jax.jacrev(mse_loss), argnums=1)
+
+    def mse_gradient_and_hessian(labels, predictions):
+      res = (
+          mse_grad(labels, predictions).block_until_ready(),
+          jnp.diagonal(mse_hessian(labels, predictions)).block_until_ready(),
+      )
+      return res
+
+    def mse_initial_predictions(labels, weights):
+      res = jax.block_until_ready(jnp.average(labels, weights=weights))
+      return res
+
+    mse_custom_loss = custom_loss.RegressionLoss(
+        initial_predictions=mse_initial_predictions,
+        gradient_and_hessian=mse_gradient_and_hessian,
+        loss=weighted_mse_loss,
+        activation=custom_loss.Activation.IDENTITY,
+    )
+
+    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
+        label="target",
+        loss=mse_custom_loss,
+        task=generic_learner.Task.REGRESSION,
+        num_trees=30,
+        early_stopping="NONE",
+        validation_ratio=0.0,
+    )
+    model_custom_loss: generic_model.GenericModel = learner_custom_loss.train(
+        self.two_center_regression.train
+    )
+
+    learner_builtin_loss = specialized_learners.GradientBoostedTreesLearner(
+        label="target",
+        task=generic_learner.Task.REGRESSION,
+        num_trees=30,
+        early_stopping="NONE",
+        validation_ratio=0.0,
+    )
+    model_builtin_loss: generic_model.GenericModel = learner_builtin_loss.train(
+        self.two_center_regression.train
+    )
+    npt.assert_allclose(
+        model_custom_loss.predict(self.two_center_regression.test),
+        model_builtin_loss.predict(self.two_center_regression.test),
+        rtol=1e-5,  # Without activation function, the predictions can be large.
+        atol=1e-6,
+    )
+
+  def test_loss_with_jax_jit(self):
+    @jax.jit
+    def mse_loss(labels, predictions):
+      numerator = jnp.sum(jnp.square(jnp.subtract(labels, predictions)))
+      denominator = jnp.size(labels)
+      res = jax.block_until_ready(jnp.divide(numerator, denominator))
+      return res
+
+    @jax.jit
+    def weighted_mse_loss(labels, predictions, _):
+      return mse_loss(labels, predictions)
+
+    mse_grad = jax.jit(jax.grad(mse_loss, argnums=1))
+    mse_hessian = jax.jit(jax.jacfwd(jax.jacrev(mse_loss), argnums=1))
+
+    def mse_gradient_and_hessian(labels, predictions):
+      return (
+          mse_grad(labels, predictions).block_until_ready(),
+          jnp.diagonal(mse_hessian(labels, predictions)).block_until_ready(),
+      )
+
+    @jax.jit
+    def mse_initial_predictions(labels, weights):
+      res = jax.block_until_ready(jnp.average(labels, weights=weights))
+      return res
+
+    mse_custom_loss = custom_loss.RegressionLoss(
+        initial_predictions=mse_initial_predictions,
+        gradient_and_hessian=mse_gradient_and_hessian,
+        loss=weighted_mse_loss,
+        activation=custom_loss.Activation.IDENTITY,
+    )
+
+    learner_custom_loss = specialized_learners.GradientBoostedTreesLearner(
+        label="target",
+        loss=mse_custom_loss,
+        task=generic_learner.Task.REGRESSION,
+        num_trees=30,
+        early_stopping="NONE",
+        validation_ratio=0.0,
+    )
+    model_custom_loss: generic_model.GenericModel = learner_custom_loss.train(
+        self.two_center_regression.train
+    )
+
+    learner_builtin_loss = specialized_learners.GradientBoostedTreesLearner(
+        label="target",
+        task=generic_learner.Task.REGRESSION,
+        num_trees=30,
+        early_stopping="NONE",
+        validation_ratio=0.0,
+    )
+    model_builtin_loss: generic_model.GenericModel = learner_builtin_loss.train(
+        self.two_center_regression.train
+    )
+    npt.assert_allclose(
+        model_custom_loss.predict(self.two_center_regression.test),
+        model_builtin_loss.predict(self.two_center_regression.test),
+        rtol=1e-5,  # Without activation function, the predictions can be large.
+        atol=1e-6,
+    )
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/learner/distributed_learner_test.py

 * *Ordering differences only*

```diff
@@ -1,169 +1,169 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Tests for distributed model learning."""
-
-import dataclasses
-import os
-import pathlib
-from typing import Callable, List, Sequence
-
-from absl import logging
-from absl.testing import absltest
-import pandas as pd
-import portpicker
-
-from ydf.learner import specialized_learners
-from ydf.learner import tuner as tuner_lib
-from ydf.learner import worker
-from ydf.utils import test_utils
-
-
-@dataclasses.dataclass
-class Workers:
-  """Locally running workers.
-
-  Attributes:
-    ips: IP addresses of the workers.
-    stop: Stops all the workers.
-  """
-
-  ips: Sequence[str]
-  stop: Callable[[], None]
-
-
-def create_in_process_workers(num_workers: int) -> Workers:
-  """Creates a set of workers running locally."""
-
-  # Select ports
-  ports = [portpicker.pick_unused_port() for _ in range(num_workers)]
-
-  # Start workers
-  stop_worker_list = [
-      worker.start_worker(port=port, blocking=False) for port in ports
-  ]
-
-  # To stop all workers
-  def stop():
-    for stop_worker in stop_worker_list:
-      stop_worker()
-
-  ips = [f"localhost:{port}" for port in ports]
-  logging.info("ips: %s", ips)
-  return Workers(
-      ips=ips,
-      stop=stop,
-  )
-
-
-def split_dataset(
-    path: pathlib.Path, tmp_dir: pathlib.Path, num_shards: int
-) -> List[pathlib.Path]:
-  """Splits a csv file into multiple csv files."""
-
-  dataset = pd.read_csv(path)
-  num_row_per_shard = (dataset.shape[0] + num_shards - 1) // num_shards
-  paths = []
-  for shard_idx in range(num_shards):
-    begin_idx = shard_idx * num_row_per_shard
-    end_idx = (shard_idx + 1) * num_row_per_shard
-    shard_dataset = dataset.iloc[begin_idx:end_idx]
-    shard_path = tmp_dir / f"shard_{shard_idx}.csv"
-    paths.append(shard_path)
-    shard_dataset.to_csv(shard_path, index=False)
-  return paths
-
-
-class Utilities(absltest.TestCase):
-
-  def test_local_worker(self):
-    workers = create_in_process_workers(4)
-    self.assertLen(workers.ips, 4)
-    workers.stop()
-
-  def test_split_dataset(self):
-    tmp_dir = pathlib.Path(self.create_tempdir().full_path)
-    dataset_path = (
-        test_utils.ydf_test_data_pathlib() / "dataset" / "adult_train.csv"
-    )
-    sharded_paths = split_dataset(dataset_path, tmp_dir, 10)
-    self.assertLen(sharded_paths, 10)
-    ds = pd.concat([pd.read_csv(path) for path in sharded_paths]).reset_index(
-        drop=True
-    )
-    expected_ds = pd.read_csv(dataset_path)
-    # Loading and concatenating the sharded dataset is equal to loading the
-    # original dataset.
-    pd.testing.assert_frame_equal(ds, expected_ds)
-
-
-class DistributedGradientBoostedTreesLearnerTest(absltest.TestCase):
-
-  def test_adult(self):
-    # Prepare datasets
-    tmp_dir = pathlib.Path(self.create_tempdir().full_path)
-    dataset_directory = test_utils.ydf_test_data_pathlib() / "dataset"
-    splitted_train_ds = split_dataset(
-        dataset_directory / "adult_train.csv", tmp_dir, 10
-    )
-    logging.info("Dataset paths: %s", splitted_train_ds)
-    test_ds = dataset_directory / "adult_test.csv"
-
-    # Start workers
-    workers = create_in_process_workers(4)
-
-    # Train model
-    model = specialized_learners.DistributedGradientBoostedTreesLearner(
-        label="income",
-        working_dir=os.path.join(tmp_dir, "work_dir"),
-        resume_training=True,
-        num_trees=10,
-        workers=workers.ips,
-    ).train(",".join(map(str, splitted_train_ds)))
-
-    workers.stop()
-    self.assertAlmostEqual(model.evaluate(str(test_ds)).accuracy, 0.850, 1)
-
-
-class HyperParameterTunerTest(absltest.TestCase):
-
-  def test_adult_from_file(self):
-    # Prepare datasets
-    tmp_dir = pathlib.Path(self.create_tempdir().full_path)
-    dataset_directory = test_utils.ydf_test_data_pathlib() / "dataset"
-    train_ds = dataset_directory / "adult_train.csv"
-    test_ds = dataset_directory / "adult_test.csv"
-
-    # Start workers
-    workers = create_in_process_workers(4)
-
-    # Tune model
-    tuner = tuner_lib.RandomSearchTuner(num_trials=10)
-    tuner.choice("shrinkage", [0.2, 0.1, 0.05])
-    tuner.choice("subsample", [1.0, 0.9, 0.8])
-
-    model = specialized_learners.GradientBoostedTreesLearner(
-        label="income",
-        working_dir=os.path.join(tmp_dir, "work_dir"),
-        num_trees=20,
-        workers=workers.ips,
-        tuner=tuner,
-    ).train(str(train_ds))
-
-    workers.stop()
-    self.assertGreater(model.evaluate(str(test_ds)).accuracy, 0.850, 1)
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Tests for distributed model learning."""
+
+import dataclasses
+import os
+import pathlib
+from typing import Callable, List, Sequence
+
+from absl import logging
+from absl.testing import absltest
+import pandas as pd
+import portpicker
+
+from ydf.learner import specialized_learners
+from ydf.learner import tuner as tuner_lib
+from ydf.learner import worker
+from ydf.utils import test_utils
+
+
+@dataclasses.dataclass
+class Workers:
+  """Locally running workers.
+
+  Attributes:
+    ips: IP addresses of the workers.
+    stop: Stops all the workers.
+  """
+
+  ips: Sequence[str]
+  stop: Callable[[], None]
+
+
+def create_in_process_workers(num_workers: int) -> Workers:
+  """Creates a set of workers running locally."""
+
+  # Select ports
+  ports = [portpicker.pick_unused_port() for _ in range(num_workers)]
+
+  # Start workers
+  stop_worker_list = [
+      worker.start_worker(port=port, blocking=False) for port in ports
+  ]
+
+  # To stop all workers
+  def stop():
+    for stop_worker in stop_worker_list:
+      stop_worker()
+
+  ips = [f"localhost:{port}" for port in ports]
+  logging.info("ips: %s", ips)
+  return Workers(
+      ips=ips,
+      stop=stop,
+  )
+
+
+def split_dataset(
+    path: pathlib.Path, tmp_dir: pathlib.Path, num_shards: int
+) -> List[pathlib.Path]:
+  """Splits a csv file into multiple csv files."""
+
+  dataset = pd.read_csv(path)
+  num_row_per_shard = (dataset.shape[0] + num_shards - 1) // num_shards
+  paths = []
+  for shard_idx in range(num_shards):
+    begin_idx = shard_idx * num_row_per_shard
+    end_idx = (shard_idx + 1) * num_row_per_shard
+    shard_dataset = dataset.iloc[begin_idx:end_idx]
+    shard_path = tmp_dir / f"shard_{shard_idx}.csv"
+    paths.append(shard_path)
+    shard_dataset.to_csv(shard_path, index=False)
+  return paths
+
+
+class Utilities(absltest.TestCase):
+
+  def test_local_worker(self):
+    workers = create_in_process_workers(4)
+    self.assertLen(workers.ips, 4)
+    workers.stop()
+
+  def test_split_dataset(self):
+    tmp_dir = pathlib.Path(self.create_tempdir().full_path)
+    dataset_path = (
+        test_utils.ydf_test_data_pathlib() / "dataset" / "adult_train.csv"
+    )
+    sharded_paths = split_dataset(dataset_path, tmp_dir, 10)
+    self.assertLen(sharded_paths, 10)
+    ds = pd.concat([pd.read_csv(path) for path in sharded_paths]).reset_index(
+        drop=True
+    )
+    expected_ds = pd.read_csv(dataset_path)
+    # Loading and concatenating the sharded dataset is equal to loading the
+    # original dataset.
+    pd.testing.assert_frame_equal(ds, expected_ds)
+
+
+class DistributedGradientBoostedTreesLearnerTest(absltest.TestCase):
+
+  def test_adult(self):
+    # Prepare datasets
+    tmp_dir = pathlib.Path(self.create_tempdir().full_path)
+    dataset_directory = test_utils.ydf_test_data_pathlib() / "dataset"
+    splitted_train_ds = split_dataset(
+        dataset_directory / "adult_train.csv", tmp_dir, 10
+    )
+    logging.info("Dataset paths: %s", splitted_train_ds)
+    test_ds = dataset_directory / "adult_test.csv"
+
+    # Start workers
+    workers = create_in_process_workers(4)
+
+    # Train model
+    model = specialized_learners.DistributedGradientBoostedTreesLearner(
+        label="income",
+        working_dir=os.path.join(tmp_dir, "work_dir"),
+        resume_training=True,
+        num_trees=10,
+        workers=workers.ips,
+    ).train(",".join(map(str, splitted_train_ds)))
+
+    workers.stop()
+    self.assertAlmostEqual(model.evaluate(str(test_ds)).accuracy, 0.850, 1)
+
+
+class HyperParameterTunerTest(absltest.TestCase):
+
+  def test_adult_from_file(self):
+    # Prepare datasets
+    tmp_dir = pathlib.Path(self.create_tempdir().full_path)
+    dataset_directory = test_utils.ydf_test_data_pathlib() / "dataset"
+    train_ds = dataset_directory / "adult_train.csv"
+    test_ds = dataset_directory / "adult_test.csv"
+
+    # Start workers
+    workers = create_in_process_workers(4)
+
+    # Tune model
+    tuner = tuner_lib.RandomSearchTuner(num_trials=10)
+    tuner.choice("shrinkage", [0.2, 0.1, 0.05])
+    tuner.choice("subsample", [1.0, 0.9, 0.8])
+
+    model = specialized_learners.GradientBoostedTreesLearner(
+        label="income",
+        working_dir=os.path.join(tmp_dir, "work_dir"),
+        num_trees=20,
+        workers=workers.ips,
+        tuner=tuner,
+    ).train(str(train_ds))
+
+    workers.stop()
+    self.assertGreater(model.evaluate(str(test_ds)).accuracy, 0.850, 1)
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/learner/generic_learner.py

```diff
@@ -1,583 +1,583 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Definitions for Generic learners."""
-
-import copy
-import datetime
-import os
-import re
-from typing import Optional, Sequence, Union
-
-from absl import logging
-
-from yggdrasil_decision_forests.dataset import data_spec_pb2
-from yggdrasil_decision_forests.dataset import weight_pb2
-from yggdrasil_decision_forests.learner import abstract_learner_pb2
-from yggdrasil_decision_forests.metric import metric_pb2
-from yggdrasil_decision_forests.model import abstract_model_pb2
-from ydf.cc import ydf
-from ydf.dataset import dataset
-from ydf.dataset import dataspec
-from ydf.learner import custom_loss
-from ydf.learner import hyperparameters
-from ydf.learner import tuner as tuner_lib
-from ydf.metric import metric
-from ydf.model import generic_model
-from ydf.model import model_lib
-from ydf.utils import log
-from yggdrasil_decision_forests.utils import fold_generator_pb2
-from yggdrasil_decision_forests.utils.distribute.implementations.grpc import grpc_pb2
-
-Task = generic_model.Task
-
-_FRAMEWORK_NAME = "Python YDF"
-
-
-class GenericLearner:
-  """A generic YDF learner."""
-
-  def __init__(
-      self,
-      learner_name: str,
-      task: Task,
-      label: str,
-      weights: Optional[str],
-      ranking_group: Optional[str],
-      uplift_treatment: Optional[str],
-      data_spec_args: dataspec.DataSpecInferenceArgs,
-      data_spec: Optional[data_spec_pb2.DataSpecification],
-      hyper_parameters: hyperparameters.HyperParameters,
-      deployment_config: abstract_learner_pb2.DeploymentConfig,
-      tuner: Optional[tuner_lib.AbstractTuner],
-  ):
-    # TODO: Refactor to a single hyperparameter dictionary with edit
-    # access to these options.
-    self._task = task
-    self._learner_name = learner_name
-    self._label = label
-    self._weights = weights
-    self._ranking_group = ranking_group
-    self._uplift_treatment = uplift_treatment
-    self._hyperparameters = hyper_parameters
-    self._data_spec = data_spec
-    self._data_spec_args = data_spec_args
-    self._deployment_config = deployment_config
-    self._tuner = tuner
-
-    if not self._label:
-      raise ValueError("Constructing the learner requires a non-empty label.")
-    if self._ranking_group is not None and task != Task.RANKING:
-      raise ValueError(
-          "The ranking group should only be specified for ranking tasks."
-          f" Got task={task.name}"
-      )
-    if self._ranking_group is None and task == Task.RANKING:
-      raise ValueError("The ranking group must be specified for ranking tasks.")
-    if self._uplift_treatment is not None and task not in [
-        Task.NUMERICAL_UPLIFT,
-        Task.CATEGORICAL_UPLIFT,
-    ]:
-      raise ValueError(
-          "The uplift treatment should only be specified for uplifting tasks."
-          f" Got task={task.name}"
-      )
-    if self._uplift_treatment is None and task in [
-        Task.NUMERICAL_UPLIFT,
-        Task.CATEGORICAL_UPLIFT,
-    ]:
-      raise ValueError(
-          "The uplift treatment must be specified for uplifting tasks."
-      )
-    if data_spec is not None:
-      logging.info(
-          "Data spec was provided explicitly, so any other dataspec"
-          " configuration options will be ignored."
-      )
-    if tuner:
-      tuner.set_base_learner(learner_name)
-
-  @property
-  def hyperparameters(self) -> hyperparameters.HyperParameters:
-    return self._hyperparameters
-
-  def train(
-      self,
-      ds: dataset.InputDataset,
-      valid: Optional[dataset.InputDataset] = None,
-  ) -> generic_model.ModelType:
-    """Trains a model on the given dataset.
-
-    Options for dataset reading are given on the learner. Consult the
-    documentation of the learner or ydf.create_vertical_dataset() for additional
-    information on dataset reading in YDF.
-
-    Usage example:
-
-    ```
-    import ydf
-    import pandas as pd
-
-    train_ds = pd.read_csv(...)
-    test_ds = pd.read_csv(...)
-
-    learner = ydf.GradientBoostedTreesLearner(label="label")
-    model = learner.train(train_ds)
-    evaluation = model.evaluate(test_ds)
-    ```
-
-    Usage example with a validation dataset:
-
-    ```
-    import ydf
-    import pandas as pd
-
-    train_ds = pd.read_csv(...)
-    valid_ds = pd.read_csv(...)
-    test_ds = pd.read_csv(...)
-
-    learner = ydf.GradientBoostedTreesLearner(label="label")
-    model = learner.train(train_ds, valid=valid_ds)
-    evaluation = model.evaluate(test_ds)
-    ```
-
-    If training is interrupted (for example, by interrupting the cell execution
-    in Colab), the model will be returned to the state it was in at the moment
-    of interruption.
-
-    Args:
-      ds: Training dataset.
-      valid: Optional validation dataset. Some learners, such as Random Forest,
-        do not need validation dataset. Some learners, such as
-        GradientBoostedTrees, automatically extract a validation dataset from
-        the training dataset if the validation dataset is not provided.
-
-    Returns:
-      A trained model.
-    """
-
-    if valid is not None:
-      if not self.__class__.capabilities().support_validation_dataset:
-        raise ValueError(
-            f"The learner {self.__class__.__name__!r} does not use a"
-            " validation dataset. If you can, add the validation examples to"
-            " the training dataset."
-        )
-
-    if isinstance(ds, str):
-      if valid is not None and not isinstance(valid, str):
-        raise ValueError(
-            "If the training dataset is a path, the validation dataset must"
-            " also be a path."
-        )
-      return self._train_from_path(ds, valid)
-    if valid is not None and isinstance(valid, str):
-      raise ValueError(
-          "The validation dataset may only be a path if the training dataset is"
-          " a path."
-      )
-    return self._train_from_dataset(ds, valid)
-
-  def __str__(self) -> str:
-    return f"""\
-Learner: {self._learner_name}
-Task: {self._task}
-Class: ydf.{self.__class__.__name__}
-Hyper-parameters: ydf.{self._hyperparameters}
-"""
-
-  def _train_from_path(
-      self, ds: str, valid: Optional[str]
-  ) -> generic_model.ModelType:
-    """Trains a model from a file path (dataset reading in YDF C++)."""
-    with log.cc_log_context():
-      if self._data_spec is not None:
-        cc_model = self._get_learner().TrainFromPathWithDataSpec(
-            ds, self._data_spec, valid
-        )
-      else:
-        guide = self._build_data_spec_args().to_proto_guide()
-        cc_model = self._get_learner().TrainFromPathWithGuide(ds, guide, valid)
-      return model_lib.load_cc_model(cc_model)
-
-  def _train_from_dataset(
-      self,
-      ds: dataset.InputDataset,
-      valid: Optional[dataset.InputDataset] = None,
-  ) -> generic_model.ModelType:
-    """Trains a model from in-memory data."""
-
-    with log.cc_log_context():
-      train_ds = self._get_vertical_dataset(ds)._dataset  # pylint: disable=protected-access
-      train_args = {"dataset": train_ds}
-
-      if valid is not None:
-        valid_ds = self._get_vertical_dataset(valid)._dataset  # pylint: disable=protected-access
-        train_args["validation_dataset"] = valid_ds
-        log.info(
-            "Train model on %d training examples and %d validation examples",
-            train_ds.nrow(),
-            valid_ds.nrow(),
-        )
-      else:
-        log.info(
-            "Train model on %d examples",
-            train_ds.nrow(),
-        )
-
-      time_begin_training_model = datetime.datetime.now()
-      learner = self._get_learner()
-      cc_model = learner.Train(**train_args)
-      log.info(
-          "Model trained in %s",
-          datetime.datetime.now() - time_begin_training_model,
-      )
-
-      return model_lib.load_cc_model(cc_model)
-
-  def _get_training_config(self) -> abstract_learner_pb2.TrainingConfig:
-    """Gets the training config proto."""
-
-    training_config = abstract_learner_pb2.TrainingConfig(
-        learner=self._learner_name,
-        label=self._label,
-        weight_definition=self._build_weight_definition(),
-        ranking_group=self._ranking_group,
-        uplift_treatment=self._uplift_treatment,
-        task=self._task._to_proto_type(),
-        metadata=abstract_model_pb2.Metadata(framework=_FRAMEWORK_NAME),
-    )
-
-    # Apply monotonic constraints.
-    if self._data_spec_args.columns:
-      for feature in self._data_spec_args.columns:
-        if not feature.normalized_monotonic:
-          continue
-
-        proto_direction = (
-            abstract_learner_pb2.MonotonicConstraint.INCREASING
-            if feature.normalized_monotonic == dataspec.Monotonic.INCREASING
-            else abstract_learner_pb2.MonotonicConstraint.DECREASING
-        )
-        training_config.monotonic_constraints.append(
-            abstract_learner_pb2.MonotonicConstraint(
-                feature=_feature_name_to_regex(feature.name),
-                direction=proto_direction,
-            )
-        )
-
-    if self._tuner:
-      training_config.MergeFrom(self._tuner.train_config)
-    return training_config
-
-  def _get_learner(self) -> ydf.GenericCCLearner:
-    """Gets a ready-to-train learner."""
-
-    training_config = self._get_training_config()
-    cc_custom_loss = None
-    if "loss" in self._hyperparameters and isinstance(
-        self._hyperparameters["loss"], custom_loss.AbstractCustomLoss
-    ):
-      log.info(
-          "Using a custom loss. Note when using custom losses, hyperparameter"
-          " `apply_link_function` is ignored. Use the losses' activation"
-          " function instead."
-      )
-      py_custom_loss: custom_loss.AbstractCustomLoss = self._hyperparameters[
-          "loss"
-      ]
-      py_custom_loss.check_is_compatible_task(training_config.task)
-      cc_custom_loss = py_custom_loss._to_cc()  # pylint: disable=protected-access
-      # TODO: b/322763329 - Fail if the user set apply_link_function.
-      if py_custom_loss.activation.name == "IDENTITY":
-        self._hyperparameters["apply_link_function"] = False
-      else:
-        self._hyperparameters["apply_link_function"] = True
-
-    hp_proto = hyperparameters.dict_to_generic_hyperparameter(
-        self._hyperparameters
-    )
-    return ydf.GetLearner(
-        training_config, hp_proto, self._deployment_config, cc_custom_loss
-    )
-
-  def _get_vertical_dataset(
-      self, ds: dataset.InputDataset
-  ) -> dataset.VerticalDataset:
-    if isinstance(ds, dataset.VerticalDataset):
-      return ds
-      # TODO: Check that the user has not specified a data spec guide.
-    else:
-
-      # List of columns that cannot be unrolled.
-      dont_unroll_columns = [self._label]
-      for column in [
-          self._weights,
-          self._ranking_group,
-          self._uplift_treatment,
-      ]:
-        if column:
-          dont_unroll_columns.append(column)
-
-      effective_data_spec_args = None
-      if self._data_spec is None:
-        effective_data_spec_args = self._build_data_spec_args()
-      return dataset.create_vertical_dataset_with_spec_or_args(
-          ds,
-          data_spec=self._data_spec,
-          inference_args=effective_data_spec_args,
-          required_columns=None,  # All columns in the dataspec are required.
-          dont_unroll_columns=dont_unroll_columns,
-      )
-
-  def cross_validation(
-      self,
-      ds: dataset.InputDataset,
-      folds: int = 10,
-      bootstrapping: Union[bool, int] = False,
-      parallel_evaluations: int = 1,
-  ) -> metric.Evaluation:
-    """Cross-validates the learner and return the evaluation.
-
-    Usage example:
-
-    ```python
-    import pandas as pd
-    import ydf
-
-    dataset = pd.read_csv("my_dataset.csv")
-    learner = ydf.RandomForestLearner(label="label")
-    evaluation = learner.cross_validation(dataset)
-
-    # In a notebook, display an interractive evaluation
-    evaluation
-
-    # Print the evaluation
-    print(evaluation)
-
-    # Look at specific metrics
-    print(evaluation.accuracy)
-    ```
-
-    Args:
-      ds: Dataset for the cross-validation.
-      folds: Number of cross-validation folds.
-      bootstrapping: Controls whether bootstrapping is used to evaluate the
-        confidence intervals and statistical tests (i.e., all the metrics ending
-        with "[B]"). If set to false, bootstrapping is disabled. If set to true,
-        bootstrapping is enabled and 2000 bootstrapping samples are used. If set
-        to an integer, it specifies the number of bootstrapping samples to use.
-        In this case, if the number is less than 100, an error is raised as
-        bootstrapping will not yield useful results.
-      parallel_evaluations: Number of model to train and evaluate in parallel
-        using multi-threading. Note that each model is potentially already
-        trained with multithreading (see `num_threads` argument of Learner
-        constructor).
-
-    Returns:
-      The cross-validation evaluation.
-    """
-
-    fold_generator = fold_generator_pb2.FoldGenerator(
-        cross_validation=fold_generator_pb2.FoldGenerator.CrossValidation(
-            num_folds=folds,
-        )
-    )
-
-    if isinstance(bootstrapping, bool):
-      bootstrapping_samples = 2000 if bootstrapping else -1
-    elif isinstance(bootstrapping, int) and bootstrapping >= 100:
-      bootstrapping_samples = bootstrapping
-    else:
-      raise ValueError(
-          "bootstrapping argument should be boolean or an integer greater than"
-          " 100 as bootstrapping will not yield useful results. Got"
-          f" {bootstrapping!r} instead"
-      )
-    evaluation_options = metric_pb2.EvaluationOptions(
-        bootstrapping_samples=bootstrapping_samples
-    )
-
-    deployment_evaluation = abstract_learner_pb2.DeploymentConfig(
-        num_threads=parallel_evaluations,
-    )
-
-    vertical_dataset = self._get_vertical_dataset(ds)
-    learner = self._get_learner()
-
-    with log.cc_log_context():
-      evaluation_proto = learner.Evaluate(
-          vertical_dataset._dataset,  # pylint: disable=protected-access
-          fold_generator,
-          evaluation_options,
-          deployment_evaluation,
-      )
-      return metric.Evaluation(evaluation_proto)
-
-  def _build_data_spec_args(self) -> dataspec.DataSpecInferenceArgs:
-    """Builds DS args with user inputs and guides for labels / special columns.
-
-    Create a copy of self._data_spec_args and adds column definitions for the
-    columns label / weights / ranking group / uplift treatment with the correct
-    semantic and dataspec inference arguments.
-
-    Returns:
-      A copy of the data spec arguments with information about the special
-      columns added.
-
-    Raises:
-      ValueError: If the label / weights / ranking group / uplift treatment
-      column are specified as features.
-    """
-
-    def create_label_column(name: str, task: Task) -> dataspec.Column:
-      if task in [Task.CLASSIFICATION, Task.CATEGORICAL_UPLIFT]:
-        return dataspec.Column(
-            name=name,
-            semantic=dataspec.Semantic.CATEGORICAL,
-            max_vocab_count=-1,
-            min_vocab_frequency=1,
-        )
-      elif task in [Task.REGRESSION, Task.RANKING, Task.NUMERICAL_UPLIFT]:
-        return dataspec.Column(name=name, semantic=dataspec.Semantic.NUMERICAL)
-      else:
-        raise ValueError(f"Unsupported task {task.name} for label column")
-
-    data_spec_args = copy.deepcopy(self._data_spec_args)
-    # If no columns have been specified, make sure that all columns are used,
-    # since this function will specify some.
-    #
-    # TODO: If `label` becomes an optional argument, this function needs
-    # to be adapted.
-    if data_spec_args.columns is None:
-      data_spec_args.include_all_columns = True
-      data_spec_args.columns = []
-    column_defs = data_spec_args.columns
-    if dataspec.column_defs_contains_column(self._label, column_defs):
-      raise ValueError(
-          f"Label column {self._label} is also an input feature. A column"
-          " cannot be both a label and input feature."
-      )
-    column_defs.append(create_label_column(self._label, self._task))
-    if self._weights is not None:
-      if dataspec.column_defs_contains_column(self._weights, column_defs):
-        raise ValueError(
-            f"Weights column {self._weights} is also an input feature. A column"
-            " cannot be both a weights and input feature."
-        )
-      column_defs.append(
-          dataspec.Column(
-              name=self._weights, semantic=dataspec.Semantic.NUMERICAL
-          )
-      )
-    if self._ranking_group is not None:
-      assert self._task == Task.RANKING
-
-      if dataspec.column_defs_contains_column(self._ranking_group, column_defs):
-        raise ValueError(
-            f"Ranking group column {self._ranking_group} is also an input"
-            " feature. A column cannot be both a ranking group and input"
-            " feature."
-        )
-      column_defs.append(
-          dataspec.Column(
-              name=self._ranking_group, semantic=dataspec.Semantic.HASH
-          )
-      )
-    if self._uplift_treatment is not None:
-      assert self._task in [Task.NUMERICAL_UPLIFT, Task.CATEGORICAL_UPLIFT]
-
-      if dataspec.column_defs_contains_column(
-          self._uplift_treatment, column_defs
-      ):
-        raise ValueError(
-            "The uplift_treatment column should not be specified as a feature"
-        )
-      column_defs.append(
-          dataspec.Column(
-              name=self._uplift_treatment,
-              semantic=dataspec.Semantic.CATEGORICAL,
-              max_vocab_count=-1,
-              min_vocab_frequency=1,
-          )
-      )
-    return data_spec_args
-
-  def _build_weight_definition(
-      self,
-  ) -> Optional[weight_pb2.WeightDefinition]:
-    weight_definition = None
-    if self._weights is not None:
-      weight_definition = weight_pb2.WeightDefinition(
-          attribute=self._weights,
-          numerical=weight_pb2.WeightDefinition.NumericalWeight(),
-      )
-    return weight_definition
-
-  def _build_deployment_config(
-      self,
-      num_threads: Optional[int],
-      resume_training: bool,
-      resume_training_snapshot_interval_seconds: int,
-      working_dir: Optional[str],
-      workers: Optional[Sequence[str]],
-  ):
-    """Merges constructor arguments into a deployment configuration."""
-
-    if num_threads is None:
-      num_threads = self._determine_optimal_num_threads()
-    config = abstract_learner_pb2.DeploymentConfig(
-        num_threads=num_threads,
-        try_resume_training=resume_training,
-        cache_path=working_dir,
-        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
-    )
-
-    if workers is not None:
-      if not workers:
-        raise ValueError("At least one worker should be provided")
-      config.distribute.implementation_key = "GRPC"
-      grpc_config = config.distribute.Extensions[grpc_pb2.grpc]
-      grpc_config.grpc_addresses.addresses[:] = workers
-
-    return config
-
-  def _determine_optimal_num_threads(self):
-    """Sets  number of threads to min(num_cpus, 32) or 6 if num_cpus unclear."""
-    num_threads = os.cpu_count()
-    if num_threads is None:
-      logging.warning("Cannot determine the number of CPUs. Set num_threads=6")
-      num_threads = 6
-    else:
-      if num_threads > 32:
-        logging.warning(
-            "The `num_threads` constructor argument is not set and the "
-            "number of CPU is os.cpu_count()=%d > 32. Setting num_threads "
-            "to 32. Set num_threads manually to use more than 32 cpus.",
-            num_threads,
-        )
-        num_threads = 32
-      else:
-        logging.info("Use %d thread(s) for training", num_threads)
-    return num_threads
-
-  @classmethod
-  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
-    raise NotImplementedError("Not implemented")
-
-
-def _feature_name_to_regex(name: str) -> str:
-  """Generates a regular expression capturing a feature by name."""
-
-  return "^" + re.escape(name) + "$"
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Definitions for Generic learners."""
+
+import copy
+import datetime
+import os
+import re
+from typing import Optional, Sequence, Union
+
+from absl import logging
+
+from ydf.proto.dataset import data_spec_pb2
+from ydf.proto.dataset import weight_pb2
+from ydf.proto.learner import abstract_learner_pb2
+from ydf.proto.metric import metric_pb2
+from ydf.proto.model import abstract_model_pb2
+from ydf.cc import ydf
+from ydf.dataset import dataset
+from ydf.dataset import dataspec
+from ydf.learner import custom_loss
+from ydf.learner import hyperparameters
+from ydf.learner import tuner as tuner_lib
+from ydf.metric import metric
+from ydf.model import generic_model
+from ydf.model import model_lib
+from ydf.utils import log
+from ydf.proto.utils import fold_generator_pb2
+from ydf.proto.utils.distribute.implementations.grpc import grpc_pb2
+
+Task = generic_model.Task
+
+_FRAMEWORK_NAME = "Python YDF"
+
+
+class GenericLearner:
+  """A generic YDF learner."""
+
+  def __init__(
+      self,
+      learner_name: str,
+      task: Task,
+      label: str,
+      weights: Optional[str],
+      ranking_group: Optional[str],
+      uplift_treatment: Optional[str],
+      data_spec_args: dataspec.DataSpecInferenceArgs,
+      data_spec: Optional[data_spec_pb2.DataSpecification],
+      hyper_parameters: hyperparameters.HyperParameters,
+      deployment_config: abstract_learner_pb2.DeploymentConfig,
+      tuner: Optional[tuner_lib.AbstractTuner],
+  ):
+    # TODO: Refactor to a single hyperparameter dictionary with edit
+    # access to these options.
+    self._task = task
+    self._learner_name = learner_name
+    self._label = label
+    self._weights = weights
+    self._ranking_group = ranking_group
+    self._uplift_treatment = uplift_treatment
+    self._hyperparameters = hyper_parameters
+    self._data_spec = data_spec
+    self._data_spec_args = data_spec_args
+    self._deployment_config = deployment_config
+    self._tuner = tuner
+
+    if not self._label:
+      raise ValueError("Constructing the learner requires a non-empty label.")
+    if self._ranking_group is not None and task != Task.RANKING:
+      raise ValueError(
+          "The ranking group should only be specified for ranking tasks."
+          f" Got task={task.name}"
+      )
+    if self._ranking_group is None and task == Task.RANKING:
+      raise ValueError("The ranking group must be specified for ranking tasks.")
+    if self._uplift_treatment is not None and task not in [
+        Task.NUMERICAL_UPLIFT,
+        Task.CATEGORICAL_UPLIFT,
+    ]:
+      raise ValueError(
+          "The uplift treatment should only be specified for uplifting tasks."
+          f" Got task={task.name}"
+      )
+    if self._uplift_treatment is None and task in [
+        Task.NUMERICAL_UPLIFT,
+        Task.CATEGORICAL_UPLIFT,
+    ]:
+      raise ValueError(
+          "The uplift treatment must be specified for uplifting tasks."
+      )
+    if data_spec is not None:
+      logging.info(
+          "Data spec was provided explicitly, so any other dataspec"
+          " configuration options will be ignored."
+      )
+    if tuner:
+      tuner.set_base_learner(learner_name)
+
+  @property
+  def hyperparameters(self) -> hyperparameters.HyperParameters:
+    return self._hyperparameters
+
+  def train(
+      self,
+      ds: dataset.InputDataset,
+      valid: Optional[dataset.InputDataset] = None,
+  ) -> generic_model.ModelType:
+    """Trains a model on the given dataset.
+
+    Options for dataset reading are given on the learner. Consult the
+    documentation of the learner or ydf.create_vertical_dataset() for additional
+    information on dataset reading in YDF.
+
+    Usage example:
+
+    ```
+    import ydf
+    import pandas as pd
+
+    train_ds = pd.read_csv(...)
+    test_ds = pd.read_csv(...)
+
+    learner = ydf.GradientBoostedTreesLearner(label="label")
+    model = learner.train(train_ds)
+    evaluation = model.evaluate(test_ds)
+    ```
+
+    Usage example with a validation dataset:
+
+    ```
+    import ydf
+    import pandas as pd
+
+    train_ds = pd.read_csv(...)
+    valid_ds = pd.read_csv(...)
+    test_ds = pd.read_csv(...)
+
+    learner = ydf.GradientBoostedTreesLearner(label="label")
+    model = learner.train(train_ds, valid=valid_ds)
+    evaluation = model.evaluate(test_ds)
+    ```
+
+    If training is interrupted (for example, by interrupting the cell execution
+    in Colab), the model will be returned to the state it was in at the moment
+    of interruption.
+
+    Args:
+      ds: Training dataset.
+      valid: Optional validation dataset. Some learners, such as Random Forest,
+        do not need validation dataset. Some learners, such as
+        GradientBoostedTrees, automatically extract a validation dataset from
+        the training dataset if the validation dataset is not provided.
+
+    Returns:
+      A trained model.
+    """
+
+    if valid is not None:
+      if not self.__class__.capabilities().support_validation_dataset:
+        raise ValueError(
+            f"The learner {self.__class__.__name__!r} does not use a"
+            " validation dataset. If you can, add the validation examples to"
+            " the training dataset."
+        )
+
+    if isinstance(ds, str):
+      if valid is not None and not isinstance(valid, str):
+        raise ValueError(
+            "If the training dataset is a path, the validation dataset must"
+            " also be a path."
+        )
+      return self._train_from_path(ds, valid)
+    if valid is not None and isinstance(valid, str):
+      raise ValueError(
+          "The validation dataset may only be a path if the training dataset is"
+          " a path."
+      )
+    return self._train_from_dataset(ds, valid)
+
+  def __str__(self) -> str:
+    return f"""\
+Learner: {self._learner_name}
+Task: {self._task}
+Class: ydf.{self.__class__.__name__}
+Hyper-parameters: ydf.{self._hyperparameters}
+"""
+
+  def _train_from_path(
+      self, ds: str, valid: Optional[str]
+  ) -> generic_model.ModelType:
+    """Trains a model from a file path (dataset reading in YDF C++)."""
+    with log.cc_log_context():
+      if self._data_spec is not None:
+        cc_model = self._get_learner().TrainFromPathWithDataSpec(
+            ds, self._data_spec, valid
+        )
+      else:
+        guide = self._build_data_spec_args().to_proto_guide()
+        cc_model = self._get_learner().TrainFromPathWithGuide(ds, guide, valid)
+      return model_lib.load_cc_model(cc_model)
+
+  def _train_from_dataset(
+      self,
+      ds: dataset.InputDataset,
+      valid: Optional[dataset.InputDataset] = None,
+  ) -> generic_model.ModelType:
+    """Trains a model from in-memory data."""
+
+    with log.cc_log_context():
+      train_ds = self._get_vertical_dataset(ds)._dataset  # pylint: disable=protected-access
+      train_args = {"dataset": train_ds}
+
+      if valid is not None:
+        valid_ds = self._get_vertical_dataset(valid)._dataset  # pylint: disable=protected-access
+        train_args["validation_dataset"] = valid_ds
+        log.info(
+            "Train model on %d training examples and %d validation examples",
+            train_ds.nrow(),
+            valid_ds.nrow(),
+        )
+      else:
+        log.info(
+            "Train model on %d examples",
+            train_ds.nrow(),
+        )
+
+      time_begin_training_model = datetime.datetime.now()
+      learner = self._get_learner()
+      cc_model = learner.Train(**train_args)
+      log.info(
+          "Model trained in %s",
+          datetime.datetime.now() - time_begin_training_model,
+      )
+
+      return model_lib.load_cc_model(cc_model)
+
+  def _get_training_config(self) -> abstract_learner_pb2.TrainingConfig:
+    """Gets the training config proto."""
+
+    training_config = abstract_learner_pb2.TrainingConfig(
+        learner=self._learner_name,
+        label=self._label,
+        weight_definition=self._build_weight_definition(),
+        ranking_group=self._ranking_group,
+        uplift_treatment=self._uplift_treatment,
+        task=self._task._to_proto_type(),
+        metadata=abstract_model_pb2.Metadata(framework=_FRAMEWORK_NAME),
+    )
+
+    # Apply monotonic constraints.
+    if self._data_spec_args.columns:
+      for feature in self._data_spec_args.columns:
+        if not feature.normalized_monotonic:
+          continue
+
+        proto_direction = (
+            abstract_learner_pb2.MonotonicConstraint.INCREASING
+            if feature.normalized_monotonic == dataspec.Monotonic.INCREASING
+            else abstract_learner_pb2.MonotonicConstraint.DECREASING
+        )
+        training_config.monotonic_constraints.append(
+            abstract_learner_pb2.MonotonicConstraint(
+                feature=_feature_name_to_regex(feature.name),
+                direction=proto_direction,
+            )
+        )
+
+    if self._tuner:
+      training_config.MergeFrom(self._tuner.train_config)
+    return training_config
+
+  def _get_learner(self) -> ydf.GenericCCLearner:
+    """Gets a ready-to-train learner."""
+
+    training_config = self._get_training_config()
+    cc_custom_loss = None
+    if "loss" in self._hyperparameters and isinstance(
+        self._hyperparameters["loss"], custom_loss.AbstractCustomLoss
+    ):
+      log.info(
+          "Using a custom loss. Note when using custom losses, hyperparameter"
+          " `apply_link_function` is ignored. Use the losses' activation"
+          " function instead."
+      )
+      py_custom_loss: custom_loss.AbstractCustomLoss = self._hyperparameters[
+          "loss"
+      ]
+      py_custom_loss.check_is_compatible_task(training_config.task)
+      cc_custom_loss = py_custom_loss._to_cc()  # pylint: disable=protected-access
+      # TODO: b/322763329 - Fail if the user set apply_link_function.
+      if py_custom_loss.activation.name == "IDENTITY":
+        self._hyperparameters["apply_link_function"] = False
+      else:
+        self._hyperparameters["apply_link_function"] = True
+
+    hp_proto = hyperparameters.dict_to_generic_hyperparameter(
+        self._hyperparameters
+    )
+    return ydf.GetLearner(
+        training_config, hp_proto, self._deployment_config, cc_custom_loss
+    )
+
+  def _get_vertical_dataset(
+      self, ds: dataset.InputDataset
+  ) -> dataset.VerticalDataset:
+    if isinstance(ds, dataset.VerticalDataset):
+      return ds
+      # TODO: Check that the user has not specified a data spec guide.
+    else:
+
+      # List of columns that cannot be unrolled.
+      dont_unroll_columns = [self._label]
+      for column in [
+          self._weights,
+          self._ranking_group,
+          self._uplift_treatment,
+      ]:
+        if column:
+          dont_unroll_columns.append(column)
+
+      effective_data_spec_args = None
+      if self._data_spec is None:
+        effective_data_spec_args = self._build_data_spec_args()
+      return dataset.create_vertical_dataset_with_spec_or_args(
+          ds,
+          data_spec=self._data_spec,
+          inference_args=effective_data_spec_args,
+          required_columns=None,  # All columns in the dataspec are required.
+          dont_unroll_columns=dont_unroll_columns,
+      )
+
+  def cross_validation(
+      self,
+      ds: dataset.InputDataset,
+      folds: int = 10,
+      bootstrapping: Union[bool, int] = False,
+      parallel_evaluations: int = 1,
+  ) -> metric.Evaluation:
+    """Cross-validates the learner and return the evaluation.
+
+    Usage example:
+
+    ```python
+    import pandas as pd
+    import ydf
+
+    dataset = pd.read_csv("my_dataset.csv")
+    learner = ydf.RandomForestLearner(label="label")
+    evaluation = learner.cross_validation(dataset)
+
+    # In a notebook, display an interractive evaluation
+    evaluation
+
+    # Print the evaluation
+    print(evaluation)
+
+    # Look at specific metrics
+    print(evaluation.accuracy)
+    ```
+
+    Args:
+      ds: Dataset for the cross-validation.
+      folds: Number of cross-validation folds.
+      bootstrapping: Controls whether bootstrapping is used to evaluate the
+        confidence intervals and statistical tests (i.e., all the metrics ending
+        with "[B]"). If set to false, bootstrapping is disabled. If set to true,
+        bootstrapping is enabled and 2000 bootstrapping samples are used. If set
+        to an integer, it specifies the number of bootstrapping samples to use.
+        In this case, if the number is less than 100, an error is raised as
+        bootstrapping will not yield useful results.
+      parallel_evaluations: Number of model to train and evaluate in parallel
+        using multi-threading. Note that each model is potentially already
+        trained with multithreading (see `num_threads` argument of Learner
+        constructor).
+
+    Returns:
+      The cross-validation evaluation.
+    """
+
+    fold_generator = fold_generator_pb2.FoldGenerator(
+        cross_validation=fold_generator_pb2.FoldGenerator.CrossValidation(
+            num_folds=folds,
+        )
+    )
+
+    if isinstance(bootstrapping, bool):
+      bootstrapping_samples = 2000 if bootstrapping else -1
+    elif isinstance(bootstrapping, int) and bootstrapping >= 100:
+      bootstrapping_samples = bootstrapping
+    else:
+      raise ValueError(
+          "bootstrapping argument should be boolean or an integer greater than"
+          " 100 as bootstrapping will not yield useful results. Got"
+          f" {bootstrapping!r} instead"
+      )
+    evaluation_options = metric_pb2.EvaluationOptions(
+        bootstrapping_samples=bootstrapping_samples
+    )
+
+    deployment_evaluation = abstract_learner_pb2.DeploymentConfig(
+        num_threads=parallel_evaluations,
+    )
+
+    vertical_dataset = self._get_vertical_dataset(ds)
+    learner = self._get_learner()
+
+    with log.cc_log_context():
+      evaluation_proto = learner.Evaluate(
+          vertical_dataset._dataset,  # pylint: disable=protected-access
+          fold_generator,
+          evaluation_options,
+          deployment_evaluation,
+      )
+      return metric.Evaluation(evaluation_proto)
+
+  def _build_data_spec_args(self) -> dataspec.DataSpecInferenceArgs:
+    """Builds DS args with user inputs and guides for labels / special columns.
+
+    Create a copy of self._data_spec_args and adds column definitions for the
+    columns label / weights / ranking group / uplift treatment with the correct
+    semantic and dataspec inference arguments.
+
+    Returns:
+      A copy of the data spec arguments with information about the special
+      columns added.
+
+    Raises:
+      ValueError: If the label / weights / ranking group / uplift treatment
+      column are specified as features.
+    """
+
+    def create_label_column(name: str, task: Task) -> dataspec.Column:
+      if task in [Task.CLASSIFICATION, Task.CATEGORICAL_UPLIFT]:
+        return dataspec.Column(
+            name=name,
+            semantic=dataspec.Semantic.CATEGORICAL,
+            max_vocab_count=-1,
+            min_vocab_frequency=1,
+        )
+      elif task in [Task.REGRESSION, Task.RANKING, Task.NUMERICAL_UPLIFT]:
+        return dataspec.Column(name=name, semantic=dataspec.Semantic.NUMERICAL)
+      else:
+        raise ValueError(f"Unsupported task {task.name} for label column")
+
+    data_spec_args = copy.deepcopy(self._data_spec_args)
+    # If no columns have been specified, make sure that all columns are used,
+    # since this function will specify some.
+    #
+    # TODO: If `label` becomes an optional argument, this function needs
+    # to be adapted.
+    if data_spec_args.columns is None:
+      data_spec_args.include_all_columns = True
+      data_spec_args.columns = []
+    column_defs = data_spec_args.columns
+    if dataspec.column_defs_contains_column(self._label, column_defs):
+      raise ValueError(
+          f"Label column {self._label} is also an input feature. A column"
+          " cannot be both a label and input feature."
+      )
+    column_defs.append(create_label_column(self._label, self._task))
+    if self._weights is not None:
+      if dataspec.column_defs_contains_column(self._weights, column_defs):
+        raise ValueError(
+            f"Weights column {self._weights} is also an input feature. A column"
+            " cannot be both a weights and input feature."
+        )
+      column_defs.append(
+          dataspec.Column(
+              name=self._weights, semantic=dataspec.Semantic.NUMERICAL
+          )
+      )
+    if self._ranking_group is not None:
+      assert self._task == Task.RANKING
+
+      if dataspec.column_defs_contains_column(self._ranking_group, column_defs):
+        raise ValueError(
+            f"Ranking group column {self._ranking_group} is also an input"
+            " feature. A column cannot be both a ranking group and input"
+            " feature."
+        )
+      column_defs.append(
+          dataspec.Column(
+              name=self._ranking_group, semantic=dataspec.Semantic.HASH
+          )
+      )
+    if self._uplift_treatment is not None:
+      assert self._task in [Task.NUMERICAL_UPLIFT, Task.CATEGORICAL_UPLIFT]
+
+      if dataspec.column_defs_contains_column(
+          self._uplift_treatment, column_defs
+      ):
+        raise ValueError(
+            "The uplift_treatment column should not be specified as a feature"
+        )
+      column_defs.append(
+          dataspec.Column(
+              name=self._uplift_treatment,
+              semantic=dataspec.Semantic.CATEGORICAL,
+              max_vocab_count=-1,
+              min_vocab_frequency=1,
+          )
+      )
+    return data_spec_args
+
+  def _build_weight_definition(
+      self,
+  ) -> Optional[weight_pb2.WeightDefinition]:
+    weight_definition = None
+    if self._weights is not None:
+      weight_definition = weight_pb2.WeightDefinition(
+          attribute=self._weights,
+          numerical=weight_pb2.WeightDefinition.NumericalWeight(),
+      )
+    return weight_definition
+
+  def _build_deployment_config(
+      self,
+      num_threads: Optional[int],
+      resume_training: bool,
+      resume_training_snapshot_interval_seconds: int,
+      working_dir: Optional[str],
+      workers: Optional[Sequence[str]],
+  ):
+    """Merges constructor arguments into a deployment configuration."""
+
+    if num_threads is None:
+      num_threads = self._determine_optimal_num_threads()
+    config = abstract_learner_pb2.DeploymentConfig(
+        num_threads=num_threads,
+        try_resume_training=resume_training,
+        cache_path=working_dir,
+        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
+    )
+
+    if workers is not None:
+      if not workers:
+        raise ValueError("At least one worker should be provided")
+      config.distribute.implementation_key = "GRPC"
+      grpc_config = config.distribute.Extensions[grpc_pb2.grpc]
+      grpc_config.grpc_addresses.addresses[:] = workers
+
+    return config
+
+  def _determine_optimal_num_threads(self):
+    """Sets  number of threads to min(num_cpus, 32) or 6 if num_cpus unclear."""
+    num_threads = os.cpu_count()
+    if num_threads is None:
+      logging.warning("Cannot determine the number of CPUs. Set num_threads=6")
+      num_threads = 6
+    else:
+      if num_threads > 32:
+        logging.warning(
+            "The `num_threads` constructor argument is not set and the "
+            "number of CPU is os.cpu_count()=%d > 32. Setting num_threads "
+            "to 32. Set num_threads manually to use more than 32 cpus.",
+            num_threads,
+        )
+        num_threads = 32
+      else:
+        logging.info("Use %d thread(s) for training", num_threads)
+    return num_threads
+
+  @classmethod
+  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
+    raise NotImplementedError("Not implemented")
+
+
+def _feature_name_to_regex(name: str) -> str:
+  """Generates a regular expression capturing a feature by name."""
+
+  return "^" + re.escape(name) + "$"
```

## ydf/learner/hyperparameters.py

```diff
@@ -1,116 +1,116 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Utility functions for YDF Hyperparameters."""
-from collections.abc import Mapping
-import dataclasses
-from typing import Dict, Union
-
-from yggdrasil_decision_forests.model import hyperparameter_pb2
-from ydf.learner import custom_loss
-
-
-HyperParameters = Dict[
-    str, Union[int, float, str, bool, custom_loss.AbstractCustomLoss]
-]
-
-
-def dict_to_generic_hyperparameter(
-    src: HyperParameters,
-) -> hyperparameter_pb2.GenericHyperParameters:
-  """Transform a dictionary of hyperparameters to the corresponding proto."""
-  generic_hps = hyperparameter_pb2.GenericHyperParameters()
-  for key, value in src.items():
-    if value is None:
-      # The value is not defined, use default.
-      continue
-    if key == "loss" and isinstance(value, custom_loss.AbstractCustomLoss):
-      # Custom Python fields must be treated separately.
-      continue
-    # Boolean has to come first, since it is a subtype of int.
-    if isinstance(value, bool):
-      # The GenericHyperParameters proto demands this conversion.
-      value_as_str = "true" if value else "false"
-      generic_hps.fields.append(
-          hyperparameter_pb2.GenericHyperParameters.Field(
-              name=key,
-              value=hyperparameter_pb2.GenericHyperParameters.Value(
-                  categorical=value_as_str
-              ),
-          )
-      )
-    elif isinstance(value, int):
-      generic_hps.fields.append(
-          hyperparameter_pb2.GenericHyperParameters.Field(
-              name=key,
-              value=hyperparameter_pb2.GenericHyperParameters.Value(
-                  integer=value
-              ),
-          )
-      )
-    elif isinstance(value, float):
-      generic_hps.fields.append(
-          hyperparameter_pb2.GenericHyperParameters.Field(
-              name=key,
-              value=hyperparameter_pb2.GenericHyperParameters.Value(real=value),
-          )
-      )
-    elif isinstance(value, str):
-      generic_hps.fields.append(
-          hyperparameter_pb2.GenericHyperParameters.Field(
-              name=key,
-              value=hyperparameter_pb2.GenericHyperParameters.Value(
-                  categorical=value
-              ),
-          )
-      )
-    else:
-      raise ValueError(f"Invalid value {value} for parameter {key}")
-  return generic_hps
-
-
-@dataclasses.dataclass
-class HyperparameterTemplate(Mapping):
-  """A named and versioned set of hyper-parameters.
-
-  List of hyper-parameter sets that outperforms the default hyper-parameters
-  (either generally or in specific scenarios). A template is also a mapping of
-  hyperparameters and may be used with the double star operator.
-
-  Usage example:
-
-  ```python
-  templates = ydf.GradientBoostedTreesLearner.hyperparameter_templates()
-  better_default = templates["better_defaultv1"]
-  # Apply the parameters of the template on the learner.
-  learner = ydf.GradientBoostedTreesLearner(label, **better_default)
-  ```
-  """
-
-  name: str
-  version: int
-  parameters: HyperParameters
-  description: str
-
-  def __iter__(self):
-    for key in self.parameters.keys():
-      yield key
-
-  def __len__(self):
-    return len(self.parameters)
-
-  def __getitem__(self, item):
-    if isinstance(self.parameters, dict) and item in self.parameters:
-      return self.parameters[item]
-    return None
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Utility functions for YDF Hyperparameters."""
+from collections.abc import Mapping
+import dataclasses
+from typing import Dict, Union
+
+from ydf.proto.model import hyperparameter_pb2
+from ydf.learner import custom_loss
+
+
+HyperParameters = Dict[
+    str, Union[int, float, str, bool, custom_loss.AbstractCustomLoss]
+]
+
+
+def dict_to_generic_hyperparameter(
+    src: HyperParameters,
+) -> hyperparameter_pb2.GenericHyperParameters:
+  """Transform a dictionary of hyperparameters to the corresponding proto."""
+  generic_hps = hyperparameter_pb2.GenericHyperParameters()
+  for key, value in src.items():
+    if value is None:
+      # The value is not defined, use default.
+      continue
+    if key == "loss" and isinstance(value, custom_loss.AbstractCustomLoss):
+      # Custom Python fields must be treated separately.
+      continue
+    # Boolean has to come first, since it is a subtype of int.
+    if isinstance(value, bool):
+      # The GenericHyperParameters proto demands this conversion.
+      value_as_str = "true" if value else "false"
+      generic_hps.fields.append(
+          hyperparameter_pb2.GenericHyperParameters.Field(
+              name=key,
+              value=hyperparameter_pb2.GenericHyperParameters.Value(
+                  categorical=value_as_str
+              ),
+          )
+      )
+    elif isinstance(value, int):
+      generic_hps.fields.append(
+          hyperparameter_pb2.GenericHyperParameters.Field(
+              name=key,
+              value=hyperparameter_pb2.GenericHyperParameters.Value(
+                  integer=value
+              ),
+          )
+      )
+    elif isinstance(value, float):
+      generic_hps.fields.append(
+          hyperparameter_pb2.GenericHyperParameters.Field(
+              name=key,
+              value=hyperparameter_pb2.GenericHyperParameters.Value(real=value),
+          )
+      )
+    elif isinstance(value, str):
+      generic_hps.fields.append(
+          hyperparameter_pb2.GenericHyperParameters.Field(
+              name=key,
+              value=hyperparameter_pb2.GenericHyperParameters.Value(
+                  categorical=value
+              ),
+          )
+      )
+    else:
+      raise ValueError(f"Invalid value {value} for parameter {key}")
+  return generic_hps
+
+
+@dataclasses.dataclass
+class HyperparameterTemplate(Mapping):
+  """A named and versioned set of hyper-parameters.
+
+  List of hyper-parameter sets that outperforms the default hyper-parameters
+  (either generally or in specific scenarios). A template is also a mapping of
+  hyperparameters and may be used with the double star operator.
+
+  Usage example:
+
+  ```python
+  templates = ydf.GradientBoostedTreesLearner.hyperparameter_templates()
+  better_default = templates["better_defaultv1"]
+  # Apply the parameters of the template on the learner.
+  learner = ydf.GradientBoostedTreesLearner(label, **better_default)
+  ```
+  """
+
+  name: str
+  version: int
+  parameters: HyperParameters
+  description: str
+
+  def __iter__(self):
+    for key in self.parameters.keys():
+      yield key
+
+  def __len__(self):
+    return len(self.parameters)
+
+  def __getitem__(self, item):
+    if isinstance(self.parameters, dict) and item in self.parameters:
+      return self.parameters[item]
+    return None
```

## ydf/learner/learner_test.py

```diff
@@ -1,876 +1,876 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Tests for model learning."""
-
-import os
-import signal
-from typing import Tuple
-
-from absl import logging
-from absl.testing import absltest
-from absl.testing import parameterized
-import numpy as np
-import numpy.testing as npt
-import pandas as pd
-
-from yggdrasil_decision_forests.dataset import data_spec_pb2
-from yggdrasil_decision_forests.dataset import data_spec_pb2 as ds_pb
-from yggdrasil_decision_forests.learner import abstract_learner_pb2
-from yggdrasil_decision_forests.model import abstract_model_pb2
-from ydf.dataset import dataspec
-from ydf.learner import generic_learner
-from ydf.learner import specialized_learners
-from ydf.learner import tuner as tuner_lib
-from ydf.metric import metric
-from ydf.model import generic_model
-from ydf.model import model_lib
-from ydf.model.decision_forest_model import decision_forest_model
-from ydf.utils import log
-from ydf.utils import test_utils
-
-ProtoMonotonicConstraint = abstract_learner_pb2.MonotonicConstraint
-Column = dataspec.Column
-
-
-class LearnerTest(parameterized.TestCase):
-
-  def setUp(self):
-    super().setUp()
-    self.dataset_directory = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset"
-    )
-
-    self.adult = test_utils.load_datasets("adult")
-    self.two_center_regression = test_utils.load_datasets(
-        "two_center_regression"
-    )
-    self.synthetic_ranking = test_utils.load_datasets(
-        "synthetic_ranking",
-        [Column("GROUP", semantic=dataspec.Semantic.HASH)],
-    )
-    self.sim_pte = test_utils.load_datasets(
-        "sim_pte",
-        [
-            Column("y", semantic=dataspec.Semantic.CATEGORICAL),
-            Column("treat", semantic=dataspec.Semantic.CATEGORICAL),
-        ],
-    )
-
-  def _check_adult_model(
-      self,
-      learner: generic_learner.GenericLearner,
-      minimum_accuracy: float,
-      check_serialization: bool = True,
-  ) -> Tuple[generic_model.GenericModel, metric.Evaluation, np.ndarray]:
-    """Runs a battery of test on a model compatible with the adult dataset.
-
-    The following tests are run:
-      - Train the model.
-      - Run and evaluate the model.
-      - Serialize the model to a YDF model.
-      - Load the serialized model.
-      - Make sure predictions of original model and serialized model match.
-
-    Args:
-      learner: A learner for on the adult dataset.
-      minimum_accuracy: minimum accuracy.
-      check_serialization: If true, check the serialization of the model.
-
-    Returns:
-      The model, its evaluation and the predictions on the test dataset.
-    """
-    # Train the model.
-    model = learner.train(self.adult.train)
-
-    # Evaluate the trained model.
-    evaluation = model.evaluate(self.adult.test)
-    self.assertGreaterEqual(evaluation.accuracy, minimum_accuracy)
-
-    predictions = model.predict(self.adult.test)
-
-    if check_serialization:
-      ydf_model_path = os.path.join(
-          self.create_tempdir().full_path, "ydf_model"
-      )
-      model.save(ydf_model_path)
-      loaded_model = model_lib.load_model(ydf_model_path)
-      npt.assert_equal(predictions, loaded_model.predict(self.adult.test))
-
-    return model, evaluation, predictions
-
-
-class RandomForestLearnerTest(LearnerTest):
-
-  def test_adult_classification(self):
-    learner = specialized_learners.RandomForestLearner(label="income")
-    model = learner.train(self.adult.train)
-    logging.info("Trained model: %s", model)
-
-    # Evaluate the trained model.
-    evaluation = model.evaluate(self.adult.test)
-    logging.info("Evaluation: %s", evaluation)
-    self.assertGreaterEqual(evaluation.accuracy, 0.864)
-
-  def test_two_center_regression(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="target", task=generic_learner.Task.REGRESSION
-    )
-    model = learner.train(self.two_center_regression.train)
-    logging.info("Trained model: %s", model)
-
-    # Evaluate the trained model.
-    evaluation = model.evaluate(self.two_center_regression.test)
-    logging.info("Evaluation: %s", evaluation)
-    self.assertAlmostEqual(evaluation.rmse, 114.54, places=0)
-
-  def test_sim_pte_uplift(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="y",
-        uplift_treatment="treat",
-        task=generic_learner.Task.CATEGORICAL_UPLIFT,
-    )
-    model = learner.train(self.sim_pte.train)
-
-    evaluation = model.evaluate(self.sim_pte.test)
-    self.assertAlmostEqual(evaluation.qini, 0.105709, places=2)
-
-  def test_sim_pte_uplift_pd(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="y",
-        uplift_treatment="treat",
-        task=generic_learner.Task.CATEGORICAL_UPLIFT,
-    )
-    model = learner.train(self.sim_pte.train_pd)
-
-    evaluation = model.evaluate(self.sim_pte.test_pd)
-    self.assertAlmostEqual(evaluation.qini, 0.105709, places=2)
-
-  def test_sim_pte_uplift_path(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="y",
-        uplift_treatment="treat",
-        task=generic_learner.Task.CATEGORICAL_UPLIFT,
-        num_threads=1,
-        max_depth=2,
-    )
-    model = learner.train(self.sim_pte.train_path)
-    evaluation = model.evaluate(self.sim_pte.test_path)
-    self.assertAlmostEqual(evaluation.qini, 0.105709, places=2)
-
-  def test_adult_classification_pd_and_vds_match(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="income", num_trees=50
-    )
-    model_pd = learner.train(self.adult.train_pd)
-    model_vds = learner.train(self.adult.train)
-
-    predictions_pd_from_vds = model_vds.predict(self.adult.test_pd)
-    predictions_pd_from_pd = model_pd.predict(self.adult.test_pd)
-    predictions_vds_from_vds = model_vds.predict(self.adult.test)
-
-    npt.assert_equal(predictions_pd_from_vds, predictions_vds_from_vds)
-    npt.assert_equal(predictions_pd_from_pd, predictions_vds_from_vds)
-
-  def test_two_center_regression_pd_and_vds_match(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="target", task=generic_learner.Task.REGRESSION, num_trees=50
-    )
-    model_pd = learner.train(self.two_center_regression.train_pd)
-    model_vds = learner.train(self.two_center_regression.train)
-
-    predictions_pd_from_vds = model_vds.predict(
-        self.two_center_regression.test_pd
-    )
-    predictions_pd_from_pd = model_pd.predict(
-        self.two_center_regression.test_pd
-    )
-    predictions_vds_from_vds = model_vds.predict(
-        self.two_center_regression.test
-    )
-
-    npt.assert_equal(predictions_pd_from_vds, predictions_vds_from_vds)
-    npt.assert_equal(predictions_pd_from_pd, predictions_vds_from_vds)
-
-  # TODO: Fix this test in OSS.
-  @absltest.skip("predictions do not match")
-  def test_adult_golden_predictions(self):
-    data_spec_path = os.path.join(
-        test_utils.ydf_test_data_path(),
-        "model",
-        "adult_binary_class_rf",
-        "data_spec.pb",
-    )
-    predictions_path = os.path.join(
-        test_utils.ydf_test_data_path(),
-        "prediction",
-        "adult_test_binary_class_rf.csv",
-    )
-    predictions_df = pd.read_csv(predictions_path)
-    data_spec = data_spec_pb2.DataSpecification()
-    with open(data_spec_path, "rb") as f:
-      data_spec.ParseFromString(f.read())
-
-    learner = specialized_learners.RandomForestLearner(
-        label="income",
-        num_trees=100,
-        winner_take_all=False,
-        data_spec=data_spec,
-    )
-    model = learner.train(self.adult.train_pd)
-    predictions = model.predict(self.adult.test_pd)
-    expected_predictions = predictions_df[">50K"].to_numpy()
-    # This is not particularly exact, but enough for a confidence check.
-    np.testing.assert_almost_equal(predictions, expected_predictions, decimal=1)
-
-  def test_model_type_regression(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="col_float",
-        num_trees=1,
-        task=generic_learner.Task.REGRESSION,
-    )
-    self.assertEqual(
-        learner.train(test_utils.toy_dataset()).task(),
-        generic_learner.Task.REGRESSION,
-    )
-
-  def test_model_type_classification_string_label(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="col_three_string",
-        num_trees=1,
-        task=generic_learner.Task.CLASSIFICATION,
-    )
-    self.assertEqual(
-        learner.train(test_utils.toy_dataset()).task(),
-        generic_learner.Task.CLASSIFICATION,
-    )
-
-  def test_model_type_classification_int_label(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="binary_int_label",
-        num_trees=1,
-        task=generic_learner.Task.CLASSIFICATION,
-    )
-    self.assertEqual(
-        learner.train(test_utils.toy_dataset()).task(),
-        generic_learner.Task.CLASSIFICATION,
-    )
-
-  def test_regression_on_categorical_fails(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="col_three_string",
-        num_trees=1,
-        task=generic_learner.Task.REGRESSION,
-    )
-    with self.assertRaises(ValueError):
-      _ = learner.train(test_utils.toy_dataset())
-
-  def test_classification_on_floats_fails(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="col_float",
-        num_trees=1,
-        task=generic_learner.Task.CLASSIFICATION,
-    )
-
-    with self.assertRaises(ValueError):
-      _ = (
-          learner.train(test_utils.toy_dataset()).task(),
-          generic_learner.Task.CLASSIFICATION,
-      )
-
-  def test_model_type_categorical_uplift(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="effect_binary",
-        uplift_treatment="treatement",
-        num_trees=1,
-        task=generic_learner.Task.CATEGORICAL_UPLIFT,
-    )
-    self.assertEqual(
-        learner.train(test_utils.toy_dataset_uplift()).task(),
-        generic_learner.Task.CATEGORICAL_UPLIFT,
-    )
-
-  def test_model_type_numerical_uplift(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="effect_numerical",
-        uplift_treatment="treatement",
-        num_trees=1,
-        task=generic_learner.Task.NUMERICAL_UPLIFT,
-    )
-    self.assertEqual(
-        learner.train(test_utils.toy_dataset_uplift()).task(),
-        generic_learner.Task.NUMERICAL_UPLIFT,
-    )
-
-  def test_adult_num_threads(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="income", num_threads=12, num_trees=50
-    )
-
-    self._check_adult_model(learner=learner, minimum_accuracy=0.860)
-
-  # TODO: b/310580458 - Fix this test in OSS.
-  @absltest.skip("Test sometimes times out")
-  def test_interrupt_training(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="income",
-        num_trees=1000000,  # Trains for a very long time
-    )
-
-    signal.alarm(3)  # Stop the training in 3 seconds
-    with self.assertRaises(ValueError):
-      _ = learner.train(self.adult.train)
-
-  def test_cross_validation(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="income", num_trees=10
-    )
-    evaluation = learner.cross_validation(
-        self.adult.train, folds=10, parallel_evaluations=2
-    )
-    logging.info("evaluation:\n%s", evaluation)
-    self.assertAlmostEqual(evaluation.accuracy, 0.87, 1)
-    # All the examples are used in the evaluation
-    self.assertEqual(
-        evaluation.num_examples, self.adult.train.data_spec().created_num_rows
-    )
-
-    with open(self.create_tempfile(), "w") as f:
-      f.write(evaluation._repr_html_())
-
-  def test_tuner_manual(self):
-    tuner = tuner_lib.RandomSearchTuner(
-        num_trials=5,
-        automatic_search_space=True,
-        parallel_trials=2,
-    )
-    learner = specialized_learners.GradientBoostedTreesLearner(
-        label="income",
-        tuner=tuner,
-        num_trees=30,
-    )
-
-    model, _, _ = self._check_adult_model(learner, minimum_accuracy=0.864)
-    logs = model.hyperparameter_optimizer_logs()
-    self.assertIsNotNone(logs)
-    self.assertLen(logs.trials, 5)
-
-  def test_tuner_predefined(self):
-    tuner = tuner_lib.RandomSearchTuner(
-        num_trials=5,
-        automatic_search_space=True,
-        parallel_trials=2,
-    )
-    learner = specialized_learners.GradientBoostedTreesLearner(
-        label="income",
-        tuner=tuner,
-        num_trees=30,
-    )
-
-    model, _, _ = self._check_adult_model(learner, minimum_accuracy=0.864)
-    logs = model.hyperparameter_optimizer_logs()
-    self.assertIsNotNone(logs)
-    self.assertLen(logs.trials, 5)
-
-  def test_label_type_error_message(self):
-    with self.assertRaisesRegex(
-        ValueError,
-        "Cannot import column 'l' with semantic=Semantic.CATEGORICAL",
-    ):
-      _ = specialized_learners.GradientBoostedTreesLearner(
-          label="l", task=generic_learner.Task.CLASSIFICATION
-      ).train(pd.DataFrame({"l": [1.0, 2.0], "f": [0, 1]}))
-
-    with self.assertRaisesRegex(
-        ValueError,
-        "Cannot convert NUMERICAL column 'l' of type numpy's array of 'object'"
-        " and with content=",
-    ):
-      _ = specialized_learners.GradientBoostedTreesLearner(
-          label="l", task=generic_learner.Task.REGRESSION
-      ).train(pd.DataFrame({"l": ["A", "B"], "f": [0, 1]}))
-
-  def test_with_validation(self):
-    with self.assertRaisesRegex(
-        ValueError,
-        "The learner 'RandomForestLearner' does not use a validation dataset",
-    ):
-      _ = specialized_learners.RandomForestLearner(
-          label="income", num_trees=5
-      ).train(self.adult.train, valid=self.adult.test)
-
-  def test_train_with_path_validation_dataset(self):
-    with self.assertRaisesRegex(
-        ValueError,
-        "The learner 'RandomForestLearner' does not use a validation dataset",
-    ):
-      _ = specialized_learners.RandomForestLearner(
-          label="income", num_trees=5
-      ).train(self.adult.train_path, valid=self.adult.test_path)
-
-  def test_compare_pandas_and_path(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="income", num_trees=50
-    )
-    model_from_pd = learner.train(self.adult.train)
-    predictions_from_pd = model_from_pd.predict(self.adult.test)
-
-    learner_from_path = specialized_learners.RandomForestLearner(
-        label="income", data_spec=model_from_pd.data_spec(), num_trees=50
-    )
-    model_from_path = learner_from_path.train(self.adult.train_path)
-    predictions_from_path = model_from_path.predict(self.adult.test)
-
-    npt.assert_equal(predictions_from_pd, predictions_from_path)
-
-  def test_default_hp_dictionary(self):
-    learner = specialized_learners.RandomForestLearner(label="l", num_trees=50)
-    self.assertDictContainsSubset(
-        {
-            "num_trees": 50,
-            "categorical_algorithm": "CART",
-            "categorical_set_split_greedy_sampling": 0.1,
-            "compute_oob_performances": True,
-            "compute_oob_variable_importances": False,
-        },
-        learner.hyperparameters,
-    )
-
-  def test_multidimensional_training_dataset(self):
-    data = {
-        "feature": np.array([[0, 1, 2, 3], [4, 5, 6, 7]]),
-        "label": np.array([0, 1]),
-    }
-    learner = specialized_learners.RandomForestLearner(label="label")
-    model = learner.train(data)
-
-    expected_columns = [
-        data_spec_pb2.Column(
-            name="feature.0_of_4",
-            type=data_spec_pb2.ColumnType.NUMERICAL,
-            dtype=ds_pb.DType.DTYPE_INT64,
-            count_nas=0,
-            numerical=data_spec_pb2.NumericalSpec(
-                mean=2,
-                standard_deviation=2,
-                min_value=0,
-                max_value=4,
-            ),
-            is_unstacked=True,
-        ),
-        data_spec_pb2.Column(
-            name="feature.1_of_4",
-            type=data_spec_pb2.ColumnType.NUMERICAL,
-            dtype=ds_pb.DType.DTYPE_INT64,
-            count_nas=0,
-            numerical=data_spec_pb2.NumericalSpec(
-                mean=3,
-                standard_deviation=2,
-                min_value=1,
-                max_value=5,
-            ),
-            is_unstacked=True,
-        ),
-        data_spec_pb2.Column(
-            name="feature.2_of_4",
-            type=data_spec_pb2.ColumnType.NUMERICAL,
-            dtype=ds_pb.DType.DTYPE_INT64,
-            count_nas=0,
-            numerical=data_spec_pb2.NumericalSpec(
-                mean=4,
-                standard_deviation=2,
-                min_value=2,
-                max_value=6,
-            ),
-            is_unstacked=True,
-        ),
-        data_spec_pb2.Column(
-            name="feature.3_of_4",
-            type=data_spec_pb2.ColumnType.NUMERICAL,
-            dtype=ds_pb.DType.DTYPE_INT64,
-            count_nas=0,
-            numerical=data_spec_pb2.NumericalSpec(
-                mean=5,
-                standard_deviation=2,
-                min_value=3,
-                max_value=7,
-            ),
-            is_unstacked=True,
-        ),
-    ]
-    # Skip the first column that contains the label.
-    self.assertEqual(model.data_spec().columns[1:], expected_columns)
-
-    predictions = model.predict(data)
-    self.assertEqual(predictions.shape, (2,))
-
-  def test_multidimensional_features_with_feature_arg(self):
-    ds = {
-        "f1": np.random.uniform(size=(100, 5)),
-        "f2": np.random.uniform(size=(100, 5)),
-        "label": np.random.randint(0, 2, size=100),
-    }
-    learner = specialized_learners.RandomForestLearner(
-        label="label",
-        features=["f1"],
-        num_trees=3,
-    )
-    model = learner.train(ds)
-    self.assertEqual(
-        model.input_feature_names(),
-        ["f1.0_of_5", "f1.1_of_5", "f1.2_of_5", "f1.3_of_5", "f1.4_of_5"],
-    )
-
-  def test_multidimensional_labels(self):
-    ds = {
-        "feature": np.array([[0, 1], [1, 0]]),
-        "label": np.array([[0, 1], [1, 0]]),
-    }
-    learner = specialized_learners.RandomForestLearner(label="label")
-    with self.assertRaisesRegex(
-        test_utils.AbslInvalidArgumentError,
-        "The column 'label' is multi-dimensional \\(shape=\\(2, 2\\)\\) while"
-        " the model requires this column to be single-dimensional",
-    ):
-      _ = learner.train(ds)
-
-  def test_multidimensional_weights(self):
-    ds = {
-        "feature": np.array([[0, 1], [1, 0]]),
-        "label": np.array([0, 1]),
-        "weight": np.array([[0, 1], [1, 0]]),
-    }
-    learner = specialized_learners.RandomForestLearner(
-        label="label", weights="weight"
-    )
-    with self.assertRaisesRegex(
-        test_utils.AbslInvalidArgumentError,
-        "The column 'weight' is multi-dimensional \\(shape=\\(2, 2\\)\\) while"
-        " the model requires this column to be single-dimensional",
-    ):
-      _ = learner.train(ds)
-
-  def test_learn_and_predict_when_label_is_not_last_column(self):
-    label = "age"
-    learner = specialized_learners.RandomForestLearner(
-        label=label, num_trees=10
-    )
-    # The age column is automatically interpreted as categorical
-    model_from_pd = learner.train(self.adult.train_pd)
-    evaluation = model_from_pd.evaluate(self.adult.test_pd)
-    self.assertGreaterEqual(evaluation.accuracy, 0.05)
-
-  def test_model_metadata_contains_framework(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="binary_int_label", num_trees=1
-    )
-    model = learner.train(test_utils.toy_dataset())
-    self.assertEqual(model.metadata().framework, "Python YDF")
-
-  def test_model_metadata_does_not_populate_owner(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="binary_int_label", num_trees=1
-    )
-    model = learner.train(test_utils.toy_dataset())
-    self.assertEqual(model.metadata().owner, "")
-
-  def test_adult_sparse_oblique(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="income",
-        num_trees=4,
-        split_axis="SPARSE_OBLIQUE",
-        sparse_oblique_weights="CONTINUOUS",
-    )
-    model = learner.train(self.adult.train)
-    logging.info("Trained model: %s", model)
-
-  def test_adult_mhld_oblique(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="income", num_trees=4, split_axis="MHLD_OBLIQUE"
-    )
-    model = learner.train(self.adult.train)
-    logging.info("Trained model: %s", model)
-
-
-class CARTLearnerTest(LearnerTest):
-
-  def test_adult(self):
-    learner = specialized_learners.CartLearner(label="income")
-
-    self._check_adult_model(learner=learner, minimum_accuracy=0.853)
-
-  def test_two_center_regression(self):
-    learner = specialized_learners.CartLearner(
-        label="target", task=generic_learner.Task.REGRESSION
-    )
-    model = learner.train(self.two_center_regression.train)
-    evaluation = model.evaluate(self.two_center_regression.test)
-    self.assertAlmostEqual(evaluation.rmse, 114.081, places=3)
-
-  def test_monotonic_non_compatible_learner(self):
-    learner = specialized_learners.CartLearner(
-        label="label", features=[dataspec.Column("feature", monotonic=+1)]
-    )
-    ds = pd.DataFrame({"feature": [0, 1], "label": [0, 1]})
-    with self.assertRaisesRegex(
-        test_utils.AbslInvalidArgumentError,
-        "The learner CART does not support monotonic constraints",
-    ):
-      _ = learner.train(ds)
-
-
-class GradientBoostedTreesLearnerTest(LearnerTest):
-
-  def test_adult(self):
-    learner = specialized_learners.GradientBoostedTreesLearner(label="income")
-
-    self._check_adult_model(learner=learner, minimum_accuracy=0.869)
-
-  def test_ranking(self):
-    learner = specialized_learners.GradientBoostedTreesLearner(
-        label="LABEL",
-        ranking_group="GROUP",
-        task=generic_learner.Task.RANKING,
-    )
-
-    model = learner.train(self.synthetic_ranking.train)
-    evaluation = model.evaluate(self.synthetic_ranking.test)
-    self.assertGreaterEqual(evaluation.ndcg, 0.70)
-    self.assertLessEqual(evaluation.ndcg, 0.74)
-
-  def test_ranking_pd(self):
-    learner = specialized_learners.GradientBoostedTreesLearner(
-        label="LABEL",
-        ranking_group="GROUP",
-        task=generic_learner.Task.RANKING,
-    )
-
-    model = learner.train(self.synthetic_ranking.train_pd)
-    evaluation = model.evaluate(self.synthetic_ranking.test_pd)
-    self.assertGreaterEqual(evaluation.ndcg, 0.70)
-    self.assertLessEqual(evaluation.ndcg, 0.74)
-
-  def test_ranking_path(self):
-    learner = specialized_learners.GradientBoostedTreesLearner(
-        label="LABEL",
-        ranking_group="GROUP",
-        task=generic_learner.Task.RANKING,
-    )
-
-    model = learner.train(self.synthetic_ranking.train_path)
-    evaluation = model.evaluate(self.synthetic_ranking.test_path)
-    self.assertGreaterEqual(evaluation.ndcg, 0.70)
-    self.assertLessEqual(evaluation.ndcg, 0.74)
-
-  def test_adult_num_threads(self):
-    learner = specialized_learners.GradientBoostedTreesLearner(
-        label="income", num_threads=12, num_trees=50
-    )
-
-    self._check_adult_model(learner=learner, minimum_accuracy=0.869)
-
-  def test_model_type_ranking(self):
-    learner = specialized_learners.GradientBoostedTreesLearner(
-        label="col_float",
-        ranking_group="col_three_string",
-        num_trees=1,
-        task=generic_learner.Task.RANKING,
-    )
-    self.assertEqual(
-        learner.train(test_utils.toy_dataset()).task(),
-        generic_learner.Task.RANKING,
-    )
-
-  def test_monotonic_non_compatible_options(self):
-    learner = specialized_learners.GradientBoostedTreesLearner(
-        label="label", features=[dataspec.Column("feature", monotonic=+1)]
-    )
-    ds = pd.DataFrame({"feature": [0, 1], "label": [0, 1]})
-    with self.assertRaisesRegex(
-        test_utils.AbslInvalidArgumentError,
-        "Gradient Boosted Trees does not support monotonic constraints with"
-        " use_hessian_gain=false",
-    ):
-      _ = learner.train(ds)
-
-  def test_monotonic_training(self):
-    learner = specialized_learners.GradientBoostedTreesLearner(
-        label="income",
-        num_trees=70,
-        use_hessian_gain=True,
-        features=[
-            dataspec.Column("age", monotonic=+1),
-            dataspec.Column("hours_per_week", monotonic=-1),
-            dataspec.Column("education_num", monotonic=+1),
-        ],
-        include_all_columns=True,
-    )
-
-    test_utils.assertProto2Equal(
-        self,
-        learner._get_training_config(),
-        abstract_learner_pb2.TrainingConfig(
-            learner="GRADIENT_BOOSTED_TREES",
-            label="income",
-            task=abstract_model_pb2.Task.CLASSIFICATION,
-            metadata=abstract_model_pb2.Metadata(framework="Python YDF"),
-            monotonic_constraints=[
-                ProtoMonotonicConstraint(
-                    feature="^age$",
-                    direction=ProtoMonotonicConstraint.INCREASING,
-                ),
-                ProtoMonotonicConstraint(
-                    feature="^hours_per_week$",
-                    direction=ProtoMonotonicConstraint.DECREASING,
-                ),
-                ProtoMonotonicConstraint(
-                    feature="^education_num$",
-                    direction=ProtoMonotonicConstraint.INCREASING,
-                ),
-            ],
-        ),
-    )
-
-    model, _, _ = self._check_adult_model(learner, minimum_accuracy=0.863)
-
-    _ = model.analyze(self.adult.test)
-
-  def test_with_validation_pd(self):
-    evaluation = (
-        specialized_learners.GradientBoostedTreesLearner(
-            label="income", num_trees=50
-        )
-        .train(self.adult.train, valid=self.adult.test)
-        .evaluate(self.adult.test)
-    )
-
-    logging.info("evaluation:\n%s", evaluation)
-    self.assertAlmostEqual(evaluation.accuracy, 0.87, 1)
-
-  def test_with_validation_path(self):
-    evaluation = (
-        specialized_learners.GradientBoostedTreesLearner(
-            label="income", num_trees=50
-        )
-        .train(self.adult.train_path, valid=self.adult.test_path)
-        .evaluate(self.adult.test_path)
-    )
-
-    logging.info("evaluation:\n%s", evaluation)
-    self.assertAlmostEqual(evaluation.accuracy, 0.87, 1)
-
-  def test_failure_train_path_validation_pd(self):
-    with self.assertRaisesRegex(
-        ValueError,
-        "If the training dataset is a path, the validation dataset must also be"
-        " a path.",
-    ):
-      specialized_learners.GradientBoostedTreesLearner(
-          label="income", num_trees=50
-      ).train(self.adult.train_path, valid=self.adult.test)
-
-  def test_failure_train_pd_validation_path(self):
-    with self.assertRaisesRegex(
-        ValueError,
-        "The validation dataset may only be a path if the training dataset is"
-        " a path.",
-    ):
-      specialized_learners.GradientBoostedTreesLearner(
-          label="income", num_trees=50
-      ).train(self.adult.train, valid=self.adult.test_path)
-
-  def test_resume_training(self):
-    learner = specialized_learners.GradientBoostedTreesLearner(
-        label="income",
-        num_trees=10,
-        resume_training=True,
-        working_dir=self.create_tempdir().full_path,
-    )
-    model_1 = learner.train(self.adult.train)
-    assert isinstance(model_1, decision_forest_model.DecisionForestModel)
-    self.assertEqual(model_1.num_trees(), 10)
-    learner.hyperparameters["num_trees"] = 50
-    model_2 = learner.train(self.adult.train)
-    assert isinstance(model_2, decision_forest_model.DecisionForestModel)
-    self.assertEqual(model_2.num_trees(), 50)
-
-  def test_predict_iris(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "iris.csv"
-    )
-    ds = pd.read_csv(dataset_path)
-    model = specialized_learners.RandomForestLearner(label="class").train(ds)
-
-    predictions = model.predict(ds)
-
-    self.assertEqual(predictions.shape, (ds.shape[0], 3))
-
-    row_sums = np.sum(predictions, axis=1)
-    # Make sure a multi-dimensional prediction always (mostly) sums to 1.
-    npt.assert_array_almost_equal(
-        row_sums, np.ones(predictions.shape[0]), decimal=5
-    )
-
-  def test_better_default_template(self):
-    ds = test_utils.toy_dataset()
-    label = "binary_int_label"
-    templates = (
-        specialized_learners.GradientBoostedTreesLearner.hyperparameter_templates()
-    )
-    self.assertIn("better_defaultv1", templates)
-    better_defaultv1 = templates["better_defaultv1"]
-    learner = specialized_learners.GradientBoostedTreesLearner(
-        label=label, **better_defaultv1
-    )
-    self.assertEqual(
-        learner.hyperparameters["growing_strategy"], "BEST_FIRST_GLOBAL"
-    )
-    _ = learner.train(ds)
-
-  def test_model_with_na_conditions_numerical(self):
-    ds = pd.DataFrame({
-        "feature": [np.nan] * 10 + [1.234] * 10,
-        "label": [0] * 10 + [1] * 10,
-    })
-    learner = specialized_learners.GradientBoostedTreesLearner(
-        label="label",
-        allow_na_conditions=True,
-        num_trees=1,
-    )
-    model = learner.train(ds)
-    evaluation = model.evaluate(ds)
-    self.assertEqual(evaluation.accuracy, 1)
-
-
-class LoggingTest(parameterized.TestCase):
-
-  @parameterized.parameters(0, 1, 2)
-  def test_logging(self, verbose):
-    save_verbose = log.verbose(verbose)
-    learner = specialized_learners.RandomForestLearner(label="label")
-    ds = pd.DataFrame({"feature": [0, 1], "label": [0, 1]})
-    _ = learner.train(ds)
-    log.verbose(save_verbose)
-
-
-class UtilityTest(LearnerTest):
-
-  def test_feature_name_to_regex(self):
-    self.assertEqual(
-        generic_learner._feature_name_to_regex("a(z)e"), r"^a\(z\)e$"
-    )
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Tests for model learning."""
+
+import os
+import signal
+from typing import Tuple
+
+from absl import logging
+from absl.testing import absltest
+from absl.testing import parameterized
+import numpy as np
+import numpy.testing as npt
+import pandas as pd
+
+from ydf.proto.dataset import data_spec_pb2
+from ydf.proto.dataset import data_spec_pb2 as ds_pb
+from ydf.proto.learner import abstract_learner_pb2
+from ydf.proto.model import abstract_model_pb2
+from ydf.dataset import dataspec
+from ydf.learner import generic_learner
+from ydf.learner import specialized_learners
+from ydf.learner import tuner as tuner_lib
+from ydf.metric import metric
+from ydf.model import generic_model
+from ydf.model import model_lib
+from ydf.model.decision_forest_model import decision_forest_model
+from ydf.utils import log
+from ydf.utils import test_utils
+
+ProtoMonotonicConstraint = abstract_learner_pb2.MonotonicConstraint
+Column = dataspec.Column
+
+
+class LearnerTest(parameterized.TestCase):
+
+  def setUp(self):
+    super().setUp()
+    self.dataset_directory = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset"
+    )
+
+    self.adult = test_utils.load_datasets("adult")
+    self.two_center_regression = test_utils.load_datasets(
+        "two_center_regression"
+    )
+    self.synthetic_ranking = test_utils.load_datasets(
+        "synthetic_ranking",
+        [Column("GROUP", semantic=dataspec.Semantic.HASH)],
+    )
+    self.sim_pte = test_utils.load_datasets(
+        "sim_pte",
+        [
+            Column("y", semantic=dataspec.Semantic.CATEGORICAL),
+            Column("treat", semantic=dataspec.Semantic.CATEGORICAL),
+        ],
+    )
+
+  def _check_adult_model(
+      self,
+      learner: generic_learner.GenericLearner,
+      minimum_accuracy: float,
+      check_serialization: bool = True,
+  ) -> Tuple[generic_model.GenericModel, metric.Evaluation, np.ndarray]:
+    """Runs a battery of test on a model compatible with the adult dataset.
+
+    The following tests are run:
+      - Train the model.
+      - Run and evaluate the model.
+      - Serialize the model to a YDF model.
+      - Load the serialized model.
+      - Make sure predictions of original model and serialized model match.
+
+    Args:
+      learner: A learner for on the adult dataset.
+      minimum_accuracy: minimum accuracy.
+      check_serialization: If true, check the serialization of the model.
+
+    Returns:
+      The model, its evaluation and the predictions on the test dataset.
+    """
+    # Train the model.
+    model = learner.train(self.adult.train)
+
+    # Evaluate the trained model.
+    evaluation = model.evaluate(self.adult.test)
+    self.assertGreaterEqual(evaluation.accuracy, minimum_accuracy)
+
+    predictions = model.predict(self.adult.test)
+
+    if check_serialization:
+      ydf_model_path = os.path.join(
+          self.create_tempdir().full_path, "ydf_model"
+      )
+      model.save(ydf_model_path)
+      loaded_model = model_lib.load_model(ydf_model_path)
+      npt.assert_equal(predictions, loaded_model.predict(self.adult.test))
+
+    return model, evaluation, predictions
+
+
+class RandomForestLearnerTest(LearnerTest):
+
+  def test_adult_classification(self):
+    learner = specialized_learners.RandomForestLearner(label="income")
+    model = learner.train(self.adult.train)
+    logging.info("Trained model: %s", model)
+
+    # Evaluate the trained model.
+    evaluation = model.evaluate(self.adult.test)
+    logging.info("Evaluation: %s", evaluation)
+    self.assertGreaterEqual(evaluation.accuracy, 0.864)
+
+  def test_two_center_regression(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="target", task=generic_learner.Task.REGRESSION
+    )
+    model = learner.train(self.two_center_regression.train)
+    logging.info("Trained model: %s", model)
+
+    # Evaluate the trained model.
+    evaluation = model.evaluate(self.two_center_regression.test)
+    logging.info("Evaluation: %s", evaluation)
+    self.assertAlmostEqual(evaluation.rmse, 114.54, places=0)
+
+  def test_sim_pte_uplift(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="y",
+        uplift_treatment="treat",
+        task=generic_learner.Task.CATEGORICAL_UPLIFT,
+    )
+    model = learner.train(self.sim_pte.train)
+
+    evaluation = model.evaluate(self.sim_pte.test)
+    self.assertAlmostEqual(evaluation.qini, 0.105709, places=2)
+
+  def test_sim_pte_uplift_pd(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="y",
+        uplift_treatment="treat",
+        task=generic_learner.Task.CATEGORICAL_UPLIFT,
+    )
+    model = learner.train(self.sim_pte.train_pd)
+
+    evaluation = model.evaluate(self.sim_pte.test_pd)
+    self.assertAlmostEqual(evaluation.qini, 0.105709, places=2)
+
+  def test_sim_pte_uplift_path(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="y",
+        uplift_treatment="treat",
+        task=generic_learner.Task.CATEGORICAL_UPLIFT,
+        num_threads=1,
+        max_depth=2,
+    )
+    model = learner.train(self.sim_pte.train_path)
+    evaluation = model.evaluate(self.sim_pte.test_path)
+    self.assertAlmostEqual(evaluation.qini, 0.105709, places=2)
+
+  def test_adult_classification_pd_and_vds_match(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="income", num_trees=50
+    )
+    model_pd = learner.train(self.adult.train_pd)
+    model_vds = learner.train(self.adult.train)
+
+    predictions_pd_from_vds = model_vds.predict(self.adult.test_pd)
+    predictions_pd_from_pd = model_pd.predict(self.adult.test_pd)
+    predictions_vds_from_vds = model_vds.predict(self.adult.test)
+
+    npt.assert_equal(predictions_pd_from_vds, predictions_vds_from_vds)
+    npt.assert_equal(predictions_pd_from_pd, predictions_vds_from_vds)
+
+  def test_two_center_regression_pd_and_vds_match(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="target", task=generic_learner.Task.REGRESSION, num_trees=50
+    )
+    model_pd = learner.train(self.two_center_regression.train_pd)
+    model_vds = learner.train(self.two_center_regression.train)
+
+    predictions_pd_from_vds = model_vds.predict(
+        self.two_center_regression.test_pd
+    )
+    predictions_pd_from_pd = model_pd.predict(
+        self.two_center_regression.test_pd
+    )
+    predictions_vds_from_vds = model_vds.predict(
+        self.two_center_regression.test
+    )
+
+    npt.assert_equal(predictions_pd_from_vds, predictions_vds_from_vds)
+    npt.assert_equal(predictions_pd_from_pd, predictions_vds_from_vds)
+
+  # TODO: Fix this test in OSS.
+  @absltest.skip("predictions do not match")
+  def test_adult_golden_predictions(self):
+    data_spec_path = os.path.join(
+        test_utils.ydf_test_data_path(),
+        "model",
+        "adult_binary_class_rf",
+        "data_spec.pb",
+    )
+    predictions_path = os.path.join(
+        test_utils.ydf_test_data_path(),
+        "prediction",
+        "adult_test_binary_class_rf.csv",
+    )
+    predictions_df = pd.read_csv(predictions_path)
+    data_spec = data_spec_pb2.DataSpecification()
+    with open(data_spec_path, "rb") as f:
+      data_spec.ParseFromString(f.read())
+
+    learner = specialized_learners.RandomForestLearner(
+        label="income",
+        num_trees=100,
+        winner_take_all=False,
+        data_spec=data_spec,
+    )
+    model = learner.train(self.adult.train_pd)
+    predictions = model.predict(self.adult.test_pd)
+    expected_predictions = predictions_df[">50K"].to_numpy()
+    # This is not particularly exact, but enough for a confidence check.
+    np.testing.assert_almost_equal(predictions, expected_predictions, decimal=1)
+
+  def test_model_type_regression(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="col_float",
+        num_trees=1,
+        task=generic_learner.Task.REGRESSION,
+    )
+    self.assertEqual(
+        learner.train(test_utils.toy_dataset()).task(),
+        generic_learner.Task.REGRESSION,
+    )
+
+  def test_model_type_classification_string_label(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="col_three_string",
+        num_trees=1,
+        task=generic_learner.Task.CLASSIFICATION,
+    )
+    self.assertEqual(
+        learner.train(test_utils.toy_dataset()).task(),
+        generic_learner.Task.CLASSIFICATION,
+    )
+
+  def test_model_type_classification_int_label(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="binary_int_label",
+        num_trees=1,
+        task=generic_learner.Task.CLASSIFICATION,
+    )
+    self.assertEqual(
+        learner.train(test_utils.toy_dataset()).task(),
+        generic_learner.Task.CLASSIFICATION,
+    )
+
+  def test_regression_on_categorical_fails(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="col_three_string",
+        num_trees=1,
+        task=generic_learner.Task.REGRESSION,
+    )
+    with self.assertRaises(ValueError):
+      _ = learner.train(test_utils.toy_dataset())
+
+  def test_classification_on_floats_fails(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="col_float",
+        num_trees=1,
+        task=generic_learner.Task.CLASSIFICATION,
+    )
+
+    with self.assertRaises(ValueError):
+      _ = (
+          learner.train(test_utils.toy_dataset()).task(),
+          generic_learner.Task.CLASSIFICATION,
+      )
+
+  def test_model_type_categorical_uplift(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="effect_binary",
+        uplift_treatment="treatement",
+        num_trees=1,
+        task=generic_learner.Task.CATEGORICAL_UPLIFT,
+    )
+    self.assertEqual(
+        learner.train(test_utils.toy_dataset_uplift()).task(),
+        generic_learner.Task.CATEGORICAL_UPLIFT,
+    )
+
+  def test_model_type_numerical_uplift(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="effect_numerical",
+        uplift_treatment="treatement",
+        num_trees=1,
+        task=generic_learner.Task.NUMERICAL_UPLIFT,
+    )
+    self.assertEqual(
+        learner.train(test_utils.toy_dataset_uplift()).task(),
+        generic_learner.Task.NUMERICAL_UPLIFT,
+    )
+
+  def test_adult_num_threads(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="income", num_threads=12, num_trees=50
+    )
+
+    self._check_adult_model(learner=learner, minimum_accuracy=0.860)
+
+  # TODO: b/310580458 - Fix this test in OSS.
+  @absltest.skip("Test sometimes times out")
+  def test_interrupt_training(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="income",
+        num_trees=1000000,  # Trains for a very long time
+    )
+
+    signal.alarm(3)  # Stop the training in 3 seconds
+    with self.assertRaises(ValueError):
+      _ = learner.train(self.adult.train)
+
+  def test_cross_validation(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="income", num_trees=10
+    )
+    evaluation = learner.cross_validation(
+        self.adult.train, folds=10, parallel_evaluations=2
+    )
+    logging.info("evaluation:\n%s", evaluation)
+    self.assertAlmostEqual(evaluation.accuracy, 0.87, 1)
+    # All the examples are used in the evaluation
+    self.assertEqual(
+        evaluation.num_examples, self.adult.train.data_spec().created_num_rows
+    )
+
+    with open(self.create_tempfile(), "w") as f:
+      f.write(evaluation._repr_html_())
+
+  def test_tuner_manual(self):
+    tuner = tuner_lib.RandomSearchTuner(
+        num_trials=5,
+        automatic_search_space=True,
+        parallel_trials=2,
+    )
+    learner = specialized_learners.GradientBoostedTreesLearner(
+        label="income",
+        tuner=tuner,
+        num_trees=30,
+    )
+
+    model, _, _ = self._check_adult_model(learner, minimum_accuracy=0.864)
+    logs = model.hyperparameter_optimizer_logs()
+    self.assertIsNotNone(logs)
+    self.assertLen(logs.trials, 5)
+
+  def test_tuner_predefined(self):
+    tuner = tuner_lib.RandomSearchTuner(
+        num_trials=5,
+        automatic_search_space=True,
+        parallel_trials=2,
+    )
+    learner = specialized_learners.GradientBoostedTreesLearner(
+        label="income",
+        tuner=tuner,
+        num_trees=30,
+    )
+
+    model, _, _ = self._check_adult_model(learner, minimum_accuracy=0.864)
+    logs = model.hyperparameter_optimizer_logs()
+    self.assertIsNotNone(logs)
+    self.assertLen(logs.trials, 5)
+
+  def test_label_type_error_message(self):
+    with self.assertRaisesRegex(
+        ValueError,
+        "Cannot import column 'l' with semantic=Semantic.CATEGORICAL",
+    ):
+      _ = specialized_learners.GradientBoostedTreesLearner(
+          label="l", task=generic_learner.Task.CLASSIFICATION
+      ).train(pd.DataFrame({"l": [1.0, 2.0], "f": [0, 1]}))
+
+    with self.assertRaisesRegex(
+        ValueError,
+        "Cannot convert NUMERICAL column 'l' of type numpy's array of 'object'"
+        " and with content=",
+    ):
+      _ = specialized_learners.GradientBoostedTreesLearner(
+          label="l", task=generic_learner.Task.REGRESSION
+      ).train(pd.DataFrame({"l": ["A", "B"], "f": [0, 1]}))
+
+  def test_with_validation(self):
+    with self.assertRaisesRegex(
+        ValueError,
+        "The learner 'RandomForestLearner' does not use a validation dataset",
+    ):
+      _ = specialized_learners.RandomForestLearner(
+          label="income", num_trees=5
+      ).train(self.adult.train, valid=self.adult.test)
+
+  def test_train_with_path_validation_dataset(self):
+    with self.assertRaisesRegex(
+        ValueError,
+        "The learner 'RandomForestLearner' does not use a validation dataset",
+    ):
+      _ = specialized_learners.RandomForestLearner(
+          label="income", num_trees=5
+      ).train(self.adult.train_path, valid=self.adult.test_path)
+
+  def test_compare_pandas_and_path(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="income", num_trees=50
+    )
+    model_from_pd = learner.train(self.adult.train)
+    predictions_from_pd = model_from_pd.predict(self.adult.test)
+
+    learner_from_path = specialized_learners.RandomForestLearner(
+        label="income", data_spec=model_from_pd.data_spec(), num_trees=50
+    )
+    model_from_path = learner_from_path.train(self.adult.train_path)
+    predictions_from_path = model_from_path.predict(self.adult.test)
+
+    npt.assert_equal(predictions_from_pd, predictions_from_path)
+
+  def test_default_hp_dictionary(self):
+    learner = specialized_learners.RandomForestLearner(label="l", num_trees=50)
+    self.assertDictContainsSubset(
+        {
+            "num_trees": 50,
+            "categorical_algorithm": "CART",
+            "categorical_set_split_greedy_sampling": 0.1,
+            "compute_oob_performances": True,
+            "compute_oob_variable_importances": False,
+        },
+        learner.hyperparameters,
+    )
+
+  def test_multidimensional_training_dataset(self):
+    data = {
+        "feature": np.array([[0, 1, 2, 3], [4, 5, 6, 7]]),
+        "label": np.array([0, 1]),
+    }
+    learner = specialized_learners.RandomForestLearner(label="label")
+    model = learner.train(data)
+
+    expected_columns = [
+        data_spec_pb2.Column(
+            name="feature.0_of_4",
+            type=data_spec_pb2.ColumnType.NUMERICAL,
+            dtype=ds_pb.DType.DTYPE_INT64,
+            count_nas=0,
+            numerical=data_spec_pb2.NumericalSpec(
+                mean=2,
+                standard_deviation=2,
+                min_value=0,
+                max_value=4,
+            ),
+            is_unstacked=True,
+        ),
+        data_spec_pb2.Column(
+            name="feature.1_of_4",
+            type=data_spec_pb2.ColumnType.NUMERICAL,
+            dtype=ds_pb.DType.DTYPE_INT64,
+            count_nas=0,
+            numerical=data_spec_pb2.NumericalSpec(
+                mean=3,
+                standard_deviation=2,
+                min_value=1,
+                max_value=5,
+            ),
+            is_unstacked=True,
+        ),
+        data_spec_pb2.Column(
+            name="feature.2_of_4",
+            type=data_spec_pb2.ColumnType.NUMERICAL,
+            dtype=ds_pb.DType.DTYPE_INT64,
+            count_nas=0,
+            numerical=data_spec_pb2.NumericalSpec(
+                mean=4,
+                standard_deviation=2,
+                min_value=2,
+                max_value=6,
+            ),
+            is_unstacked=True,
+        ),
+        data_spec_pb2.Column(
+            name="feature.3_of_4",
+            type=data_spec_pb2.ColumnType.NUMERICAL,
+            dtype=ds_pb.DType.DTYPE_INT64,
+            count_nas=0,
+            numerical=data_spec_pb2.NumericalSpec(
+                mean=5,
+                standard_deviation=2,
+                min_value=3,
+                max_value=7,
+            ),
+            is_unstacked=True,
+        ),
+    ]
+    # Skip the first column that contains the label.
+    self.assertEqual(model.data_spec().columns[1:], expected_columns)
+
+    predictions = model.predict(data)
+    self.assertEqual(predictions.shape, (2,))
+
+  def test_multidimensional_features_with_feature_arg(self):
+    ds = {
+        "f1": np.random.uniform(size=(100, 5)),
+        "f2": np.random.uniform(size=(100, 5)),
+        "label": np.random.randint(0, 2, size=100),
+    }
+    learner = specialized_learners.RandomForestLearner(
+        label="label",
+        features=["f1"],
+        num_trees=3,
+    )
+    model = learner.train(ds)
+    self.assertEqual(
+        model.input_feature_names(),
+        ["f1.0_of_5", "f1.1_of_5", "f1.2_of_5", "f1.3_of_5", "f1.4_of_5"],
+    )
+
+  def test_multidimensional_labels(self):
+    ds = {
+        "feature": np.array([[0, 1], [1, 0]]),
+        "label": np.array([[0, 1], [1, 0]]),
+    }
+    learner = specialized_learners.RandomForestLearner(label="label")
+    with self.assertRaisesRegex(
+        test_utils.AbslInvalidArgumentError,
+        "The column 'label' is multi-dimensional \\(shape=\\(2, 2\\)\\) while"
+        " the model requires this column to be single-dimensional",
+    ):
+      _ = learner.train(ds)
+
+  def test_multidimensional_weights(self):
+    ds = {
+        "feature": np.array([[0, 1], [1, 0]]),
+        "label": np.array([0, 1]),
+        "weight": np.array([[0, 1], [1, 0]]),
+    }
+    learner = specialized_learners.RandomForestLearner(
+        label="label", weights="weight"
+    )
+    with self.assertRaisesRegex(
+        test_utils.AbslInvalidArgumentError,
+        "The column 'weight' is multi-dimensional \\(shape=\\(2, 2\\)\\) while"
+        " the model requires this column to be single-dimensional",
+    ):
+      _ = learner.train(ds)
+
+  def test_learn_and_predict_when_label_is_not_last_column(self):
+    label = "age"
+    learner = specialized_learners.RandomForestLearner(
+        label=label, num_trees=10
+    )
+    # The age column is automatically interpreted as categorical
+    model_from_pd = learner.train(self.adult.train_pd)
+    evaluation = model_from_pd.evaluate(self.adult.test_pd)
+    self.assertGreaterEqual(evaluation.accuracy, 0.05)
+
+  def test_model_metadata_contains_framework(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="binary_int_label", num_trees=1
+    )
+    model = learner.train(test_utils.toy_dataset())
+    self.assertEqual(model.metadata().framework, "Python YDF")
+
+  def test_model_metadata_does_not_populate_owner(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="binary_int_label", num_trees=1
+    )
+    model = learner.train(test_utils.toy_dataset())
+    self.assertEqual(model.metadata().owner, "")
+
+  def test_adult_sparse_oblique(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="income",
+        num_trees=4,
+        split_axis="SPARSE_OBLIQUE",
+        sparse_oblique_weights="CONTINUOUS",
+    )
+    model = learner.train(self.adult.train)
+    logging.info("Trained model: %s", model)
+
+  def test_adult_mhld_oblique(self):
+    learner = specialized_learners.RandomForestLearner(
+        label="income", num_trees=4, split_axis="MHLD_OBLIQUE"
+    )
+    model = learner.train(self.adult.train)
+    logging.info("Trained model: %s", model)
+
+
+class CARTLearnerTest(LearnerTest):
+
+  def test_adult(self):
+    learner = specialized_learners.CartLearner(label="income")
+
+    self._check_adult_model(learner=learner, minimum_accuracy=0.853)
+
+  def test_two_center_regression(self):
+    learner = specialized_learners.CartLearner(
+        label="target", task=generic_learner.Task.REGRESSION
+    )
+    model = learner.train(self.two_center_regression.train)
+    evaluation = model.evaluate(self.two_center_regression.test)
+    self.assertAlmostEqual(evaluation.rmse, 114.081, places=3)
+
+  def test_monotonic_non_compatible_learner(self):
+    learner = specialized_learners.CartLearner(
+        label="label", features=[dataspec.Column("feature", monotonic=+1)]
+    )
+    ds = pd.DataFrame({"feature": [0, 1], "label": [0, 1]})
+    with self.assertRaisesRegex(
+        test_utils.AbslInvalidArgumentError,
+        "The learner CART does not support monotonic constraints",
+    ):
+      _ = learner.train(ds)
+
+
+class GradientBoostedTreesLearnerTest(LearnerTest):
+
+  def test_adult(self):
+    learner = specialized_learners.GradientBoostedTreesLearner(label="income")
+
+    self._check_adult_model(learner=learner, minimum_accuracy=0.869)
+
+  def test_ranking(self):
+    learner = specialized_learners.GradientBoostedTreesLearner(
+        label="LABEL",
+        ranking_group="GROUP",
+        task=generic_learner.Task.RANKING,
+    )
+
+    model = learner.train(self.synthetic_ranking.train)
+    evaluation = model.evaluate(self.synthetic_ranking.test)
+    self.assertGreaterEqual(evaluation.ndcg, 0.70)
+    self.assertLessEqual(evaluation.ndcg, 0.74)
+
+  def test_ranking_pd(self):
+    learner = specialized_learners.GradientBoostedTreesLearner(
+        label="LABEL",
+        ranking_group="GROUP",
+        task=generic_learner.Task.RANKING,
+    )
+
+    model = learner.train(self.synthetic_ranking.train_pd)
+    evaluation = model.evaluate(self.synthetic_ranking.test_pd)
+    self.assertGreaterEqual(evaluation.ndcg, 0.70)
+    self.assertLessEqual(evaluation.ndcg, 0.74)
+
+  def test_ranking_path(self):
+    learner = specialized_learners.GradientBoostedTreesLearner(
+        label="LABEL",
+        ranking_group="GROUP",
+        task=generic_learner.Task.RANKING,
+    )
+
+    model = learner.train(self.synthetic_ranking.train_path)
+    evaluation = model.evaluate(self.synthetic_ranking.test_path)
+    self.assertGreaterEqual(evaluation.ndcg, 0.70)
+    self.assertLessEqual(evaluation.ndcg, 0.74)
+
+  def test_adult_num_threads(self):
+    learner = specialized_learners.GradientBoostedTreesLearner(
+        label="income", num_threads=12, num_trees=50
+    )
+
+    self._check_adult_model(learner=learner, minimum_accuracy=0.869)
+
+  def test_model_type_ranking(self):
+    learner = specialized_learners.GradientBoostedTreesLearner(
+        label="col_float",
+        ranking_group="col_three_string",
+        num_trees=1,
+        task=generic_learner.Task.RANKING,
+    )
+    self.assertEqual(
+        learner.train(test_utils.toy_dataset()).task(),
+        generic_learner.Task.RANKING,
+    )
+
+  def test_monotonic_non_compatible_options(self):
+    learner = specialized_learners.GradientBoostedTreesLearner(
+        label="label", features=[dataspec.Column("feature", monotonic=+1)]
+    )
+    ds = pd.DataFrame({"feature": [0, 1], "label": [0, 1]})
+    with self.assertRaisesRegex(
+        test_utils.AbslInvalidArgumentError,
+        "Gradient Boosted Trees does not support monotonic constraints with"
+        " use_hessian_gain=false",
+    ):
+      _ = learner.train(ds)
+
+  def test_monotonic_training(self):
+    learner = specialized_learners.GradientBoostedTreesLearner(
+        label="income",
+        num_trees=70,
+        use_hessian_gain=True,
+        features=[
+            dataspec.Column("age", monotonic=+1),
+            dataspec.Column("hours_per_week", monotonic=-1),
+            dataspec.Column("education_num", monotonic=+1),
+        ],
+        include_all_columns=True,
+    )
+
+    test_utils.assertProto2Equal(
+        self,
+        learner._get_training_config(),
+        abstract_learner_pb2.TrainingConfig(
+            learner="GRADIENT_BOOSTED_TREES",
+            label="income",
+            task=abstract_model_pb2.Task.CLASSIFICATION,
+            metadata=abstract_model_pb2.Metadata(framework="Python YDF"),
+            monotonic_constraints=[
+                ProtoMonotonicConstraint(
+                    feature="^age$",
+                    direction=ProtoMonotonicConstraint.INCREASING,
+                ),
+                ProtoMonotonicConstraint(
+                    feature="^hours_per_week$",
+                    direction=ProtoMonotonicConstraint.DECREASING,
+                ),
+                ProtoMonotonicConstraint(
+                    feature="^education_num$",
+                    direction=ProtoMonotonicConstraint.INCREASING,
+                ),
+            ],
+        ),
+    )
+
+    model, _, _ = self._check_adult_model(learner, minimum_accuracy=0.863)
+
+    _ = model.analyze(self.adult.test)
+
+  def test_with_validation_pd(self):
+    evaluation = (
+        specialized_learners.GradientBoostedTreesLearner(
+            label="income", num_trees=50
+        )
+        .train(self.adult.train, valid=self.adult.test)
+        .evaluate(self.adult.test)
+    )
+
+    logging.info("evaluation:\n%s", evaluation)
+    self.assertAlmostEqual(evaluation.accuracy, 0.87, 1)
+
+  def test_with_validation_path(self):
+    evaluation = (
+        specialized_learners.GradientBoostedTreesLearner(
+            label="income", num_trees=50
+        )
+        .train(self.adult.train_path, valid=self.adult.test_path)
+        .evaluate(self.adult.test_path)
+    )
+
+    logging.info("evaluation:\n%s", evaluation)
+    self.assertAlmostEqual(evaluation.accuracy, 0.87, 1)
+
+  def test_failure_train_path_validation_pd(self):
+    with self.assertRaisesRegex(
+        ValueError,
+        "If the training dataset is a path, the validation dataset must also be"
+        " a path.",
+    ):
+      specialized_learners.GradientBoostedTreesLearner(
+          label="income", num_trees=50
+      ).train(self.adult.train_path, valid=self.adult.test)
+
+  def test_failure_train_pd_validation_path(self):
+    with self.assertRaisesRegex(
+        ValueError,
+        "The validation dataset may only be a path if the training dataset is"
+        " a path.",
+    ):
+      specialized_learners.GradientBoostedTreesLearner(
+          label="income", num_trees=50
+      ).train(self.adult.train, valid=self.adult.test_path)
+
+  def test_resume_training(self):
+    learner = specialized_learners.GradientBoostedTreesLearner(
+        label="income",
+        num_trees=10,
+        resume_training=True,
+        working_dir=self.create_tempdir().full_path,
+    )
+    model_1 = learner.train(self.adult.train)
+    assert isinstance(model_1, decision_forest_model.DecisionForestModel)
+    self.assertEqual(model_1.num_trees(), 10)
+    learner.hyperparameters["num_trees"] = 50
+    model_2 = learner.train(self.adult.train)
+    assert isinstance(model_2, decision_forest_model.DecisionForestModel)
+    self.assertEqual(model_2.num_trees(), 50)
+
+  def test_predict_iris(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "iris.csv"
+    )
+    ds = pd.read_csv(dataset_path)
+    model = specialized_learners.RandomForestLearner(label="class").train(ds)
+
+    predictions = model.predict(ds)
+
+    self.assertEqual(predictions.shape, (ds.shape[0], 3))
+
+    row_sums = np.sum(predictions, axis=1)
+    # Make sure a multi-dimensional prediction always (mostly) sums to 1.
+    npt.assert_array_almost_equal(
+        row_sums, np.ones(predictions.shape[0]), decimal=5
+    )
+
+  def test_better_default_template(self):
+    ds = test_utils.toy_dataset()
+    label = "binary_int_label"
+    templates = (
+        specialized_learners.GradientBoostedTreesLearner.hyperparameter_templates()
+    )
+    self.assertIn("better_defaultv1", templates)
+    better_defaultv1 = templates["better_defaultv1"]
+    learner = specialized_learners.GradientBoostedTreesLearner(
+        label=label, **better_defaultv1
+    )
+    self.assertEqual(
+        learner.hyperparameters["growing_strategy"], "BEST_FIRST_GLOBAL"
+    )
+    _ = learner.train(ds)
+
+  def test_model_with_na_conditions_numerical(self):
+    ds = pd.DataFrame({
+        "feature": [np.nan] * 10 + [1.234] * 10,
+        "label": [0] * 10 + [1] * 10,
+    })
+    learner = specialized_learners.GradientBoostedTreesLearner(
+        label="label",
+        allow_na_conditions=True,
+        num_trees=1,
+    )
+    model = learner.train(ds)
+    evaluation = model.evaluate(ds)
+    self.assertEqual(evaluation.accuracy, 1)
+
+
+class LoggingTest(parameterized.TestCase):
+
+  @parameterized.parameters(0, 1, 2)
+  def test_logging(self, verbose):
+    save_verbose = log.verbose(verbose)
+    learner = specialized_learners.RandomForestLearner(label="label")
+    ds = pd.DataFrame({"feature": [0, 1], "label": [0, 1]})
+    _ = learner.train(ds)
+    log.verbose(save_verbose)
+
+
+class UtilityTest(LearnerTest):
+
+  def test_feature_name_to_regex(self):
+    self.assertEqual(
+        generic_learner._feature_name_to_regex("a(z)e"), r"^a\(z\)e$"
+    )
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/learner/learner_with_tf_test.py

```diff
@@ -1,54 +1,61 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Tests for model learning."""
-
-
-from absl.testing import absltest
-import pandas as pd
-import tensorflow as tf
-
-from ydf.learner import generic_learner
-from ydf.learner import specialized_learners
-
-
-def toy_dataset():
-  df = pd.DataFrame({
-      "col1": ["A", "A", "B", "B", "C"],
-      "col2": [1, 2.1, 1.3, 5.5, 2.4],
-      "col3": ["bar", "foo", "foo", "foo", "foo"],
-      "weights": [3, 2, 3.1, 28, 3],
-      "label": [0, 0, 0, 1, 1],
-  })
-  return df
-
-
-class RandomForestLearnerTest(absltest.TestCase):
-
-  def test_tensorflow_dataset(self):
-    learner = specialized_learners.RandomForestLearner(
-        label="label", num_trees=1
-    )
-    tf_dataset = tf.data.Dataset.from_tensor_slices(dict(toy_dataset())).batch(
-        10
-    )
-    for x in tf_dataset.take(2):
-      print(x)
-    self.assertEqual(
-        learner.train(tf_dataset).task(), generic_learner.Task.CLASSIFICATION
-    )
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Tests for model learning."""
+
+from absl.testing import absltest
+from absl.testing import parameterized
+import pandas as pd
+import tensorflow as tf
+
+from ydf.learner import generic_learner
+from ydf.learner import specialized_learners
+
+
+def toy_dataset():
+  df = pd.DataFrame({
+      "col1": ["A", "A", "B", "B", "C"],
+      "col2": [1, 2.1, 1.3, 5.5, 2.4],
+      "col3": ["bar", "foo", "foo", "foo", "foo"],
+      "weights": [3, 2, 3.1, 28, 3],
+      "label": [0, 0, 0, 1, 1],
+  })
+  return df
+
+
+class RandomForestLearnerTest(parameterized.TestCase):
+
+  @parameterized.parameters({"use_cache": True}, {"use_filter": True})
+  def test_tensorflow_dataset(
+      self, use_cache: bool = False, use_filter: bool = False
+  ):
+    learner = specialized_learners.RandomForestLearner(
+        label="label", num_trees=1
+    )
+    tf_dataset = tf.data.Dataset.from_tensor_slices(dict(toy_dataset())).batch(
+        10
+    )
+    for x in tf_dataset.take(2):
+      print(x)
+    if use_cache:
+      tf_dataset = tf_dataset.cache()
+    if use_filter:
+      tf_dataset = tf_dataset.filter(lambda x: True)
+    self.assertEqual(
+        learner.train(tf_dataset).task(), generic_learner.Task.CLASSIFICATION
+    )
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/learner/specialized_learners.py

```diff
@@ -1,2167 +1,2167 @@
-r"""Wrappers around the YDF learners.
-
-This file is generated automatically by running the following commands:
-  bazel build //ydf/learner:specialized_learners\
-  && bazel-bin/ydf/learner/specialized_learners_generator\
-  > ydf/learner/specialized_learners_pre_generated.py
-
-Please don't change this file directly. Instead, changes the source. The
-documentation source is contained in the "GetGenericHyperParameterSpecification"
-method of each learner e.g. GetGenericHyperParameterSpecification in
-learner/gradient_boosted_trees/gradient_boosted_trees.cc contains the
-documentation (and meta-data) used to generate this file.
-
-In particular, these pre-generated wrappers included in the source code are 
-included for reference only. The actual wrappers are re-generated during
-compilation.
-"""
-
-from typing import Dict, Optional, Sequence, Union
-
-from yggdrasil_decision_forests.dataset import data_spec_pb2
-from yggdrasil_decision_forests.learner import abstract_learner_pb2
-from ydf.dataset import dataset
-from ydf.dataset import dataspec
-from ydf.learner import custom_loss
-from ydf.learner import generic_learner
-from ydf.learner import hyperparameters
-from ydf.learner import tuner as tuner_lib
-from ydf.model import generic_model
-from ydf.model.gradient_boosted_trees_model import gradient_boosted_trees_model
-from ydf.model.random_forest_model import random_forest_model
-
-
-
-class CartLearner(generic_learner.GenericLearner):
-  r"""Cart learning algorithm.
-
-  A CART (Classification and Regression Trees) a decision tree. The non-leaf
-  nodes contains conditions (also known as splits) while the leaf nodes contain
-  prediction values. The training dataset is divided in two parts. The first is
-  used to grow the tree while the second is used to prune the tree.
-
-  Usage example:
-
-  ```python
-  import ydf
-  import pandas as pd
-
-  dataset = pd.read_csv("project/dataset.csv")
-
-  model = ydf.CartLearner().train(dataset)
-
-  print(model.summary())
-  ```
-
-  Hyperparameters are configured to give reasonable results for typical
-  datasets. Hyperparameters can also be modified manually (see descriptions)
-  below or by applying the hyperparameter templates available with
-  `CartLearner.hyperparameter_templates()` (see this function's documentation for
-  details).
-
-  Attributes:
-    label: Label of the dataset. The label column
-      should not be identified as a feature in the `features` parameter.
-    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,
-      Task.RANKING, Task.CATEGORICAL_UPLIFT, Task.NUMERICAL_UPLIFT).
-    weights: Name of a feature that identifies the weight of each example. If
-      weights are not specified, unit weights are assumed. The weight column
-      should not be identified as a feature in the `features` parameter.
-    ranking_group: Only for `task=Task.RANKING`. Name of a feature
-      that identifies queries in a query/document ranking task. The ranking
-      group should not be identified as a feature in the `features` parameter.
-    uplift_treatment: Only for `task=Task.CATEGORICAL_UPLIFT` and `task=Task`.
-      NUMERICAL_UPLIFT. Name of a numerical feature that identifies the
-      treatment in an uplift problem. The value 0 is reserved for the control
-      treatment. Currently, only 0/1 binary treatments are supported.
-    features: If None, all columns are used as features. The semantic of the
-      features is determined automatically. Otherwise, if
-      include_all_columns=False (default) only the column listed in `features`
-      are imported. If include_all_columns=True, all the columns are imported as
-      features and only the semantic of the columns NOT in `columns` is 
-      determined automatically. If specified,  defines the order of the features
-      - any non-listed features are appended in-order after the specified
-      features (if include_all_columns=True).
-      The label, weights, uplift treatment and ranking_group columns should not
-      be specified as features.
-    include_all_columns: See `features`.
-    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
-      CATEGORICAL_SET columns stored as strings. If more unique values exist,
-      only the most frequent values are kept, and the remaining values are
-      considered as out-of-vocabulary.
-    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
-      and CATEGORICAL_SET columns. Value observed less than
-      `min_vocab_frequency` are considered as out-of-vocabulary.
-    discretize_numerical_columns: If true, discretize all the numerical columns
-      before training. Discretized numerical columns are faster to train with,
-      but they can have a negative impact on the model quality. Using
-      `discretize_numerical_columns=True` is equivalent as setting the column
-      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
-      definition of DISCRETIZED_NUMERICAL for more details.
-    num_discretized_numerical_bins: Number of bins used when disretizing
-      numerical columns.
-    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
-      inferring the column's semantic if it is not explicitly specified. Only
-      used when reading from file, in-memory datasets are always read in full.
-      Setting this to a lower number will speed up dataset reading, but might
-      result in incorrect column semantics. Set to -1 to scan the entire
-      dataset.
-    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
-      computing a column's statistics. Only used when reading from file,
-      in-memory datasets are always read in full. A column's statistics include
-      the dictionary for categorical features and the mean / min / max for
-      numerical features. Setting this to a lower number will speed up dataset
-      reading, but skew statistics in the dataspec, which can hurt model quality
-      (e.g. if an important category of a categorical feature is considered
-      OOV). Set to -1 to scan the entire dataset.
-    data_spec: Dataspec to be used (advanced). If a data spec is given,
-      `columns`, `include_all_columns`, `max_vocab_count`,
-      `min_vocab_frequency`, `discretize_numerical_columns` and 
-      `num_discretized_numerical_bins` will be ignored.
-    allow_na_conditions: If true, the tree training evaluates conditions of the
-      type `X is NA` i.e. `X is missing`. Default: False.
-    categorical_algorithm: How to learn splits on categorical attributes.
-      - `CART`: CART algorithm. Find categorical splits of the form "value \\in
-        mask". The solution is exact for binary classification, regression and
-        ranking. It is approximated for multi-class classification. This is a
-        good first algorithm to use. In case of overfitting (very small
-        dataset, large dictionary), the "random" algorithm is a good
-        alternative.
-      - `ONE_HOT`: One-hot encoding. Find the optimal categorical split of the
-        form "attribute == param". This method is similar (but more efficient)
-        than converting converting each possible categorical value into a
-        boolean feature. This method is available for comparison purpose and
-        generally performs worse than other alternatives.
-      - `RANDOM`: Best splits among a set of random candidate. Find the a
-        categorical split of the form "value \\in mask" using a random search.
-        This solution can be seen as an approximation of the CART algorithm.
-        This method is a strong alternative to CART. This algorithm is inspired
-        from section "5.1 Categorical Variables" of "Random Forest", 2001.
-        Default: "CART".
-    categorical_set_split_greedy_sampling: For categorical set splits e.g.
-      texts. Probability for a categorical value to be a candidate for the
-      positive set. The sampling is applied once per node (i.e. not at every
-      step of the greedy optimization). Default: 0.1.
-    categorical_set_split_max_num_items: For categorical set splits e.g. texts.
-      Maximum number of items (prior to the sampling). If more items are
-      available, the least frequent items are ignored. Changing this value is
-      similar to change the "max_vocab_count" before loading the dataset, with
-      the following exception: With `max_vocab_count`, all the remaining items
-      are grouped in a special Out-of-vocabulary item. With `max_num_items`,
-      this is not the case. Default: -1.
-    categorical_set_split_min_item_frequency: For categorical set splits e.g.
-      texts. Minimum number of occurrences of an item to be considered.
-      Default: 1.
-    growing_strategy: How to grow the tree.
-      - `LOCAL`: Each node is split independently of the other nodes. In other
-        words, as long as a node satisfy the splits "constraints (e.g. maximum
-        depth, minimum number of observations), the node will be split. This is
-        the "classical" way to grow decision trees.
-      - `BEST_FIRST_GLOBAL`: The node with the best loss reduction among all
-        the nodes of the tree is selected for splitting. This method is also
-        called "best first" or "leaf-wise growth". See "Best-first decision
-        tree learning", Shi and "Additive logistic regression : A statistical
-        view of boosting", Friedman for more details. Default: "LOCAL".
-    honest: In honest trees, different training examples are used to infer the
-      structure and the leaf values. This regularization technique trades
-      examples for bias estimates. It might increase or reduce the quality of
-      the model. See "Generalized Random Forests", Athey et al. In this paper,
-      Honest trees are trained with the Random Forest algorithm with a sampling
-      without replacement. Default: False.
-    honest_fixed_separation: For honest trees only i.e. honest=true. If true, a
-      new random separation is generated for each tree. If false, the same
-      separation is used for all the trees (e.g., in Gradient Boosted Trees
-      containing multiple trees). Default: False.
-    honest_ratio_leaf_examples: For honest trees only i.e. honest=true. Ratio
-      of examples used to set the leaf values. Default: 0.5.
-    in_split_min_examples_check: Whether to check the `min_examples` constraint
-      in the split search (i.e. splits leading to one child having less than
-      `min_examples` examples are considered invalid) or before the split
-      search (i.e. a node can be derived only if it contains more than
-      `min_examples` examples). If false, there can be nodes with less than
-      `min_examples` training examples. Default: True.
-    keep_non_leaf_label_distribution: Whether to keep the node value (i.e. the
-      distribution of the labels of the training examples) of non-leaf nodes.
-      This information is not used during serving, however it can be used for
-      model interpretation as well as hyper parameter tuning. This can take
-      lots of space, sometimes accounting for half of the model size. Default:
-      True.
-    max_depth: Maximum depth of the tree. `max_depth=1` means that all trees
-      will be roots. `max_depth=-1` means that tree depth is not restricted by
-      this parameter. Values <= -2 will be ignored. Default: 16.
-    max_num_nodes: Maximum number of nodes in the tree. Set to -1 to disable
-      this limit. Only available for `growing_strategy=BEST_FIRST_GLOBAL`.
-      Default: None.
-    maximum_model_size_in_memory_in_bytes: Limit the size of the model when
-      stored in ram. Different algorithms can enforce this limit differently.
-      Note that when models are compiled into an inference, the size of the
-      inference engine is generally much smaller than the original model.
-      Default: -1.0.
-    maximum_training_duration_seconds: Maximum training duration of the model
-      expressed in seconds. Each learning algorithm is free to use this
-      parameter at it sees fit. Enabling maximum training duration makes the
-      model training non-deterministic. Default: -1.0.
-    mhld_oblique_max_num_attributes: For MHLD oblique splits i.e.
-      `split_axis=MHLD_OBLIQUE`. Maximum number of attributes in the
-      projection. Increasing this value increases the training time. Decreasing
-      this value acts as a regularization. The value should be in [2,
-      num_numerical_features]. If the value is above the total number of
-      numerical features, the value is capped automatically. The value 1 is
-      allowed but results in ordinary (non-oblique) splits. Default: None.
-    mhld_oblique_sample_attributes: For MHLD oblique splits i.e.
-      `split_axis=MHLD_OBLIQUE`. If true, applies the attribute sampling
-      controlled by the "num_candidate_attributes" or
-      "num_candidate_attributes_ratio" parameters. If false, all the attributes
-      are tested. Default: None.
-    min_examples: Minimum number of examples in a node. Default: 5.
-    missing_value_policy: Method used to handle missing attribute values.
-      - `GLOBAL_IMPUTATION`: Missing attribute values are imputed, with the
-        mean (in case of numerical attribute) or the most-frequent-item (in
-        case of categorical attribute) computed on the entire dataset (i.e. the
-        information contained in the data spec).
-      - `LOCAL_IMPUTATION`: Missing attribute values are imputed with the mean
-        (numerical attribute) or most-frequent-item (in the case of categorical
-        attribute) evaluated on the training examples in the current node.
-      - `RANDOM_LOCAL_IMPUTATION`: Missing attribute values are imputed from
-        randomly sampled values from the training examples in the current node.
-        This method was proposed by Clinic et al. in "Random Survival Forests"
-        (https://projecteuclid.org/download/pdfview_1/euclid.aoas/1223908043).
-        Default: "GLOBAL_IMPUTATION".
-    num_candidate_attributes: Number of unique valid attributes tested for each
-      node. An attribute is valid if it has at least a valid split. If
-      `num_candidate_attributes=0`, the value is set to the classical default
-      value for Random Forest: `sqrt(number of input attributes)` in case of
-      classification and `number_of_input_attributes / 3` in case of
-      regression. If `num_candidate_attributes=-1`, all the attributes are
-      tested. Default: 0.
-    num_candidate_attributes_ratio: Ratio of attributes tested at each node. If
-      set, it is equivalent to `num_candidate_attributes =
-      number_of_input_features x num_candidate_attributes_ratio`. The possible
-      values are between ]0, and 1] as well as -1. If not set or equal to -1,
-      the `num_candidate_attributes` is used. Default: -1.0.
-    pure_serving_model: Clear the model from any information that is not
-      required for model serving. This includes debugging, model interpretation
-      and other meta-data. The size of the serialized model can be reduced
-      significatively (50% model size reduction is common). This parameter has
-      no impact on the quality, serving speed or RAM usage of model serving.
-      Default: False.
-    random_seed: Random seed for the training of the model. Learners are
-      expected to be deterministic by the random seed. Default: 123456.
-    sorting_strategy: How are sorted the numerical features in order to find
-      the splits
-      - PRESORT: The features are pre-sorted at the start of the training. This
-        solution is faster but consumes much more memory than IN_NODE.
-      - IN_NODE: The features are sorted just before being used in the node.
-        This solution is slow but consumes little amount of memory.
-      . Default: "PRESORT".
-    sparse_oblique_max_num_projections: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Maximum number of projections (applied after
-      the num_projections_exponent).
-      Oblique splits try out max(p^num_projections_exponent,
-      max_num_projections) random projections for choosing a split, where p is
-      the number of numerical features. Increasing "max_num_projections"
-      increases the training time but not the inference time. In late stage
-      model development, if every bit of accuracy if important, increase this
-      value.
-      The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
-      does not define this hyperparameter. Default: None.
-    sparse_oblique_normalization: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Normalization applied on the features,
-      before applying the sparse oblique projections.
-      - `NONE`: No normalization.
-      - `STANDARD_DEVIATION`: Normalize the feature by the estimated standard
-        deviation on the entire train dataset. Also known as Z-Score
-        normalization.
-      - `MIN_MAX`: Normalize the feature by the range (i.e. max-min) estimated
-        on the entire train dataset. Default: None.
-    sparse_oblique_num_projections_exponent: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections
-      to test at each node.
-      Increasing this value very likely improves the quality of the model,
-      drastically increases the training time, and doe not impact the inference
-      time.
-      Oblique splits try out max(p^num_projections_exponent,
-      max_num_projections) random projections for choosing a split, where p is
-      the number of numerical features. Therefore, increasing this
-      `num_projections_exponent` and possibly `max_num_projections` may improve
-      model quality, but will also significantly increase training time.
-      Note that the complexity of (classic) Random Forests is roughly
-      proportional to `num_projections_exponent=0.5`, since it considers
-      sqrt(num_features) for a split. The complexity of (classic) GBDT is
-      roughly proportional to `num_projections_exponent=1`, since it considers
-      all features for a split.
-      The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
-      recommends values in [1/4, 2]. Default: None.
-    sparse_oblique_projection_density_factor: Density of the projections as an
-      exponent of the number of features. Independently for each projection,
-      each feature has a probability "projection_density_factor / num_features"
-      to be considered in the projection.
-      The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
-      calls this parameter `lambda` and recommends values in [1, 5].
-      Increasing this value increases training and inference time (on average).
-      This value is best tuned for each dataset. Default: None.
-    sparse_oblique_weights: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Possible values:
-      - `BINARY`: The oblique weights are sampled in {-1,1} (default).
-      - `CONTINUOUS`: The oblique weights are be sampled in [-1,1]. Default:
-        None.
-    split_axis: What structure of split to consider for numerical features.
-      - `AXIS_ALIGNED`: Axis aligned splits (i.e. one condition at a time).
-        This is the "classical" way to train a tree. Default value.
-      - `SPARSE_OBLIQUE`: Sparse oblique splits (i.e. random splits one a small
-        number of features) from "Sparse Projection Oblique Random Forests",
-        Tomita et al., 2020.
-      - `MHLD_OBLIQUE`: Multi-class Hellinger Linear Discriminant splits from
-        "Classification Based on Multivariate Contrast Patterns",
-        Canete-Sifuentes et al., 2029 Default: "AXIS_ALIGNED".
-    uplift_min_examples_in_treatment: For uplift models only. Minimum number of
-      examples per treatment in a node. Default: 5.
-    uplift_split_score: For uplift models only. Splitter score i.e. score
-      optimized by the splitters. The scores are introduced in "Decision trees
-      for uplift modeling with single and multiple treatments", Rzepakowski et
-      al. Notation: `p` probability / average value of the positive outcome,
-      `q` probability / average value in the control group.
-      - `KULLBACK_LEIBLER` or `KL`: - p log (p/q)
-      - `EUCLIDEAN_DISTANCE` or `ED`: (p-q)^2
-      - `CHI_SQUARED` or `CS`: (p-q)^2/q
-        Default: "KULLBACK_LEIBLER".
-    validation_ratio: Ratio of the training dataset used to create the
-      validation dataset for pruning the tree. If set to 0, the entire dataset
-      is used for training, and the tree is not pruned. Default: 0.1.
-
-    num_threads: Number of threads used to train the model. Different learning
-      algorithms use multi-threading differently and with different degree of
-      efficiency. If `None`, `num_threads` will be automatically set to the
-      number of processors (up to a maximum of 32; or set to 6 if the number of
-      processors is not available). Making `num_threads` significantly larger
-      than the number of processors can slow-down the training speed. The
-      default value logic might change in the future.
-    resume_training: If true, the model training resumes from the checkpoint
-      stored in the `working_dir` directory. If `working_dir` does not
-      contain any model checkpoint, the training starts from the beginning.
-      Resuming training is useful in the following situations: (1) The training
-      was interrupted by the user (e.g. ctrl+c or "stop" button in a notebook)
-      or rescheduled, or (2) the hyper-parameter of the learner was changed e.g.
-      increasing the number of trees.
-    working_dir: Path to a directory available for the learning algorithm to
-      store intermediate computation results. Depending on the learning
-      algorithm and parameters, the working_dir might be optional, required, or
-      ignored. For instance, distributed training algorithm always need a
-      "working_dir", and the gradient boosted tree and hyper-parameter tuners
-      will export artefacts to the "working_dir" if provided.
-    resume_training_snapshot_interval_seconds: Indicative number of seconds in 
-      between snapshots when `resume_training=True`. Might be ignored by
-      some learners.
-    tuner: If set, automatically select the best hyperparameters using the
-      provided tuner. When using distributed training, the tuning is
-      distributed.
-    workers: If set, enable distributed training. "workers" is the list of IP
-      addresses of the workers. A worker is a process running
-      `ydf.start_worker(port)`.
-  """
-
-  def __init__(self,
-      label: str,
-      task: generic_learner.Task = generic_learner.Task.CLASSIFICATION,
-      weights: Optional[str] = None,
-      ranking_group: Optional[str] = None,
-      uplift_treatment: Optional[str] = None,
-      features: dataspec.ColumnDefs = None,
-      include_all_columns: bool = False,
-      max_vocab_count: int = 2000,
-      min_vocab_frequency: int = 5,
-      discretize_numerical_columns: bool = False,
-      num_discretized_numerical_bins: int = 255,
-      max_num_scanned_rows_to_infer_semantic: int = 10000,
-      max_num_scanned_rows_to_compute_statistics: int = 10000,
-      data_spec: Optional[data_spec_pb2.DataSpecification] = None,
-      allow_na_conditions: Optional[bool] = False,
-      categorical_algorithm: Optional[str] = "CART",
-      categorical_set_split_greedy_sampling: Optional[float] = 0.1,
-      categorical_set_split_max_num_items: Optional[int] = -1,
-      categorical_set_split_min_item_frequency: Optional[int] = 1,
-      growing_strategy: Optional[str] = "LOCAL",
-      honest: Optional[bool] = False,
-      honest_fixed_separation: Optional[bool] = False,
-      honest_ratio_leaf_examples: Optional[float] = 0.5,
-      in_split_min_examples_check: Optional[bool] = True,
-      keep_non_leaf_label_distribution: Optional[bool] = True,
-      max_depth: Optional[int] = 16,
-      max_num_nodes: Optional[int] = None,
-      maximum_model_size_in_memory_in_bytes: Optional[float] = -1.0,
-      maximum_training_duration_seconds: Optional[float] = -1.0,
-      mhld_oblique_max_num_attributes: Optional[int] = None,
-      mhld_oblique_sample_attributes: Optional[bool] = None,
-      min_examples: Optional[int] = 5,
-      missing_value_policy: Optional[str] = "GLOBAL_IMPUTATION",
-      num_candidate_attributes: Optional[int] = 0,
-      num_candidate_attributes_ratio: Optional[float] = -1.0,
-      pure_serving_model: Optional[bool] = False,
-      random_seed: Optional[int] = 123456,
-      sorting_strategy: Optional[str] = "PRESORT",
-      sparse_oblique_max_num_projections: Optional[int] = None,
-      sparse_oblique_normalization: Optional[str] = None,
-      sparse_oblique_num_projections_exponent: Optional[float] = None,
-      sparse_oblique_projection_density_factor: Optional[float] = None,
-      sparse_oblique_weights: Optional[str] = None,
-      split_axis: Optional[str] = "AXIS_ALIGNED",
-      uplift_min_examples_in_treatment: Optional[int] = 5,
-      uplift_split_score: Optional[str] = "KULLBACK_LEIBLER",
-      validation_ratio: Optional[float] = 0.1,
-      num_threads: Optional[int] = None,
-      working_dir: Optional[str] = None,
-      resume_training: bool = False,
-      resume_training_snapshot_interval_seconds: int = 1800,
-      tuner: Optional[tuner_lib.AbstractTuner] = None,
-      workers: Optional[Sequence[str]] = None,
-      ):
-
-    hyper_parameters = {
-                      "allow_na_conditions" : allow_na_conditions,
-                      "categorical_algorithm" : categorical_algorithm,
-                      "categorical_set_split_greedy_sampling" : categorical_set_split_greedy_sampling,
-                      "categorical_set_split_max_num_items" : categorical_set_split_max_num_items,
-                      "categorical_set_split_min_item_frequency" : categorical_set_split_min_item_frequency,
-                      "growing_strategy" : growing_strategy,
-                      "honest" : honest,
-                      "honest_fixed_separation" : honest_fixed_separation,
-                      "honest_ratio_leaf_examples" : honest_ratio_leaf_examples,
-                      "in_split_min_examples_check" : in_split_min_examples_check,
-                      "keep_non_leaf_label_distribution" : keep_non_leaf_label_distribution,
-                      "max_depth" : max_depth,
-                      "max_num_nodes" : max_num_nodes,
-                      "maximum_model_size_in_memory_in_bytes" : maximum_model_size_in_memory_in_bytes,
-                      "maximum_training_duration_seconds" : maximum_training_duration_seconds,
-                      "mhld_oblique_max_num_attributes" : mhld_oblique_max_num_attributes,
-                      "mhld_oblique_sample_attributes" : mhld_oblique_sample_attributes,
-                      "min_examples" : min_examples,
-                      "missing_value_policy" : missing_value_policy,
-                      "num_candidate_attributes" : num_candidate_attributes,
-                      "num_candidate_attributes_ratio" : num_candidate_attributes_ratio,
-                      "pure_serving_model" : pure_serving_model,
-                      "random_seed" : random_seed,
-                      "sorting_strategy" : sorting_strategy,
-                      "sparse_oblique_max_num_projections" : sparse_oblique_max_num_projections,
-                      "sparse_oblique_normalization" : sparse_oblique_normalization,
-                      "sparse_oblique_num_projections_exponent" : sparse_oblique_num_projections_exponent,
-                      "sparse_oblique_projection_density_factor" : sparse_oblique_projection_density_factor,
-                      "sparse_oblique_weights" : sparse_oblique_weights,
-                      "split_axis" : split_axis,
-                      "uplift_min_examples_in_treatment" : uplift_min_examples_in_treatment,
-                      "uplift_split_score" : uplift_split_score,
-                      "validation_ratio" : validation_ratio,
-
-      }
-
-    data_spec_args = dataspec.DataSpecInferenceArgs(
-        columns=dataspec.normalize_column_defs(features),
-        include_all_columns=include_all_columns,
-        max_vocab_count=max_vocab_count,
-        min_vocab_frequency=min_vocab_frequency,
-        discretize_numerical_columns=discretize_numerical_columns,
-        num_discretized_numerical_bins=num_discretized_numerical_bins,
-        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
-        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
-    )
-
-    deployment_config = self._build_deployment_config(
-        num_threads=num_threads,
-        resume_training=resume_training,
-        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
-        working_dir=working_dir,
-        workers=workers,
-    )
-
-    super().__init__(learner_name="CART",
-      task=task,
-      label=label,
-      weights=weights,
-      ranking_group=ranking_group,
-      uplift_treatment=uplift_treatment,
-      data_spec_args=data_spec_args,
-      data_spec=data_spec,
-      hyper_parameters=hyper_parameters,
-      deployment_config=deployment_config,
-      tuner=tuner,
-    )
-
-  def train(
-      self,
-      ds: dataset.InputDataset,
-      valid: Optional[dataset.InputDataset] = None,
-  ) -> random_forest_model.RandomForestModel:
-    """Trains a model on the given dataset.
-
-    Options for dataset reading are given on the learner. Consult the
-    documentation of the learner or ydf.create_vertical_dataset() for additional
-    information on dataset reading in YDF.
-
-    Usage example:
-
-    ```
-    import ydf
-    import pandas as pd
-
-    train_ds = pd.read_csv(...)
-
-    learner = ydf.CartLearner(label="label")
-    model = learner.train(train_ds)
-    print(model.summary())
-    ```
-
-    If training is interrupted (for example, by interrupting the cell execution
-    in Colab), the model will be returned to the state it was in at the moment
-    of interruption.
-
-    Args:
-      ds: Training dataset.
-      valid: Optional validation dataset. Some learners, such as Random Forest,
-        do not need validation dataset. Some learners, such as
-        GradientBoostedTrees, automatically extract a validation dataset from
-        the training dataset if the validation dataset is not provided.
-
-    Returns:
-      A trained model.
-    """
-    return super().train(ds, valid)
-
-  @classmethod
-  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
-    return abstract_learner_pb2.LearnerCapabilities(
-      support_max_training_duration=True,
-      resume_training=False,
-      support_validation_dataset=False,
-      support_partial_cache_dataset_format=False,
-      support_max_model_size_in_memory=False,
-      support_monotonic_constraints=False,
-    )
-
-  @classmethod
-  def hyperparameter_templates(cls) -> Dict[str, hyperparameters.HyperparameterTemplate]:
-    r"""Hyperparameter templates for this Learner.
-    
-    This learner currently does not provide any hyperparameter templates, this
-    method is provided for consistency with other learners.
-    
-    Returns:
-      Empty dictionary.
-    """
-    return {}
-
-class DistributedGradientBoostedTreesLearner(generic_learner.GenericLearner):
-  r"""Distributed Gradient Boosted Trees learning algorithm.
-
-  Exact distributed version of the Gradient Boosted Tree learning algorithm. See
-  the documentation of the non-distributed Gradient Boosted Tree learning
-  algorithm for an introduction to GBTs.
-
-  Usage example:
-
-  ```python
-  import ydf
-  import pandas as pd
-
-  dataset = pd.read_csv("project/dataset.csv")
-
-  model = ydf.DistributedGradientBoostedTreesLearner().train(dataset)
-
-  print(model.summary())
-  ```
-
-  Hyperparameters are configured to give reasonable results for typical
-  datasets. Hyperparameters can also be modified manually (see descriptions)
-  below or by applying the hyperparameter templates available with
-  `DistributedGradientBoostedTreesLearner.hyperparameter_templates()` (see this function's documentation for
-  details).
-
-  Attributes:
-    label: Label of the dataset. The label column
-      should not be identified as a feature in the `features` parameter.
-    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,
-      Task.RANKING, Task.CATEGORICAL_UPLIFT, Task.NUMERICAL_UPLIFT).
-    weights: Name of a feature that identifies the weight of each example. If
-      weights are not specified, unit weights are assumed. The weight column
-      should not be identified as a feature in the `features` parameter.
-    ranking_group: Only for `task=Task.RANKING`. Name of a feature
-      that identifies queries in a query/document ranking task. The ranking
-      group should not be identified as a feature in the `features` parameter.
-    uplift_treatment: Only for `task=Task.CATEGORICAL_UPLIFT` and `task=Task`.
-      NUMERICAL_UPLIFT. Name of a numerical feature that identifies the
-      treatment in an uplift problem. The value 0 is reserved for the control
-      treatment. Currently, only 0/1 binary treatments are supported.
-    features: If None, all columns are used as features. The semantic of the
-      features is determined automatically. Otherwise, if
-      include_all_columns=False (default) only the column listed in `features`
-      are imported. If include_all_columns=True, all the columns are imported as
-      features and only the semantic of the columns NOT in `columns` is 
-      determined automatically. If specified,  defines the order of the features
-      - any non-listed features are appended in-order after the specified
-      features (if include_all_columns=True).
-      The label, weights, uplift treatment and ranking_group columns should not
-      be specified as features.
-    include_all_columns: See `features`.
-    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
-      CATEGORICAL_SET columns stored as strings. If more unique values exist,
-      only the most frequent values are kept, and the remaining values are
-      considered as out-of-vocabulary.
-    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
-      and CATEGORICAL_SET columns. Value observed less than
-      `min_vocab_frequency` are considered as out-of-vocabulary.
-    discretize_numerical_columns: If true, discretize all the numerical columns
-      before training. Discretized numerical columns are faster to train with,
-      but they can have a negative impact on the model quality. Using
-      `discretize_numerical_columns=True` is equivalent as setting the column
-      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
-      definition of DISCRETIZED_NUMERICAL for more details.
-    num_discretized_numerical_bins: Number of bins used when disretizing
-      numerical columns.
-    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
-      inferring the column's semantic if it is not explicitly specified. Only
-      used when reading from file, in-memory datasets are always read in full.
-      Setting this to a lower number will speed up dataset reading, but might
-      result in incorrect column semantics. Set to -1 to scan the entire
-      dataset.
-    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
-      computing a column's statistics. Only used when reading from file,
-      in-memory datasets are always read in full. A column's statistics include
-      the dictionary for categorical features and the mean / min / max for
-      numerical features. Setting this to a lower number will speed up dataset
-      reading, but skew statistics in the dataspec, which can hurt model quality
-      (e.g. if an important category of a categorical feature is considered
-      OOV). Set to -1 to scan the entire dataset.
-    data_spec: Dataspec to be used (advanced). If a data spec is given,
-      `columns`, `include_all_columns`, `max_vocab_count`,
-      `min_vocab_frequency`, `discretize_numerical_columns` and 
-      `num_discretized_numerical_bins` will be ignored.
-    apply_link_function: If true, applies the link function (a.k.a. activation
-      function), if any, before returning the model prediction. If false,
-      returns the pre-link function model output.
-      For example, in the case of binary classification, the pre-link function
-      output is a logic while the post-link function is a probability. Default:
-      True.
-    force_numerical_discretization: If false, only the numerical column
-      safisfying "max_unique_values_for_discretized_numerical" will be
-      discretized. If true, all the numerical columns will be discretized.
-      Columns with more than "max_unique_values_for_discretized_numerical"
-      unique values will be approximated with
-      "max_unique_values_for_discretized_numerical" bins. This parameter will
-      impact the model training. Default: False.
-    max_depth: Maximum depth of the tree. `max_depth=1` means that all trees
-      will be roots. `max_depth=-1` means that tree depth is not restricted by
-      this parameter. Values <= -2 will be ignored. Default: 6.
-    max_unique_values_for_discretized_numerical: Maximum number of unique value
-      of a numerical feature to allow its pre-discretization. In case of large
-      datasets, discretized numerical features with a small number of unique
-      values are more efficient to learn than classical / non-discretized
-      numerical features. This parameter does not impact the final model.
-      However, it can speed-up or slown the training. Default: 16000.
-    maximum_model_size_in_memory_in_bytes: Limit the size of the model when
-      stored in ram. Different algorithms can enforce this limit differently.
-      Note that when models are compiled into an inference, the size of the
-      inference engine is generally much smaller than the original model.
-      Default: -1.0.
-    maximum_training_duration_seconds: Maximum training duration of the model
-      expressed in seconds. Each learning algorithm is free to use this
-      parameter at it sees fit. Enabling maximum training duration makes the
-      model training non-deterministic. Default: -1.0.
-    min_examples: Minimum number of examples in a node. Default: 5.
-    num_candidate_attributes: Number of unique valid attributes tested for each
-      node. An attribute is valid if it has at least a valid split. If
-      `num_candidate_attributes=0`, the value is set to the classical default
-      value for Random Forest: `sqrt(number of input attributes)` in case of
-      classification and `number_of_input_attributes / 3` in case of
-      regression. If `num_candidate_attributes=-1`, all the attributes are
-      tested. Default: -1.
-    num_candidate_attributes_ratio: Ratio of attributes tested at each node. If
-      set, it is equivalent to `num_candidate_attributes =
-      number_of_input_features x num_candidate_attributes_ratio`. The possible
-      values are between ]0, and 1] as well as -1. If not set or equal to -1,
-      the `num_candidate_attributes` is used. Default: -1.0.
-    num_trees: Maximum number of decision trees. The effective number of
-      trained tree can be smaller if early stopping is enabled. Default: 300.
-    pure_serving_model: Clear the model from any information that is not
-      required for model serving. This includes debugging, model interpretation
-      and other meta-data. The size of the serialized model can be reduced
-      significatively (50% model size reduction is common). This parameter has
-      no impact on the quality, serving speed or RAM usage of model serving.
-      Default: False.
-    random_seed: Random seed for the training of the model. Learners are
-      expected to be deterministic by the random seed. Default: 123456.
-    shrinkage: Coefficient applied to each tree prediction. A small value
-      (0.02) tends to give more accurate results (assuming enough trees are
-      trained), but results in larger models. Analogous to neural network
-      learning rate. Default: 0.1.
-    use_hessian_gain: Use true, uses a formulation of split gain with a hessian
-      term i.e. optimizes the splits to minimize the variance of "gradient /
-      hessian. Available for all losses except regression. Default: False.
-    worker_logs: If true, workers will print training logs. Default: True.
-
-    num_threads: Number of threads used to train the model. Different learning
-      algorithms use multi-threading differently and with different degree of
-      efficiency. If `None`, `num_threads` will be automatically set to the
-      number of processors (up to a maximum of 32; or set to 6 if the number of
-      processors is not available). Making `num_threads` significantly larger
-      than the number of processors can slow-down the training speed. The
-      default value logic might change in the future.
-    resume_training: If true, the model training resumes from the checkpoint
-      stored in the `working_dir` directory. If `working_dir` does not
-      contain any model checkpoint, the training starts from the beginning.
-      Resuming training is useful in the following situations: (1) The training
-      was interrupted by the user (e.g. ctrl+c or "stop" button in a notebook)
-      or rescheduled, or (2) the hyper-parameter of the learner was changed e.g.
-      increasing the number of trees.
-    working_dir: Path to a directory available for the learning algorithm to
-      store intermediate computation results. Depending on the learning
-      algorithm and parameters, the working_dir might be optional, required, or
-      ignored. For instance, distributed training algorithm always need a
-      "working_dir", and the gradient boosted tree and hyper-parameter tuners
-      will export artefacts to the "working_dir" if provided.
-    resume_training_snapshot_interval_seconds: Indicative number of seconds in 
-      between snapshots when `resume_training=True`. Might be ignored by
-      some learners.
-    tuner: If set, automatically select the best hyperparameters using the
-      provided tuner. When using distributed training, the tuning is
-      distributed.
-    workers: If set, enable distributed training. "workers" is the list of IP
-      addresses of the workers. A worker is a process running
-      `ydf.start_worker(port)`.
-  """
-
-  def __init__(self,
-      label: str,
-      task: generic_learner.Task = generic_learner.Task.CLASSIFICATION,
-      weights: Optional[str] = None,
-      ranking_group: Optional[str] = None,
-      uplift_treatment: Optional[str] = None,
-      features: dataspec.ColumnDefs = None,
-      include_all_columns: bool = False,
-      max_vocab_count: int = 2000,
-      min_vocab_frequency: int = 5,
-      discretize_numerical_columns: bool = False,
-      num_discretized_numerical_bins: int = 255,
-      max_num_scanned_rows_to_infer_semantic: int = 10000,
-      max_num_scanned_rows_to_compute_statistics: int = 10000,
-      data_spec: Optional[data_spec_pb2.DataSpecification] = None,
-      apply_link_function: Optional[bool] = True,
-      force_numerical_discretization: Optional[bool] = False,
-      max_depth: Optional[int] = 6,
-      max_unique_values_for_discretized_numerical: Optional[int] = 16000,
-      maximum_model_size_in_memory_in_bytes: Optional[float] = -1.0,
-      maximum_training_duration_seconds: Optional[float] = -1.0,
-      min_examples: Optional[int] = 5,
-      num_candidate_attributes: Optional[int] = -1,
-      num_candidate_attributes_ratio: Optional[float] = -1.0,
-      num_trees: Optional[int] = 300,
-      pure_serving_model: Optional[bool] = False,
-      random_seed: Optional[int] = 123456,
-      shrinkage: Optional[float] = 0.1,
-      use_hessian_gain: Optional[bool] = False,
-      worker_logs: Optional[bool] = True,
-      num_threads: Optional[int] = None,
-      working_dir: Optional[str] = None,
-      resume_training: bool = False,
-      resume_training_snapshot_interval_seconds: int = 1800,
-      tuner: Optional[tuner_lib.AbstractTuner] = None,
-      workers: Optional[Sequence[str]] = None,
-      ):
-
-    hyper_parameters = {
-                      "apply_link_function" : apply_link_function,
-                      "force_numerical_discretization" : force_numerical_discretization,
-                      "max_depth" : max_depth,
-                      "max_unique_values_for_discretized_numerical" : max_unique_values_for_discretized_numerical,
-                      "maximum_model_size_in_memory_in_bytes" : maximum_model_size_in_memory_in_bytes,
-                      "maximum_training_duration_seconds" : maximum_training_duration_seconds,
-                      "min_examples" : min_examples,
-                      "num_candidate_attributes" : num_candidate_attributes,
-                      "num_candidate_attributes_ratio" : num_candidate_attributes_ratio,
-                      "num_trees" : num_trees,
-                      "pure_serving_model" : pure_serving_model,
-                      "random_seed" : random_seed,
-                      "shrinkage" : shrinkage,
-                      "use_hessian_gain" : use_hessian_gain,
-                      "worker_logs" : worker_logs,
-
-      }
-
-    data_spec_args = dataspec.DataSpecInferenceArgs(
-        columns=dataspec.normalize_column_defs(features),
-        include_all_columns=include_all_columns,
-        max_vocab_count=max_vocab_count,
-        min_vocab_frequency=min_vocab_frequency,
-        discretize_numerical_columns=discretize_numerical_columns,
-        num_discretized_numerical_bins=num_discretized_numerical_bins,
-        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
-        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
-    )
-
-    deployment_config = self._build_deployment_config(
-        num_threads=num_threads,
-        resume_training=resume_training,
-        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
-        working_dir=working_dir,
-        workers=workers,
-    )
-
-    super().__init__(learner_name="DISTRIBUTED_GRADIENT_BOOSTED_TREES",
-      task=task,
-      label=label,
-      weights=weights,
-      ranking_group=ranking_group,
-      uplift_treatment=uplift_treatment,
-      data_spec_args=data_spec_args,
-      data_spec=data_spec,
-      hyper_parameters=hyper_parameters,
-      deployment_config=deployment_config,
-      tuner=tuner,
-    )
-
-  def train(
-      self,
-      ds: dataset.InputDataset,
-      valid: Optional[dataset.InputDataset] = None,
-  ) -> gradient_boosted_trees_model.GradientBoostedTreesModel:
-    """Trains a model on the given dataset.
-
-    Options for dataset reading are given on the learner. Consult the
-    documentation of the learner or ydf.create_vertical_dataset() for additional
-    information on dataset reading in YDF.
-
-    Usage example:
-
-    ```
-    import ydf
-    import pandas as pd
-
-    train_ds = pd.read_csv(...)
-
-    learner = ydf.DistributedGradientBoostedTreesLearner(label="label")
-    model = learner.train(train_ds)
-    print(model.summary())
-    ```
-
-    If training is interrupted (for example, by interrupting the cell execution
-    in Colab), the model will be returned to the state it was in at the moment
-    of interruption.
-
-    Args:
-      ds: Training dataset.
-      valid: Optional validation dataset. Some learners, such as Random Forest,
-        do not need validation dataset. Some learners, such as
-        GradientBoostedTrees, automatically extract a validation dataset from
-        the training dataset if the validation dataset is not provided.
-
-    Returns:
-      A trained model.
-    """
-    return super().train(ds, valid)
-
-  @classmethod
-  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
-    return abstract_learner_pb2.LearnerCapabilities(
-      support_max_training_duration=False,
-      resume_training=True,
-      support_validation_dataset=False,
-      support_partial_cache_dataset_format=True,
-      support_max_model_size_in_memory=False,
-      support_monotonic_constraints=False,
-    )
-
-  @classmethod
-  def hyperparameter_templates(cls) -> Dict[str, hyperparameters.HyperparameterTemplate]:
-    r"""Hyperparameter templates for this Learner.
-    
-    This learner currently does not provide any hyperparameter templates, this
-    method is provided for consistency with other learners.
-    
-    Returns:
-      Empty dictionary.
-    """
-    return {}
-
-class GradientBoostedTreesLearner(generic_learner.GenericLearner):
-  r"""Gradient Boosted Trees learning algorithm.
-
-  A [Gradient Boosted Trees](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)
-  (GBT), also known as Gradient Boosted Decision Trees (GBDT) or Gradient
-  Boosted Machines (GBM),  is a set of shallow decision trees trained
-  sequentially. Each tree is trained to predict and then "correct" for the
-  errors of the previously trained trees (more precisely each tree predict the
-  gradient of the loss relative to the model output).
-
-  Usage example:
-
-  ```python
-  import ydf
-  import pandas as pd
-
-  dataset = pd.read_csv("project/dataset.csv")
-
-  model = ydf.GradientBoostedTreesLearner().train(dataset)
-
-  print(model.summary())
-  ```
-
-  Hyperparameters are configured to give reasonable results for typical
-  datasets. Hyperparameters can also be modified manually (see descriptions)
-  below or by applying the hyperparameter templates available with
-  `GradientBoostedTreesLearner.hyperparameter_templates()` (see this function's documentation for
-  details).
-
-  Attributes:
-    label: Label of the dataset. The label column
-      should not be identified as a feature in the `features` parameter.
-    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,
-      Task.RANKING, Task.CATEGORICAL_UPLIFT, Task.NUMERICAL_UPLIFT).
-    weights: Name of a feature that identifies the weight of each example. If
-      weights are not specified, unit weights are assumed. The weight column
-      should not be identified as a feature in the `features` parameter.
-    ranking_group: Only for `task=Task.RANKING`. Name of a feature
-      that identifies queries in a query/document ranking task. The ranking
-      group should not be identified as a feature in the `features` parameter.
-    uplift_treatment: Only for `task=Task.CATEGORICAL_UPLIFT` and `task=Task`.
-      NUMERICAL_UPLIFT. Name of a numerical feature that identifies the
-      treatment in an uplift problem. The value 0 is reserved for the control
-      treatment. Currently, only 0/1 binary treatments are supported.
-    features: If None, all columns are used as features. The semantic of the
-      features is determined automatically. Otherwise, if
-      include_all_columns=False (default) only the column listed in `features`
-      are imported. If include_all_columns=True, all the columns are imported as
-      features and only the semantic of the columns NOT in `columns` is 
-      determined automatically. If specified,  defines the order of the features
-      - any non-listed features are appended in-order after the specified
-      features (if include_all_columns=True).
-      The label, weights, uplift treatment and ranking_group columns should not
-      be specified as features.
-    include_all_columns: See `features`.
-    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
-      CATEGORICAL_SET columns stored as strings. If more unique values exist,
-      only the most frequent values are kept, and the remaining values are
-      considered as out-of-vocabulary.
-    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
-      and CATEGORICAL_SET columns. Value observed less than
-      `min_vocab_frequency` are considered as out-of-vocabulary.
-    discretize_numerical_columns: If true, discretize all the numerical columns
-      before training. Discretized numerical columns are faster to train with,
-      but they can have a negative impact on the model quality. Using
-      `discretize_numerical_columns=True` is equivalent as setting the column
-      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
-      definition of DISCRETIZED_NUMERICAL for more details.
-    num_discretized_numerical_bins: Number of bins used when disretizing
-      numerical columns.
-    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
-      inferring the column's semantic if it is not explicitly specified. Only
-      used when reading from file, in-memory datasets are always read in full.
-      Setting this to a lower number will speed up dataset reading, but might
-      result in incorrect column semantics. Set to -1 to scan the entire
-      dataset.
-    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
-      computing a column's statistics. Only used when reading from file,
-      in-memory datasets are always read in full. A column's statistics include
-      the dictionary for categorical features and the mean / min / max for
-      numerical features. Setting this to a lower number will speed up dataset
-      reading, but skew statistics in the dataspec, which can hurt model quality
-      (e.g. if an important category of a categorical feature is considered
-      OOV). Set to -1 to scan the entire dataset.
-    data_spec: Dataspec to be used (advanced). If a data spec is given,
-      `columns`, `include_all_columns`, `max_vocab_count`,
-      `min_vocab_frequency`, `discretize_numerical_columns` and 
-      `num_discretized_numerical_bins` will be ignored.
-    adapt_subsample_for_maximum_training_duration: Control how the maximum
-      training duration (if set) is applied. If false, the training stop when
-      the time is used. If true, the size of the sampled datasets used train
-      individual trees are adapted dynamically so that all the trees are
-      trained in time. Default: False.
-    allow_na_conditions: If true, the tree training evaluates conditions of the
-      type `X is NA` i.e. `X is missing`. Default: False.
-    apply_link_function: If true, applies the link function (a.k.a. activation
-      function), if any, before returning the model prediction. If false,
-      returns the pre-link function model output.
-      For example, in the case of binary classification, the pre-link function
-      output is a logic while the post-link function is a probability. Default:
-      True.
-    categorical_algorithm: How to learn splits on categorical attributes.
-      - `CART`: CART algorithm. Find categorical splits of the form "value \\in
-        mask". The solution is exact for binary classification, regression and
-        ranking. It is approximated for multi-class classification. This is a
-        good first algorithm to use. In case of overfitting (very small
-        dataset, large dictionary), the "random" algorithm is a good
-        alternative.
-      - `ONE_HOT`: One-hot encoding. Find the optimal categorical split of the
-        form "attribute == param". This method is similar (but more efficient)
-        than converting converting each possible categorical value into a
-        boolean feature. This method is available for comparison purpose and
-        generally performs worse than other alternatives.
-      - `RANDOM`: Best splits among a set of random candidate. Find the a
-        categorical split of the form "value \\in mask" using a random search.
-        This solution can be seen as an approximation of the CART algorithm.
-        This method is a strong alternative to CART. This algorithm is inspired
-        from section "5.1 Categorical Variables" of "Random Forest", 2001.
-        Default: "CART".
-    categorical_set_split_greedy_sampling: For categorical set splits e.g.
-      texts. Probability for a categorical value to be a candidate for the
-      positive set. The sampling is applied once per node (i.e. not at every
-      step of the greedy optimization). Default: 0.1.
-    categorical_set_split_max_num_items: For categorical set splits e.g. texts.
-      Maximum number of items (prior to the sampling). If more items are
-      available, the least frequent items are ignored. Changing this value is
-      similar to change the "max_vocab_count" before loading the dataset, with
-      the following exception: With `max_vocab_count`, all the remaining items
-      are grouped in a special Out-of-vocabulary item. With `max_num_items`,
-      this is not the case. Default: -1.
-    categorical_set_split_min_item_frequency: For categorical set splits e.g.
-      texts. Minimum number of occurrences of an item to be considered.
-      Default: 1.
-    compute_permutation_variable_importance: If true, compute the permutation
-      variable importance of the model at the end of the training using the
-      validation dataset. Enabling this feature can increase the training time
-      significantly. Default: False.
-    dart_dropout: Dropout rate applied when using the DART i.e. when
-      forest_extraction=DART. Default: 0.01.
-    early_stopping: Early stopping detects the overfitting of the model and
-      halts it training using the validation dataset. If not provided directly,
-      the validation dataset is extracted from the training dataset (see
-      "validation_ratio" parameter):
-      - `NONE`: No early stopping. All the num_trees are trained and kept.
-      - `MIN_LOSS_FINAL`: All the num_trees are trained. The model is then
-        truncated to minimize the validation loss i.e. some of the trees are
-        discarded as to minimum the validation loss.
-      - `LOSS_INCREASE`: Classical early stopping. Stop the training when the
-        validation does not decrease for `early_stopping_num_trees_look_ahead`
-        trees. Default: "LOSS_INCREASE".
-    early_stopping_initial_iteration: 0-based index of the first iteration
-      considered for early stopping computation. Increasing this value prevents
-      too early stopping due to noisy initial iterations of the learner.
-      Default: 10.
-    early_stopping_num_trees_look_ahead: Rolling number of trees used to detect
-      validation loss increase and trigger early stopping. Default: 30.
-    focal_loss_alpha: EXPERIMENTAL. Weighting parameter for focal loss,
-      positive samples weighted by alpha, negative samples by (1-alpha). The
-      default 0.5 value means no active class-level weighting. Only used with
-      focal loss i.e. `loss="BINARY_FOCAL_LOSS"` Default: 0.5.
-    focal_loss_gamma: EXPERIMENTAL. Exponent of the misprediction exponent term
-      in focal loss, corresponds to gamma parameter in
-      https://arxiv.org/pdf/1708.02002.pdf. Only used with focal loss i.e.
-      `loss="BINARY_FOCAL_LOSS"` Default: 2.0.
-    forest_extraction: How to construct the forest:
-      - MART: For Multiple Additive Regression Trees. The "classical" way to
-        build a GBDT i.e. each tree tries to "correct" the mistakes of the
-        previous trees.
-      - DART: For Dropout Additive Regression Trees. A modification of MART
-        proposed in http://proceedings.mlr.press/v38/korlakaivinayak15.pdf.
-        Here, each tree tries to "correct" the mistakes of a random subset of
-        the previous trees. Default: "MART".
-    goss_alpha: Alpha parameter for the GOSS (Gradient-based One-Side Sampling;
-      "See LightGBM: A Highly Efficient Gradient Boosting Decision Tree")
-      sampling method. Default: 0.2.
-    goss_beta: Beta parameter for the GOSS (Gradient-based One-Side Sampling)
-      sampling method. Default: 0.1.
-    growing_strategy: How to grow the tree.
-      - `LOCAL`: Each node is split independently of the other nodes. In other
-        words, as long as a node satisfy the splits "constraints (e.g. maximum
-        depth, minimum number of observations), the node will be split. This is
-        the "classical" way to grow decision trees.
-      - `BEST_FIRST_GLOBAL`: The node with the best loss reduction among all
-        the nodes of the tree is selected for splitting. This method is also
-        called "best first" or "leaf-wise growth". See "Best-first decision
-        tree learning", Shi and "Additive logistic regression : A statistical
-        view of boosting", Friedman for more details. Default: "LOCAL".
-    honest: In honest trees, different training examples are used to infer the
-      structure and the leaf values. This regularization technique trades
-      examples for bias estimates. It might increase or reduce the quality of
-      the model. See "Generalized Random Forests", Athey et al. In this paper,
-      Honest trees are trained with the Random Forest algorithm with a sampling
-      without replacement. Default: False.
-    honest_fixed_separation: For honest trees only i.e. honest=true. If true, a
-      new random separation is generated for each tree. If false, the same
-      separation is used for all the trees (e.g., in Gradient Boosted Trees
-      containing multiple trees). Default: False.
-    honest_ratio_leaf_examples: For honest trees only i.e. honest=true. Ratio
-      of examples used to set the leaf values. Default: 0.5.
-    in_split_min_examples_check: Whether to check the `min_examples` constraint
-      in the split search (i.e. splits leading to one child having less than
-      `min_examples` examples are considered invalid) or before the split
-      search (i.e. a node can be derived only if it contains more than
-      `min_examples` examples). If false, there can be nodes with less than
-      `min_examples` training examples. Default: True.
-    keep_non_leaf_label_distribution: Whether to keep the node value (i.e. the
-      distribution of the labels of the training examples) of non-leaf nodes.
-      This information is not used during serving, however it can be used for
-      model interpretation as well as hyper parameter tuning. This can take
-      lots of space, sometimes accounting for half of the model size. Default:
-      True.
-    l1_regularization: L1 regularization applied to the training loss. Impact
-      the tree structures and lead values. Default: 0.0.
-    l2_categorical_regularization: L2 regularization applied to the training
-      loss for categorical features. Impact the tree structures and lead
-      values. Default: 1.0.
-    l2_regularization: L2 regularization applied to the training loss for all
-      features except the categorical ones. Default: 0.0.
-    lambda_loss: Lambda regularization applied to certain training loss
-      functions. Only for NDCG loss. Default: 1.0.
-    loss: The loss optimized by the model. If not specified (DEFAULT) the loss
-      is selected automatically according to the \\"task\\" and label
-      statistics. For example, if task=CLASSIFICATION and the label has two
-      possible values, the loss will be set to BINOMIAL_LOG_LIKELIHOOD.
-      Possible values are:
-      - `DEFAULT`: Select the loss automatically according to the task and
-        label statistics.
-      - `BINOMIAL_LOG_LIKELIHOOD`: Binomial log likelihood. Only valid for
-        binary classification.
-      - `SQUARED_ERROR`: Least square loss. Only valid for regression.
-      - `POISSON`: Poisson log likelihood loss. Mainly used for counting
-        problems. Only valid for regression.
-      - `MULTINOMIAL_LOG_LIKELIHOOD`: Multinomial log likelihood i.e.
-        cross-entropy. Only valid for binary or multi-class classification.
-      - `LAMBDA_MART_NDCG5`: LambdaMART with NDCG5.
-      - `XE_NDCG_MART`:  Cross Entropy Loss NDCG. See arxiv.org/abs/1911.09798.
-      - `BINARY_FOCAL_LOSS`: Focal loss. Only valid for binary classification.
-        See https://arxiv.org/pdf/1708.02002.pdf.
-      - `POISSON`: Poisson log likelihood. Only valid for regression.
-      - `MEAN_AVERAGE_ERROR`: Mean average error a.k.a. MAE. For custom losses, pass the loss object here. Note that when using custom losses, the link function is deactivated (aka apply_link_function is always False).
-        Default: "DEFAULT".
-    max_depth: Maximum depth of the tree. `max_depth=1` means that all trees
-      will be roots. `max_depth=-1` means that tree depth is not restricted by
-      this parameter. Values <= -2 will be ignored. Default: 6.
-    max_num_nodes: Maximum number of nodes in the tree. Set to -1 to disable
-      this limit. Only available for `growing_strategy=BEST_FIRST_GLOBAL`.
-      Default: None.
-    maximum_model_size_in_memory_in_bytes: Limit the size of the model when
-      stored in ram. Different algorithms can enforce this limit differently.
-      Note that when models are compiled into an inference, the size of the
-      inference engine is generally much smaller than the original model.
-      Default: -1.0.
-    maximum_training_duration_seconds: Maximum training duration of the model
-      expressed in seconds. Each learning algorithm is free to use this
-      parameter at it sees fit. Enabling maximum training duration makes the
-      model training non-deterministic. Default: -1.0.
-    mhld_oblique_max_num_attributes: For MHLD oblique splits i.e.
-      `split_axis=MHLD_OBLIQUE`. Maximum number of attributes in the
-      projection. Increasing this value increases the training time. Decreasing
-      this value acts as a regularization. The value should be in [2,
-      num_numerical_features]. If the value is above the total number of
-      numerical features, the value is capped automatically. The value 1 is
-      allowed but results in ordinary (non-oblique) splits. Default: None.
-    mhld_oblique_sample_attributes: For MHLD oblique splits i.e.
-      `split_axis=MHLD_OBLIQUE`. If true, applies the attribute sampling
-      controlled by the "num_candidate_attributes" or
-      "num_candidate_attributes_ratio" parameters. If false, all the attributes
-      are tested. Default: None.
-    min_examples: Minimum number of examples in a node. Default: 5.
-    missing_value_policy: Method used to handle missing attribute values.
-      - `GLOBAL_IMPUTATION`: Missing attribute values are imputed, with the
-        mean (in case of numerical attribute) or the most-frequent-item (in
-        case of categorical attribute) computed on the entire dataset (i.e. the
-        information contained in the data spec).
-      - `LOCAL_IMPUTATION`: Missing attribute values are imputed with the mean
-        (numerical attribute) or most-frequent-item (in the case of categorical
-        attribute) evaluated on the training examples in the current node.
-      - `RANDOM_LOCAL_IMPUTATION`: Missing attribute values are imputed from
-        randomly sampled values from the training examples in the current node.
-        This method was proposed by Clinic et al. in "Random Survival Forests"
-        (https://projecteuclid.org/download/pdfview_1/euclid.aoas/1223908043).
-        Default: "GLOBAL_IMPUTATION".
-    num_candidate_attributes: Number of unique valid attributes tested for each
-      node. An attribute is valid if it has at least a valid split. If
-      `num_candidate_attributes=0`, the value is set to the classical default
-      value for Random Forest: `sqrt(number of input attributes)` in case of
-      classification and `number_of_input_attributes / 3` in case of
-      regression. If `num_candidate_attributes=-1`, all the attributes are
-      tested. Default: -1.
-    num_candidate_attributes_ratio: Ratio of attributes tested at each node. If
-      set, it is equivalent to `num_candidate_attributes =
-      number_of_input_features x num_candidate_attributes_ratio`. The possible
-      values are between ]0, and 1] as well as -1. If not set or equal to -1,
-      the `num_candidate_attributes` is used. Default: -1.0.
-    num_trees: Maximum number of decision trees. The effective number of
-      trained tree can be smaller if early stopping is enabled. Default: 300.
-    pure_serving_model: Clear the model from any information that is not
-      required for model serving. This includes debugging, model interpretation
-      and other meta-data. The size of the serialized model can be reduced
-      significatively (50% model size reduction is common). This parameter has
-      no impact on the quality, serving speed or RAM usage of model serving.
-      Default: False.
-    random_seed: Random seed for the training of the model. Learners are
-      expected to be deterministic by the random seed. Default: 123456.
-    sampling_method: Control the sampling of the datasets used to train
-      individual trees.
-      - NONE: No sampling is applied. This is equivalent to RANDOM sampling
-        with \\"subsample=1\\".
-      - RANDOM (default): Uniform random sampling. Automatically selected if
-        "subsample" is set.
-      - GOSS: Gradient-based One-Side Sampling. Automatically selected if
-        "goss_alpha" or "goss_beta" is set.
-      - SELGB: Selective Gradient Boosting. Automatically selected if
-        "selective_gradient_boosting_ratio" is set. Only valid for ranking.
-        Default: "RANDOM".
-    selective_gradient_boosting_ratio: Ratio of the dataset used to train
-      individual tree for the selective Gradient Boosting (Selective Gradient
-      Boosting for Effective Learning to Rank; Lucchese et al;
-      http://quickrank.isti.cnr.it/selective-data/selective-SIGIR2018.pdf)
-      sampling method. Default: 0.01.
-    shrinkage: Coefficient applied to each tree prediction. A small value
-      (0.02) tends to give more accurate results (assuming enough trees are
-      trained), but results in larger models. Analogous to neural network
-      learning rate. Default: 0.1.
-    sorting_strategy: How are sorted the numerical features in order to find
-      the splits
-      - PRESORT: The features are pre-sorted at the start of the training. This
-        solution is faster but consumes much more memory than IN_NODE.
-      - IN_NODE: The features are sorted just before being used in the node.
-        This solution is slow but consumes little amount of memory.
-      . Default: "PRESORT".
-    sparse_oblique_max_num_projections: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Maximum number of projections (applied after
-      the num_projections_exponent).
-      Oblique splits try out max(p^num_projections_exponent,
-      max_num_projections) random projections for choosing a split, where p is
-      the number of numerical features. Increasing "max_num_projections"
-      increases the training time but not the inference time. In late stage
-      model development, if every bit of accuracy if important, increase this
-      value.
-      The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
-      does not define this hyperparameter. Default: None.
-    sparse_oblique_normalization: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Normalization applied on the features,
-      before applying the sparse oblique projections.
-      - `NONE`: No normalization.
-      - `STANDARD_DEVIATION`: Normalize the feature by the estimated standard
-        deviation on the entire train dataset. Also known as Z-Score
-        normalization.
-      - `MIN_MAX`: Normalize the feature by the range (i.e. max-min) estimated
-        on the entire train dataset. Default: None.
-    sparse_oblique_num_projections_exponent: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections
-      to test at each node.
-      Increasing this value very likely improves the quality of the model,
-      drastically increases the training time, and doe not impact the inference
-      time.
-      Oblique splits try out max(p^num_projections_exponent,
-      max_num_projections) random projections for choosing a split, where p is
-      the number of numerical features. Therefore, increasing this
-      `num_projections_exponent` and possibly `max_num_projections` may improve
-      model quality, but will also significantly increase training time.
-      Note that the complexity of (classic) Random Forests is roughly
-      proportional to `num_projections_exponent=0.5`, since it considers
-      sqrt(num_features) for a split. The complexity of (classic) GBDT is
-      roughly proportional to `num_projections_exponent=1`, since it considers
-      all features for a split.
-      The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
-      recommends values in [1/4, 2]. Default: None.
-    sparse_oblique_projection_density_factor: Density of the projections as an
-      exponent of the number of features. Independently for each projection,
-      each feature has a probability "projection_density_factor / num_features"
-      to be considered in the projection.
-      The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
-      calls this parameter `lambda` and recommends values in [1, 5].
-      Increasing this value increases training and inference time (on average).
-      This value is best tuned for each dataset. Default: None.
-    sparse_oblique_weights: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Possible values:
-      - `BINARY`: The oblique weights are sampled in {-1,1} (default).
-      - `CONTINUOUS`: The oblique weights are be sampled in [-1,1]. Default:
-        None.
-    split_axis: What structure of split to consider for numerical features.
-      - `AXIS_ALIGNED`: Axis aligned splits (i.e. one condition at a time).
-        This is the "classical" way to train a tree. Default value.
-      - `SPARSE_OBLIQUE`: Sparse oblique splits (i.e. random splits one a small
-        number of features) from "Sparse Projection Oblique Random Forests",
-        Tomita et al., 2020.
-      - `MHLD_OBLIQUE`: Multi-class Hellinger Linear Discriminant splits from
-        "Classification Based on Multivariate Contrast Patterns",
-        Canete-Sifuentes et al., 2029 Default: "AXIS_ALIGNED".
-    subsample: Ratio of the dataset (sampling without replacement) used to
-      train individual trees for the random sampling method. If \\"subsample\\"
-      is set and if \\"sampling_method\\" is NOT set or set to \\"NONE\\", then
-      \\"sampling_method\\" is implicitly set to \\"RANDOM\\". In other words,
-      to enable random subsampling, you only need to set "\\"subsample\\".
-      Default: 1.0.
-    uplift_min_examples_in_treatment: For uplift models only. Minimum number of
-      examples per treatment in a node. Default: 5.
-    uplift_split_score: For uplift models only. Splitter score i.e. score
-      optimized by the splitters. The scores are introduced in "Decision trees
-      for uplift modeling with single and multiple treatments", Rzepakowski et
-      al. Notation: `p` probability / average value of the positive outcome,
-      `q` probability / average value in the control group.
-      - `KULLBACK_LEIBLER` or `KL`: - p log (p/q)
-      - `EUCLIDEAN_DISTANCE` or `ED`: (p-q)^2
-      - `CHI_SQUARED` or `CS`: (p-q)^2/q
-        Default: "KULLBACK_LEIBLER".
-    use_hessian_gain: Use true, uses a formulation of split gain with a hessian
-      term i.e. optimizes the splits to minimize the variance of "gradient /
-      hessian. Available for all losses except regression. Default: False.
-    validation_interval_in_trees: Evaluate the model on the validation set
-      every "validation_interval_in_trees" trees. Increasing this value reduce
-      the cost of validation and can impact the early stopping policy (as early
-      stopping is only tested during the validation). Default: 1.
-    validation_ratio: Fraction of the training dataset used for validation if
-      not validation dataset is provided. The validation dataset, whether
-      provided directly or extracted from the training dataset, is used to
-      compute the validation loss, other validation metrics, and possibly
-      trigger early stopping (if enabled). When early stopping is disabled, the
-      validation dataset is only used for monitoring and does not influence the
-      model directly. If the "validation_ratio" is set to 0, early stopping is
-      disabled (i.e., it implies setting early_stopping=NONE). Default: 0.1.
-
-    num_threads: Number of threads used to train the model. Different learning
-      algorithms use multi-threading differently and with different degree of
-      efficiency. If `None`, `num_threads` will be automatically set to the
-      number of processors (up to a maximum of 32; or set to 6 if the number of
-      processors is not available). Making `num_threads` significantly larger
-      than the number of processors can slow-down the training speed. The
-      default value logic might change in the future.
-    resume_training: If true, the model training resumes from the checkpoint
-      stored in the `working_dir` directory. If `working_dir` does not
-      contain any model checkpoint, the training starts from the beginning.
-      Resuming training is useful in the following situations: (1) The training
-      was interrupted by the user (e.g. ctrl+c or "stop" button in a notebook)
-      or rescheduled, or (2) the hyper-parameter of the learner was changed e.g.
-      increasing the number of trees.
-    working_dir: Path to a directory available for the learning algorithm to
-      store intermediate computation results. Depending on the learning
-      algorithm and parameters, the working_dir might be optional, required, or
-      ignored. For instance, distributed training algorithm always need a
-      "working_dir", and the gradient boosted tree and hyper-parameter tuners
-      will export artefacts to the "working_dir" if provided.
-    resume_training_snapshot_interval_seconds: Indicative number of seconds in 
-      between snapshots when `resume_training=True`. Might be ignored by
-      some learners.
-    tuner: If set, automatically select the best hyperparameters using the
-      provided tuner. When using distributed training, the tuning is
-      distributed.
-    workers: If set, enable distributed training. "workers" is the list of IP
-      addresses of the workers. A worker is a process running
-      `ydf.start_worker(port)`.
-  """
-
-  def __init__(self,
-      label: str,
-      task: generic_learner.Task = generic_learner.Task.CLASSIFICATION,
-      weights: Optional[str] = None,
-      ranking_group: Optional[str] = None,
-      uplift_treatment: Optional[str] = None,
-      features: dataspec.ColumnDefs = None,
-      include_all_columns: bool = False,
-      max_vocab_count: int = 2000,
-      min_vocab_frequency: int = 5,
-      discretize_numerical_columns: bool = False,
-      num_discretized_numerical_bins: int = 255,
-      max_num_scanned_rows_to_infer_semantic: int = 10000,
-      max_num_scanned_rows_to_compute_statistics: int = 10000,
-      data_spec: Optional[data_spec_pb2.DataSpecification] = None,
-      adapt_subsample_for_maximum_training_duration: Optional[bool] = False,
-      allow_na_conditions: Optional[bool] = False,
-      apply_link_function: Optional[bool] = True,
-      categorical_algorithm: Optional[str] = "CART",
-      categorical_set_split_greedy_sampling: Optional[float] = 0.1,
-      categorical_set_split_max_num_items: Optional[int] = -1,
-      categorical_set_split_min_item_frequency: Optional[int] = 1,
-      compute_permutation_variable_importance: Optional[bool] = False,
-      dart_dropout: Optional[float] = 0.01,
-      early_stopping: Optional[str] = "LOSS_INCREASE",
-      early_stopping_initial_iteration: Optional[int] = 10,
-      early_stopping_num_trees_look_ahead: Optional[int] = 30,
-      focal_loss_alpha: Optional[float] = 0.5,
-      focal_loss_gamma: Optional[float] = 2.0,
-      forest_extraction: Optional[str] = "MART",
-      goss_alpha: Optional[float] = 0.2,
-      goss_beta: Optional[float] = 0.1,
-      growing_strategy: Optional[str] = "LOCAL",
-      honest: Optional[bool] = False,
-      honest_fixed_separation: Optional[bool] = False,
-      honest_ratio_leaf_examples: Optional[float] = 0.5,
-      in_split_min_examples_check: Optional[bool] = True,
-      keep_non_leaf_label_distribution: Optional[bool] = True,
-      l1_regularization: Optional[float] = 0.0,
-      l2_categorical_regularization: Optional[float] = 1.0,
-      l2_regularization: Optional[float] = 0.0,
-      lambda_loss: Optional[float] = 1.0,
-      loss: Optional[Union[str, custom_loss.AbstractCustomLoss]] = "DEFAULT",
-      max_depth: Optional[int] = 6,
-      max_num_nodes: Optional[int] = None,
-      maximum_model_size_in_memory_in_bytes: Optional[float] = -1.0,
-      maximum_training_duration_seconds: Optional[float] = -1.0,
-      mhld_oblique_max_num_attributes: Optional[int] = None,
-      mhld_oblique_sample_attributes: Optional[bool] = None,
-      min_examples: Optional[int] = 5,
-      missing_value_policy: Optional[str] = "GLOBAL_IMPUTATION",
-      num_candidate_attributes: Optional[int] = -1,
-      num_candidate_attributes_ratio: Optional[float] = -1.0,
-      num_trees: Optional[int] = 300,
-      pure_serving_model: Optional[bool] = False,
-      random_seed: Optional[int] = 123456,
-      sampling_method: Optional[str] = "RANDOM",
-      selective_gradient_boosting_ratio: Optional[float] = 0.01,
-      shrinkage: Optional[float] = 0.1,
-      sorting_strategy: Optional[str] = "PRESORT",
-      sparse_oblique_max_num_projections: Optional[int] = None,
-      sparse_oblique_normalization: Optional[str] = None,
-      sparse_oblique_num_projections_exponent: Optional[float] = None,
-      sparse_oblique_projection_density_factor: Optional[float] = None,
-      sparse_oblique_weights: Optional[str] = None,
-      split_axis: Optional[str] = "AXIS_ALIGNED",
-      subsample: Optional[float] = 1.0,
-      uplift_min_examples_in_treatment: Optional[int] = 5,
-      uplift_split_score: Optional[str] = "KULLBACK_LEIBLER",
-      use_hessian_gain: Optional[bool] = False,
-      validation_interval_in_trees: Optional[int] = 1,
-      validation_ratio: Optional[float] = 0.1,
-      num_threads: Optional[int] = None,
-      working_dir: Optional[str] = None,
-      resume_training: bool = False,
-      resume_training_snapshot_interval_seconds: int = 1800,
-      tuner: Optional[tuner_lib.AbstractTuner] = None,
-      workers: Optional[Sequence[str]] = None,
-      ):
-
-    hyper_parameters = {
-                      "adapt_subsample_for_maximum_training_duration" : adapt_subsample_for_maximum_training_duration,
-                      "allow_na_conditions" : allow_na_conditions,
-                      "apply_link_function" : apply_link_function,
-                      "categorical_algorithm" : categorical_algorithm,
-                      "categorical_set_split_greedy_sampling" : categorical_set_split_greedy_sampling,
-                      "categorical_set_split_max_num_items" : categorical_set_split_max_num_items,
-                      "categorical_set_split_min_item_frequency" : categorical_set_split_min_item_frequency,
-                      "compute_permutation_variable_importance" : compute_permutation_variable_importance,
-                      "dart_dropout" : dart_dropout,
-                      "early_stopping" : early_stopping,
-                      "early_stopping_initial_iteration" : early_stopping_initial_iteration,
-                      "early_stopping_num_trees_look_ahead" : early_stopping_num_trees_look_ahead,
-                      "focal_loss_alpha" : focal_loss_alpha,
-                      "focal_loss_gamma" : focal_loss_gamma,
-                      "forest_extraction" : forest_extraction,
-                      "goss_alpha" : goss_alpha,
-                      "goss_beta" : goss_beta,
-                      "growing_strategy" : growing_strategy,
-                      "honest" : honest,
-                      "honest_fixed_separation" : honest_fixed_separation,
-                      "honest_ratio_leaf_examples" : honest_ratio_leaf_examples,
-                      "in_split_min_examples_check" : in_split_min_examples_check,
-                      "keep_non_leaf_label_distribution" : keep_non_leaf_label_distribution,
-                      "l1_regularization" : l1_regularization,
-                      "l2_categorical_regularization" : l2_categorical_regularization,
-                      "l2_regularization" : l2_regularization,
-                      "lambda_loss" : lambda_loss,
-                      "loss" : loss,
-                      "max_depth" : max_depth,
-                      "max_num_nodes" : max_num_nodes,
-                      "maximum_model_size_in_memory_in_bytes" : maximum_model_size_in_memory_in_bytes,
-                      "maximum_training_duration_seconds" : maximum_training_duration_seconds,
-                      "mhld_oblique_max_num_attributes" : mhld_oblique_max_num_attributes,
-                      "mhld_oblique_sample_attributes" : mhld_oblique_sample_attributes,
-                      "min_examples" : min_examples,
-                      "missing_value_policy" : missing_value_policy,
-                      "num_candidate_attributes" : num_candidate_attributes,
-                      "num_candidate_attributes_ratio" : num_candidate_attributes_ratio,
-                      "num_trees" : num_trees,
-                      "pure_serving_model" : pure_serving_model,
-                      "random_seed" : random_seed,
-                      "sampling_method" : sampling_method,
-                      "selective_gradient_boosting_ratio" : selective_gradient_boosting_ratio,
-                      "shrinkage" : shrinkage,
-                      "sorting_strategy" : sorting_strategy,
-                      "sparse_oblique_max_num_projections" : sparse_oblique_max_num_projections,
-                      "sparse_oblique_normalization" : sparse_oblique_normalization,
-                      "sparse_oblique_num_projections_exponent" : sparse_oblique_num_projections_exponent,
-                      "sparse_oblique_projection_density_factor" : sparse_oblique_projection_density_factor,
-                      "sparse_oblique_weights" : sparse_oblique_weights,
-                      "split_axis" : split_axis,
-                      "subsample" : subsample,
-                      "uplift_min_examples_in_treatment" : uplift_min_examples_in_treatment,
-                      "uplift_split_score" : uplift_split_score,
-                      "use_hessian_gain" : use_hessian_gain,
-                      "validation_interval_in_trees" : validation_interval_in_trees,
-                      "validation_ratio" : validation_ratio,
-
-      }
-
-    data_spec_args = dataspec.DataSpecInferenceArgs(
-        columns=dataspec.normalize_column_defs(features),
-        include_all_columns=include_all_columns,
-        max_vocab_count=max_vocab_count,
-        min_vocab_frequency=min_vocab_frequency,
-        discretize_numerical_columns=discretize_numerical_columns,
-        num_discretized_numerical_bins=num_discretized_numerical_bins,
-        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
-        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
-    )
-
-    deployment_config = self._build_deployment_config(
-        num_threads=num_threads,
-        resume_training=resume_training,
-        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
-        working_dir=working_dir,
-        workers=workers,
-    )
-
-    super().__init__(learner_name="GRADIENT_BOOSTED_TREES",
-      task=task,
-      label=label,
-      weights=weights,
-      ranking_group=ranking_group,
-      uplift_treatment=uplift_treatment,
-      data_spec_args=data_spec_args,
-      data_spec=data_spec,
-      hyper_parameters=hyper_parameters,
-      deployment_config=deployment_config,
-      tuner=tuner,
-    )
-
-  def train(
-      self,
-      ds: dataset.InputDataset,
-      valid: Optional[dataset.InputDataset] = None,
-  ) -> gradient_boosted_trees_model.GradientBoostedTreesModel:
-    """Trains a model on the given dataset.
-
-    Options for dataset reading are given on the learner. Consult the
-    documentation of the learner or ydf.create_vertical_dataset() for additional
-    information on dataset reading in YDF.
-
-    Usage example:
-
-    ```
-    import ydf
-    import pandas as pd
-
-    train_ds = pd.read_csv(...)
-
-    learner = ydf.GradientBoostedTreesLearner(label="label")
-    model = learner.train(train_ds)
-    print(model.summary())
-    ```
-
-    If training is interrupted (for example, by interrupting the cell execution
-    in Colab), the model will be returned to the state it was in at the moment
-    of interruption.
-
-    Args:
-      ds: Training dataset.
-      valid: Optional validation dataset. Some learners, such as Random Forest,
-        do not need validation dataset. Some learners, such as
-        GradientBoostedTrees, automatically extract a validation dataset from
-        the training dataset if the validation dataset is not provided.
-
-    Returns:
-      A trained model.
-    """
-    return super().train(ds, valid)
-
-  @classmethod
-  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
-    return abstract_learner_pb2.LearnerCapabilities(
-      support_max_training_duration=True,
-      resume_training=True,
-      support_validation_dataset=True,
-      support_partial_cache_dataset_format=False,
-      support_max_model_size_in_memory=False,
-      support_monotonic_constraints=True,
-    )
-
-  @classmethod
-  def hyperparameter_templates(cls) -> Dict[str, hyperparameters.HyperparameterTemplate]:
-    r"""Hyperparameter templates for this Learner.
-    
-    Hyperparameter templates are sets of pre-defined hyperparameters for easy
-    access to different variants of the learner. Each template is a mapping to a
-    set of hyperparameters and can be applied directly on the learner.
-    
-    Usage example:
-    ```python
-    templates = ydf.GradientBoostedTreesLearner.hyperparameter_templates()
-    better_defaultv1 = templates["better_defaultv1"]
-    # Print a description of the template
-    print(better_defaultv1.description)
-    # Apply the template's settings on the learner.
-    learner = ydf.GradientBoostedTreesLearner(label, **better_defaultv1)
-    ```
-    
-    Returns:
-      Dictionary of the available templates
-    """
-    return {"better_defaultv1": hyperparameters.HyperparameterTemplate(name="better_default", version=1, description="A configuration that is generally better than the default parameters without being more expensive.", parameters={"growing_strategy" :"BEST_FIRST_GLOBAL"}), "benchmark_rank1v1": hyperparameters.HyperparameterTemplate(name="benchmark_rank1", version=1, description="Top ranking hyper-parameters on our benchmark slightly modified to run in reasonable time.", parameters={"growing_strategy" :"BEST_FIRST_GLOBAL", "categorical_algorithm" :"RANDOM", "split_axis" :"SPARSE_OBLIQUE", "sparse_oblique_normalization" :"MIN_MAX", "sparse_oblique_num_projections_exponent" :1.0}), }
-
-class RandomForestLearner(generic_learner.GenericLearner):
-  r"""Random Forest learning algorithm.
-
-  A Random Forest (https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)
-  is a collection of deep CART decision trees trained independently and without
-  pruning. Each tree is trained on a random subset of the original training 
-  dataset (sampled with replacement).
-  
-  The algorithm is unique in that it is robust to overfitting, even in extreme
-  cases e.g. when there are more features than training examples.
-  
-  It is probably the most well-known of the Decision Forest training
-  algorithms.
-
-  Usage example:
-
-  ```python
-  import ydf
-  import pandas as pd
-
-  dataset = pd.read_csv("project/dataset.csv")
-
-  model = ydf.RandomForestLearner().train(dataset)
-
-  print(model.summary())
-  ```
-
-  Hyperparameters are configured to give reasonable results for typical
-  datasets. Hyperparameters can also be modified manually (see descriptions)
-  below or by applying the hyperparameter templates available with
-  `RandomForestLearner.hyperparameter_templates()` (see this function's documentation for
-  details).
-
-  Attributes:
-    label: Label of the dataset. The label column
-      should not be identified as a feature in the `features` parameter.
-    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,
-      Task.RANKING, Task.CATEGORICAL_UPLIFT, Task.NUMERICAL_UPLIFT).
-    weights: Name of a feature that identifies the weight of each example. If
-      weights are not specified, unit weights are assumed. The weight column
-      should not be identified as a feature in the `features` parameter.
-    ranking_group: Only for `task=Task.RANKING`. Name of a feature
-      that identifies queries in a query/document ranking task. The ranking
-      group should not be identified as a feature in the `features` parameter.
-    uplift_treatment: Only for `task=Task.CATEGORICAL_UPLIFT` and `task=Task`.
-      NUMERICAL_UPLIFT. Name of a numerical feature that identifies the
-      treatment in an uplift problem. The value 0 is reserved for the control
-      treatment. Currently, only 0/1 binary treatments are supported.
-    features: If None, all columns are used as features. The semantic of the
-      features is determined automatically. Otherwise, if
-      include_all_columns=False (default) only the column listed in `features`
-      are imported. If include_all_columns=True, all the columns are imported as
-      features and only the semantic of the columns NOT in `columns` is 
-      determined automatically. If specified,  defines the order of the features
-      - any non-listed features are appended in-order after the specified
-      features (if include_all_columns=True).
-      The label, weights, uplift treatment and ranking_group columns should not
-      be specified as features.
-    include_all_columns: See `features`.
-    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
-      CATEGORICAL_SET columns stored as strings. If more unique values exist,
-      only the most frequent values are kept, and the remaining values are
-      considered as out-of-vocabulary.
-    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
-      and CATEGORICAL_SET columns. Value observed less than
-      `min_vocab_frequency` are considered as out-of-vocabulary.
-    discretize_numerical_columns: If true, discretize all the numerical columns
-      before training. Discretized numerical columns are faster to train with,
-      but they can have a negative impact on the model quality. Using
-      `discretize_numerical_columns=True` is equivalent as setting the column
-      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
-      definition of DISCRETIZED_NUMERICAL for more details.
-    num_discretized_numerical_bins: Number of bins used when disretizing
-      numerical columns.
-    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
-      inferring the column's semantic if it is not explicitly specified. Only
-      used when reading from file, in-memory datasets are always read in full.
-      Setting this to a lower number will speed up dataset reading, but might
-      result in incorrect column semantics. Set to -1 to scan the entire
-      dataset.
-    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
-      computing a column's statistics. Only used when reading from file,
-      in-memory datasets are always read in full. A column's statistics include
-      the dictionary for categorical features and the mean / min / max for
-      numerical features. Setting this to a lower number will speed up dataset
-      reading, but skew statistics in the dataspec, which can hurt model quality
-      (e.g. if an important category of a categorical feature is considered
-      OOV). Set to -1 to scan the entire dataset.
-    data_spec: Dataspec to be used (advanced). If a data spec is given,
-      `columns`, `include_all_columns`, `max_vocab_count`,
-      `min_vocab_frequency`, `discretize_numerical_columns` and 
-      `num_discretized_numerical_bins` will be ignored.
-    adapt_bootstrap_size_ratio_for_maximum_training_duration: Control how the
-      maximum training duration (if set) is applied. If false, the training
-      stop when the time is used. If true, adapts the size of the sampled
-      dataset used to train each tree such that `num_trees` will train within
-      `maximum_training_duration`. Has no effect if there is no maximum
-      training duration specified. Default: False.
-    allow_na_conditions: If true, the tree training evaluates conditions of the
-      type `X is NA` i.e. `X is missing`. Default: False.
-    bootstrap_size_ratio: Number of examples used to train each trees;
-      expressed as a ratio of the training dataset size. Default: 1.0.
-    bootstrap_training_dataset: If true (default), each tree is trained on a
-      separate dataset sampled with replacement from the original dataset. If
-      false, all the trees are trained on the entire same dataset. If
-      bootstrap_training_dataset:false, OOB metrics are not available.
-      bootstrap_training_dataset=false is used in "Extremely randomized trees"
-      (https://link.springer.com/content/pdf/10.1007%2Fs10994-006-6226-1.pdf).
-      Default: True.
-    categorical_algorithm: How to learn splits on categorical attributes.
-      - `CART`: CART algorithm. Find categorical splits of the form "value \\in
-        mask". The solution is exact for binary classification, regression and
-        ranking. It is approximated for multi-class classification. This is a
-        good first algorithm to use. In case of overfitting (very small
-        dataset, large dictionary), the "random" algorithm is a good
-        alternative.
-      - `ONE_HOT`: One-hot encoding. Find the optimal categorical split of the
-        form "attribute == param". This method is similar (but more efficient)
-        than converting converting each possible categorical value into a
-        boolean feature. This method is available for comparison purpose and
-        generally performs worse than other alternatives.
-      - `RANDOM`: Best splits among a set of random candidate. Find the a
-        categorical split of the form "value \\in mask" using a random search.
-        This solution can be seen as an approximation of the CART algorithm.
-        This method is a strong alternative to CART. This algorithm is inspired
-        from section "5.1 Categorical Variables" of "Random Forest", 2001.
-        Default: "CART".
-    categorical_set_split_greedy_sampling: For categorical set splits e.g.
-      texts. Probability for a categorical value to be a candidate for the
-      positive set. The sampling is applied once per node (i.e. not at every
-      step of the greedy optimization). Default: 0.1.
-    categorical_set_split_max_num_items: For categorical set splits e.g. texts.
-      Maximum number of items (prior to the sampling). If more items are
-      available, the least frequent items are ignored. Changing this value is
-      similar to change the "max_vocab_count" before loading the dataset, with
-      the following exception: With `max_vocab_count`, all the remaining items
-      are grouped in a special Out-of-vocabulary item. With `max_num_items`,
-      this is not the case. Default: -1.
-    categorical_set_split_min_item_frequency: For categorical set splits e.g.
-      texts. Minimum number of occurrences of an item to be considered.
-      Default: 1.
-    compute_oob_performances: If true, compute the Out-of-bag evaluation (then
-      available in the summary and model inspector). This evaluation is a cheap
-      alternative to cross-validation evaluation. Default: True.
-    compute_oob_variable_importances: If true, compute the Out-of-bag feature
-      importance (then available in the summary and model inspector). Note that
-      the OOB feature importance can be expensive to compute. Default: False.
-    growing_strategy: How to grow the tree.
-      - `LOCAL`: Each node is split independently of the other nodes. In other
-        words, as long as a node satisfy the splits "constraints (e.g. maximum
-        depth, minimum number of observations), the node will be split. This is
-        the "classical" way to grow decision trees.
-      - `BEST_FIRST_GLOBAL`: The node with the best loss reduction among all
-        the nodes of the tree is selected for splitting. This method is also
-        called "best first" or "leaf-wise growth". See "Best-first decision
-        tree learning", Shi and "Additive logistic regression : A statistical
-        view of boosting", Friedman for more details. Default: "LOCAL".
-    honest: In honest trees, different training examples are used to infer the
-      structure and the leaf values. This regularization technique trades
-      examples for bias estimates. It might increase or reduce the quality of
-      the model. See "Generalized Random Forests", Athey et al. In this paper,
-      Honest trees are trained with the Random Forest algorithm with a sampling
-      without replacement. Default: False.
-    honest_fixed_separation: For honest trees only i.e. honest=true. If true, a
-      new random separation is generated for each tree. If false, the same
-      separation is used for all the trees (e.g., in Gradient Boosted Trees
-      containing multiple trees). Default: False.
-    honest_ratio_leaf_examples: For honest trees only i.e. honest=true. Ratio
-      of examples used to set the leaf values. Default: 0.5.
-    in_split_min_examples_check: Whether to check the `min_examples` constraint
-      in the split search (i.e. splits leading to one child having less than
-      `min_examples` examples are considered invalid) or before the split
-      search (i.e. a node can be derived only if it contains more than
-      `min_examples` examples). If false, there can be nodes with less than
-      `min_examples` training examples. Default: True.
-    keep_non_leaf_label_distribution: Whether to keep the node value (i.e. the
-      distribution of the labels of the training examples) of non-leaf nodes.
-      This information is not used during serving, however it can be used for
-      model interpretation as well as hyper parameter tuning. This can take
-      lots of space, sometimes accounting for half of the model size. Default:
-      True.
-    max_depth: Maximum depth of the tree. `max_depth=1` means that all trees
-      will be roots. `max_depth=-1` means that tree depth is not restricted by
-      this parameter. Values <= -2 will be ignored. Default: 16.
-    max_num_nodes: Maximum number of nodes in the tree. Set to -1 to disable
-      this limit. Only available for `growing_strategy=BEST_FIRST_GLOBAL`.
-      Default: None.
-    maximum_model_size_in_memory_in_bytes: Limit the size of the model when
-      stored in ram. Different algorithms can enforce this limit differently.
-      Note that when models are compiled into an inference, the size of the
-      inference engine is generally much smaller than the original model.
-      Default: -1.0.
-    maximum_training_duration_seconds: Maximum training duration of the model
-      expressed in seconds. Each learning algorithm is free to use this
-      parameter at it sees fit. Enabling maximum training duration makes the
-      model training non-deterministic. Default: -1.0.
-    mhld_oblique_max_num_attributes: For MHLD oblique splits i.e.
-      `split_axis=MHLD_OBLIQUE`. Maximum number of attributes in the
-      projection. Increasing this value increases the training time. Decreasing
-      this value acts as a regularization. The value should be in [2,
-      num_numerical_features]. If the value is above the total number of
-      numerical features, the value is capped automatically. The value 1 is
-      allowed but results in ordinary (non-oblique) splits. Default: None.
-    mhld_oblique_sample_attributes: For MHLD oblique splits i.e.
-      `split_axis=MHLD_OBLIQUE`. If true, applies the attribute sampling
-      controlled by the "num_candidate_attributes" or
-      "num_candidate_attributes_ratio" parameters. If false, all the attributes
-      are tested. Default: None.
-    min_examples: Minimum number of examples in a node. Default: 5.
-    missing_value_policy: Method used to handle missing attribute values.
-      - `GLOBAL_IMPUTATION`: Missing attribute values are imputed, with the
-        mean (in case of numerical attribute) or the most-frequent-item (in
-        case of categorical attribute) computed on the entire dataset (i.e. the
-        information contained in the data spec).
-      - `LOCAL_IMPUTATION`: Missing attribute values are imputed with the mean
-        (numerical attribute) or most-frequent-item (in the case of categorical
-        attribute) evaluated on the training examples in the current node.
-      - `RANDOM_LOCAL_IMPUTATION`: Missing attribute values are imputed from
-        randomly sampled values from the training examples in the current node.
-        This method was proposed by Clinic et al. in "Random Survival Forests"
-        (https://projecteuclid.org/download/pdfview_1/euclid.aoas/1223908043).
-        Default: "GLOBAL_IMPUTATION".
-    num_candidate_attributes: Number of unique valid attributes tested for each
-      node. An attribute is valid if it has at least a valid split. If
-      `num_candidate_attributes=0`, the value is set to the classical default
-      value for Random Forest: `sqrt(number of input attributes)` in case of
-      classification and `number_of_input_attributes / 3` in case of
-      regression. If `num_candidate_attributes=-1`, all the attributes are
-      tested. Default: 0.
-    num_candidate_attributes_ratio: Ratio of attributes tested at each node. If
-      set, it is equivalent to `num_candidate_attributes =
-      number_of_input_features x num_candidate_attributes_ratio`. The possible
-      values are between ]0, and 1] as well as -1. If not set or equal to -1,
-      the `num_candidate_attributes` is used. Default: -1.0.
-    num_oob_variable_importances_permutations: Number of time the dataset is
-      re-shuffled to compute the permutation variable importances. Increasing
-      this value increase the training time (if
-      "compute_oob_variable_importances:true") as well as the stability of the
-      oob variable importance metrics. Default: 1.
-    num_trees: Number of individual decision trees. Increasing the number of
-      trees can increase the quality of the model at the expense of size,
-      training speed, and inference latency. Default: 300.
-    pure_serving_model: Clear the model from any information that is not
-      required for model serving. This includes debugging, model interpretation
-      and other meta-data. The size of the serialized model can be reduced
-      significatively (50% model size reduction is common). This parameter has
-      no impact on the quality, serving speed or RAM usage of model serving.
-      Default: False.
-    random_seed: Random seed for the training of the model. Learners are
-      expected to be deterministic by the random seed. Default: 123456.
-    sampling_with_replacement: If true, the training examples are sampled with
-      replacement. If false, the training samples are sampled without
-      replacement. Only used when "bootstrap_training_dataset=true". If false
-      (sampling without replacement) and if "bootstrap_size_ratio=1" (default),
-      all the examples are used to train all the trees (you probably do not
-      want that). Default: True.
-    sorting_strategy: How are sorted the numerical features in order to find
-      the splits
-      - PRESORT: The features are pre-sorted at the start of the training. This
-        solution is faster but consumes much more memory than IN_NODE.
-      - IN_NODE: The features are sorted just before being used in the node.
-        This solution is slow but consumes little amount of memory.
-      . Default: "PRESORT".
-    sparse_oblique_max_num_projections: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Maximum number of projections (applied after
-      the num_projections_exponent).
-      Oblique splits try out max(p^num_projections_exponent,
-      max_num_projections) random projections for choosing a split, where p is
-      the number of numerical features. Increasing "max_num_projections"
-      increases the training time but not the inference time. In late stage
-      model development, if every bit of accuracy if important, increase this
-      value.
-      The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
-      does not define this hyperparameter. Default: None.
-    sparse_oblique_normalization: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Normalization applied on the features,
-      before applying the sparse oblique projections.
-      - `NONE`: No normalization.
-      - `STANDARD_DEVIATION`: Normalize the feature by the estimated standard
-        deviation on the entire train dataset. Also known as Z-Score
-        normalization.
-      - `MIN_MAX`: Normalize the feature by the range (i.e. max-min) estimated
-        on the entire train dataset. Default: None.
-    sparse_oblique_num_projections_exponent: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections
-      to test at each node.
-      Increasing this value very likely improves the quality of the model,
-      drastically increases the training time, and doe not impact the inference
-      time.
-      Oblique splits try out max(p^num_projections_exponent,
-      max_num_projections) random projections for choosing a split, where p is
-      the number of numerical features. Therefore, increasing this
-      `num_projections_exponent` and possibly `max_num_projections` may improve
-      model quality, but will also significantly increase training time.
-      Note that the complexity of (classic) Random Forests is roughly
-      proportional to `num_projections_exponent=0.5`, since it considers
-      sqrt(num_features) for a split. The complexity of (classic) GBDT is
-      roughly proportional to `num_projections_exponent=1`, since it considers
-      all features for a split.
-      The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
-      recommends values in [1/4, 2]. Default: None.
-    sparse_oblique_projection_density_factor: Density of the projections as an
-      exponent of the number of features. Independently for each projection,
-      each feature has a probability "projection_density_factor / num_features"
-      to be considered in the projection.
-      The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
-      calls this parameter `lambda` and recommends values in [1, 5].
-      Increasing this value increases training and inference time (on average).
-      This value is best tuned for each dataset. Default: None.
-    sparse_oblique_weights: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Possible values:
-      - `BINARY`: The oblique weights are sampled in {-1,1} (default).
-      - `CONTINUOUS`: The oblique weights are be sampled in [-1,1]. Default:
-        None.
-    split_axis: What structure of split to consider for numerical features.
-      - `AXIS_ALIGNED`: Axis aligned splits (i.e. one condition at a time).
-        This is the "classical" way to train a tree. Default value.
-      - `SPARSE_OBLIQUE`: Sparse oblique splits (i.e. random splits one a small
-        number of features) from "Sparse Projection Oblique Random Forests",
-        Tomita et al., 2020.
-      - `MHLD_OBLIQUE`: Multi-class Hellinger Linear Discriminant splits from
-        "Classification Based on Multivariate Contrast Patterns",
-        Canete-Sifuentes et al., 2029 Default: "AXIS_ALIGNED".
-    uplift_min_examples_in_treatment: For uplift models only. Minimum number of
-      examples per treatment in a node. Default: 5.
-    uplift_split_score: For uplift models only. Splitter score i.e. score
-      optimized by the splitters. The scores are introduced in "Decision trees
-      for uplift modeling with single and multiple treatments", Rzepakowski et
-      al. Notation: `p` probability / average value of the positive outcome,
-      `q` probability / average value in the control group.
-      - `KULLBACK_LEIBLER` or `KL`: - p log (p/q)
-      - `EUCLIDEAN_DISTANCE` or `ED`: (p-q)^2
-      - `CHI_SQUARED` or `CS`: (p-q)^2/q
-        Default: "KULLBACK_LEIBLER".
-    winner_take_all: Control how classification trees vote. If true, each tree
-      votes for one class. If false, each tree vote for a distribution of
-      classes. winner_take_all_inference=false is often preferable. Default:
-      True.
-
-    num_threads: Number of threads used to train the model. Different learning
-      algorithms use multi-threading differently and with different degree of
-      efficiency. If `None`, `num_threads` will be automatically set to the
-      number of processors (up to a maximum of 32; or set to 6 if the number of
-      processors is not available). Making `num_threads` significantly larger
-      than the number of processors can slow-down the training speed. The
-      default value logic might change in the future.
-    resume_training: If true, the model training resumes from the checkpoint
-      stored in the `working_dir` directory. If `working_dir` does not
-      contain any model checkpoint, the training starts from the beginning.
-      Resuming training is useful in the following situations: (1) The training
-      was interrupted by the user (e.g. ctrl+c or "stop" button in a notebook)
-      or rescheduled, or (2) the hyper-parameter of the learner was changed e.g.
-      increasing the number of trees.
-    working_dir: Path to a directory available for the learning algorithm to
-      store intermediate computation results. Depending on the learning
-      algorithm and parameters, the working_dir might be optional, required, or
-      ignored. For instance, distributed training algorithm always need a
-      "working_dir", and the gradient boosted tree and hyper-parameter tuners
-      will export artefacts to the "working_dir" if provided.
-    resume_training_snapshot_interval_seconds: Indicative number of seconds in 
-      between snapshots when `resume_training=True`. Might be ignored by
-      some learners.
-    tuner: If set, automatically select the best hyperparameters using the
-      provided tuner. When using distributed training, the tuning is
-      distributed.
-    workers: If set, enable distributed training. "workers" is the list of IP
-      addresses of the workers. A worker is a process running
-      `ydf.start_worker(port)`.
-  """
-
-  def __init__(self,
-      label: str,
-      task: generic_learner.Task = generic_learner.Task.CLASSIFICATION,
-      weights: Optional[str] = None,
-      ranking_group: Optional[str] = None,
-      uplift_treatment: Optional[str] = None,
-      features: dataspec.ColumnDefs = None,
-      include_all_columns: bool = False,
-      max_vocab_count: int = 2000,
-      min_vocab_frequency: int = 5,
-      discretize_numerical_columns: bool = False,
-      num_discretized_numerical_bins: int = 255,
-      max_num_scanned_rows_to_infer_semantic: int = 10000,
-      max_num_scanned_rows_to_compute_statistics: int = 10000,
-      data_spec: Optional[data_spec_pb2.DataSpecification] = None,
-      adapt_bootstrap_size_ratio_for_maximum_training_duration: Optional[bool] = False,
-      allow_na_conditions: Optional[bool] = False,
-      bootstrap_size_ratio: Optional[float] = 1.0,
-      bootstrap_training_dataset: Optional[bool] = True,
-      categorical_algorithm: Optional[str] = "CART",
-      categorical_set_split_greedy_sampling: Optional[float] = 0.1,
-      categorical_set_split_max_num_items: Optional[int] = -1,
-      categorical_set_split_min_item_frequency: Optional[int] = 1,
-      compute_oob_performances: Optional[bool] = True,
-      compute_oob_variable_importances: Optional[bool] = False,
-      growing_strategy: Optional[str] = "LOCAL",
-      honest: Optional[bool] = False,
-      honest_fixed_separation: Optional[bool] = False,
-      honest_ratio_leaf_examples: Optional[float] = 0.5,
-      in_split_min_examples_check: Optional[bool] = True,
-      keep_non_leaf_label_distribution: Optional[bool] = True,
-      max_depth: Optional[int] = 16,
-      max_num_nodes: Optional[int] = None,
-      maximum_model_size_in_memory_in_bytes: Optional[float] = -1.0,
-      maximum_training_duration_seconds: Optional[float] = -1.0,
-      mhld_oblique_max_num_attributes: Optional[int] = None,
-      mhld_oblique_sample_attributes: Optional[bool] = None,
-      min_examples: Optional[int] = 5,
-      missing_value_policy: Optional[str] = "GLOBAL_IMPUTATION",
-      num_candidate_attributes: Optional[int] = 0,
-      num_candidate_attributes_ratio: Optional[float] = -1.0,
-      num_oob_variable_importances_permutations: Optional[int] = 1,
-      num_trees: Optional[int] = 300,
-      pure_serving_model: Optional[bool] = False,
-      random_seed: Optional[int] = 123456,
-      sampling_with_replacement: Optional[bool] = True,
-      sorting_strategy: Optional[str] = "PRESORT",
-      sparse_oblique_max_num_projections: Optional[int] = None,
-      sparse_oblique_normalization: Optional[str] = None,
-      sparse_oblique_num_projections_exponent: Optional[float] = None,
-      sparse_oblique_projection_density_factor: Optional[float] = None,
-      sparse_oblique_weights: Optional[str] = None,
-      split_axis: Optional[str] = "AXIS_ALIGNED",
-      uplift_min_examples_in_treatment: Optional[int] = 5,
-      uplift_split_score: Optional[str] = "KULLBACK_LEIBLER",
-      winner_take_all: Optional[bool] = True,
-      num_threads: Optional[int] = None,
-      working_dir: Optional[str] = None,
-      resume_training: bool = False,
-      resume_training_snapshot_interval_seconds: int = 1800,
-      tuner: Optional[tuner_lib.AbstractTuner] = None,
-      workers: Optional[Sequence[str]] = None,
-      ):
-
-    hyper_parameters = {
-                      "adapt_bootstrap_size_ratio_for_maximum_training_duration" : adapt_bootstrap_size_ratio_for_maximum_training_duration,
-                      "allow_na_conditions" : allow_na_conditions,
-                      "bootstrap_size_ratio" : bootstrap_size_ratio,
-                      "bootstrap_training_dataset" : bootstrap_training_dataset,
-                      "categorical_algorithm" : categorical_algorithm,
-                      "categorical_set_split_greedy_sampling" : categorical_set_split_greedy_sampling,
-                      "categorical_set_split_max_num_items" : categorical_set_split_max_num_items,
-                      "categorical_set_split_min_item_frequency" : categorical_set_split_min_item_frequency,
-                      "compute_oob_performances" : compute_oob_performances,
-                      "compute_oob_variable_importances" : compute_oob_variable_importances,
-                      "growing_strategy" : growing_strategy,
-                      "honest" : honest,
-                      "honest_fixed_separation" : honest_fixed_separation,
-                      "honest_ratio_leaf_examples" : honest_ratio_leaf_examples,
-                      "in_split_min_examples_check" : in_split_min_examples_check,
-                      "keep_non_leaf_label_distribution" : keep_non_leaf_label_distribution,
-                      "max_depth" : max_depth,
-                      "max_num_nodes" : max_num_nodes,
-                      "maximum_model_size_in_memory_in_bytes" : maximum_model_size_in_memory_in_bytes,
-                      "maximum_training_duration_seconds" : maximum_training_duration_seconds,
-                      "mhld_oblique_max_num_attributes" : mhld_oblique_max_num_attributes,
-                      "mhld_oblique_sample_attributes" : mhld_oblique_sample_attributes,
-                      "min_examples" : min_examples,
-                      "missing_value_policy" : missing_value_policy,
-                      "num_candidate_attributes" : num_candidate_attributes,
-                      "num_candidate_attributes_ratio" : num_candidate_attributes_ratio,
-                      "num_oob_variable_importances_permutations" : num_oob_variable_importances_permutations,
-                      "num_trees" : num_trees,
-                      "pure_serving_model" : pure_serving_model,
-                      "random_seed" : random_seed,
-                      "sampling_with_replacement" : sampling_with_replacement,
-                      "sorting_strategy" : sorting_strategy,
-                      "sparse_oblique_max_num_projections" : sparse_oblique_max_num_projections,
-                      "sparse_oblique_normalization" : sparse_oblique_normalization,
-                      "sparse_oblique_num_projections_exponent" : sparse_oblique_num_projections_exponent,
-                      "sparse_oblique_projection_density_factor" : sparse_oblique_projection_density_factor,
-                      "sparse_oblique_weights" : sparse_oblique_weights,
-                      "split_axis" : split_axis,
-                      "uplift_min_examples_in_treatment" : uplift_min_examples_in_treatment,
-                      "uplift_split_score" : uplift_split_score,
-                      "winner_take_all" : winner_take_all,
-
-      }
-
-    data_spec_args = dataspec.DataSpecInferenceArgs(
-        columns=dataspec.normalize_column_defs(features),
-        include_all_columns=include_all_columns,
-        max_vocab_count=max_vocab_count,
-        min_vocab_frequency=min_vocab_frequency,
-        discretize_numerical_columns=discretize_numerical_columns,
-        num_discretized_numerical_bins=num_discretized_numerical_bins,
-        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
-        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
-    )
-
-    deployment_config = self._build_deployment_config(
-        num_threads=num_threads,
-        resume_training=resume_training,
-        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
-        working_dir=working_dir,
-        workers=workers,
-    )
-
-    super().__init__(learner_name="RANDOM_FOREST",
-      task=task,
-      label=label,
-      weights=weights,
-      ranking_group=ranking_group,
-      uplift_treatment=uplift_treatment,
-      data_spec_args=data_spec_args,
-      data_spec=data_spec,
-      hyper_parameters=hyper_parameters,
-      deployment_config=deployment_config,
-      tuner=tuner,
-    )
-
-  def train(
-      self,
-      ds: dataset.InputDataset,
-      valid: Optional[dataset.InputDataset] = None,
-  ) -> random_forest_model.RandomForestModel:
-    """Trains a model on the given dataset.
-
-    Options for dataset reading are given on the learner. Consult the
-    documentation of the learner or ydf.create_vertical_dataset() for additional
-    information on dataset reading in YDF.
-
-    Usage example:
-
-    ```
-    import ydf
-    import pandas as pd
-
-    train_ds = pd.read_csv(...)
-
-    learner = ydf.RandomForestLearner(label="label")
-    model = learner.train(train_ds)
-    print(model.summary())
-    ```
-
-    If training is interrupted (for example, by interrupting the cell execution
-    in Colab), the model will be returned to the state it was in at the moment
-    of interruption.
-
-    Args:
-      ds: Training dataset.
-      valid: Optional validation dataset. Some learners, such as Random Forest,
-        do not need validation dataset. Some learners, such as
-        GradientBoostedTrees, automatically extract a validation dataset from
-        the training dataset if the validation dataset is not provided.
-
-    Returns:
-      A trained model.
-    """
-    return super().train(ds, valid)
-
-  @classmethod
-  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
-    return abstract_learner_pb2.LearnerCapabilities(
-      support_max_training_duration=True,
-      resume_training=False,
-      support_validation_dataset=False,
-      support_partial_cache_dataset_format=False,
-      support_max_model_size_in_memory=True,
-      support_monotonic_constraints=False,
-    )
-
-  @classmethod
-  def hyperparameter_templates(cls) -> Dict[str, hyperparameters.HyperparameterTemplate]:
-    r"""Hyperparameter templates for this Learner.
-    
-    Hyperparameter templates are sets of pre-defined hyperparameters for easy
-    access to different variants of the learner. Each template is a mapping to a
-    set of hyperparameters and can be applied directly on the learner.
-    
-    Usage example:
-    ```python
-    templates = ydf.RandomForestLearner.hyperparameter_templates()
-    better_defaultv1 = templates["better_defaultv1"]
-    # Print a description of the template
-    print(better_defaultv1.description)
-    # Apply the template's settings on the learner.
-    learner = ydf.RandomForestLearner(label, **better_defaultv1)
-    ```
-    
-    Returns:
-      Dictionary of the available templates
-    """
-    return {"better_defaultv1": hyperparameters.HyperparameterTemplate(name="better_default", version=1, description="A configuration that is generally better than the default parameters without being more expensive.", parameters={"winner_take_all" :True}), "benchmark_rank1v1": hyperparameters.HyperparameterTemplate(name="benchmark_rank1", version=1, description="Top ranking hyper-parameters on our benchmark slightly modified to run in reasonable time.", parameters={"winner_take_all" :True, "categorical_algorithm" :"RANDOM", "split_axis" :"SPARSE_OBLIQUE", "sparse_oblique_normalization" :"MIN_MAX", "sparse_oblique_num_projections_exponent" :1.0}), }
+r"""Wrappers around the YDF learners.
+
+This file is generated automatically by running the following commands:
+  bazel build //ydf/learner:specialized_learners\
+  && bazel-bin/ydf/learner/specialized_learners_generator\
+  > ydf/learner/specialized_learners_pre_generated.py
+
+Please don't change this file directly. Instead, changes the source. The
+documentation source is contained in the "GetGenericHyperParameterSpecification"
+method of each learner e.g. GetGenericHyperParameterSpecification in
+learner/gradient_boosted_trees/gradient_boosted_trees.cc contains the
+documentation (and meta-data) used to generate this file.
+
+In particular, these pre-generated wrappers included in the source code are 
+included for reference only. The actual wrappers are re-generated during
+compilation.
+"""
+
+from typing import Dict, Optional, Sequence, Union
+
+from ydf.proto.dataset import data_spec_pb2
+from ydf.proto.learner import abstract_learner_pb2
+from ydf.dataset import dataset
+from ydf.dataset import dataspec
+from ydf.learner import custom_loss
+from ydf.learner import generic_learner
+from ydf.learner import hyperparameters
+from ydf.learner import tuner as tuner_lib
+from ydf.model import generic_model
+from ydf.model.gradient_boosted_trees_model import gradient_boosted_trees_model
+from ydf.model.random_forest_model import random_forest_model
+
+
+
+class CartLearner(generic_learner.GenericLearner):
+  r"""Cart learning algorithm.
+
+  A CART (Classification and Regression Trees) a decision tree. The non-leaf
+  nodes contains conditions (also known as splits) while the leaf nodes contain
+  prediction values. The training dataset is divided in two parts. The first is
+  used to grow the tree while the second is used to prune the tree.
+
+  Usage example:
+
+  ```python
+  import ydf
+  import pandas as pd
+
+  dataset = pd.read_csv("project/dataset.csv")
+
+  model = ydf.CartLearner().train(dataset)
+
+  print(model.summary())
+  ```
+
+  Hyperparameters are configured to give reasonable results for typical
+  datasets. Hyperparameters can also be modified manually (see descriptions)
+  below or by applying the hyperparameter templates available with
+  `CartLearner.hyperparameter_templates()` (see this function's documentation for
+  details).
+
+  Attributes:
+    label: Label of the dataset. The label column
+      should not be identified as a feature in the `features` parameter.
+    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,
+      Task.RANKING, Task.CATEGORICAL_UPLIFT, Task.NUMERICAL_UPLIFT).
+    weights: Name of a feature that identifies the weight of each example. If
+      weights are not specified, unit weights are assumed. The weight column
+      should not be identified as a feature in the `features` parameter.
+    ranking_group: Only for `task=Task.RANKING`. Name of a feature
+      that identifies queries in a query/document ranking task. The ranking
+      group should not be identified as a feature in the `features` parameter.
+    uplift_treatment: Only for `task=Task.CATEGORICAL_UPLIFT` and `task=Task`.
+      NUMERICAL_UPLIFT. Name of a numerical feature that identifies the
+      treatment in an uplift problem. The value 0 is reserved for the control
+      treatment. Currently, only 0/1 binary treatments are supported.
+    features: If None, all columns are used as features. The semantic of the
+      features is determined automatically. Otherwise, if
+      include_all_columns=False (default) only the column listed in `features`
+      are imported. If include_all_columns=True, all the columns are imported as
+      features and only the semantic of the columns NOT in `columns` is 
+      determined automatically. If specified,  defines the order of the features
+      - any non-listed features are appended in-order after the specified
+      features (if include_all_columns=True).
+      The label, weights, uplift treatment and ranking_group columns should not
+      be specified as features.
+    include_all_columns: See `features`.
+    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
+      CATEGORICAL_SET columns stored as strings. If more unique values exist,
+      only the most frequent values are kept, and the remaining values are
+      considered as out-of-vocabulary.
+    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
+      and CATEGORICAL_SET columns. Value observed less than
+      `min_vocab_frequency` are considered as out-of-vocabulary.
+    discretize_numerical_columns: If true, discretize all the numerical columns
+      before training. Discretized numerical columns are faster to train with,
+      but they can have a negative impact on the model quality. Using
+      `discretize_numerical_columns=True` is equivalent as setting the column
+      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
+      definition of DISCRETIZED_NUMERICAL for more details.
+    num_discretized_numerical_bins: Number of bins used when disretizing
+      numerical columns.
+    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
+      inferring the column's semantic if it is not explicitly specified. Only
+      used when reading from file, in-memory datasets are always read in full.
+      Setting this to a lower number will speed up dataset reading, but might
+      result in incorrect column semantics. Set to -1 to scan the entire
+      dataset.
+    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
+      computing a column's statistics. Only used when reading from file,
+      in-memory datasets are always read in full. A column's statistics include
+      the dictionary for categorical features and the mean / min / max for
+      numerical features. Setting this to a lower number will speed up dataset
+      reading, but skew statistics in the dataspec, which can hurt model quality
+      (e.g. if an important category of a categorical feature is considered
+      OOV). Set to -1 to scan the entire dataset.
+    data_spec: Dataspec to be used (advanced). If a data spec is given,
+      `columns`, `include_all_columns`, `max_vocab_count`,
+      `min_vocab_frequency`, `discretize_numerical_columns` and 
+      `num_discretized_numerical_bins` will be ignored.
+    allow_na_conditions: If true, the tree training evaluates conditions of the
+      type `X is NA` i.e. `X is missing`. Default: False.
+    categorical_algorithm: How to learn splits on categorical attributes.
+      - `CART`: CART algorithm. Find categorical splits of the form "value \\in
+        mask". The solution is exact for binary classification, regression and
+        ranking. It is approximated for multi-class classification. This is a
+        good first algorithm to use. In case of overfitting (very small
+        dataset, large dictionary), the "random" algorithm is a good
+        alternative.
+      - `ONE_HOT`: One-hot encoding. Find the optimal categorical split of the
+        form "attribute == param". This method is similar (but more efficient)
+        than converting converting each possible categorical value into a
+        boolean feature. This method is available for comparison purpose and
+        generally performs worse than other alternatives.
+      - `RANDOM`: Best splits among a set of random candidate. Find the a
+        categorical split of the form "value \\in mask" using a random search.
+        This solution can be seen as an approximation of the CART algorithm.
+        This method is a strong alternative to CART. This algorithm is inspired
+        from section "5.1 Categorical Variables" of "Random Forest", 2001.
+        Default: "CART".
+    categorical_set_split_greedy_sampling: For categorical set splits e.g.
+      texts. Probability for a categorical value to be a candidate for the
+      positive set. The sampling is applied once per node (i.e. not at every
+      step of the greedy optimization). Default: 0.1.
+    categorical_set_split_max_num_items: For categorical set splits e.g. texts.
+      Maximum number of items (prior to the sampling). If more items are
+      available, the least frequent items are ignored. Changing this value is
+      similar to change the "max_vocab_count" before loading the dataset, with
+      the following exception: With `max_vocab_count`, all the remaining items
+      are grouped in a special Out-of-vocabulary item. With `max_num_items`,
+      this is not the case. Default: -1.
+    categorical_set_split_min_item_frequency: For categorical set splits e.g.
+      texts. Minimum number of occurrences of an item to be considered.
+      Default: 1.
+    growing_strategy: How to grow the tree.
+      - `LOCAL`: Each node is split independently of the other nodes. In other
+        words, as long as a node satisfy the splits "constraints (e.g. maximum
+        depth, minimum number of observations), the node will be split. This is
+        the "classical" way to grow decision trees.
+      - `BEST_FIRST_GLOBAL`: The node with the best loss reduction among all
+        the nodes of the tree is selected for splitting. This method is also
+        called "best first" or "leaf-wise growth". See "Best-first decision
+        tree learning", Shi and "Additive logistic regression : A statistical
+        view of boosting", Friedman for more details. Default: "LOCAL".
+    honest: In honest trees, different training examples are used to infer the
+      structure and the leaf values. This regularization technique trades
+      examples for bias estimates. It might increase or reduce the quality of
+      the model. See "Generalized Random Forests", Athey et al. In this paper,
+      Honest trees are trained with the Random Forest algorithm with a sampling
+      without replacement. Default: False.
+    honest_fixed_separation: For honest trees only i.e. honest=true. If true, a
+      new random separation is generated for each tree. If false, the same
+      separation is used for all the trees (e.g., in Gradient Boosted Trees
+      containing multiple trees). Default: False.
+    honest_ratio_leaf_examples: For honest trees only i.e. honest=true. Ratio
+      of examples used to set the leaf values. Default: 0.5.
+    in_split_min_examples_check: Whether to check the `min_examples` constraint
+      in the split search (i.e. splits leading to one child having less than
+      `min_examples` examples are considered invalid) or before the split
+      search (i.e. a node can be derived only if it contains more than
+      `min_examples` examples). If false, there can be nodes with less than
+      `min_examples` training examples. Default: True.
+    keep_non_leaf_label_distribution: Whether to keep the node value (i.e. the
+      distribution of the labels of the training examples) of non-leaf nodes.
+      This information is not used during serving, however it can be used for
+      model interpretation as well as hyper parameter tuning. This can take
+      lots of space, sometimes accounting for half of the model size. Default:
+      True.
+    max_depth: Maximum depth of the tree. `max_depth=1` means that all trees
+      will be roots. `max_depth=-1` means that tree depth is not restricted by
+      this parameter. Values <= -2 will be ignored. Default: 16.
+    max_num_nodes: Maximum number of nodes in the tree. Set to -1 to disable
+      this limit. Only available for `growing_strategy=BEST_FIRST_GLOBAL`.
+      Default: None.
+    maximum_model_size_in_memory_in_bytes: Limit the size of the model when
+      stored in ram. Different algorithms can enforce this limit differently.
+      Note that when models are compiled into an inference, the size of the
+      inference engine is generally much smaller than the original model.
+      Default: -1.0.
+    maximum_training_duration_seconds: Maximum training duration of the model
+      expressed in seconds. Each learning algorithm is free to use this
+      parameter at it sees fit. Enabling maximum training duration makes the
+      model training non-deterministic. Default: -1.0.
+    mhld_oblique_max_num_attributes: For MHLD oblique splits i.e.
+      `split_axis=MHLD_OBLIQUE`. Maximum number of attributes in the
+      projection. Increasing this value increases the training time. Decreasing
+      this value acts as a regularization. The value should be in [2,
+      num_numerical_features]. If the value is above the total number of
+      numerical features, the value is capped automatically. The value 1 is
+      allowed but results in ordinary (non-oblique) splits. Default: None.
+    mhld_oblique_sample_attributes: For MHLD oblique splits i.e.
+      `split_axis=MHLD_OBLIQUE`. If true, applies the attribute sampling
+      controlled by the "num_candidate_attributes" or
+      "num_candidate_attributes_ratio" parameters. If false, all the attributes
+      are tested. Default: None.
+    min_examples: Minimum number of examples in a node. Default: 5.
+    missing_value_policy: Method used to handle missing attribute values.
+      - `GLOBAL_IMPUTATION`: Missing attribute values are imputed, with the
+        mean (in case of numerical attribute) or the most-frequent-item (in
+        case of categorical attribute) computed on the entire dataset (i.e. the
+        information contained in the data spec).
+      - `LOCAL_IMPUTATION`: Missing attribute values are imputed with the mean
+        (numerical attribute) or most-frequent-item (in the case of categorical
+        attribute) evaluated on the training examples in the current node.
+      - `RANDOM_LOCAL_IMPUTATION`: Missing attribute values are imputed from
+        randomly sampled values from the training examples in the current node.
+        This method was proposed by Clinic et al. in "Random Survival Forests"
+        (https://projecteuclid.org/download/pdfview_1/euclid.aoas/1223908043).
+        Default: "GLOBAL_IMPUTATION".
+    num_candidate_attributes: Number of unique valid attributes tested for each
+      node. An attribute is valid if it has at least a valid split. If
+      `num_candidate_attributes=0`, the value is set to the classical default
+      value for Random Forest: `sqrt(number of input attributes)` in case of
+      classification and `number_of_input_attributes / 3` in case of
+      regression. If `num_candidate_attributes=-1`, all the attributes are
+      tested. Default: 0.
+    num_candidate_attributes_ratio: Ratio of attributes tested at each node. If
+      set, it is equivalent to `num_candidate_attributes =
+      number_of_input_features x num_candidate_attributes_ratio`. The possible
+      values are between ]0, and 1] as well as -1. If not set or equal to -1,
+      the `num_candidate_attributes` is used. Default: -1.0.
+    pure_serving_model: Clear the model from any information that is not
+      required for model serving. This includes debugging, model interpretation
+      and other meta-data. The size of the serialized model can be reduced
+      significatively (50% model size reduction is common). This parameter has
+      no impact on the quality, serving speed or RAM usage of model serving.
+      Default: False.
+    random_seed: Random seed for the training of the model. Learners are
+      expected to be deterministic by the random seed. Default: 123456.
+    sorting_strategy: How are sorted the numerical features in order to find
+      the splits
+      - PRESORT: The features are pre-sorted at the start of the training. This
+        solution is faster but consumes much more memory than IN_NODE.
+      - IN_NODE: The features are sorted just before being used in the node.
+        This solution is slow but consumes little amount of memory.
+      . Default: "PRESORT".
+    sparse_oblique_max_num_projections: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Maximum number of projections (applied after
+      the num_projections_exponent).
+      Oblique splits try out max(p^num_projections_exponent,
+      max_num_projections) random projections for choosing a split, where p is
+      the number of numerical features. Increasing "max_num_projections"
+      increases the training time but not the inference time. In late stage
+      model development, if every bit of accuracy if important, increase this
+      value.
+      The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
+      does not define this hyperparameter. Default: None.
+    sparse_oblique_normalization: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Normalization applied on the features,
+      before applying the sparse oblique projections.
+      - `NONE`: No normalization.
+      - `STANDARD_DEVIATION`: Normalize the feature by the estimated standard
+        deviation on the entire train dataset. Also known as Z-Score
+        normalization.
+      - `MIN_MAX`: Normalize the feature by the range (i.e. max-min) estimated
+        on the entire train dataset. Default: None.
+    sparse_oblique_num_projections_exponent: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections
+      to test at each node.
+      Increasing this value very likely improves the quality of the model,
+      drastically increases the training time, and doe not impact the inference
+      time.
+      Oblique splits try out max(p^num_projections_exponent,
+      max_num_projections) random projections for choosing a split, where p is
+      the number of numerical features. Therefore, increasing this
+      `num_projections_exponent` and possibly `max_num_projections` may improve
+      model quality, but will also significantly increase training time.
+      Note that the complexity of (classic) Random Forests is roughly
+      proportional to `num_projections_exponent=0.5`, since it considers
+      sqrt(num_features) for a split. The complexity of (classic) GBDT is
+      roughly proportional to `num_projections_exponent=1`, since it considers
+      all features for a split.
+      The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
+      recommends values in [1/4, 2]. Default: None.
+    sparse_oblique_projection_density_factor: Density of the projections as an
+      exponent of the number of features. Independently for each projection,
+      each feature has a probability "projection_density_factor / num_features"
+      to be considered in the projection.
+      The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
+      calls this parameter `lambda` and recommends values in [1, 5].
+      Increasing this value increases training and inference time (on average).
+      This value is best tuned for each dataset. Default: None.
+    sparse_oblique_weights: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Possible values:
+      - `BINARY`: The oblique weights are sampled in {-1,1} (default).
+      - `CONTINUOUS`: The oblique weights are be sampled in [-1,1]. Default:
+        None.
+    split_axis: What structure of split to consider for numerical features.
+      - `AXIS_ALIGNED`: Axis aligned splits (i.e. one condition at a time).
+        This is the "classical" way to train a tree. Default value.
+      - `SPARSE_OBLIQUE`: Sparse oblique splits (i.e. random splits one a small
+        number of features) from "Sparse Projection Oblique Random Forests",
+        Tomita et al., 2020.
+      - `MHLD_OBLIQUE`: Multi-class Hellinger Linear Discriminant splits from
+        "Classification Based on Multivariate Contrast Patterns",
+        Canete-Sifuentes et al., 2029 Default: "AXIS_ALIGNED".
+    uplift_min_examples_in_treatment: For uplift models only. Minimum number of
+      examples per treatment in a node. Default: 5.
+    uplift_split_score: For uplift models only. Splitter score i.e. score
+      optimized by the splitters. The scores are introduced in "Decision trees
+      for uplift modeling with single and multiple treatments", Rzepakowski et
+      al. Notation: `p` probability / average value of the positive outcome,
+      `q` probability / average value in the control group.
+      - `KULLBACK_LEIBLER` or `KL`: - p log (p/q)
+      - `EUCLIDEAN_DISTANCE` or `ED`: (p-q)^2
+      - `CHI_SQUARED` or `CS`: (p-q)^2/q
+        Default: "KULLBACK_LEIBLER".
+    validation_ratio: Ratio of the training dataset used to create the
+      validation dataset for pruning the tree. If set to 0, the entire dataset
+      is used for training, and the tree is not pruned. Default: 0.1.
+
+    num_threads: Number of threads used to train the model. Different learning
+      algorithms use multi-threading differently and with different degree of
+      efficiency. If `None`, `num_threads` will be automatically set to the
+      number of processors (up to a maximum of 32; or set to 6 if the number of
+      processors is not available). Making `num_threads` significantly larger
+      than the number of processors can slow-down the training speed. The
+      default value logic might change in the future.
+    resume_training: If true, the model training resumes from the checkpoint
+      stored in the `working_dir` directory. If `working_dir` does not
+      contain any model checkpoint, the training starts from the beginning.
+      Resuming training is useful in the following situations: (1) The training
+      was interrupted by the user (e.g. ctrl+c or "stop" button in a notebook)
+      or rescheduled, or (2) the hyper-parameter of the learner was changed e.g.
+      increasing the number of trees.
+    working_dir: Path to a directory available for the learning algorithm to
+      store intermediate computation results. Depending on the learning
+      algorithm and parameters, the working_dir might be optional, required, or
+      ignored. For instance, distributed training algorithm always need a
+      "working_dir", and the gradient boosted tree and hyper-parameter tuners
+      will export artefacts to the "working_dir" if provided.
+    resume_training_snapshot_interval_seconds: Indicative number of seconds in 
+      between snapshots when `resume_training=True`. Might be ignored by
+      some learners.
+    tuner: If set, automatically select the best hyperparameters using the
+      provided tuner. When using distributed training, the tuning is
+      distributed.
+    workers: If set, enable distributed training. "workers" is the list of IP
+      addresses of the workers. A worker is a process running
+      `ydf.start_worker(port)`.
+  """
+
+  def __init__(self,
+      label: str,
+      task: generic_learner.Task = generic_learner.Task.CLASSIFICATION,
+      weights: Optional[str] = None,
+      ranking_group: Optional[str] = None,
+      uplift_treatment: Optional[str] = None,
+      features: dataspec.ColumnDefs = None,
+      include_all_columns: bool = False,
+      max_vocab_count: int = 2000,
+      min_vocab_frequency: int = 5,
+      discretize_numerical_columns: bool = False,
+      num_discretized_numerical_bins: int = 255,
+      max_num_scanned_rows_to_infer_semantic: int = 10000,
+      max_num_scanned_rows_to_compute_statistics: int = 10000,
+      data_spec: Optional[data_spec_pb2.DataSpecification] = None,
+      allow_na_conditions: Optional[bool] = False,
+      categorical_algorithm: Optional[str] = "CART",
+      categorical_set_split_greedy_sampling: Optional[float] = 0.1,
+      categorical_set_split_max_num_items: Optional[int] = -1,
+      categorical_set_split_min_item_frequency: Optional[int] = 1,
+      growing_strategy: Optional[str] = "LOCAL",
+      honest: Optional[bool] = False,
+      honest_fixed_separation: Optional[bool] = False,
+      honest_ratio_leaf_examples: Optional[float] = 0.5,
+      in_split_min_examples_check: Optional[bool] = True,
+      keep_non_leaf_label_distribution: Optional[bool] = True,
+      max_depth: Optional[int] = 16,
+      max_num_nodes: Optional[int] = None,
+      maximum_model_size_in_memory_in_bytes: Optional[float] = -1.0,
+      maximum_training_duration_seconds: Optional[float] = -1.0,
+      mhld_oblique_max_num_attributes: Optional[int] = None,
+      mhld_oblique_sample_attributes: Optional[bool] = None,
+      min_examples: Optional[int] = 5,
+      missing_value_policy: Optional[str] = "GLOBAL_IMPUTATION",
+      num_candidate_attributes: Optional[int] = 0,
+      num_candidate_attributes_ratio: Optional[float] = -1.0,
+      pure_serving_model: Optional[bool] = False,
+      random_seed: Optional[int] = 123456,
+      sorting_strategy: Optional[str] = "PRESORT",
+      sparse_oblique_max_num_projections: Optional[int] = None,
+      sparse_oblique_normalization: Optional[str] = None,
+      sparse_oblique_num_projections_exponent: Optional[float] = None,
+      sparse_oblique_projection_density_factor: Optional[float] = None,
+      sparse_oblique_weights: Optional[str] = None,
+      split_axis: Optional[str] = "AXIS_ALIGNED",
+      uplift_min_examples_in_treatment: Optional[int] = 5,
+      uplift_split_score: Optional[str] = "KULLBACK_LEIBLER",
+      validation_ratio: Optional[float] = 0.1,
+      num_threads: Optional[int] = None,
+      working_dir: Optional[str] = None,
+      resume_training: bool = False,
+      resume_training_snapshot_interval_seconds: int = 1800,
+      tuner: Optional[tuner_lib.AbstractTuner] = None,
+      workers: Optional[Sequence[str]] = None,
+      ):
+
+    hyper_parameters = {
+                      "allow_na_conditions" : allow_na_conditions,
+                      "categorical_algorithm" : categorical_algorithm,
+                      "categorical_set_split_greedy_sampling" : categorical_set_split_greedy_sampling,
+                      "categorical_set_split_max_num_items" : categorical_set_split_max_num_items,
+                      "categorical_set_split_min_item_frequency" : categorical_set_split_min_item_frequency,
+                      "growing_strategy" : growing_strategy,
+                      "honest" : honest,
+                      "honest_fixed_separation" : honest_fixed_separation,
+                      "honest_ratio_leaf_examples" : honest_ratio_leaf_examples,
+                      "in_split_min_examples_check" : in_split_min_examples_check,
+                      "keep_non_leaf_label_distribution" : keep_non_leaf_label_distribution,
+                      "max_depth" : max_depth,
+                      "max_num_nodes" : max_num_nodes,
+                      "maximum_model_size_in_memory_in_bytes" : maximum_model_size_in_memory_in_bytes,
+                      "maximum_training_duration_seconds" : maximum_training_duration_seconds,
+                      "mhld_oblique_max_num_attributes" : mhld_oblique_max_num_attributes,
+                      "mhld_oblique_sample_attributes" : mhld_oblique_sample_attributes,
+                      "min_examples" : min_examples,
+                      "missing_value_policy" : missing_value_policy,
+                      "num_candidate_attributes" : num_candidate_attributes,
+                      "num_candidate_attributes_ratio" : num_candidate_attributes_ratio,
+                      "pure_serving_model" : pure_serving_model,
+                      "random_seed" : random_seed,
+                      "sorting_strategy" : sorting_strategy,
+                      "sparse_oblique_max_num_projections" : sparse_oblique_max_num_projections,
+                      "sparse_oblique_normalization" : sparse_oblique_normalization,
+                      "sparse_oblique_num_projections_exponent" : sparse_oblique_num_projections_exponent,
+                      "sparse_oblique_projection_density_factor" : sparse_oblique_projection_density_factor,
+                      "sparse_oblique_weights" : sparse_oblique_weights,
+                      "split_axis" : split_axis,
+                      "uplift_min_examples_in_treatment" : uplift_min_examples_in_treatment,
+                      "uplift_split_score" : uplift_split_score,
+                      "validation_ratio" : validation_ratio,
+
+      }
+
+    data_spec_args = dataspec.DataSpecInferenceArgs(
+        columns=dataspec.normalize_column_defs(features),
+        include_all_columns=include_all_columns,
+        max_vocab_count=max_vocab_count,
+        min_vocab_frequency=min_vocab_frequency,
+        discretize_numerical_columns=discretize_numerical_columns,
+        num_discretized_numerical_bins=num_discretized_numerical_bins,
+        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
+        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
+    )
+
+    deployment_config = self._build_deployment_config(
+        num_threads=num_threads,
+        resume_training=resume_training,
+        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
+        working_dir=working_dir,
+        workers=workers,
+    )
+
+    super().__init__(learner_name="CART",
+      task=task,
+      label=label,
+      weights=weights,
+      ranking_group=ranking_group,
+      uplift_treatment=uplift_treatment,
+      data_spec_args=data_spec_args,
+      data_spec=data_spec,
+      hyper_parameters=hyper_parameters,
+      deployment_config=deployment_config,
+      tuner=tuner,
+    )
+
+  def train(
+      self,
+      ds: dataset.InputDataset,
+      valid: Optional[dataset.InputDataset] = None,
+  ) -> random_forest_model.RandomForestModel:
+    """Trains a model on the given dataset.
+
+    Options for dataset reading are given on the learner. Consult the
+    documentation of the learner or ydf.create_vertical_dataset() for additional
+    information on dataset reading in YDF.
+
+    Usage example:
+
+    ```
+    import ydf
+    import pandas as pd
+
+    train_ds = pd.read_csv(...)
+
+    learner = ydf.CartLearner(label="label")
+    model = learner.train(train_ds)
+    print(model.summary())
+    ```
+
+    If training is interrupted (for example, by interrupting the cell execution
+    in Colab), the model will be returned to the state it was in at the moment
+    of interruption.
+
+    Args:
+      ds: Training dataset.
+      valid: Optional validation dataset. Some learners, such as Random Forest,
+        do not need validation dataset. Some learners, such as
+        GradientBoostedTrees, automatically extract a validation dataset from
+        the training dataset if the validation dataset is not provided.
+
+    Returns:
+      A trained model.
+    """
+    return super().train(ds, valid)
+
+  @classmethod
+  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
+    return abstract_learner_pb2.LearnerCapabilities(
+      support_max_training_duration=True,
+      resume_training=False,
+      support_validation_dataset=False,
+      support_partial_cache_dataset_format=False,
+      support_max_model_size_in_memory=False,
+      support_monotonic_constraints=False,
+    )
+
+  @classmethod
+  def hyperparameter_templates(cls) -> Dict[str, hyperparameters.HyperparameterTemplate]:
+    r"""Hyperparameter templates for this Learner.
+    
+    This learner currently does not provide any hyperparameter templates, this
+    method is provided for consistency with other learners.
+    
+    Returns:
+      Empty dictionary.
+    """
+    return {}
+
+class DistributedGradientBoostedTreesLearner(generic_learner.GenericLearner):
+  r"""Distributed Gradient Boosted Trees learning algorithm.
+
+  Exact distributed version of the Gradient Boosted Tree learning algorithm. See
+  the documentation of the non-distributed Gradient Boosted Tree learning
+  algorithm for an introduction to GBTs.
+
+  Usage example:
+
+  ```python
+  import ydf
+  import pandas as pd
+
+  dataset = pd.read_csv("project/dataset.csv")
+
+  model = ydf.DistributedGradientBoostedTreesLearner().train(dataset)
+
+  print(model.summary())
+  ```
+
+  Hyperparameters are configured to give reasonable results for typical
+  datasets. Hyperparameters can also be modified manually (see descriptions)
+  below or by applying the hyperparameter templates available with
+  `DistributedGradientBoostedTreesLearner.hyperparameter_templates()` (see this function's documentation for
+  details).
+
+  Attributes:
+    label: Label of the dataset. The label column
+      should not be identified as a feature in the `features` parameter.
+    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,
+      Task.RANKING, Task.CATEGORICAL_UPLIFT, Task.NUMERICAL_UPLIFT).
+    weights: Name of a feature that identifies the weight of each example. If
+      weights are not specified, unit weights are assumed. The weight column
+      should not be identified as a feature in the `features` parameter.
+    ranking_group: Only for `task=Task.RANKING`. Name of a feature
+      that identifies queries in a query/document ranking task. The ranking
+      group should not be identified as a feature in the `features` parameter.
+    uplift_treatment: Only for `task=Task.CATEGORICAL_UPLIFT` and `task=Task`.
+      NUMERICAL_UPLIFT. Name of a numerical feature that identifies the
+      treatment in an uplift problem. The value 0 is reserved for the control
+      treatment. Currently, only 0/1 binary treatments are supported.
+    features: If None, all columns are used as features. The semantic of the
+      features is determined automatically. Otherwise, if
+      include_all_columns=False (default) only the column listed in `features`
+      are imported. If include_all_columns=True, all the columns are imported as
+      features and only the semantic of the columns NOT in `columns` is 
+      determined automatically. If specified,  defines the order of the features
+      - any non-listed features are appended in-order after the specified
+      features (if include_all_columns=True).
+      The label, weights, uplift treatment and ranking_group columns should not
+      be specified as features.
+    include_all_columns: See `features`.
+    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
+      CATEGORICAL_SET columns stored as strings. If more unique values exist,
+      only the most frequent values are kept, and the remaining values are
+      considered as out-of-vocabulary.
+    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
+      and CATEGORICAL_SET columns. Value observed less than
+      `min_vocab_frequency` are considered as out-of-vocabulary.
+    discretize_numerical_columns: If true, discretize all the numerical columns
+      before training. Discretized numerical columns are faster to train with,
+      but they can have a negative impact on the model quality. Using
+      `discretize_numerical_columns=True` is equivalent as setting the column
+      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
+      definition of DISCRETIZED_NUMERICAL for more details.
+    num_discretized_numerical_bins: Number of bins used when disretizing
+      numerical columns.
+    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
+      inferring the column's semantic if it is not explicitly specified. Only
+      used when reading from file, in-memory datasets are always read in full.
+      Setting this to a lower number will speed up dataset reading, but might
+      result in incorrect column semantics. Set to -1 to scan the entire
+      dataset.
+    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
+      computing a column's statistics. Only used when reading from file,
+      in-memory datasets are always read in full. A column's statistics include
+      the dictionary for categorical features and the mean / min / max for
+      numerical features. Setting this to a lower number will speed up dataset
+      reading, but skew statistics in the dataspec, which can hurt model quality
+      (e.g. if an important category of a categorical feature is considered
+      OOV). Set to -1 to scan the entire dataset.
+    data_spec: Dataspec to be used (advanced). If a data spec is given,
+      `columns`, `include_all_columns`, `max_vocab_count`,
+      `min_vocab_frequency`, `discretize_numerical_columns` and 
+      `num_discretized_numerical_bins` will be ignored.
+    apply_link_function: If true, applies the link function (a.k.a. activation
+      function), if any, before returning the model prediction. If false,
+      returns the pre-link function model output.
+      For example, in the case of binary classification, the pre-link function
+      output is a logic while the post-link function is a probability. Default:
+      True.
+    force_numerical_discretization: If false, only the numerical column
+      safisfying "max_unique_values_for_discretized_numerical" will be
+      discretized. If true, all the numerical columns will be discretized.
+      Columns with more than "max_unique_values_for_discretized_numerical"
+      unique values will be approximated with
+      "max_unique_values_for_discretized_numerical" bins. This parameter will
+      impact the model training. Default: False.
+    max_depth: Maximum depth of the tree. `max_depth=1` means that all trees
+      will be roots. `max_depth=-1` means that tree depth is not restricted by
+      this parameter. Values <= -2 will be ignored. Default: 6.
+    max_unique_values_for_discretized_numerical: Maximum number of unique value
+      of a numerical feature to allow its pre-discretization. In case of large
+      datasets, discretized numerical features with a small number of unique
+      values are more efficient to learn than classical / non-discretized
+      numerical features. This parameter does not impact the final model.
+      However, it can speed-up or slown the training. Default: 16000.
+    maximum_model_size_in_memory_in_bytes: Limit the size of the model when
+      stored in ram. Different algorithms can enforce this limit differently.
+      Note that when models are compiled into an inference, the size of the
+      inference engine is generally much smaller than the original model.
+      Default: -1.0.
+    maximum_training_duration_seconds: Maximum training duration of the model
+      expressed in seconds. Each learning algorithm is free to use this
+      parameter at it sees fit. Enabling maximum training duration makes the
+      model training non-deterministic. Default: -1.0.
+    min_examples: Minimum number of examples in a node. Default: 5.
+    num_candidate_attributes: Number of unique valid attributes tested for each
+      node. An attribute is valid if it has at least a valid split. If
+      `num_candidate_attributes=0`, the value is set to the classical default
+      value for Random Forest: `sqrt(number of input attributes)` in case of
+      classification and `number_of_input_attributes / 3` in case of
+      regression. If `num_candidate_attributes=-1`, all the attributes are
+      tested. Default: -1.
+    num_candidate_attributes_ratio: Ratio of attributes tested at each node. If
+      set, it is equivalent to `num_candidate_attributes =
+      number_of_input_features x num_candidate_attributes_ratio`. The possible
+      values are between ]0, and 1] as well as -1. If not set or equal to -1,
+      the `num_candidate_attributes` is used. Default: -1.0.
+    num_trees: Maximum number of decision trees. The effective number of
+      trained tree can be smaller if early stopping is enabled. Default: 300.
+    pure_serving_model: Clear the model from any information that is not
+      required for model serving. This includes debugging, model interpretation
+      and other meta-data. The size of the serialized model can be reduced
+      significatively (50% model size reduction is common). This parameter has
+      no impact on the quality, serving speed or RAM usage of model serving.
+      Default: False.
+    random_seed: Random seed for the training of the model. Learners are
+      expected to be deterministic by the random seed. Default: 123456.
+    shrinkage: Coefficient applied to each tree prediction. A small value
+      (0.02) tends to give more accurate results (assuming enough trees are
+      trained), but results in larger models. Analogous to neural network
+      learning rate. Default: 0.1.
+    use_hessian_gain: Use true, uses a formulation of split gain with a hessian
+      term i.e. optimizes the splits to minimize the variance of "gradient /
+      hessian. Available for all losses except regression. Default: False.
+    worker_logs: If true, workers will print training logs. Default: True.
+
+    num_threads: Number of threads used to train the model. Different learning
+      algorithms use multi-threading differently and with different degree of
+      efficiency. If `None`, `num_threads` will be automatically set to the
+      number of processors (up to a maximum of 32; or set to 6 if the number of
+      processors is not available). Making `num_threads` significantly larger
+      than the number of processors can slow-down the training speed. The
+      default value logic might change in the future.
+    resume_training: If true, the model training resumes from the checkpoint
+      stored in the `working_dir` directory. If `working_dir` does not
+      contain any model checkpoint, the training starts from the beginning.
+      Resuming training is useful in the following situations: (1) The training
+      was interrupted by the user (e.g. ctrl+c or "stop" button in a notebook)
+      or rescheduled, or (2) the hyper-parameter of the learner was changed e.g.
+      increasing the number of trees.
+    working_dir: Path to a directory available for the learning algorithm to
+      store intermediate computation results. Depending on the learning
+      algorithm and parameters, the working_dir might be optional, required, or
+      ignored. For instance, distributed training algorithm always need a
+      "working_dir", and the gradient boosted tree and hyper-parameter tuners
+      will export artefacts to the "working_dir" if provided.
+    resume_training_snapshot_interval_seconds: Indicative number of seconds in 
+      between snapshots when `resume_training=True`. Might be ignored by
+      some learners.
+    tuner: If set, automatically select the best hyperparameters using the
+      provided tuner. When using distributed training, the tuning is
+      distributed.
+    workers: If set, enable distributed training. "workers" is the list of IP
+      addresses of the workers. A worker is a process running
+      `ydf.start_worker(port)`.
+  """
+
+  def __init__(self,
+      label: str,
+      task: generic_learner.Task = generic_learner.Task.CLASSIFICATION,
+      weights: Optional[str] = None,
+      ranking_group: Optional[str] = None,
+      uplift_treatment: Optional[str] = None,
+      features: dataspec.ColumnDefs = None,
+      include_all_columns: bool = False,
+      max_vocab_count: int = 2000,
+      min_vocab_frequency: int = 5,
+      discretize_numerical_columns: bool = False,
+      num_discretized_numerical_bins: int = 255,
+      max_num_scanned_rows_to_infer_semantic: int = 10000,
+      max_num_scanned_rows_to_compute_statistics: int = 10000,
+      data_spec: Optional[data_spec_pb2.DataSpecification] = None,
+      apply_link_function: Optional[bool] = True,
+      force_numerical_discretization: Optional[bool] = False,
+      max_depth: Optional[int] = 6,
+      max_unique_values_for_discretized_numerical: Optional[int] = 16000,
+      maximum_model_size_in_memory_in_bytes: Optional[float] = -1.0,
+      maximum_training_duration_seconds: Optional[float] = -1.0,
+      min_examples: Optional[int] = 5,
+      num_candidate_attributes: Optional[int] = -1,
+      num_candidate_attributes_ratio: Optional[float] = -1.0,
+      num_trees: Optional[int] = 300,
+      pure_serving_model: Optional[bool] = False,
+      random_seed: Optional[int] = 123456,
+      shrinkage: Optional[float] = 0.1,
+      use_hessian_gain: Optional[bool] = False,
+      worker_logs: Optional[bool] = True,
+      num_threads: Optional[int] = None,
+      working_dir: Optional[str] = None,
+      resume_training: bool = False,
+      resume_training_snapshot_interval_seconds: int = 1800,
+      tuner: Optional[tuner_lib.AbstractTuner] = None,
+      workers: Optional[Sequence[str]] = None,
+      ):
+
+    hyper_parameters = {
+                      "apply_link_function" : apply_link_function,
+                      "force_numerical_discretization" : force_numerical_discretization,
+                      "max_depth" : max_depth,
+                      "max_unique_values_for_discretized_numerical" : max_unique_values_for_discretized_numerical,
+                      "maximum_model_size_in_memory_in_bytes" : maximum_model_size_in_memory_in_bytes,
+                      "maximum_training_duration_seconds" : maximum_training_duration_seconds,
+                      "min_examples" : min_examples,
+                      "num_candidate_attributes" : num_candidate_attributes,
+                      "num_candidate_attributes_ratio" : num_candidate_attributes_ratio,
+                      "num_trees" : num_trees,
+                      "pure_serving_model" : pure_serving_model,
+                      "random_seed" : random_seed,
+                      "shrinkage" : shrinkage,
+                      "use_hessian_gain" : use_hessian_gain,
+                      "worker_logs" : worker_logs,
+
+      }
+
+    data_spec_args = dataspec.DataSpecInferenceArgs(
+        columns=dataspec.normalize_column_defs(features),
+        include_all_columns=include_all_columns,
+        max_vocab_count=max_vocab_count,
+        min_vocab_frequency=min_vocab_frequency,
+        discretize_numerical_columns=discretize_numerical_columns,
+        num_discretized_numerical_bins=num_discretized_numerical_bins,
+        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
+        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
+    )
+
+    deployment_config = self._build_deployment_config(
+        num_threads=num_threads,
+        resume_training=resume_training,
+        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
+        working_dir=working_dir,
+        workers=workers,
+    )
+
+    super().__init__(learner_name="DISTRIBUTED_GRADIENT_BOOSTED_TREES",
+      task=task,
+      label=label,
+      weights=weights,
+      ranking_group=ranking_group,
+      uplift_treatment=uplift_treatment,
+      data_spec_args=data_spec_args,
+      data_spec=data_spec,
+      hyper_parameters=hyper_parameters,
+      deployment_config=deployment_config,
+      tuner=tuner,
+    )
+
+  def train(
+      self,
+      ds: dataset.InputDataset,
+      valid: Optional[dataset.InputDataset] = None,
+  ) -> gradient_boosted_trees_model.GradientBoostedTreesModel:
+    """Trains a model on the given dataset.
+
+    Options for dataset reading are given on the learner. Consult the
+    documentation of the learner or ydf.create_vertical_dataset() for additional
+    information on dataset reading in YDF.
+
+    Usage example:
+
+    ```
+    import ydf
+    import pandas as pd
+
+    train_ds = pd.read_csv(...)
+
+    learner = ydf.DistributedGradientBoostedTreesLearner(label="label")
+    model = learner.train(train_ds)
+    print(model.summary())
+    ```
+
+    If training is interrupted (for example, by interrupting the cell execution
+    in Colab), the model will be returned to the state it was in at the moment
+    of interruption.
+
+    Args:
+      ds: Training dataset.
+      valid: Optional validation dataset. Some learners, such as Random Forest,
+        do not need validation dataset. Some learners, such as
+        GradientBoostedTrees, automatically extract a validation dataset from
+        the training dataset if the validation dataset is not provided.
+
+    Returns:
+      A trained model.
+    """
+    return super().train(ds, valid)
+
+  @classmethod
+  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
+    return abstract_learner_pb2.LearnerCapabilities(
+      support_max_training_duration=False,
+      resume_training=True,
+      support_validation_dataset=False,
+      support_partial_cache_dataset_format=True,
+      support_max_model_size_in_memory=False,
+      support_monotonic_constraints=False,
+    )
+
+  @classmethod
+  def hyperparameter_templates(cls) -> Dict[str, hyperparameters.HyperparameterTemplate]:
+    r"""Hyperparameter templates for this Learner.
+    
+    This learner currently does not provide any hyperparameter templates, this
+    method is provided for consistency with other learners.
+    
+    Returns:
+      Empty dictionary.
+    """
+    return {}
+
+class GradientBoostedTreesLearner(generic_learner.GenericLearner):
+  r"""Gradient Boosted Trees learning algorithm.
+
+  A [Gradient Boosted Trees](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)
+  (GBT), also known as Gradient Boosted Decision Trees (GBDT) or Gradient
+  Boosted Machines (GBM),  is a set of shallow decision trees trained
+  sequentially. Each tree is trained to predict and then "correct" for the
+  errors of the previously trained trees (more precisely each tree predict the
+  gradient of the loss relative to the model output).
+
+  Usage example:
+
+  ```python
+  import ydf
+  import pandas as pd
+
+  dataset = pd.read_csv("project/dataset.csv")
+
+  model = ydf.GradientBoostedTreesLearner().train(dataset)
+
+  print(model.summary())
+  ```
+
+  Hyperparameters are configured to give reasonable results for typical
+  datasets. Hyperparameters can also be modified manually (see descriptions)
+  below or by applying the hyperparameter templates available with
+  `GradientBoostedTreesLearner.hyperparameter_templates()` (see this function's documentation for
+  details).
+
+  Attributes:
+    label: Label of the dataset. The label column
+      should not be identified as a feature in the `features` parameter.
+    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,
+      Task.RANKING, Task.CATEGORICAL_UPLIFT, Task.NUMERICAL_UPLIFT).
+    weights: Name of a feature that identifies the weight of each example. If
+      weights are not specified, unit weights are assumed. The weight column
+      should not be identified as a feature in the `features` parameter.
+    ranking_group: Only for `task=Task.RANKING`. Name of a feature
+      that identifies queries in a query/document ranking task. The ranking
+      group should not be identified as a feature in the `features` parameter.
+    uplift_treatment: Only for `task=Task.CATEGORICAL_UPLIFT` and `task=Task`.
+      NUMERICAL_UPLIFT. Name of a numerical feature that identifies the
+      treatment in an uplift problem. The value 0 is reserved for the control
+      treatment. Currently, only 0/1 binary treatments are supported.
+    features: If None, all columns are used as features. The semantic of the
+      features is determined automatically. Otherwise, if
+      include_all_columns=False (default) only the column listed in `features`
+      are imported. If include_all_columns=True, all the columns are imported as
+      features and only the semantic of the columns NOT in `columns` is 
+      determined automatically. If specified,  defines the order of the features
+      - any non-listed features are appended in-order after the specified
+      features (if include_all_columns=True).
+      The label, weights, uplift treatment and ranking_group columns should not
+      be specified as features.
+    include_all_columns: See `features`.
+    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
+      CATEGORICAL_SET columns stored as strings. If more unique values exist,
+      only the most frequent values are kept, and the remaining values are
+      considered as out-of-vocabulary.
+    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
+      and CATEGORICAL_SET columns. Value observed less than
+      `min_vocab_frequency` are considered as out-of-vocabulary.
+    discretize_numerical_columns: If true, discretize all the numerical columns
+      before training. Discretized numerical columns are faster to train with,
+      but they can have a negative impact on the model quality. Using
+      `discretize_numerical_columns=True` is equivalent as setting the column
+      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
+      definition of DISCRETIZED_NUMERICAL for more details.
+    num_discretized_numerical_bins: Number of bins used when disretizing
+      numerical columns.
+    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
+      inferring the column's semantic if it is not explicitly specified. Only
+      used when reading from file, in-memory datasets are always read in full.
+      Setting this to a lower number will speed up dataset reading, but might
+      result in incorrect column semantics. Set to -1 to scan the entire
+      dataset.
+    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
+      computing a column's statistics. Only used when reading from file,
+      in-memory datasets are always read in full. A column's statistics include
+      the dictionary for categorical features and the mean / min / max for
+      numerical features. Setting this to a lower number will speed up dataset
+      reading, but skew statistics in the dataspec, which can hurt model quality
+      (e.g. if an important category of a categorical feature is considered
+      OOV). Set to -1 to scan the entire dataset.
+    data_spec: Dataspec to be used (advanced). If a data spec is given,
+      `columns`, `include_all_columns`, `max_vocab_count`,
+      `min_vocab_frequency`, `discretize_numerical_columns` and 
+      `num_discretized_numerical_bins` will be ignored.
+    adapt_subsample_for_maximum_training_duration: Control how the maximum
+      training duration (if set) is applied. If false, the training stop when
+      the time is used. If true, the size of the sampled datasets used train
+      individual trees are adapted dynamically so that all the trees are
+      trained in time. Default: False.
+    allow_na_conditions: If true, the tree training evaluates conditions of the
+      type `X is NA` i.e. `X is missing`. Default: False.
+    apply_link_function: If true, applies the link function (a.k.a. activation
+      function), if any, before returning the model prediction. If false,
+      returns the pre-link function model output.
+      For example, in the case of binary classification, the pre-link function
+      output is a logic while the post-link function is a probability. Default:
+      True.
+    categorical_algorithm: How to learn splits on categorical attributes.
+      - `CART`: CART algorithm. Find categorical splits of the form "value \\in
+        mask". The solution is exact for binary classification, regression and
+        ranking. It is approximated for multi-class classification. This is a
+        good first algorithm to use. In case of overfitting (very small
+        dataset, large dictionary), the "random" algorithm is a good
+        alternative.
+      - `ONE_HOT`: One-hot encoding. Find the optimal categorical split of the
+        form "attribute == param". This method is similar (but more efficient)
+        than converting converting each possible categorical value into a
+        boolean feature. This method is available for comparison purpose and
+        generally performs worse than other alternatives.
+      - `RANDOM`: Best splits among a set of random candidate. Find the a
+        categorical split of the form "value \\in mask" using a random search.
+        This solution can be seen as an approximation of the CART algorithm.
+        This method is a strong alternative to CART. This algorithm is inspired
+        from section "5.1 Categorical Variables" of "Random Forest", 2001.
+        Default: "CART".
+    categorical_set_split_greedy_sampling: For categorical set splits e.g.
+      texts. Probability for a categorical value to be a candidate for the
+      positive set. The sampling is applied once per node (i.e. not at every
+      step of the greedy optimization). Default: 0.1.
+    categorical_set_split_max_num_items: For categorical set splits e.g. texts.
+      Maximum number of items (prior to the sampling). If more items are
+      available, the least frequent items are ignored. Changing this value is
+      similar to change the "max_vocab_count" before loading the dataset, with
+      the following exception: With `max_vocab_count`, all the remaining items
+      are grouped in a special Out-of-vocabulary item. With `max_num_items`,
+      this is not the case. Default: -1.
+    categorical_set_split_min_item_frequency: For categorical set splits e.g.
+      texts. Minimum number of occurrences of an item to be considered.
+      Default: 1.
+    compute_permutation_variable_importance: If true, compute the permutation
+      variable importance of the model at the end of the training using the
+      validation dataset. Enabling this feature can increase the training time
+      significantly. Default: False.
+    dart_dropout: Dropout rate applied when using the DART i.e. when
+      forest_extraction=DART. Default: 0.01.
+    early_stopping: Early stopping detects the overfitting of the model and
+      halts it training using the validation dataset. If not provided directly,
+      the validation dataset is extracted from the training dataset (see
+      "validation_ratio" parameter):
+      - `NONE`: No early stopping. All the num_trees are trained and kept.
+      - `MIN_LOSS_FINAL`: All the num_trees are trained. The model is then
+        truncated to minimize the validation loss i.e. some of the trees are
+        discarded as to minimum the validation loss.
+      - `LOSS_INCREASE`: Classical early stopping. Stop the training when the
+        validation does not decrease for `early_stopping_num_trees_look_ahead`
+        trees. Default: "LOSS_INCREASE".
+    early_stopping_initial_iteration: 0-based index of the first iteration
+      considered for early stopping computation. Increasing this value prevents
+      too early stopping due to noisy initial iterations of the learner.
+      Default: 10.
+    early_stopping_num_trees_look_ahead: Rolling number of trees used to detect
+      validation loss increase and trigger early stopping. Default: 30.
+    focal_loss_alpha: EXPERIMENTAL. Weighting parameter for focal loss,
+      positive samples weighted by alpha, negative samples by (1-alpha). The
+      default 0.5 value means no active class-level weighting. Only used with
+      focal loss i.e. `loss="BINARY_FOCAL_LOSS"` Default: 0.5.
+    focal_loss_gamma: EXPERIMENTAL. Exponent of the misprediction exponent term
+      in focal loss, corresponds to gamma parameter in
+      https://arxiv.org/pdf/1708.02002.pdf. Only used with focal loss i.e.
+      `loss="BINARY_FOCAL_LOSS"` Default: 2.0.
+    forest_extraction: How to construct the forest:
+      - MART: For Multiple Additive Regression Trees. The "classical" way to
+        build a GBDT i.e. each tree tries to "correct" the mistakes of the
+        previous trees.
+      - DART: For Dropout Additive Regression Trees. A modification of MART
+        proposed in http://proceedings.mlr.press/v38/korlakaivinayak15.pdf.
+        Here, each tree tries to "correct" the mistakes of a random subset of
+        the previous trees. Default: "MART".
+    goss_alpha: Alpha parameter for the GOSS (Gradient-based One-Side Sampling;
+      "See LightGBM: A Highly Efficient Gradient Boosting Decision Tree")
+      sampling method. Default: 0.2.
+    goss_beta: Beta parameter for the GOSS (Gradient-based One-Side Sampling)
+      sampling method. Default: 0.1.
+    growing_strategy: How to grow the tree.
+      - `LOCAL`: Each node is split independently of the other nodes. In other
+        words, as long as a node satisfy the splits "constraints (e.g. maximum
+        depth, minimum number of observations), the node will be split. This is
+        the "classical" way to grow decision trees.
+      - `BEST_FIRST_GLOBAL`: The node with the best loss reduction among all
+        the nodes of the tree is selected for splitting. This method is also
+        called "best first" or "leaf-wise growth". See "Best-first decision
+        tree learning", Shi and "Additive logistic regression : A statistical
+        view of boosting", Friedman for more details. Default: "LOCAL".
+    honest: In honest trees, different training examples are used to infer the
+      structure and the leaf values. This regularization technique trades
+      examples for bias estimates. It might increase or reduce the quality of
+      the model. See "Generalized Random Forests", Athey et al. In this paper,
+      Honest trees are trained with the Random Forest algorithm with a sampling
+      without replacement. Default: False.
+    honest_fixed_separation: For honest trees only i.e. honest=true. If true, a
+      new random separation is generated for each tree. If false, the same
+      separation is used for all the trees (e.g., in Gradient Boosted Trees
+      containing multiple trees). Default: False.
+    honest_ratio_leaf_examples: For honest trees only i.e. honest=true. Ratio
+      of examples used to set the leaf values. Default: 0.5.
+    in_split_min_examples_check: Whether to check the `min_examples` constraint
+      in the split search (i.e. splits leading to one child having less than
+      `min_examples` examples are considered invalid) or before the split
+      search (i.e. a node can be derived only if it contains more than
+      `min_examples` examples). If false, there can be nodes with less than
+      `min_examples` training examples. Default: True.
+    keep_non_leaf_label_distribution: Whether to keep the node value (i.e. the
+      distribution of the labels of the training examples) of non-leaf nodes.
+      This information is not used during serving, however it can be used for
+      model interpretation as well as hyper parameter tuning. This can take
+      lots of space, sometimes accounting for half of the model size. Default:
+      True.
+    l1_regularization: L1 regularization applied to the training loss. Impact
+      the tree structures and lead values. Default: 0.0.
+    l2_categorical_regularization: L2 regularization applied to the training
+      loss for categorical features. Impact the tree structures and lead
+      values. Default: 1.0.
+    l2_regularization: L2 regularization applied to the training loss for all
+      features except the categorical ones. Default: 0.0.
+    lambda_loss: Lambda regularization applied to certain training loss
+      functions. Only for NDCG loss. Default: 1.0.
+    loss: The loss optimized by the model. If not specified (DEFAULT) the loss
+      is selected automatically according to the \\"task\\" and label
+      statistics. For example, if task=CLASSIFICATION and the label has two
+      possible values, the loss will be set to BINOMIAL_LOG_LIKELIHOOD.
+      Possible values are:
+      - `DEFAULT`: Select the loss automatically according to the task and
+        label statistics.
+      - `BINOMIAL_LOG_LIKELIHOOD`: Binomial log likelihood. Only valid for
+        binary classification.
+      - `SQUARED_ERROR`: Least square loss. Only valid for regression.
+      - `POISSON`: Poisson log likelihood loss. Mainly used for counting
+        problems. Only valid for regression.
+      - `MULTINOMIAL_LOG_LIKELIHOOD`: Multinomial log likelihood i.e.
+        cross-entropy. Only valid for binary or multi-class classification.
+      - `LAMBDA_MART_NDCG5`: LambdaMART with NDCG5.
+      - `XE_NDCG_MART`:  Cross Entropy Loss NDCG. See arxiv.org/abs/1911.09798.
+      - `BINARY_FOCAL_LOSS`: Focal loss. Only valid for binary classification.
+        See https://arxiv.org/pdf/1708.02002.pdf.
+      - `POISSON`: Poisson log likelihood. Only valid for regression.
+      - `MEAN_AVERAGE_ERROR`: Mean average error a.k.a. MAE. For custom losses, pass the loss object here. Note that when using custom losses, the link function is deactivated (aka apply_link_function is always False).
+        Default: "DEFAULT".
+    max_depth: Maximum depth of the tree. `max_depth=1` means that all trees
+      will be roots. `max_depth=-1` means that tree depth is not restricted by
+      this parameter. Values <= -2 will be ignored. Default: 6.
+    max_num_nodes: Maximum number of nodes in the tree. Set to -1 to disable
+      this limit. Only available for `growing_strategy=BEST_FIRST_GLOBAL`.
+      Default: None.
+    maximum_model_size_in_memory_in_bytes: Limit the size of the model when
+      stored in ram. Different algorithms can enforce this limit differently.
+      Note that when models are compiled into an inference, the size of the
+      inference engine is generally much smaller than the original model.
+      Default: -1.0.
+    maximum_training_duration_seconds: Maximum training duration of the model
+      expressed in seconds. Each learning algorithm is free to use this
+      parameter at it sees fit. Enabling maximum training duration makes the
+      model training non-deterministic. Default: -1.0.
+    mhld_oblique_max_num_attributes: For MHLD oblique splits i.e.
+      `split_axis=MHLD_OBLIQUE`. Maximum number of attributes in the
+      projection. Increasing this value increases the training time. Decreasing
+      this value acts as a regularization. The value should be in [2,
+      num_numerical_features]. If the value is above the total number of
+      numerical features, the value is capped automatically. The value 1 is
+      allowed but results in ordinary (non-oblique) splits. Default: None.
+    mhld_oblique_sample_attributes: For MHLD oblique splits i.e.
+      `split_axis=MHLD_OBLIQUE`. If true, applies the attribute sampling
+      controlled by the "num_candidate_attributes" or
+      "num_candidate_attributes_ratio" parameters. If false, all the attributes
+      are tested. Default: None.
+    min_examples: Minimum number of examples in a node. Default: 5.
+    missing_value_policy: Method used to handle missing attribute values.
+      - `GLOBAL_IMPUTATION`: Missing attribute values are imputed, with the
+        mean (in case of numerical attribute) or the most-frequent-item (in
+        case of categorical attribute) computed on the entire dataset (i.e. the
+        information contained in the data spec).
+      - `LOCAL_IMPUTATION`: Missing attribute values are imputed with the mean
+        (numerical attribute) or most-frequent-item (in the case of categorical
+        attribute) evaluated on the training examples in the current node.
+      - `RANDOM_LOCAL_IMPUTATION`: Missing attribute values are imputed from
+        randomly sampled values from the training examples in the current node.
+        This method was proposed by Clinic et al. in "Random Survival Forests"
+        (https://projecteuclid.org/download/pdfview_1/euclid.aoas/1223908043).
+        Default: "GLOBAL_IMPUTATION".
+    num_candidate_attributes: Number of unique valid attributes tested for each
+      node. An attribute is valid if it has at least a valid split. If
+      `num_candidate_attributes=0`, the value is set to the classical default
+      value for Random Forest: `sqrt(number of input attributes)` in case of
+      classification and `number_of_input_attributes / 3` in case of
+      regression. If `num_candidate_attributes=-1`, all the attributes are
+      tested. Default: -1.
+    num_candidate_attributes_ratio: Ratio of attributes tested at each node. If
+      set, it is equivalent to `num_candidate_attributes =
+      number_of_input_features x num_candidate_attributes_ratio`. The possible
+      values are between ]0, and 1] as well as -1. If not set or equal to -1,
+      the `num_candidate_attributes` is used. Default: -1.0.
+    num_trees: Maximum number of decision trees. The effective number of
+      trained tree can be smaller if early stopping is enabled. Default: 300.
+    pure_serving_model: Clear the model from any information that is not
+      required for model serving. This includes debugging, model interpretation
+      and other meta-data. The size of the serialized model can be reduced
+      significatively (50% model size reduction is common). This parameter has
+      no impact on the quality, serving speed or RAM usage of model serving.
+      Default: False.
+    random_seed: Random seed for the training of the model. Learners are
+      expected to be deterministic by the random seed. Default: 123456.
+    sampling_method: Control the sampling of the datasets used to train
+      individual trees.
+      - NONE: No sampling is applied. This is equivalent to RANDOM sampling
+        with \\"subsample=1\\".
+      - RANDOM (default): Uniform random sampling. Automatically selected if
+        "subsample" is set.
+      - GOSS: Gradient-based One-Side Sampling. Automatically selected if
+        "goss_alpha" or "goss_beta" is set.
+      - SELGB: Selective Gradient Boosting. Automatically selected if
+        "selective_gradient_boosting_ratio" is set. Only valid for ranking.
+        Default: "RANDOM".
+    selective_gradient_boosting_ratio: Ratio of the dataset used to train
+      individual tree for the selective Gradient Boosting (Selective Gradient
+      Boosting for Effective Learning to Rank; Lucchese et al;
+      http://quickrank.isti.cnr.it/selective-data/selective-SIGIR2018.pdf)
+      sampling method. Default: 0.01.
+    shrinkage: Coefficient applied to each tree prediction. A small value
+      (0.02) tends to give more accurate results (assuming enough trees are
+      trained), but results in larger models. Analogous to neural network
+      learning rate. Default: 0.1.
+    sorting_strategy: How are sorted the numerical features in order to find
+      the splits
+      - PRESORT: The features are pre-sorted at the start of the training. This
+        solution is faster but consumes much more memory than IN_NODE.
+      - IN_NODE: The features are sorted just before being used in the node.
+        This solution is slow but consumes little amount of memory.
+      . Default: "PRESORT".
+    sparse_oblique_max_num_projections: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Maximum number of projections (applied after
+      the num_projections_exponent).
+      Oblique splits try out max(p^num_projections_exponent,
+      max_num_projections) random projections for choosing a split, where p is
+      the number of numerical features. Increasing "max_num_projections"
+      increases the training time but not the inference time. In late stage
+      model development, if every bit of accuracy if important, increase this
+      value.
+      The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
+      does not define this hyperparameter. Default: None.
+    sparse_oblique_normalization: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Normalization applied on the features,
+      before applying the sparse oblique projections.
+      - `NONE`: No normalization.
+      - `STANDARD_DEVIATION`: Normalize the feature by the estimated standard
+        deviation on the entire train dataset. Also known as Z-Score
+        normalization.
+      - `MIN_MAX`: Normalize the feature by the range (i.e. max-min) estimated
+        on the entire train dataset. Default: None.
+    sparse_oblique_num_projections_exponent: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections
+      to test at each node.
+      Increasing this value very likely improves the quality of the model,
+      drastically increases the training time, and doe not impact the inference
+      time.
+      Oblique splits try out max(p^num_projections_exponent,
+      max_num_projections) random projections for choosing a split, where p is
+      the number of numerical features. Therefore, increasing this
+      `num_projections_exponent` and possibly `max_num_projections` may improve
+      model quality, but will also significantly increase training time.
+      Note that the complexity of (classic) Random Forests is roughly
+      proportional to `num_projections_exponent=0.5`, since it considers
+      sqrt(num_features) for a split. The complexity of (classic) GBDT is
+      roughly proportional to `num_projections_exponent=1`, since it considers
+      all features for a split.
+      The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
+      recommends values in [1/4, 2]. Default: None.
+    sparse_oblique_projection_density_factor: Density of the projections as an
+      exponent of the number of features. Independently for each projection,
+      each feature has a probability "projection_density_factor / num_features"
+      to be considered in the projection.
+      The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
+      calls this parameter `lambda` and recommends values in [1, 5].
+      Increasing this value increases training and inference time (on average).
+      This value is best tuned for each dataset. Default: None.
+    sparse_oblique_weights: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Possible values:
+      - `BINARY`: The oblique weights are sampled in {-1,1} (default).
+      - `CONTINUOUS`: The oblique weights are be sampled in [-1,1]. Default:
+        None.
+    split_axis: What structure of split to consider for numerical features.
+      - `AXIS_ALIGNED`: Axis aligned splits (i.e. one condition at a time).
+        This is the "classical" way to train a tree. Default value.
+      - `SPARSE_OBLIQUE`: Sparse oblique splits (i.e. random splits one a small
+        number of features) from "Sparse Projection Oblique Random Forests",
+        Tomita et al., 2020.
+      - `MHLD_OBLIQUE`: Multi-class Hellinger Linear Discriminant splits from
+        "Classification Based on Multivariate Contrast Patterns",
+        Canete-Sifuentes et al., 2029 Default: "AXIS_ALIGNED".
+    subsample: Ratio of the dataset (sampling without replacement) used to
+      train individual trees for the random sampling method. If \\"subsample\\"
+      is set and if \\"sampling_method\\" is NOT set or set to \\"NONE\\", then
+      \\"sampling_method\\" is implicitly set to \\"RANDOM\\". In other words,
+      to enable random subsampling, you only need to set "\\"subsample\\".
+      Default: 1.0.
+    uplift_min_examples_in_treatment: For uplift models only. Minimum number of
+      examples per treatment in a node. Default: 5.
+    uplift_split_score: For uplift models only. Splitter score i.e. score
+      optimized by the splitters. The scores are introduced in "Decision trees
+      for uplift modeling with single and multiple treatments", Rzepakowski et
+      al. Notation: `p` probability / average value of the positive outcome,
+      `q` probability / average value in the control group.
+      - `KULLBACK_LEIBLER` or `KL`: - p log (p/q)
+      - `EUCLIDEAN_DISTANCE` or `ED`: (p-q)^2
+      - `CHI_SQUARED` or `CS`: (p-q)^2/q
+        Default: "KULLBACK_LEIBLER".
+    use_hessian_gain: Use true, uses a formulation of split gain with a hessian
+      term i.e. optimizes the splits to minimize the variance of "gradient /
+      hessian. Available for all losses except regression. Default: False.
+    validation_interval_in_trees: Evaluate the model on the validation set
+      every "validation_interval_in_trees" trees. Increasing this value reduce
+      the cost of validation and can impact the early stopping policy (as early
+      stopping is only tested during the validation). Default: 1.
+    validation_ratio: Fraction of the training dataset used for validation if
+      not validation dataset is provided. The validation dataset, whether
+      provided directly or extracted from the training dataset, is used to
+      compute the validation loss, other validation metrics, and possibly
+      trigger early stopping (if enabled). When early stopping is disabled, the
+      validation dataset is only used for monitoring and does not influence the
+      model directly. If the "validation_ratio" is set to 0, early stopping is
+      disabled (i.e., it implies setting early_stopping=NONE). Default: 0.1.
+
+    num_threads: Number of threads used to train the model. Different learning
+      algorithms use multi-threading differently and with different degree of
+      efficiency. If `None`, `num_threads` will be automatically set to the
+      number of processors (up to a maximum of 32; or set to 6 if the number of
+      processors is not available). Making `num_threads` significantly larger
+      than the number of processors can slow-down the training speed. The
+      default value logic might change in the future.
+    resume_training: If true, the model training resumes from the checkpoint
+      stored in the `working_dir` directory. If `working_dir` does not
+      contain any model checkpoint, the training starts from the beginning.
+      Resuming training is useful in the following situations: (1) The training
+      was interrupted by the user (e.g. ctrl+c or "stop" button in a notebook)
+      or rescheduled, or (2) the hyper-parameter of the learner was changed e.g.
+      increasing the number of trees.
+    working_dir: Path to a directory available for the learning algorithm to
+      store intermediate computation results. Depending on the learning
+      algorithm and parameters, the working_dir might be optional, required, or
+      ignored. For instance, distributed training algorithm always need a
+      "working_dir", and the gradient boosted tree and hyper-parameter tuners
+      will export artefacts to the "working_dir" if provided.
+    resume_training_snapshot_interval_seconds: Indicative number of seconds in 
+      between snapshots when `resume_training=True`. Might be ignored by
+      some learners.
+    tuner: If set, automatically select the best hyperparameters using the
+      provided tuner. When using distributed training, the tuning is
+      distributed.
+    workers: If set, enable distributed training. "workers" is the list of IP
+      addresses of the workers. A worker is a process running
+      `ydf.start_worker(port)`.
+  """
+
+  def __init__(self,
+      label: str,
+      task: generic_learner.Task = generic_learner.Task.CLASSIFICATION,
+      weights: Optional[str] = None,
+      ranking_group: Optional[str] = None,
+      uplift_treatment: Optional[str] = None,
+      features: dataspec.ColumnDefs = None,
+      include_all_columns: bool = False,
+      max_vocab_count: int = 2000,
+      min_vocab_frequency: int = 5,
+      discretize_numerical_columns: bool = False,
+      num_discretized_numerical_bins: int = 255,
+      max_num_scanned_rows_to_infer_semantic: int = 10000,
+      max_num_scanned_rows_to_compute_statistics: int = 10000,
+      data_spec: Optional[data_spec_pb2.DataSpecification] = None,
+      adapt_subsample_for_maximum_training_duration: Optional[bool] = False,
+      allow_na_conditions: Optional[bool] = False,
+      apply_link_function: Optional[bool] = True,
+      categorical_algorithm: Optional[str] = "CART",
+      categorical_set_split_greedy_sampling: Optional[float] = 0.1,
+      categorical_set_split_max_num_items: Optional[int] = -1,
+      categorical_set_split_min_item_frequency: Optional[int] = 1,
+      compute_permutation_variable_importance: Optional[bool] = False,
+      dart_dropout: Optional[float] = 0.01,
+      early_stopping: Optional[str] = "LOSS_INCREASE",
+      early_stopping_initial_iteration: Optional[int] = 10,
+      early_stopping_num_trees_look_ahead: Optional[int] = 30,
+      focal_loss_alpha: Optional[float] = 0.5,
+      focal_loss_gamma: Optional[float] = 2.0,
+      forest_extraction: Optional[str] = "MART",
+      goss_alpha: Optional[float] = 0.2,
+      goss_beta: Optional[float] = 0.1,
+      growing_strategy: Optional[str] = "LOCAL",
+      honest: Optional[bool] = False,
+      honest_fixed_separation: Optional[bool] = False,
+      honest_ratio_leaf_examples: Optional[float] = 0.5,
+      in_split_min_examples_check: Optional[bool] = True,
+      keep_non_leaf_label_distribution: Optional[bool] = True,
+      l1_regularization: Optional[float] = 0.0,
+      l2_categorical_regularization: Optional[float] = 1.0,
+      l2_regularization: Optional[float] = 0.0,
+      lambda_loss: Optional[float] = 1.0,
+      loss: Optional[Union[str, custom_loss.AbstractCustomLoss]] = "DEFAULT",
+      max_depth: Optional[int] = 6,
+      max_num_nodes: Optional[int] = None,
+      maximum_model_size_in_memory_in_bytes: Optional[float] = -1.0,
+      maximum_training_duration_seconds: Optional[float] = -1.0,
+      mhld_oblique_max_num_attributes: Optional[int] = None,
+      mhld_oblique_sample_attributes: Optional[bool] = None,
+      min_examples: Optional[int] = 5,
+      missing_value_policy: Optional[str] = "GLOBAL_IMPUTATION",
+      num_candidate_attributes: Optional[int] = -1,
+      num_candidate_attributes_ratio: Optional[float] = -1.0,
+      num_trees: Optional[int] = 300,
+      pure_serving_model: Optional[bool] = False,
+      random_seed: Optional[int] = 123456,
+      sampling_method: Optional[str] = "RANDOM",
+      selective_gradient_boosting_ratio: Optional[float] = 0.01,
+      shrinkage: Optional[float] = 0.1,
+      sorting_strategy: Optional[str] = "PRESORT",
+      sparse_oblique_max_num_projections: Optional[int] = None,
+      sparse_oblique_normalization: Optional[str] = None,
+      sparse_oblique_num_projections_exponent: Optional[float] = None,
+      sparse_oblique_projection_density_factor: Optional[float] = None,
+      sparse_oblique_weights: Optional[str] = None,
+      split_axis: Optional[str] = "AXIS_ALIGNED",
+      subsample: Optional[float] = 1.0,
+      uplift_min_examples_in_treatment: Optional[int] = 5,
+      uplift_split_score: Optional[str] = "KULLBACK_LEIBLER",
+      use_hessian_gain: Optional[bool] = False,
+      validation_interval_in_trees: Optional[int] = 1,
+      validation_ratio: Optional[float] = 0.1,
+      num_threads: Optional[int] = None,
+      working_dir: Optional[str] = None,
+      resume_training: bool = False,
+      resume_training_snapshot_interval_seconds: int = 1800,
+      tuner: Optional[tuner_lib.AbstractTuner] = None,
+      workers: Optional[Sequence[str]] = None,
+      ):
+
+    hyper_parameters = {
+                      "adapt_subsample_for_maximum_training_duration" : adapt_subsample_for_maximum_training_duration,
+                      "allow_na_conditions" : allow_na_conditions,
+                      "apply_link_function" : apply_link_function,
+                      "categorical_algorithm" : categorical_algorithm,
+                      "categorical_set_split_greedy_sampling" : categorical_set_split_greedy_sampling,
+                      "categorical_set_split_max_num_items" : categorical_set_split_max_num_items,
+                      "categorical_set_split_min_item_frequency" : categorical_set_split_min_item_frequency,
+                      "compute_permutation_variable_importance" : compute_permutation_variable_importance,
+                      "dart_dropout" : dart_dropout,
+                      "early_stopping" : early_stopping,
+                      "early_stopping_initial_iteration" : early_stopping_initial_iteration,
+                      "early_stopping_num_trees_look_ahead" : early_stopping_num_trees_look_ahead,
+                      "focal_loss_alpha" : focal_loss_alpha,
+                      "focal_loss_gamma" : focal_loss_gamma,
+                      "forest_extraction" : forest_extraction,
+                      "goss_alpha" : goss_alpha,
+                      "goss_beta" : goss_beta,
+                      "growing_strategy" : growing_strategy,
+                      "honest" : honest,
+                      "honest_fixed_separation" : honest_fixed_separation,
+                      "honest_ratio_leaf_examples" : honest_ratio_leaf_examples,
+                      "in_split_min_examples_check" : in_split_min_examples_check,
+                      "keep_non_leaf_label_distribution" : keep_non_leaf_label_distribution,
+                      "l1_regularization" : l1_regularization,
+                      "l2_categorical_regularization" : l2_categorical_regularization,
+                      "l2_regularization" : l2_regularization,
+                      "lambda_loss" : lambda_loss,
+                      "loss" : loss,
+                      "max_depth" : max_depth,
+                      "max_num_nodes" : max_num_nodes,
+                      "maximum_model_size_in_memory_in_bytes" : maximum_model_size_in_memory_in_bytes,
+                      "maximum_training_duration_seconds" : maximum_training_duration_seconds,
+                      "mhld_oblique_max_num_attributes" : mhld_oblique_max_num_attributes,
+                      "mhld_oblique_sample_attributes" : mhld_oblique_sample_attributes,
+                      "min_examples" : min_examples,
+                      "missing_value_policy" : missing_value_policy,
+                      "num_candidate_attributes" : num_candidate_attributes,
+                      "num_candidate_attributes_ratio" : num_candidate_attributes_ratio,
+                      "num_trees" : num_trees,
+                      "pure_serving_model" : pure_serving_model,
+                      "random_seed" : random_seed,
+                      "sampling_method" : sampling_method,
+                      "selective_gradient_boosting_ratio" : selective_gradient_boosting_ratio,
+                      "shrinkage" : shrinkage,
+                      "sorting_strategy" : sorting_strategy,
+                      "sparse_oblique_max_num_projections" : sparse_oblique_max_num_projections,
+                      "sparse_oblique_normalization" : sparse_oblique_normalization,
+                      "sparse_oblique_num_projections_exponent" : sparse_oblique_num_projections_exponent,
+                      "sparse_oblique_projection_density_factor" : sparse_oblique_projection_density_factor,
+                      "sparse_oblique_weights" : sparse_oblique_weights,
+                      "split_axis" : split_axis,
+                      "subsample" : subsample,
+                      "uplift_min_examples_in_treatment" : uplift_min_examples_in_treatment,
+                      "uplift_split_score" : uplift_split_score,
+                      "use_hessian_gain" : use_hessian_gain,
+                      "validation_interval_in_trees" : validation_interval_in_trees,
+                      "validation_ratio" : validation_ratio,
+
+      }
+
+    data_spec_args = dataspec.DataSpecInferenceArgs(
+        columns=dataspec.normalize_column_defs(features),
+        include_all_columns=include_all_columns,
+        max_vocab_count=max_vocab_count,
+        min_vocab_frequency=min_vocab_frequency,
+        discretize_numerical_columns=discretize_numerical_columns,
+        num_discretized_numerical_bins=num_discretized_numerical_bins,
+        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
+        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
+    )
+
+    deployment_config = self._build_deployment_config(
+        num_threads=num_threads,
+        resume_training=resume_training,
+        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
+        working_dir=working_dir,
+        workers=workers,
+    )
+
+    super().__init__(learner_name="GRADIENT_BOOSTED_TREES",
+      task=task,
+      label=label,
+      weights=weights,
+      ranking_group=ranking_group,
+      uplift_treatment=uplift_treatment,
+      data_spec_args=data_spec_args,
+      data_spec=data_spec,
+      hyper_parameters=hyper_parameters,
+      deployment_config=deployment_config,
+      tuner=tuner,
+    )
+
+  def train(
+      self,
+      ds: dataset.InputDataset,
+      valid: Optional[dataset.InputDataset] = None,
+  ) -> gradient_boosted_trees_model.GradientBoostedTreesModel:
+    """Trains a model on the given dataset.
+
+    Options for dataset reading are given on the learner. Consult the
+    documentation of the learner or ydf.create_vertical_dataset() for additional
+    information on dataset reading in YDF.
+
+    Usage example:
+
+    ```
+    import ydf
+    import pandas as pd
+
+    train_ds = pd.read_csv(...)
+
+    learner = ydf.GradientBoostedTreesLearner(label="label")
+    model = learner.train(train_ds)
+    print(model.summary())
+    ```
+
+    If training is interrupted (for example, by interrupting the cell execution
+    in Colab), the model will be returned to the state it was in at the moment
+    of interruption.
+
+    Args:
+      ds: Training dataset.
+      valid: Optional validation dataset. Some learners, such as Random Forest,
+        do not need validation dataset. Some learners, such as
+        GradientBoostedTrees, automatically extract a validation dataset from
+        the training dataset if the validation dataset is not provided.
+
+    Returns:
+      A trained model.
+    """
+    return super().train(ds, valid)
+
+  @classmethod
+  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
+    return abstract_learner_pb2.LearnerCapabilities(
+      support_max_training_duration=True,
+      resume_training=True,
+      support_validation_dataset=True,
+      support_partial_cache_dataset_format=False,
+      support_max_model_size_in_memory=False,
+      support_monotonic_constraints=True,
+    )
+
+  @classmethod
+  def hyperparameter_templates(cls) -> Dict[str, hyperparameters.HyperparameterTemplate]:
+    r"""Hyperparameter templates for this Learner.
+    
+    Hyperparameter templates are sets of pre-defined hyperparameters for easy
+    access to different variants of the learner. Each template is a mapping to a
+    set of hyperparameters and can be applied directly on the learner.
+    
+    Usage example:
+    ```python
+    templates = ydf.GradientBoostedTreesLearner.hyperparameter_templates()
+    better_defaultv1 = templates["better_defaultv1"]
+    # Print a description of the template
+    print(better_defaultv1.description)
+    # Apply the template's settings on the learner.
+    learner = ydf.GradientBoostedTreesLearner(label, **better_defaultv1)
+    ```
+    
+    Returns:
+      Dictionary of the available templates
+    """
+    return {"better_defaultv1": hyperparameters.HyperparameterTemplate(name="better_default", version=1, description="A configuration that is generally better than the default parameters without being more expensive.", parameters={"growing_strategy" :"BEST_FIRST_GLOBAL"}), "benchmark_rank1v1": hyperparameters.HyperparameterTemplate(name="benchmark_rank1", version=1, description="Top ranking hyper-parameters on our benchmark slightly modified to run in reasonable time.", parameters={"growing_strategy" :"BEST_FIRST_GLOBAL", "categorical_algorithm" :"RANDOM", "split_axis" :"SPARSE_OBLIQUE", "sparse_oblique_normalization" :"MIN_MAX", "sparse_oblique_num_projections_exponent" :1.0}), }
+
+class RandomForestLearner(generic_learner.GenericLearner):
+  r"""Random Forest learning algorithm.
+
+  A Random Forest (https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)
+  is a collection of deep CART decision trees trained independently and without
+  pruning. Each tree is trained on a random subset of the original training 
+  dataset (sampled with replacement).
+  
+  The algorithm is unique in that it is robust to overfitting, even in extreme
+  cases e.g. when there are more features than training examples.
+  
+  It is probably the most well-known of the Decision Forest training
+  algorithms.
+
+  Usage example:
+
+  ```python
+  import ydf
+  import pandas as pd
+
+  dataset = pd.read_csv("project/dataset.csv")
+
+  model = ydf.RandomForestLearner().train(dataset)
+
+  print(model.summary())
+  ```
+
+  Hyperparameters are configured to give reasonable results for typical
+  datasets. Hyperparameters can also be modified manually (see descriptions)
+  below or by applying the hyperparameter templates available with
+  `RandomForestLearner.hyperparameter_templates()` (see this function's documentation for
+  details).
+
+  Attributes:
+    label: Label of the dataset. The label column
+      should not be identified as a feature in the `features` parameter.
+    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,
+      Task.RANKING, Task.CATEGORICAL_UPLIFT, Task.NUMERICAL_UPLIFT).
+    weights: Name of a feature that identifies the weight of each example. If
+      weights are not specified, unit weights are assumed. The weight column
+      should not be identified as a feature in the `features` parameter.
+    ranking_group: Only for `task=Task.RANKING`. Name of a feature
+      that identifies queries in a query/document ranking task. The ranking
+      group should not be identified as a feature in the `features` parameter.
+    uplift_treatment: Only for `task=Task.CATEGORICAL_UPLIFT` and `task=Task`.
+      NUMERICAL_UPLIFT. Name of a numerical feature that identifies the
+      treatment in an uplift problem. The value 0 is reserved for the control
+      treatment. Currently, only 0/1 binary treatments are supported.
+    features: If None, all columns are used as features. The semantic of the
+      features is determined automatically. Otherwise, if
+      include_all_columns=False (default) only the column listed in `features`
+      are imported. If include_all_columns=True, all the columns are imported as
+      features and only the semantic of the columns NOT in `columns` is 
+      determined automatically. If specified,  defines the order of the features
+      - any non-listed features are appended in-order after the specified
+      features (if include_all_columns=True).
+      The label, weights, uplift treatment and ranking_group columns should not
+      be specified as features.
+    include_all_columns: See `features`.
+    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
+      CATEGORICAL_SET columns stored as strings. If more unique values exist,
+      only the most frequent values are kept, and the remaining values are
+      considered as out-of-vocabulary.
+    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
+      and CATEGORICAL_SET columns. Value observed less than
+      `min_vocab_frequency` are considered as out-of-vocabulary.
+    discretize_numerical_columns: If true, discretize all the numerical columns
+      before training. Discretized numerical columns are faster to train with,
+      but they can have a negative impact on the model quality. Using
+      `discretize_numerical_columns=True` is equivalent as setting the column
+      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
+      definition of DISCRETIZED_NUMERICAL for more details.
+    num_discretized_numerical_bins: Number of bins used when disretizing
+      numerical columns.
+    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
+      inferring the column's semantic if it is not explicitly specified. Only
+      used when reading from file, in-memory datasets are always read in full.
+      Setting this to a lower number will speed up dataset reading, but might
+      result in incorrect column semantics. Set to -1 to scan the entire
+      dataset.
+    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
+      computing a column's statistics. Only used when reading from file,
+      in-memory datasets are always read in full. A column's statistics include
+      the dictionary for categorical features and the mean / min / max for
+      numerical features. Setting this to a lower number will speed up dataset
+      reading, but skew statistics in the dataspec, which can hurt model quality
+      (e.g. if an important category of a categorical feature is considered
+      OOV). Set to -1 to scan the entire dataset.
+    data_spec: Dataspec to be used (advanced). If a data spec is given,
+      `columns`, `include_all_columns`, `max_vocab_count`,
+      `min_vocab_frequency`, `discretize_numerical_columns` and 
+      `num_discretized_numerical_bins` will be ignored.
+    adapt_bootstrap_size_ratio_for_maximum_training_duration: Control how the
+      maximum training duration (if set) is applied. If false, the training
+      stop when the time is used. If true, adapts the size of the sampled
+      dataset used to train each tree such that `num_trees` will train within
+      `maximum_training_duration`. Has no effect if there is no maximum
+      training duration specified. Default: False.
+    allow_na_conditions: If true, the tree training evaluates conditions of the
+      type `X is NA` i.e. `X is missing`. Default: False.
+    bootstrap_size_ratio: Number of examples used to train each trees;
+      expressed as a ratio of the training dataset size. Default: 1.0.
+    bootstrap_training_dataset: If true (default), each tree is trained on a
+      separate dataset sampled with replacement from the original dataset. If
+      false, all the trees are trained on the entire same dataset. If
+      bootstrap_training_dataset:false, OOB metrics are not available.
+      bootstrap_training_dataset=false is used in "Extremely randomized trees"
+      (https://link.springer.com/content/pdf/10.1007%2Fs10994-006-6226-1.pdf).
+      Default: True.
+    categorical_algorithm: How to learn splits on categorical attributes.
+      - `CART`: CART algorithm. Find categorical splits of the form "value \\in
+        mask". The solution is exact for binary classification, regression and
+        ranking. It is approximated for multi-class classification. This is a
+        good first algorithm to use. In case of overfitting (very small
+        dataset, large dictionary), the "random" algorithm is a good
+        alternative.
+      - `ONE_HOT`: One-hot encoding. Find the optimal categorical split of the
+        form "attribute == param". This method is similar (but more efficient)
+        than converting converting each possible categorical value into a
+        boolean feature. This method is available for comparison purpose and
+        generally performs worse than other alternatives.
+      - `RANDOM`: Best splits among a set of random candidate. Find the a
+        categorical split of the form "value \\in mask" using a random search.
+        This solution can be seen as an approximation of the CART algorithm.
+        This method is a strong alternative to CART. This algorithm is inspired
+        from section "5.1 Categorical Variables" of "Random Forest", 2001.
+        Default: "CART".
+    categorical_set_split_greedy_sampling: For categorical set splits e.g.
+      texts. Probability for a categorical value to be a candidate for the
+      positive set. The sampling is applied once per node (i.e. not at every
+      step of the greedy optimization). Default: 0.1.
+    categorical_set_split_max_num_items: For categorical set splits e.g. texts.
+      Maximum number of items (prior to the sampling). If more items are
+      available, the least frequent items are ignored. Changing this value is
+      similar to change the "max_vocab_count" before loading the dataset, with
+      the following exception: With `max_vocab_count`, all the remaining items
+      are grouped in a special Out-of-vocabulary item. With `max_num_items`,
+      this is not the case. Default: -1.
+    categorical_set_split_min_item_frequency: For categorical set splits e.g.
+      texts. Minimum number of occurrences of an item to be considered.
+      Default: 1.
+    compute_oob_performances: If true, compute the Out-of-bag evaluation (then
+      available in the summary and model inspector). This evaluation is a cheap
+      alternative to cross-validation evaluation. Default: True.
+    compute_oob_variable_importances: If true, compute the Out-of-bag feature
+      importance (then available in the summary and model inspector). Note that
+      the OOB feature importance can be expensive to compute. Default: False.
+    growing_strategy: How to grow the tree.
+      - `LOCAL`: Each node is split independently of the other nodes. In other
+        words, as long as a node satisfy the splits "constraints (e.g. maximum
+        depth, minimum number of observations), the node will be split. This is
+        the "classical" way to grow decision trees.
+      - `BEST_FIRST_GLOBAL`: The node with the best loss reduction among all
+        the nodes of the tree is selected for splitting. This method is also
+        called "best first" or "leaf-wise growth". See "Best-first decision
+        tree learning", Shi and "Additive logistic regression : A statistical
+        view of boosting", Friedman for more details. Default: "LOCAL".
+    honest: In honest trees, different training examples are used to infer the
+      structure and the leaf values. This regularization technique trades
+      examples for bias estimates. It might increase or reduce the quality of
+      the model. See "Generalized Random Forests", Athey et al. In this paper,
+      Honest trees are trained with the Random Forest algorithm with a sampling
+      without replacement. Default: False.
+    honest_fixed_separation: For honest trees only i.e. honest=true. If true, a
+      new random separation is generated for each tree. If false, the same
+      separation is used for all the trees (e.g., in Gradient Boosted Trees
+      containing multiple trees). Default: False.
+    honest_ratio_leaf_examples: For honest trees only i.e. honest=true. Ratio
+      of examples used to set the leaf values. Default: 0.5.
+    in_split_min_examples_check: Whether to check the `min_examples` constraint
+      in the split search (i.e. splits leading to one child having less than
+      `min_examples` examples are considered invalid) or before the split
+      search (i.e. a node can be derived only if it contains more than
+      `min_examples` examples). If false, there can be nodes with less than
+      `min_examples` training examples. Default: True.
+    keep_non_leaf_label_distribution: Whether to keep the node value (i.e. the
+      distribution of the labels of the training examples) of non-leaf nodes.
+      This information is not used during serving, however it can be used for
+      model interpretation as well as hyper parameter tuning. This can take
+      lots of space, sometimes accounting for half of the model size. Default:
+      True.
+    max_depth: Maximum depth of the tree. `max_depth=1` means that all trees
+      will be roots. `max_depth=-1` means that tree depth is not restricted by
+      this parameter. Values <= -2 will be ignored. Default: 16.
+    max_num_nodes: Maximum number of nodes in the tree. Set to -1 to disable
+      this limit. Only available for `growing_strategy=BEST_FIRST_GLOBAL`.
+      Default: None.
+    maximum_model_size_in_memory_in_bytes: Limit the size of the model when
+      stored in ram. Different algorithms can enforce this limit differently.
+      Note that when models are compiled into an inference, the size of the
+      inference engine is generally much smaller than the original model.
+      Default: -1.0.
+    maximum_training_duration_seconds: Maximum training duration of the model
+      expressed in seconds. Each learning algorithm is free to use this
+      parameter at it sees fit. Enabling maximum training duration makes the
+      model training non-deterministic. Default: -1.0.
+    mhld_oblique_max_num_attributes: For MHLD oblique splits i.e.
+      `split_axis=MHLD_OBLIQUE`. Maximum number of attributes in the
+      projection. Increasing this value increases the training time. Decreasing
+      this value acts as a regularization. The value should be in [2,
+      num_numerical_features]. If the value is above the total number of
+      numerical features, the value is capped automatically. The value 1 is
+      allowed but results in ordinary (non-oblique) splits. Default: None.
+    mhld_oblique_sample_attributes: For MHLD oblique splits i.e.
+      `split_axis=MHLD_OBLIQUE`. If true, applies the attribute sampling
+      controlled by the "num_candidate_attributes" or
+      "num_candidate_attributes_ratio" parameters. If false, all the attributes
+      are tested. Default: None.
+    min_examples: Minimum number of examples in a node. Default: 5.
+    missing_value_policy: Method used to handle missing attribute values.
+      - `GLOBAL_IMPUTATION`: Missing attribute values are imputed, with the
+        mean (in case of numerical attribute) or the most-frequent-item (in
+        case of categorical attribute) computed on the entire dataset (i.e. the
+        information contained in the data spec).
+      - `LOCAL_IMPUTATION`: Missing attribute values are imputed with the mean
+        (numerical attribute) or most-frequent-item (in the case of categorical
+        attribute) evaluated on the training examples in the current node.
+      - `RANDOM_LOCAL_IMPUTATION`: Missing attribute values are imputed from
+        randomly sampled values from the training examples in the current node.
+        This method was proposed by Clinic et al. in "Random Survival Forests"
+        (https://projecteuclid.org/download/pdfview_1/euclid.aoas/1223908043).
+        Default: "GLOBAL_IMPUTATION".
+    num_candidate_attributes: Number of unique valid attributes tested for each
+      node. An attribute is valid if it has at least a valid split. If
+      `num_candidate_attributes=0`, the value is set to the classical default
+      value for Random Forest: `sqrt(number of input attributes)` in case of
+      classification and `number_of_input_attributes / 3` in case of
+      regression. If `num_candidate_attributes=-1`, all the attributes are
+      tested. Default: 0.
+    num_candidate_attributes_ratio: Ratio of attributes tested at each node. If
+      set, it is equivalent to `num_candidate_attributes =
+      number_of_input_features x num_candidate_attributes_ratio`. The possible
+      values are between ]0, and 1] as well as -1. If not set or equal to -1,
+      the `num_candidate_attributes` is used. Default: -1.0.
+    num_oob_variable_importances_permutations: Number of time the dataset is
+      re-shuffled to compute the permutation variable importances. Increasing
+      this value increase the training time (if
+      "compute_oob_variable_importances:true") as well as the stability of the
+      oob variable importance metrics. Default: 1.
+    num_trees: Number of individual decision trees. Increasing the number of
+      trees can increase the quality of the model at the expense of size,
+      training speed, and inference latency. Default: 300.
+    pure_serving_model: Clear the model from any information that is not
+      required for model serving. This includes debugging, model interpretation
+      and other meta-data. The size of the serialized model can be reduced
+      significatively (50% model size reduction is common). This parameter has
+      no impact on the quality, serving speed or RAM usage of model serving.
+      Default: False.
+    random_seed: Random seed for the training of the model. Learners are
+      expected to be deterministic by the random seed. Default: 123456.
+    sampling_with_replacement: If true, the training examples are sampled with
+      replacement. If false, the training samples are sampled without
+      replacement. Only used when "bootstrap_training_dataset=true". If false
+      (sampling without replacement) and if "bootstrap_size_ratio=1" (default),
+      all the examples are used to train all the trees (you probably do not
+      want that). Default: True.
+    sorting_strategy: How are sorted the numerical features in order to find
+      the splits
+      - PRESORT: The features are pre-sorted at the start of the training. This
+        solution is faster but consumes much more memory than IN_NODE.
+      - IN_NODE: The features are sorted just before being used in the node.
+        This solution is slow but consumes little amount of memory.
+      . Default: "PRESORT".
+    sparse_oblique_max_num_projections: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Maximum number of projections (applied after
+      the num_projections_exponent).
+      Oblique splits try out max(p^num_projections_exponent,
+      max_num_projections) random projections for choosing a split, where p is
+      the number of numerical features. Increasing "max_num_projections"
+      increases the training time but not the inference time. In late stage
+      model development, if every bit of accuracy if important, increase this
+      value.
+      The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
+      does not define this hyperparameter. Default: None.
+    sparse_oblique_normalization: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Normalization applied on the features,
+      before applying the sparse oblique projections.
+      - `NONE`: No normalization.
+      - `STANDARD_DEVIATION`: Normalize the feature by the estimated standard
+        deviation on the entire train dataset. Also known as Z-Score
+        normalization.
+      - `MIN_MAX`: Normalize the feature by the range (i.e. max-min) estimated
+        on the entire train dataset. Default: None.
+    sparse_oblique_num_projections_exponent: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections
+      to test at each node.
+      Increasing this value very likely improves the quality of the model,
+      drastically increases the training time, and doe not impact the inference
+      time.
+      Oblique splits try out max(p^num_projections_exponent,
+      max_num_projections) random projections for choosing a split, where p is
+      the number of numerical features. Therefore, increasing this
+      `num_projections_exponent` and possibly `max_num_projections` may improve
+      model quality, but will also significantly increase training time.
+      Note that the complexity of (classic) Random Forests is roughly
+      proportional to `num_projections_exponent=0.5`, since it considers
+      sqrt(num_features) for a split. The complexity of (classic) GBDT is
+      roughly proportional to `num_projections_exponent=1`, since it considers
+      all features for a split.
+      The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
+      recommends values in [1/4, 2]. Default: None.
+    sparse_oblique_projection_density_factor: Density of the projections as an
+      exponent of the number of features. Independently for each projection,
+      each feature has a probability "projection_density_factor / num_features"
+      to be considered in the projection.
+      The paper "Sparse Projection Oblique Random Forests" (Tomita et al, 2020)
+      calls this parameter `lambda` and recommends values in [1, 5].
+      Increasing this value increases training and inference time (on average).
+      This value is best tuned for each dataset. Default: None.
+    sparse_oblique_weights: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Possible values:
+      - `BINARY`: The oblique weights are sampled in {-1,1} (default).
+      - `CONTINUOUS`: The oblique weights are be sampled in [-1,1]. Default:
+        None.
+    split_axis: What structure of split to consider for numerical features.
+      - `AXIS_ALIGNED`: Axis aligned splits (i.e. one condition at a time).
+        This is the "classical" way to train a tree. Default value.
+      - `SPARSE_OBLIQUE`: Sparse oblique splits (i.e. random splits one a small
+        number of features) from "Sparse Projection Oblique Random Forests",
+        Tomita et al., 2020.
+      - `MHLD_OBLIQUE`: Multi-class Hellinger Linear Discriminant splits from
+        "Classification Based on Multivariate Contrast Patterns",
+        Canete-Sifuentes et al., 2029 Default: "AXIS_ALIGNED".
+    uplift_min_examples_in_treatment: For uplift models only. Minimum number of
+      examples per treatment in a node. Default: 5.
+    uplift_split_score: For uplift models only. Splitter score i.e. score
+      optimized by the splitters. The scores are introduced in "Decision trees
+      for uplift modeling with single and multiple treatments", Rzepakowski et
+      al. Notation: `p` probability / average value of the positive outcome,
+      `q` probability / average value in the control group.
+      - `KULLBACK_LEIBLER` or `KL`: - p log (p/q)
+      - `EUCLIDEAN_DISTANCE` or `ED`: (p-q)^2
+      - `CHI_SQUARED` or `CS`: (p-q)^2/q
+        Default: "KULLBACK_LEIBLER".
+    winner_take_all: Control how classification trees vote. If true, each tree
+      votes for one class. If false, each tree vote for a distribution of
+      classes. winner_take_all_inference=false is often preferable. Default:
+      True.
+
+    num_threads: Number of threads used to train the model. Different learning
+      algorithms use multi-threading differently and with different degree of
+      efficiency. If `None`, `num_threads` will be automatically set to the
+      number of processors (up to a maximum of 32; or set to 6 if the number of
+      processors is not available). Making `num_threads` significantly larger
+      than the number of processors can slow-down the training speed. The
+      default value logic might change in the future.
+    resume_training: If true, the model training resumes from the checkpoint
+      stored in the `working_dir` directory. If `working_dir` does not
+      contain any model checkpoint, the training starts from the beginning.
+      Resuming training is useful in the following situations: (1) The training
+      was interrupted by the user (e.g. ctrl+c or "stop" button in a notebook)
+      or rescheduled, or (2) the hyper-parameter of the learner was changed e.g.
+      increasing the number of trees.
+    working_dir: Path to a directory available for the learning algorithm to
+      store intermediate computation results. Depending on the learning
+      algorithm and parameters, the working_dir might be optional, required, or
+      ignored. For instance, distributed training algorithm always need a
+      "working_dir", and the gradient boosted tree and hyper-parameter tuners
+      will export artefacts to the "working_dir" if provided.
+    resume_training_snapshot_interval_seconds: Indicative number of seconds in 
+      between snapshots when `resume_training=True`. Might be ignored by
+      some learners.
+    tuner: If set, automatically select the best hyperparameters using the
+      provided tuner. When using distributed training, the tuning is
+      distributed.
+    workers: If set, enable distributed training. "workers" is the list of IP
+      addresses of the workers. A worker is a process running
+      `ydf.start_worker(port)`.
+  """
+
+  def __init__(self,
+      label: str,
+      task: generic_learner.Task = generic_learner.Task.CLASSIFICATION,
+      weights: Optional[str] = None,
+      ranking_group: Optional[str] = None,
+      uplift_treatment: Optional[str] = None,
+      features: dataspec.ColumnDefs = None,
+      include_all_columns: bool = False,
+      max_vocab_count: int = 2000,
+      min_vocab_frequency: int = 5,
+      discretize_numerical_columns: bool = False,
+      num_discretized_numerical_bins: int = 255,
+      max_num_scanned_rows_to_infer_semantic: int = 10000,
+      max_num_scanned_rows_to_compute_statistics: int = 10000,
+      data_spec: Optional[data_spec_pb2.DataSpecification] = None,
+      adapt_bootstrap_size_ratio_for_maximum_training_duration: Optional[bool] = False,
+      allow_na_conditions: Optional[bool] = False,
+      bootstrap_size_ratio: Optional[float] = 1.0,
+      bootstrap_training_dataset: Optional[bool] = True,
+      categorical_algorithm: Optional[str] = "CART",
+      categorical_set_split_greedy_sampling: Optional[float] = 0.1,
+      categorical_set_split_max_num_items: Optional[int] = -1,
+      categorical_set_split_min_item_frequency: Optional[int] = 1,
+      compute_oob_performances: Optional[bool] = True,
+      compute_oob_variable_importances: Optional[bool] = False,
+      growing_strategy: Optional[str] = "LOCAL",
+      honest: Optional[bool] = False,
+      honest_fixed_separation: Optional[bool] = False,
+      honest_ratio_leaf_examples: Optional[float] = 0.5,
+      in_split_min_examples_check: Optional[bool] = True,
+      keep_non_leaf_label_distribution: Optional[bool] = True,
+      max_depth: Optional[int] = 16,
+      max_num_nodes: Optional[int] = None,
+      maximum_model_size_in_memory_in_bytes: Optional[float] = -1.0,
+      maximum_training_duration_seconds: Optional[float] = -1.0,
+      mhld_oblique_max_num_attributes: Optional[int] = None,
+      mhld_oblique_sample_attributes: Optional[bool] = None,
+      min_examples: Optional[int] = 5,
+      missing_value_policy: Optional[str] = "GLOBAL_IMPUTATION",
+      num_candidate_attributes: Optional[int] = 0,
+      num_candidate_attributes_ratio: Optional[float] = -1.0,
+      num_oob_variable_importances_permutations: Optional[int] = 1,
+      num_trees: Optional[int] = 300,
+      pure_serving_model: Optional[bool] = False,
+      random_seed: Optional[int] = 123456,
+      sampling_with_replacement: Optional[bool] = True,
+      sorting_strategy: Optional[str] = "PRESORT",
+      sparse_oblique_max_num_projections: Optional[int] = None,
+      sparse_oblique_normalization: Optional[str] = None,
+      sparse_oblique_num_projections_exponent: Optional[float] = None,
+      sparse_oblique_projection_density_factor: Optional[float] = None,
+      sparse_oblique_weights: Optional[str] = None,
+      split_axis: Optional[str] = "AXIS_ALIGNED",
+      uplift_min_examples_in_treatment: Optional[int] = 5,
+      uplift_split_score: Optional[str] = "KULLBACK_LEIBLER",
+      winner_take_all: Optional[bool] = True,
+      num_threads: Optional[int] = None,
+      working_dir: Optional[str] = None,
+      resume_training: bool = False,
+      resume_training_snapshot_interval_seconds: int = 1800,
+      tuner: Optional[tuner_lib.AbstractTuner] = None,
+      workers: Optional[Sequence[str]] = None,
+      ):
+
+    hyper_parameters = {
+                      "adapt_bootstrap_size_ratio_for_maximum_training_duration" : adapt_bootstrap_size_ratio_for_maximum_training_duration,
+                      "allow_na_conditions" : allow_na_conditions,
+                      "bootstrap_size_ratio" : bootstrap_size_ratio,
+                      "bootstrap_training_dataset" : bootstrap_training_dataset,
+                      "categorical_algorithm" : categorical_algorithm,
+                      "categorical_set_split_greedy_sampling" : categorical_set_split_greedy_sampling,
+                      "categorical_set_split_max_num_items" : categorical_set_split_max_num_items,
+                      "categorical_set_split_min_item_frequency" : categorical_set_split_min_item_frequency,
+                      "compute_oob_performances" : compute_oob_performances,
+                      "compute_oob_variable_importances" : compute_oob_variable_importances,
+                      "growing_strategy" : growing_strategy,
+                      "honest" : honest,
+                      "honest_fixed_separation" : honest_fixed_separation,
+                      "honest_ratio_leaf_examples" : honest_ratio_leaf_examples,
+                      "in_split_min_examples_check" : in_split_min_examples_check,
+                      "keep_non_leaf_label_distribution" : keep_non_leaf_label_distribution,
+                      "max_depth" : max_depth,
+                      "max_num_nodes" : max_num_nodes,
+                      "maximum_model_size_in_memory_in_bytes" : maximum_model_size_in_memory_in_bytes,
+                      "maximum_training_duration_seconds" : maximum_training_duration_seconds,
+                      "mhld_oblique_max_num_attributes" : mhld_oblique_max_num_attributes,
+                      "mhld_oblique_sample_attributes" : mhld_oblique_sample_attributes,
+                      "min_examples" : min_examples,
+                      "missing_value_policy" : missing_value_policy,
+                      "num_candidate_attributes" : num_candidate_attributes,
+                      "num_candidate_attributes_ratio" : num_candidate_attributes_ratio,
+                      "num_oob_variable_importances_permutations" : num_oob_variable_importances_permutations,
+                      "num_trees" : num_trees,
+                      "pure_serving_model" : pure_serving_model,
+                      "random_seed" : random_seed,
+                      "sampling_with_replacement" : sampling_with_replacement,
+                      "sorting_strategy" : sorting_strategy,
+                      "sparse_oblique_max_num_projections" : sparse_oblique_max_num_projections,
+                      "sparse_oblique_normalization" : sparse_oblique_normalization,
+                      "sparse_oblique_num_projections_exponent" : sparse_oblique_num_projections_exponent,
+                      "sparse_oblique_projection_density_factor" : sparse_oblique_projection_density_factor,
+                      "sparse_oblique_weights" : sparse_oblique_weights,
+                      "split_axis" : split_axis,
+                      "uplift_min_examples_in_treatment" : uplift_min_examples_in_treatment,
+                      "uplift_split_score" : uplift_split_score,
+                      "winner_take_all" : winner_take_all,
+
+      }
+
+    data_spec_args = dataspec.DataSpecInferenceArgs(
+        columns=dataspec.normalize_column_defs(features),
+        include_all_columns=include_all_columns,
+        max_vocab_count=max_vocab_count,
+        min_vocab_frequency=min_vocab_frequency,
+        discretize_numerical_columns=discretize_numerical_columns,
+        num_discretized_numerical_bins=num_discretized_numerical_bins,
+        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
+        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
+    )
+
+    deployment_config = self._build_deployment_config(
+        num_threads=num_threads,
+        resume_training=resume_training,
+        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
+        working_dir=working_dir,
+        workers=workers,
+    )
+
+    super().__init__(learner_name="RANDOM_FOREST",
+      task=task,
+      label=label,
+      weights=weights,
+      ranking_group=ranking_group,
+      uplift_treatment=uplift_treatment,
+      data_spec_args=data_spec_args,
+      data_spec=data_spec,
+      hyper_parameters=hyper_parameters,
+      deployment_config=deployment_config,
+      tuner=tuner,
+    )
+
+  def train(
+      self,
+      ds: dataset.InputDataset,
+      valid: Optional[dataset.InputDataset] = None,
+  ) -> random_forest_model.RandomForestModel:
+    """Trains a model on the given dataset.
+
+    Options for dataset reading are given on the learner. Consult the
+    documentation of the learner or ydf.create_vertical_dataset() for additional
+    information on dataset reading in YDF.
+
+    Usage example:
+
+    ```
+    import ydf
+    import pandas as pd
+
+    train_ds = pd.read_csv(...)
+
+    learner = ydf.RandomForestLearner(label="label")
+    model = learner.train(train_ds)
+    print(model.summary())
+    ```
+
+    If training is interrupted (for example, by interrupting the cell execution
+    in Colab), the model will be returned to the state it was in at the moment
+    of interruption.
+
+    Args:
+      ds: Training dataset.
+      valid: Optional validation dataset. Some learners, such as Random Forest,
+        do not need validation dataset. Some learners, such as
+        GradientBoostedTrees, automatically extract a validation dataset from
+        the training dataset if the validation dataset is not provided.
+
+    Returns:
+      A trained model.
+    """
+    return super().train(ds, valid)
+
+  @classmethod
+  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
+    return abstract_learner_pb2.LearnerCapabilities(
+      support_max_training_duration=True,
+      resume_training=False,
+      support_validation_dataset=False,
+      support_partial_cache_dataset_format=False,
+      support_max_model_size_in_memory=True,
+      support_monotonic_constraints=False,
+    )
+
+  @classmethod
+  def hyperparameter_templates(cls) -> Dict[str, hyperparameters.HyperparameterTemplate]:
+    r"""Hyperparameter templates for this Learner.
+    
+    Hyperparameter templates are sets of pre-defined hyperparameters for easy
+    access to different variants of the learner. Each template is a mapping to a
+    set of hyperparameters and can be applied directly on the learner.
+    
+    Usage example:
+    ```python
+    templates = ydf.RandomForestLearner.hyperparameter_templates()
+    better_defaultv1 = templates["better_defaultv1"]
+    # Print a description of the template
+    print(better_defaultv1.description)
+    # Apply the template's settings on the learner.
+    learner = ydf.RandomForestLearner(label, **better_defaultv1)
+    ```
+    
+    Returns:
+      Dictionary of the available templates
+    """
+    return {"better_defaultv1": hyperparameters.HyperparameterTemplate(name="better_default", version=1, description="A configuration that is generally better than the default parameters without being more expensive.", parameters={"winner_take_all" :True}), "benchmark_rank1v1": hyperparameters.HyperparameterTemplate(name="benchmark_rank1", version=1, description="Top ranking hyper-parameters on our benchmark slightly modified to run in reasonable time.", parameters={"winner_take_all" :True, "categorical_algorithm" :"RANDOM", "split_axis" :"SPARSE_OBLIQUE", "sparse_oblique_normalization" :"MIN_MAX", "sparse_oblique_num_projections_exponent" :1.0}), }
```

## ydf/learner/specialized_learners_pre_generated.py

```diff
@@ -1,2381 +1,2381 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-r"""Wrappers around the YDF learners.
-
-This file is generated automatically by running the following commands:
-  bazel build //external/ydf_cc/yggdrasil_decision_forests/port/python/ydf/learner:specialized_learners\
-  && bazel-bin/external/ydf_cc/yggdrasil_decision_forests/port/python/ydf/learner/specialized_learners_generator\
-  > external/ydf_cc/yggdrasil_decision_forests/port/python/ydf/learner/specialized_learners_pre_generated.py
-
-Please don't change this file directly. Instead, changes the source. The
-documentation source is contained in the "GetGenericHyperParameterSpecification"
-method of each learner e.g. GetGenericHyperParameterSpecification in
-learner/gradient_boosted_trees/gradient_boosted_trees.cc contains the
-documentation (and meta-data) used to generate this file.
-
-In particular, these pre-generated wrappers included in the source code are 
-included for reference only. The actual wrappers are re-generated during
-compilation.
-"""
-
-from typing import Dict, Optional, Sequence, Union
-
-from yggdrasil_decision_forests.dataset import data_spec_pb2
-from yggdrasil_decision_forests.learner import abstract_learner_pb2
-from ydf.dataset import dataset
-from ydf.dataset import dataspec
-from ydf.learner import custom_loss
-from ydf.learner import generic_learner
-from ydf.learner import hyperparameters
-from ydf.learner import tuner as tuner_lib
-from ydf.model import generic_model
-from ydf.model.gradient_boosted_trees_model import gradient_boosted_trees_model
-from ydf.model.random_forest_model import random_forest_model
-
-
-class RandomForestLearner(generic_learner.GenericLearner):
-  r"""Random Forest learning algorithm.
-
-  A Random Forest (https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)
-  is a collection of deep CART decision trees trained independently and without
-  pruning. Each tree is trained on a random subset of the original training
-  dataset (sampled with replacement).
-
-  The algorithm is unique in that it is robust to overfitting, even in extreme
-  cases e.g. when there are more features than training examples.
-
-  It is probably the most well-known of the Decision Forest training
-  algorithms.
-
-  Usage example:
-
-  ```python
-  import ydf
-  import pandas as pd
-
-  dataset = pd.read_csv("project/dataset.csv")
-
-  model = ydf.RandomForestLearner().train(dataset)
-
-  print(model.summary())
-  ```
-
-  Hyperparameters are configured to give reasonable results for typical
-  datasets. Hyperparameters can also be modified manually (see descriptions)
-  below or by applying the hyperparameter templates available with
-  `RandomForestLearner.hyperparameter_templates()` (see this function's
-  documentation for
-  details).
-
-  Attributes:
-    label: Label of the dataset. The label column should not be identified as a
-      feature in the `features` parameter.
-    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,
-      Task.RANKING, Task.CATEGORICAL_UPLIFT, Task.NUMERICAL_UPLIFT).
-    weights: Name of a feature that identifies the weight of each example. If
-      weights are not specified, unit weights are assumed. The weight column
-      should not be identified as a feature in the `features` parameter.
-    ranking_group: Only for `task=Task.RANKING`. Name of a feature that
-      identifies queries in a query/document ranking task. The ranking group
-      should not be identified as a feature in the `features` parameter.
-    uplift_treatment: Only for `task=Task.CATEGORICAL_UPLIFT` and `task=Task`.
-      NUMERICAL_UPLIFT. Name of a numerical feature that identifies the
-      treatment in an uplift problem. The value 0 is reserved for the control
-      treatment. Currently, only 0/1 binary treatments are supported.
-    features: If None, all columns are used as features. The semantic of the
-      features is determined automatically. Otherwise, if
-      include_all_columns=False (default) only the column listed in `features`
-      are imported. If include_all_columns=True, all the columns are imported as
-      features and only the semantic of the columns NOT in `columns` is
-      determined automatically. If specified,  defines the order of the features
-      - any non-listed features are appended in-order after the specified
-      features (if include_all_columns=True). The label, weights, uplift
-      treatment and ranking_group columns should not be specified as features.
-    include_all_columns: See `features`.
-    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
-      CATEGORICAL_SET columns stored as strings. If more unique values exist,
-      only the most frequent values are kept, and the remaining values are
-      considered as out-of-vocabulary.
-    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
-      and CATEGORICAL_SET columns. Value observed less than
-      `min_vocab_frequency` are considered as out-of-vocabulary.
-    discretize_numerical_columns: If true, discretize all the numerical columns
-      before training. Discretized numerical columns are faster to train with,
-      but they can have a negative impact on the model quality. Using
-      `discretize_numerical_columns=True` is equivalent as setting the column
-      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
-      definition of DISCRETIZED_NUMERICAL for more details.
-    num_discretized_numerical_bins: Number of bins used when disretizing
-      numerical columns.
-    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
-      inferring the column's semantic if it is not explicitly specified. Only
-      used when reading from file, in-memory datasets are always read in full.
-      Setting this to a lower number will speed up dataset reading, but might
-      result in incorrect column semantics. Set to -1 to scan the entire
-      dataset.
-    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
-      computing a column's statistics. Only used when reading from file,
-      in-memory datasets are always read in full. A column's statistics include
-      the dictionary for categorical features and the mean / min / max for
-      numerical features. Setting this to a lower number will speed up dataset
-      reading, but skew statistics in the dataspec, which can hurt model quality
-      (e.g. if an important category of a categorical feature is considered
-      OOV). Set to -1 to scan the entire dataset.
-    data_spec: Dataspec to be used (advanced). If a data spec is given,
-      `columns`, `include_all_columns`, `max_vocab_count`,
-      `min_vocab_frequency`, `discretize_numerical_columns` and
-      `num_discretized_numerical_bins` will be ignored.
-    adapt_bootstrap_size_ratio_for_maximum_training_duration: Control how the
-      maximum training duration (if set) is applied. If false, the training stop
-      when the time is used. If true, adapts the size of the sampled dataset
-      used to train each tree such that `num_trees` will train within
-      `maximum_training_duration`. Has no effect if there is no maximum training
-      duration specified. Default: False.
-    allow_na_conditions: If true, the tree training evaluates conditions of the
-      type `X is NA` i.e. `X is missing`. Default: False.
-    bootstrap_size_ratio: Number of examples used to train each trees; expressed
-      as a ratio of the training dataset size. Default: 1.0.
-    bootstrap_training_dataset: If true (default), each tree is trained on a
-      separate dataset sampled with replacement from the original dataset. If
-      false, all the trees are trained on the entire same dataset. If
-      bootstrap_training_dataset:false, OOB metrics are not available.
-        bootstrap_training_dataset=false is used in "Extremely randomized trees"
-        (https://link.springer.com/content/pdf/10.1007%2Fs10994-006-6226-1.pdf).
-      Default: True.
-    categorical_algorithm: How to learn splits on categorical attributes. -
-      `CART`: CART algorithm. Find categorical splits of the form "value \\in
-      mask". The solution is exact for binary classification, regression and
-      ranking. It is approximated for multi-class classification. This is a good
-      first algorithm to use. In case of overfitting (very small dataset, large
-      dictionary), the "random" algorithm is a good alternative. - `ONE_HOT`:
-      One-hot encoding. Find the optimal categorical split of the form
-      "attribute == param". This method is similar (but more efficient) than
-      converting converting each possible categorical value into a boolean
-      feature. This method is available for comparison purpose and generally
-      performs worse than other alternatives. - `RANDOM`: Best splits among a
-      set of random candidate. Find the a categorical split of the form "value
-      \\in mask" using a random search. This solution can be seen as an
-      approximation of the CART algorithm. This method is a strong alternative
-      to CART. This algorithm is inspired from section "5.1 Categorical
-      Variables" of "Random Forest", 2001.
-        Default: "CART".
-    categorical_set_split_greedy_sampling: For categorical set splits e.g.
-      texts. Probability for a categorical value to be a candidate for the
-      positive set. The sampling is applied once per node (i.e. not at every
-      step of the greedy optimization). Default: 0.1.
-    categorical_set_split_max_num_items: For categorical set splits e.g. texts.
-      Maximum number of items (prior to the sampling). If more items are
-      available, the least frequent items are ignored. Changing this value is
-      similar to change the "max_vocab_count" before loading the dataset, with
-      the following exception: With `max_vocab_count`, all the remaining items
-      are grouped in a special Out-of-vocabulary item. With `max_num_items`,
-      this is not the case. Default: -1.
-    categorical_set_split_min_item_frequency: For categorical set splits e.g.
-      texts. Minimum number of occurrences of an item to be considered.
-      Default: 1.
-    compute_oob_performances: If true, compute the Out-of-bag evaluation (then
-      available in the summary and model inspector). This evaluation is a cheap
-      alternative to cross-validation evaluation. Default: True.
-    compute_oob_variable_importances: If true, compute the Out-of-bag feature
-      importance (then available in the summary and model inspector). Note that
-      the OOB feature importance can be expensive to compute. Default: False.
-    growing_strategy: How to grow the tree. - `LOCAL`: Each node is split
-      independently of the other nodes. In other words, as long as a node
-      satisfy the splits "constraints (e.g. maximum depth, minimum number of
-      observations), the node will be split. This is the "classical" way to grow
-      decision trees. - `BEST_FIRST_GLOBAL`: The node with the best loss
-      reduction among all the nodes of the tree is selected for splitting. This
-      method is also called "best first" or "leaf-wise growth". See "Best-first
-      decision tree learning", Shi and "Additive logistic regression : A
-      statistical view of boosting", Friedman for more details. Default:
-      "LOCAL".
-    honest: In honest trees, different training examples are used to infer the
-      structure and the leaf values. This regularization technique trades
-      examples for bias estimates. It might increase or reduce the quality of
-      the model. See "Generalized Random Forests", Athey et al. In this paper,
-      Honest trees are trained with the Random Forest algorithm with a sampling
-      without replacement. Default: False.
-    honest_fixed_separation: For honest trees only i.e. honest=true. If true, a
-      new random separation is generated for each tree. If false, the same
-      separation is used for all the trees (e.g., in Gradient Boosted Trees
-      containing multiple trees). Default: False.
-    honest_ratio_leaf_examples: For honest trees only i.e. honest=true. Ratio of
-      examples used to set the leaf values. Default: 0.5.
-    in_split_min_examples_check: Whether to check the `min_examples` constraint
-      in the split search (i.e. splits leading to one child having less than
-      `min_examples` examples are considered invalid) or before the split search
-      (i.e. a node can be derived only if it contains more than `min_examples`
-      examples). If false, there can be nodes with less than `min_examples`
-      training examples. Default: True.
-    keep_non_leaf_label_distribution: Whether to keep the node value (i.e. the
-      distribution of the labels of the training examples) of non-leaf nodes.
-      This information is not used during serving, however it can be used for
-      model interpretation as well as hyper parameter tuning. This can take lots
-      of space, sometimes accounting for half of the model size. Default: True.
-    max_depth: Maximum depth of the tree. `max_depth=1` means that all trees
-      will be roots. `max_depth=-1` means that tree depth is not restricted by
-      this parameter. Values <= -2 will be ignored. Default: 16.
-    max_num_nodes: Maximum number of nodes in the tree. Set to -1 to disable
-      this limit. Only available for `growing_strategy=BEST_FIRST_GLOBAL`.
-      Default: None.
-    maximum_model_size_in_memory_in_bytes: Limit the size of the model when
-      stored in ram. Different algorithms can enforce this limit differently.
-      Note that when models are compiled into an inference, the size of the
-      inference engine is generally much smaller than the original model.
-      Default: -1.0.
-    maximum_training_duration_seconds: Maximum training duration of the model
-      expressed in seconds. Each learning algorithm is free to use this
-      parameter at it sees fit. Enabling maximum training duration makes the
-      model training non-deterministic. Default: -1.0.
-    min_examples: Minimum number of examples in a node. Default: 5.
-    missing_value_policy: Method used to handle missing attribute values. -
-      `GLOBAL_IMPUTATION`: Missing attribute values are imputed, with the mean
-      (in case of numerical attribute) or the most-frequent-item (in case of
-      categorical attribute) computed on the entire dataset (i.e. the
-      information contained in the data spec). - `LOCAL_IMPUTATION`: Missing
-      attribute values are imputed with the mean (numerical attribute) or
-      most-frequent-item (in the case of categorical attribute) evaluated on the
-      training examples in the current node. - `RANDOM_LOCAL_IMPUTATION`:
-      Missing attribute values are imputed from randomly sampled values from the
-      training examples in the current node. This method was proposed by Clinic
-      et al. in "Random Survival Forests"
-      (https://projecteuclid.org/download/pdfview_1/euclid.aoas/1223908043).
-        Default: "GLOBAL_IMPUTATION".
-    num_candidate_attributes: Number of unique valid attributes tested for each
-      node. An attribute is valid if it has at least a valid split. If
-      `num_candidate_attributes=0`, the value is set to the classical default
-      value for Random Forest: `sqrt(number of input attributes)` in case of
-      classification and `number_of_input_attributes / 3` in case of regression.
-      If `num_candidate_attributes=-1`, all the attributes are tested. Default:
-      0.
-    num_candidate_attributes_ratio: Ratio of attributes tested at each node. If
-      set, it is equivalent to `num_candidate_attributes =
-      number_of_input_features x num_candidate_attributes_ratio`. The possible
-      values are between ]0, and 1] as well as -1. If not set or equal to -1,
-      the `num_candidate_attributes` is used. Default: -1.0.
-    num_oob_variable_importances_permutations: Number of time the dataset is
-      re-shuffled to compute the permutation variable importances. Increasing
-      this value increase the training time (if
-      "compute_oob_variable_importances:true") as well as the stability of the
-      oob variable importance metrics. Default: 1.
-    num_trees: Number of individual decision trees. Increasing the number of
-      trees can increase the quality of the model at the expense of size,
-      training speed, and inference latency. Default: 300.
-    pure_serving_model: Clear the model from any information that is not
-      required for model serving. This includes debugging, model interpretation
-      and other meta-data. The size of the serialized model can be reduced
-      significatively (50% model size reduction is common). This parameter has
-      no impact on the quality, serving speed or RAM usage of model serving.
-      Default: False.
-    random_seed: Random seed for the training of the model. Learners are
-      expected to be deterministic by the random seed. Default: 123456.
-    sampling_with_replacement: If true, the training examples are sampled with
-      replacement. If false, the training samples are sampled without
-      replacement. Only used when "bootstrap_training_dataset=true". If false
-      (sampling without replacement) and if "bootstrap_size_ratio=1" (default),
-      all the examples are used to train all the trees (you probably do not want
-      that). Default: True.
-    sorting_strategy: How are sorted the numerical features in order to find the
-      splits - PRESORT: The features are pre-sorted at the start of the
-      training. This solution is faster but consumes much more memory than
-      IN_NODE. - IN_NODE: The features are sorted just before being used in the
-      node. This solution is slow but consumes little amount of memory. .
-      Default: "PRESORT".
-    sparse_oblique_normalization: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Normalization applied on the features, before
-      applying the sparse oblique projections. - `NONE`: No normalization. -
-      `STANDARD_DEVIATION`: Normalize the feature by the estimated standard
-      deviation on the entire train dataset. Also known as Z-Score
-      normalization. - `MIN_MAX`: Normalize the feature by the range (i.e.
-      max-min) estimated on the entire train dataset. Default: None.
-    sparse_oblique_num_projections_exponent: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections
-      to test at each node as `num_features^num_projections_exponent`. Default:
-      None.
-    sparse_oblique_projection_density_factor: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections
-      to test at each node as `num_features^num_projections_exponent`. Default:
-      None.
-    sparse_oblique_weights: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Possible values: - `BINARY`: The oblique
-      weights are sampled in {-1,1} (default). - `CONTINUOUS`: The oblique
-      weights are be sampled in [-1,1]. Default: None.
-    split_axis: What structure of split to consider for numerical features. -
-      `AXIS_ALIGNED`: Axis aligned splits (i.e. one condition at a time). This
-      is the "classical" way to train a tree. Default value. - `SPARSE_OBLIQUE`:
-      Sparse oblique splits (i.e. splits one a small number of features) from
-      "Sparse Projection Oblique Random Forests", Tomita et al., 2020. Default:
-      "AXIS_ALIGNED".
-    uplift_min_examples_in_treatment: For uplift models only. Minimum number of
-      examples per treatment in a node. Default: 5.
-    uplift_split_score: For uplift models only. Splitter score i.e. score
-      optimized by the splitters. The scores are introduced in "Decision trees
-      for uplift modeling with single and multiple treatments", Rzepakowski et
-      al. Notation: `p` probability / average value of the positive outcome, `q`
-      probability / average value in the control group. - `KULLBACK_LEIBLER` or
-      `KL`: - p log (p/q) - `EUCLIDEAN_DISTANCE` or `ED`: (p-q)^2 -
-      `CHI_SQUARED` or `CS`: (p-q)^2/q
-        Default: "KULLBACK_LEIBLER".
-    winner_take_all: Control how classification trees vote. If true, each tree
-      votes for one class. If false, each tree vote for a distribution of
-      classes. winner_take_all_inference=false is often preferable. Default:
-      True.
-    num_threads: Number of threads used to train the model. Different learning
-      algorithms use multi-threading differently and with different degree of
-      efficiency. If `None`, `num_threads` will be automatically set to the
-      number of processors (up to a maximum of 32; or set to 6 if the number of
-      processors is not available). Making `num_threads` significantly larger
-      than the number of processors can slow-down the training speed. The
-      default value logic might change in the future.
-    resume_training: If true, the model training resumes from the checkpoint
-      stored in the `working_dir` directory. If `working_dir` does not contain
-      any model checkpoint, the training starts from the beginning. Resuming
-      training is useful in the following situations: (1) The training was
-      interrupted by the user (e.g. ctrl+c or "stop" button in a notebook) or
-      rescheduled, or (2) the hyper-parameter of the learner was changed e.g.
-      increasing the number of trees.
-    working_dir: Path to a directory available for the learning algorithm to
-      store intermediate computation results. Depending on the learning
-      algorithm and parameters, the working_dir might be optional, required, or
-      ignored. For instance, distributed training algorithm always need a
-      "working_dir", and the gradient boosted tree and hyper-parameter tuners
-      will export artefacts to the "working_dir" if provided.
-    resume_training_snapshot_interval_seconds: Indicative number of seconds in
-      between snapshots when `resume_training=True`. Might be ignored by some
-      learners.
-    tuner: If set, automatically select the best hyperparameters using the
-      provided tuner. When using distributed training, the tuning is
-      distributed.
-    workers: If set, enable distributed training. "workers" is the list of IP
-      addresses of the workers. A worker is a process running
-      `ydf.start_worker(port)`.
-  """
-
-  def __init__(
-      self,
-      label: str,
-      task: generic_learner.Task = generic_learner.Task.CLASSIFICATION,
-      weights: Optional[str] = None,
-      ranking_group: Optional[str] = None,
-      uplift_treatment: Optional[str] = None,
-      features: dataspec.ColumnDefs = None,
-      include_all_columns: bool = False,
-      max_vocab_count: int = 2000,
-      min_vocab_frequency: int = 5,
-      discretize_numerical_columns: bool = False,
-      num_discretized_numerical_bins: int = 255,
-      max_num_scanned_rows_to_infer_semantic: int = 10000,
-      max_num_scanned_rows_to_compute_statistics: int = 10000,
-      data_spec: Optional[data_spec_pb2.DataSpecification] = None,
-      adapt_bootstrap_size_ratio_for_maximum_training_duration: Optional[
-          bool
-      ] = False,
-      allow_na_conditions: Optional[bool] = False,
-      bootstrap_size_ratio: Optional[float] = 1.0,
-      bootstrap_training_dataset: Optional[bool] = True,
-      categorical_algorithm: Optional[str] = "CART",
-      categorical_set_split_greedy_sampling: Optional[float] = 0.1,
-      categorical_set_split_max_num_items: Optional[int] = -1,
-      categorical_set_split_min_item_frequency: Optional[int] = 1,
-      compute_oob_performances: Optional[bool] = True,
-      compute_oob_variable_importances: Optional[bool] = False,
-      growing_strategy: Optional[str] = "LOCAL",
-      honest: Optional[bool] = False,
-      honest_fixed_separation: Optional[bool] = False,
-      honest_ratio_leaf_examples: Optional[float] = 0.5,
-      in_split_min_examples_check: Optional[bool] = True,
-      keep_non_leaf_label_distribution: Optional[bool] = True,
-      max_depth: Optional[int] = 16,
-      max_num_nodes: Optional[int] = None,
-      maximum_model_size_in_memory_in_bytes: Optional[float] = -1.0,
-      maximum_training_duration_seconds: Optional[float] = -1.0,
-      min_examples: Optional[int] = 5,
-      missing_value_policy: Optional[str] = "GLOBAL_IMPUTATION",
-      num_candidate_attributes: Optional[int] = 0,
-      num_candidate_attributes_ratio: Optional[float] = -1.0,
-      num_oob_variable_importances_permutations: Optional[int] = 1,
-      num_trees: Optional[int] = 300,
-      pure_serving_model: Optional[bool] = False,
-      random_seed: Optional[int] = 123456,
-      sampling_with_replacement: Optional[bool] = True,
-      sorting_strategy: Optional[str] = "PRESORT",
-      sparse_oblique_normalization: Optional[str] = None,
-      sparse_oblique_num_projections_exponent: Optional[float] = None,
-      sparse_oblique_projection_density_factor: Optional[float] = None,
-      sparse_oblique_weights: Optional[str] = None,
-      split_axis: Optional[str] = "AXIS_ALIGNED",
-      uplift_min_examples_in_treatment: Optional[int] = 5,
-      uplift_split_score: Optional[str] = "KULLBACK_LEIBLER",
-      winner_take_all: Optional[bool] = True,
-      num_threads: Optional[int] = None,
-      working_dir: Optional[str] = None,
-      resume_training: bool = False,
-      resume_training_snapshot_interval_seconds: int = 1800,
-      tuner: Optional[tuner_lib.AbstractTuner] = None,
-      workers: Optional[Sequence[str]] = None,
-  ):
-
-    hyper_parameters = {
-        "adapt_bootstrap_size_ratio_for_maximum_training_duration": (
-            adapt_bootstrap_size_ratio_for_maximum_training_duration
-        ),
-        "allow_na_conditions": allow_na_conditions,
-        "bootstrap_size_ratio": bootstrap_size_ratio,
-        "bootstrap_training_dataset": bootstrap_training_dataset,
-        "categorical_algorithm": categorical_algorithm,
-        "categorical_set_split_greedy_sampling": (
-            categorical_set_split_greedy_sampling
-        ),
-        "categorical_set_split_max_num_items": (
-            categorical_set_split_max_num_items
-        ),
-        "categorical_set_split_min_item_frequency": (
-            categorical_set_split_min_item_frequency
-        ),
-        "compute_oob_performances": compute_oob_performances,
-        "compute_oob_variable_importances": compute_oob_variable_importances,
-        "growing_strategy": growing_strategy,
-        "honest": honest,
-        "honest_fixed_separation": honest_fixed_separation,
-        "honest_ratio_leaf_examples": honest_ratio_leaf_examples,
-        "in_split_min_examples_check": in_split_min_examples_check,
-        "keep_non_leaf_label_distribution": keep_non_leaf_label_distribution,
-        "max_depth": max_depth,
-        "max_num_nodes": max_num_nodes,
-        "maximum_model_size_in_memory_in_bytes": (
-            maximum_model_size_in_memory_in_bytes
-        ),
-        "maximum_training_duration_seconds": maximum_training_duration_seconds,
-        "min_examples": min_examples,
-        "missing_value_policy": missing_value_policy,
-        "num_candidate_attributes": num_candidate_attributes,
-        "num_candidate_attributes_ratio": num_candidate_attributes_ratio,
-        "num_oob_variable_importances_permutations": (
-            num_oob_variable_importances_permutations
-        ),
-        "num_trees": num_trees,
-        "pure_serving_model": pure_serving_model,
-        "random_seed": random_seed,
-        "sampling_with_replacement": sampling_with_replacement,
-        "sorting_strategy": sorting_strategy,
-        "sparse_oblique_normalization": sparse_oblique_normalization,
-        "sparse_oblique_num_projections_exponent": (
-            sparse_oblique_num_projections_exponent
-        ),
-        "sparse_oblique_projection_density_factor": (
-            sparse_oblique_projection_density_factor
-        ),
-        "sparse_oblique_weights": sparse_oblique_weights,
-        "split_axis": split_axis,
-        "uplift_min_examples_in_treatment": uplift_min_examples_in_treatment,
-        "uplift_split_score": uplift_split_score,
-        "winner_take_all": winner_take_all,
-    }
-
-    data_spec_args = dataspec.DataSpecInferenceArgs(
-        columns=dataspec.normalize_column_defs(features),
-        include_all_columns=include_all_columns,
-        max_vocab_count=max_vocab_count,
-        min_vocab_frequency=min_vocab_frequency,
-        discretize_numerical_columns=discretize_numerical_columns,
-        num_discretized_numerical_bins=num_discretized_numerical_bins,
-        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
-        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
-    )
-
-    deployment_config = self._build_deployment_config(
-        num_threads=num_threads,
-        resume_training=resume_training,
-        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
-        working_dir=working_dir,
-        workers=workers,
-    )
-
-    super().__init__(
-        learner_name="RANDOM_FOREST",
-        task=task,
-        label=label,
-        weights=weights,
-        ranking_group=ranking_group,
-        uplift_treatment=uplift_treatment,
-        data_spec_args=data_spec_args,
-        data_spec=data_spec,
-        hyper_parameters=hyper_parameters,
-        deployment_config=deployment_config,
-        tuner=tuner,
-    )
-
-  def train(
-      self,
-      ds: dataset.InputDataset,
-      valid: Optional[dataset.InputDataset] = None,
-  ) -> random_forest_model.RandomForestModel:
-    """Trains a model on the given dataset.
-
-    Options for dataset reading are given on the learner. Consult the
-    documentation of the learner or ydf.create_vertical_dataset() for additional
-    information on dataset reading in YDF.
-
-    Usage example:
-
-    ```
-    import ydf
-    import pandas as pd
-
-    train_ds = pd.read_csv(...)
-
-    learner = ydf.RandomForestLearner(label="label")
-    model = learner.train(train_ds)
-    print(model.summary())
-    ```
-
-    If training is interrupted (for example, by interrupting the cell execution
-    in Colab), the model will be returned to the state it was in at the moment
-    of interruption.
-
-    Args:
-      ds: Training dataset.
-      valid: Optional validation dataset. Some learners, such as Random Forest,
-        do not need validation dataset. Some learners, such as
-        GradientBoostedTrees, automatically extract a validation dataset from
-        the training dataset if the validation dataset is not provided.
-
-    Returns:
-      A trained model.
-    """
-    return super().train(ds, valid)
-
-  @classmethod
-  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
-    return abstract_learner_pb2.LearnerCapabilities(
-        support_max_training_duration=True,
-        resume_training=False,
-        support_validation_dataset=False,
-        support_partial_cache_dataset_format=False,
-        support_max_model_size_in_memory=True,
-        support_monotonic_constraints=False,
-    )
-
-  @classmethod
-  def hyperparameter_templates(
-      cls,
-  ) -> Dict[str, hyperparameters.HyperparameterTemplate]:
-    r"""Hyperparameter templates for this Learner.
-
-    Hyperparameter templates are sets of pre-defined hyperparameters for easy
-    access to different variants of the learner. Each template is a mapping to a
-    set of hyperparameters and can be applied directly on the learner.
-
-    Usage example:
-    ```python
-    templates = ydf.RandomForestLearner.hyperparameter_templates()
-    better_defaultv1 = templates["better_defaultv1"]
-    # Print a description of the template
-    print(better_defaultv1.description)
-    # Apply the template's settings on the learner.
-    learner = ydf.RandomForestLearner(label, **better_defaultv1)
-    ```
-
-    Returns:
-      Dictionary of the available templates
-    """
-    return {
-        "better_defaultv1": hyperparameters.HyperparameterTemplate(
-            name="better_default",
-            version=1,
-            description=(
-                "A configuration that is generally better than the default"
-                " parameters without being more expensive."
-            ),
-            parameters={"winner_take_all": True},
-        ),
-        "benchmark_rank1v1": hyperparameters.HyperparameterTemplate(
-            name="benchmark_rank1",
-            version=1,
-            description=(
-                "Top ranking hyper-parameters on our benchmark slightly"
-                " modified to run in reasonable time."
-            ),
-            parameters={
-                "winner_take_all": True,
-                "categorical_algorithm": "RANDOM",
-                "split_axis": "SPARSE_OBLIQUE",
-                "sparse_oblique_normalization": "MIN_MAX",
-                "sparse_oblique_num_projections_exponent": 1.0,
-            },
-        ),
-    }
-
-
-class HyperparameterOptimizerLearner(generic_learner.GenericLearner):
-  r"""Hyperparameter Optimizer learning algorithm.
-
-  Usage example:
-
-  ```python
-  import ydf
-  import pandas as pd
-
-  dataset = pd.read_csv("project/dataset.csv")
-
-  model = ydf.HyperparameterOptimizerLearner().train(dataset)
-
-  print(model.summary())
-  ```
-
-  Hyperparameters are configured to give reasonable results for typical
-  datasets. Hyperparameters can also be modified manually (see descriptions)
-  below or by applying the hyperparameter templates available with
-  `HyperparameterOptimizerLearner.hyperparameter_templates()` (see this
-  function's documentation for
-  details).
-
-  Attributes:
-    label: Label of the dataset. The label column should not be identified as a
-      feature in the `features` parameter.
-    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,
-      Task.RANKING, Task.CATEGORICAL_UPLIFT, Task.NUMERICAL_UPLIFT).
-    weights: Name of a feature that identifies the weight of each example. If
-      weights are not specified, unit weights are assumed. The weight column
-      should not be identified as a feature in the `features` parameter.
-    ranking_group: Only for `task=Task.RANKING`. Name of a feature that
-      identifies queries in a query/document ranking task. The ranking group
-      should not be identified as a feature in the `features` parameter.
-    uplift_treatment: Only for `task=Task.CATEGORICAL_UPLIFT` and `task=Task`.
-      NUMERICAL_UPLIFT. Name of a numerical feature that identifies the
-      treatment in an uplift problem. The value 0 is reserved for the control
-      treatment. Currently, only 0/1 binary treatments are supported.
-    features: If None, all columns are used as features. The semantic of the
-      features is determined automatically. Otherwise, if
-      include_all_columns=False (default) only the column listed in `features`
-      are imported. If include_all_columns=True, all the columns are imported as
-      features and only the semantic of the columns NOT in `columns` is
-      determined automatically. If specified,  defines the order of the features
-      - any non-listed features are appended in-order after the specified
-      features (if include_all_columns=True). The label, weights, uplift
-      treatment and ranking_group columns should not be specified as features.
-    include_all_columns: See `features`.
-    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
-      CATEGORICAL_SET columns stored as strings. If more unique values exist,
-      only the most frequent values are kept, and the remaining values are
-      considered as out-of-vocabulary.
-    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
-      and CATEGORICAL_SET columns. Value observed less than
-      `min_vocab_frequency` are considered as out-of-vocabulary.
-    discretize_numerical_columns: If true, discretize all the numerical columns
-      before training. Discretized numerical columns are faster to train with,
-      but they can have a negative impact on the model quality. Using
-      `discretize_numerical_columns=True` is equivalent as setting the column
-      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
-      definition of DISCRETIZED_NUMERICAL for more details.
-    num_discretized_numerical_bins: Number of bins used when disretizing
-      numerical columns.
-    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
-      inferring the column's semantic if it is not explicitly specified. Only
-      used when reading from file, in-memory datasets are always read in full.
-      Setting this to a lower number will speed up dataset reading, but might
-      result in incorrect column semantics. Set to -1 to scan the entire
-      dataset.
-    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
-      computing a column's statistics. Only used when reading from file,
-      in-memory datasets are always read in full. A column's statistics include
-      the dictionary for categorical features and the mean / min / max for
-      numerical features. Setting this to a lower number will speed up dataset
-      reading, but skew statistics in the dataspec, which can hurt model quality
-      (e.g. if an important category of a categorical feature is considered
-      OOV). Set to -1 to scan the entire dataset.
-    data_spec: Dataspec to be used (advanced). If a data spec is given,
-      `columns`, `include_all_columns`, `max_vocab_count`,
-      `min_vocab_frequency`, `discretize_numerical_columns` and
-      `num_discretized_numerical_bins` will be ignored.
-    maximum_model_size_in_memory_in_bytes: Limit the size of the model when
-      stored in ram. Different algorithms can enforce this limit differently.
-      Note that when models are compiled into an inference, the size of the
-      inference engine is generally much smaller than the original model.
-      Default: -1.0.
-    maximum_training_duration_seconds: Maximum training duration of the model
-      expressed in seconds. Each learning algorithm is free to use this
-      parameter at it sees fit. Enabling maximum training duration makes the
-      model training non-deterministic. Default: -1.0.
-    pure_serving_model: Clear the model from any information that is not
-      required for model serving. This includes debugging, model interpretation
-      and other meta-data. The size of the serialized model can be reduced
-      significatively (50% model size reduction is common). This parameter has
-      no impact on the quality, serving speed or RAM usage of model serving.
-      Default: False.
-    random_seed: Random seed for the training of the model. Learners are
-      expected to be deterministic by the random seed. Default: 123456.
-    num_threads: Number of threads used to train the model. Different learning
-      algorithms use multi-threading differently and with different degree of
-      efficiency. If `None`, `num_threads` will be automatically set to the
-      number of processors (up to a maximum of 32; or set to 6 if the number of
-      processors is not available). Making `num_threads` significantly larger
-      than the number of processors can slow-down the training speed. The
-      default value logic might change in the future.
-    resume_training: If true, the model training resumes from the checkpoint
-      stored in the `working_dir` directory. If `working_dir` does not contain
-      any model checkpoint, the training starts from the beginning. Resuming
-      training is useful in the following situations: (1) The training was
-      interrupted by the user (e.g. ctrl+c or "stop" button in a notebook) or
-      rescheduled, or (2) the hyper-parameter of the learner was changed e.g.
-      increasing the number of trees.
-    working_dir: Path to a directory available for the learning algorithm to
-      store intermediate computation results. Depending on the learning
-      algorithm and parameters, the working_dir might be optional, required, or
-      ignored. For instance, distributed training algorithm always need a
-      "working_dir", and the gradient boosted tree and hyper-parameter tuners
-      will export artefacts to the "working_dir" if provided.
-    resume_training_snapshot_interval_seconds: Indicative number of seconds in
-      between snapshots when `resume_training=True`. Might be ignored by some
-      learners.
-    tuner: If set, automatically select the best hyperparameters using the
-      provided tuner. When using distributed training, the tuning is
-      distributed.
-    workers: If set, enable distributed training. "workers" is the list of IP
-      addresses of the workers. A worker is a process running
-      `ydf.start_worker(port)`.
-  """
-
-  def __init__(
-      self,
-      label: str,
-      task: generic_learner.Task = generic_learner.Task.CLASSIFICATION,
-      weights: Optional[str] = None,
-      ranking_group: Optional[str] = None,
-      uplift_treatment: Optional[str] = None,
-      features: dataspec.ColumnDefs = None,
-      include_all_columns: bool = False,
-      max_vocab_count: int = 2000,
-      min_vocab_frequency: int = 5,
-      discretize_numerical_columns: bool = False,
-      num_discretized_numerical_bins: int = 255,
-      max_num_scanned_rows_to_infer_semantic: int = 10000,
-      max_num_scanned_rows_to_compute_statistics: int = 10000,
-      data_spec: Optional[data_spec_pb2.DataSpecification] = None,
-      maximum_model_size_in_memory_in_bytes: Optional[float] = -1.0,
-      maximum_training_duration_seconds: Optional[float] = -1.0,
-      pure_serving_model: Optional[bool] = False,
-      random_seed: Optional[int] = 123456,
-      num_threads: Optional[int] = None,
-      working_dir: Optional[str] = None,
-      resume_training: bool = False,
-      resume_training_snapshot_interval_seconds: int = 1800,
-      tuner: Optional[tuner_lib.AbstractTuner] = None,
-      workers: Optional[Sequence[str]] = None,
-  ):
-
-    hyper_parameters = {
-        "maximum_model_size_in_memory_in_bytes": (
-            maximum_model_size_in_memory_in_bytes
-        ),
-        "maximum_training_duration_seconds": maximum_training_duration_seconds,
-        "pure_serving_model": pure_serving_model,
-        "random_seed": random_seed,
-    }
-
-    data_spec_args = dataspec.DataSpecInferenceArgs(
-        columns=dataspec.normalize_column_defs(features),
-        include_all_columns=include_all_columns,
-        max_vocab_count=max_vocab_count,
-        min_vocab_frequency=min_vocab_frequency,
-        discretize_numerical_columns=discretize_numerical_columns,
-        num_discretized_numerical_bins=num_discretized_numerical_bins,
-        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
-        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
-    )
-
-    deployment_config = self._build_deployment_config(
-        num_threads=num_threads,
-        resume_training=resume_training,
-        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
-        working_dir=working_dir,
-        workers=workers,
-    )
-
-    super().__init__(
-        learner_name="HYPERPARAMETER_OPTIMIZER",
-        task=task,
-        label=label,
-        weights=weights,
-        ranking_group=ranking_group,
-        uplift_treatment=uplift_treatment,
-        data_spec_args=data_spec_args,
-        data_spec=data_spec,
-        hyper_parameters=hyper_parameters,
-        deployment_config=deployment_config,
-        tuner=tuner,
-    )
-
-  def train(
-      self,
-      ds: dataset.InputDataset,
-      valid: Optional[dataset.InputDataset] = None,
-  ) -> generic_model.GenericModel:
-    """Trains a model on the given dataset.
-
-    Options for dataset reading are given on the learner. Consult the
-    documentation of the learner or ydf.create_vertical_dataset() for additional
-    information on dataset reading in YDF.
-
-    Usage example:
-
-    ```
-    import ydf
-    import pandas as pd
-
-    train_ds = pd.read_csv(...)
-
-    learner = ydf.HyperparameterOptimizerLearner(label="label")
-    model = learner.train(train_ds)
-    print(model.summary())
-    ```
-
-    If training is interrupted (for example, by interrupting the cell execution
-    in Colab), the model will be returned to the state it was in at the moment
-    of interruption.
-
-    Args:
-      ds: Training dataset.
-      valid: Optional validation dataset. Some learners, such as Random Forest,
-        do not need validation dataset. Some learners, such as
-        GradientBoostedTrees, automatically extract a validation dataset from
-        the training dataset if the validation dataset is not provided.
-
-    Returns:
-      A trained model.
-    """
-    return super().train(ds, valid)
-
-  @classmethod
-  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
-    return abstract_learner_pb2.LearnerCapabilities(
-        support_max_training_duration=True,
-        resume_training=False,
-        support_validation_dataset=False,
-        support_partial_cache_dataset_format=False,
-        support_max_model_size_in_memory=False,
-        support_monotonic_constraints=False,
-    )
-
-  @classmethod
-  def hyperparameter_templates(
-      cls,
-  ) -> Dict[str, hyperparameters.HyperparameterTemplate]:
-    r"""Hyperparameter templates for this Learner.
-
-    This learner currently does not provide any hyperparameter templates, this
-    method is provided for consistency with other learners.
-
-    Returns:
-      Empty dictionary.
-    """
-    return {}
-
-
-class GradientBoostedTreesLearner(generic_learner.GenericLearner):
-  r"""Gradient Boosted Trees learning algorithm.
-
-  A [Gradient Boosted Trees](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)
-  (GBT), also known as Gradient Boosted Decision Trees (GBDT) or Gradient
-  Boosted Machines (GBM),  is a set of shallow decision trees trained
-  sequentially. Each tree is trained to predict and then "correct" for the
-  errors of the previously trained trees (more precisely each tree predict the
-  gradient of the loss relative to the model output).
-
-  Usage example:
-
-  ```python
-  import ydf
-  import pandas as pd
-
-  dataset = pd.read_csv("project/dataset.csv")
-
-  model = ydf.GradientBoostedTreesLearner().train(dataset)
-
-  print(model.summary())
-  ```
-
-  Hyperparameters are configured to give reasonable results for typical
-  datasets. Hyperparameters can also be modified manually (see descriptions)
-  below or by applying the hyperparameter templates available with
-  `GradientBoostedTreesLearner.hyperparameter_templates()` (see this function's
-  documentation for
-  details).
-
-  Attributes:
-    label: Label of the dataset. The label column should not be identified as a
-      feature in the `features` parameter.
-    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,
-      Task.RANKING, Task.CATEGORICAL_UPLIFT, Task.NUMERICAL_UPLIFT).
-    weights: Name of a feature that identifies the weight of each example. If
-      weights are not specified, unit weights are assumed. The weight column
-      should not be identified as a feature in the `features` parameter.
-    ranking_group: Only for `task=Task.RANKING`. Name of a feature that
-      identifies queries in a query/document ranking task. The ranking group
-      should not be identified as a feature in the `features` parameter.
-    uplift_treatment: Only for `task=Task.CATEGORICAL_UPLIFT` and `task=Task`.
-      NUMERICAL_UPLIFT. Name of a numerical feature that identifies the
-      treatment in an uplift problem. The value 0 is reserved for the control
-      treatment. Currently, only 0/1 binary treatments are supported.
-    features: If None, all columns are used as features. The semantic of the
-      features is determined automatically. Otherwise, if
-      include_all_columns=False (default) only the column listed in `features`
-      are imported. If include_all_columns=True, all the columns are imported as
-      features and only the semantic of the columns NOT in `columns` is
-      determined automatically. If specified,  defines the order of the features
-      - any non-listed features are appended in-order after the specified
-      features (if include_all_columns=True). The label, weights, uplift
-      treatment and ranking_group columns should not be specified as features.
-    include_all_columns: See `features`.
-    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
-      CATEGORICAL_SET columns stored as strings. If more unique values exist,
-      only the most frequent values are kept, and the remaining values are
-      considered as out-of-vocabulary.
-    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
-      and CATEGORICAL_SET columns. Value observed less than
-      `min_vocab_frequency` are considered as out-of-vocabulary.
-    discretize_numerical_columns: If true, discretize all the numerical columns
-      before training. Discretized numerical columns are faster to train with,
-      but they can have a negative impact on the model quality. Using
-      `discretize_numerical_columns=True` is equivalent as setting the column
-      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
-      definition of DISCRETIZED_NUMERICAL for more details.
-    num_discretized_numerical_bins: Number of bins used when disretizing
-      numerical columns.
-    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
-      inferring the column's semantic if it is not explicitly specified. Only
-      used when reading from file, in-memory datasets are always read in full.
-      Setting this to a lower number will speed up dataset reading, but might
-      result in incorrect column semantics. Set to -1 to scan the entire
-      dataset.
-    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
-      computing a column's statistics. Only used when reading from file,
-      in-memory datasets are always read in full. A column's statistics include
-      the dictionary for categorical features and the mean / min / max for
-      numerical features. Setting this to a lower number will speed up dataset
-      reading, but skew statistics in the dataspec, which can hurt model quality
-      (e.g. if an important category of a categorical feature is considered
-      OOV). Set to -1 to scan the entire dataset.
-    data_spec: Dataspec to be used (advanced). If a data spec is given,
-      `columns`, `include_all_columns`, `max_vocab_count`,
-      `min_vocab_frequency`, `discretize_numerical_columns` and
-      `num_discretized_numerical_bins` will be ignored.
-    adapt_subsample_for_maximum_training_duration: Control how the maximum
-      training duration (if set) is applied. If false, the training stop when
-      the time is used. If true, the size of the sampled datasets used train
-      individual trees are adapted dynamically so that all the trees are trained
-      in time. Default: False.
-    allow_na_conditions: If true, the tree training evaluates conditions of the
-      type `X is NA` i.e. `X is missing`. Default: False.
-    apply_link_function: If true, applies the link function (a.k.a. activation
-      function), if any, before returning the model prediction. If false,
-      returns the pre-link function model output. For example, in the case of
-      binary classification, the pre-link function output is a logic while the
-      post-link function is a probability. Default: True.
-    categorical_algorithm: How to learn splits on categorical attributes. -
-      `CART`: CART algorithm. Find categorical splits of the form "value \\in
-      mask". The solution is exact for binary classification, regression and
-      ranking. It is approximated for multi-class classification. This is a good
-      first algorithm to use. In case of overfitting (very small dataset, large
-      dictionary), the "random" algorithm is a good alternative. - `ONE_HOT`:
-      One-hot encoding. Find the optimal categorical split of the form
-      "attribute == param". This method is similar (but more efficient) than
-      converting converting each possible categorical value into a boolean
-      feature. This method is available for comparison purpose and generally
-      performs worse than other alternatives. - `RANDOM`: Best splits among a
-      set of random candidate. Find the a categorical split of the form "value
-      \\in mask" using a random search. This solution can be seen as an
-      approximation of the CART algorithm. This method is a strong alternative
-      to CART. This algorithm is inspired from section "5.1 Categorical
-      Variables" of "Random Forest", 2001.
-        Default: "CART".
-    categorical_set_split_greedy_sampling: For categorical set splits e.g.
-      texts. Probability for a categorical value to be a candidate for the
-      positive set. The sampling is applied once per node (i.e. not at every
-      step of the greedy optimization). Default: 0.1.
-    categorical_set_split_max_num_items: For categorical set splits e.g. texts.
-      Maximum number of items (prior to the sampling). If more items are
-      available, the least frequent items are ignored. Changing this value is
-      similar to change the "max_vocab_count" before loading the dataset, with
-      the following exception: With `max_vocab_count`, all the remaining items
-      are grouped in a special Out-of-vocabulary item. With `max_num_items`,
-      this is not the case. Default: -1.
-    categorical_set_split_min_item_frequency: For categorical set splits e.g.
-      texts. Minimum number of occurrences of an item to be considered.
-      Default: 1.
-    compute_permutation_variable_importance: If true, compute the permutation
-      variable importance of the model at the end of the training using the
-      validation dataset. Enabling this feature can increase the training time
-      significantly. Default: False.
-    dart_dropout: Dropout rate applied when using the DART i.e. when
-      forest_extraction=DART. Default: 0.01.
-    early_stopping: Early stopping detects the overfitting of the model and
-      halts it training using the validation dataset. If not provided directly,
-      the validation dataset is extracted from the training dataset (see
-      "validation_ratio" parameter): - `NONE`: No early stopping. All the
-      num_trees are trained and kept. - `MIN_LOSS_FINAL`: All the num_trees are
-      trained. The model is then truncated to minimize the validation loss i.e.
-      some of the trees are discarded as to minimum the validation loss. -
-      `LOSS_INCREASE`: Classical early stopping. Stop the training when the
-      validation does not decrease for `early_stopping_num_trees_look_ahead`
-      trees. Default: "LOSS_INCREASE".
-    early_stopping_initial_iteration: 0-based index of the first iteration
-      considered for early stopping computation. Increasing this value prevents
-      too early stopping due to noisy initial iterations of the learner.
-      Default: 10.
-    early_stopping_num_trees_look_ahead: Rolling number of trees used to detect
-      validation loss increase and trigger early stopping. Default: 30.
-    focal_loss_alpha: EXPERIMENTAL. Weighting parameter for focal loss, positive
-      samples weighted by alpha, negative samples by (1-alpha). The default 0.5
-      value means no active class-level weighting. Only used with focal loss
-      i.e. `loss="BINARY_FOCAL_LOSS"` Default: 0.5.
-    focal_loss_gamma: EXPERIMENTAL. Exponent of the misprediction exponent term
-      in focal loss, corresponds to gamma parameter in
-      https://arxiv.org/pdf/1708.02002.pdf. Only used with focal loss i.e.
-        `loss="BINARY_FOCAL_LOSS"` Default: 2.0.
-    forest_extraction: How to construct the forest: - MART: For Multiple
-      Additive Regression Trees. The "classical" way to build a GBDT i.e. each
-      tree tries to "correct" the mistakes of the previous trees. - DART: For
-      Dropout Additive Regression Trees. A modification of MART proposed in
-      http://proceedings.mlr.press/v38/korlakaivinayak15.pdf. Here, each tree
-        tries to "correct" the mistakes of a random subset of the previous
-        trees.
-      Default: "MART".
-    goss_alpha: Alpha parameter for the GOSS (Gradient-based One-Side Sampling;
-      "See LightGBM: A Highly Efficient Gradient Boosting Decision Tree")
-      sampling method. Default: 0.2.
-    goss_beta: Beta parameter for the GOSS (Gradient-based One-Side Sampling)
-      sampling method. Default: 0.1.
-    growing_strategy: How to grow the tree. - `LOCAL`: Each node is split
-      independently of the other nodes. In other words, as long as a node
-      satisfy the splits "constraints (e.g. maximum depth, minimum number of
-      observations), the node will be split. This is the "classical" way to grow
-      decision trees. - `BEST_FIRST_GLOBAL`: The node with the best loss
-      reduction among all the nodes of the tree is selected for splitting. This
-      method is also called "best first" or "leaf-wise growth". See "Best-first
-      decision tree learning", Shi and "Additive logistic regression : A
-      statistical view of boosting", Friedman for more details. Default:
-      "LOCAL".
-    honest: In honest trees, different training examples are used to infer the
-      structure and the leaf values. This regularization technique trades
-      examples for bias estimates. It might increase or reduce the quality of
-      the model. See "Generalized Random Forests", Athey et al. In this paper,
-      Honest trees are trained with the Random Forest algorithm with a sampling
-      without replacement. Default: False.
-    honest_fixed_separation: For honest trees only i.e. honest=true. If true, a
-      new random separation is generated for each tree. If false, the same
-      separation is used for all the trees (e.g., in Gradient Boosted Trees
-      containing multiple trees). Default: False.
-    honest_ratio_leaf_examples: For honest trees only i.e. honest=true. Ratio of
-      examples used to set the leaf values. Default: 0.5.
-    in_split_min_examples_check: Whether to check the `min_examples` constraint
-      in the split search (i.e. splits leading to one child having less than
-      `min_examples` examples are considered invalid) or before the split search
-      (i.e. a node can be derived only if it contains more than `min_examples`
-      examples). If false, there can be nodes with less than `min_examples`
-      training examples. Default: True.
-    keep_non_leaf_label_distribution: Whether to keep the node value (i.e. the
-      distribution of the labels of the training examples) of non-leaf nodes.
-      This information is not used during serving, however it can be used for
-      model interpretation as well as hyper parameter tuning. This can take lots
-      of space, sometimes accounting for half of the model size. Default: True.
-    l1_regularization: L1 regularization applied to the training loss. Impact
-      the tree structures and lead values. Default: 0.0.
-    l2_categorical_regularization: L2 regularization applied to the training
-      loss for categorical features. Impact the tree structures and lead values.
-      Default: 1.0.
-    l2_regularization: L2 regularization applied to the training loss for all
-      features except the categorical ones. Default: 0.0.
-    lambda_loss: Lambda regularization applied to certain training loss
-      functions. Only for NDCG loss. Default: 1.0.
-    loss: The loss optimized by the model. If not specified (DEFAULT) the loss
-      is selected automatically according to the \\"task\\" and label
-      statistics. For example, if task=CLASSIFICATION and the label has two
-      possible values, the loss will be set to BINOMIAL_LOG_LIKELIHOOD. Possible
-      values are: - `DEFAULT`: Select the loss automatically according to the
-      task and label statistics. - `BINOMIAL_LOG_LIKELIHOOD`: Binomial log
-      likelihood. Only valid for binary classification. - `SQUARED_ERROR`: Least
-      square loss. Only valid for regression. - `POISSON`: Poisson log
-      likelihood loss. Mainly used for counting problems. Only valid for
-      regression. - `MULTINOMIAL_LOG_LIKELIHOOD`: Multinomial log likelihood
-      i.e. cross-entropy. Only valid for binary or multi-class classification. -
-      `LAMBDA_MART_NDCG5`: LambdaMART with NDCG5. - `XE_NDCG_MART`:  Cross
-      Entropy Loss NDCG. See arxiv.org/abs/1911.09798. - `BINARY_FOCAL_LOSS`:
-      Focal loss. Only valid for binary classification. See
-      https://arxiv.org/pdf/1708.02002.pdf. - `POISSON`: Poisson log likelihood.
-        Only valid for regression. - `MEAN_AVERAGE_ERROR`: Mean average error
-        a.k.a. MAE. For custom losses, pass the loss object here. Note that when
-        using custom losses, the link function is deactivated (aka
-        apply_link_function is always False).
-        Default: "DEFAULT".
-    max_depth: Maximum depth of the tree. `max_depth=1` means that all trees
-      will be roots. `max_depth=-1` means that tree depth is not restricted by
-      this parameter. Values <= -2 will be ignored. Default: 6.
-    max_num_nodes: Maximum number of nodes in the tree. Set to -1 to disable
-      this limit. Only available for `growing_strategy=BEST_FIRST_GLOBAL`.
-      Default: None.
-    maximum_model_size_in_memory_in_bytes: Limit the size of the model when
-      stored in ram. Different algorithms can enforce this limit differently.
-      Note that when models are compiled into an inference, the size of the
-      inference engine is generally much smaller than the original model.
-      Default: -1.0.
-    maximum_training_duration_seconds: Maximum training duration of the model
-      expressed in seconds. Each learning algorithm is free to use this
-      parameter at it sees fit. Enabling maximum training duration makes the
-      model training non-deterministic. Default: -1.0.
-    min_examples: Minimum number of examples in a node. Default: 5.
-    missing_value_policy: Method used to handle missing attribute values. -
-      `GLOBAL_IMPUTATION`: Missing attribute values are imputed, with the mean
-      (in case of numerical attribute) or the most-frequent-item (in case of
-      categorical attribute) computed on the entire dataset (i.e. the
-      information contained in the data spec). - `LOCAL_IMPUTATION`: Missing
-      attribute values are imputed with the mean (numerical attribute) or
-      most-frequent-item (in the case of categorical attribute) evaluated on the
-      training examples in the current node. - `RANDOM_LOCAL_IMPUTATION`:
-      Missing attribute values are imputed from randomly sampled values from the
-      training examples in the current node. This method was proposed by Clinic
-      et al. in "Random Survival Forests"
-      (https://projecteuclid.org/download/pdfview_1/euclid.aoas/1223908043).
-        Default: "GLOBAL_IMPUTATION".
-    num_candidate_attributes: Number of unique valid attributes tested for each
-      node. An attribute is valid if it has at least a valid split. If
-      `num_candidate_attributes=0`, the value is set to the classical default
-      value for Random Forest: `sqrt(number of input attributes)` in case of
-      classification and `number_of_input_attributes / 3` in case of regression.
-      If `num_candidate_attributes=-1`, all the attributes are tested. Default:
-      -1.
-    num_candidate_attributes_ratio: Ratio of attributes tested at each node. If
-      set, it is equivalent to `num_candidate_attributes =
-      number_of_input_features x num_candidate_attributes_ratio`. The possible
-      values are between ]0, and 1] as well as -1. If not set or equal to -1,
-      the `num_candidate_attributes` is used. Default: -1.0.
-    num_trees: Maximum number of decision trees. The effective number of trained
-      tree can be smaller if early stopping is enabled. Default: 300.
-    pure_serving_model: Clear the model from any information that is not
-      required for model serving. This includes debugging, model interpretation
-      and other meta-data. The size of the serialized model can be reduced
-      significatively (50% model size reduction is common). This parameter has
-      no impact on the quality, serving speed or RAM usage of model serving.
-      Default: False.
-    random_seed: Random seed for the training of the model. Learners are
-      expected to be deterministic by the random seed. Default: 123456.
-    sampling_method: Control the sampling of the datasets used to train
-      individual trees. - NONE: No sampling is applied. This is equivalent to
-      RANDOM sampling with \\"subsample=1\\". - RANDOM (default): Uniform random
-      sampling. Automatically selected if "subsample" is set. - GOSS:
-      Gradient-based One-Side Sampling. Automatically selected if "goss_alpha"
-      or "goss_beta" is set. - SELGB: Selective Gradient Boosting. Automatically
-      selected if "selective_gradient_boosting_ratio" is set. Only valid for
-      ranking.
-        Default: "RANDOM".
-    selective_gradient_boosting_ratio: Ratio of the dataset used to train
-      individual tree for the selective Gradient Boosting (Selective Gradient
-      Boosting for Effective Learning to Rank; Lucchese et al;
-      http://quickrank.isti.cnr.it/selective-data/selective-SIGIR2018.pdf)
-        sampling method. Default: 0.01.
-    shrinkage: Coefficient applied to each tree prediction. A small value (0.02)
-      tends to give more accurate results (assuming enough trees are trained),
-      but results in larger models. Analogous to neural network learning rate.
-      Default: 0.1.
-    sorting_strategy: How are sorted the numerical features in order to find the
-      splits - PRESORT: The features are pre-sorted at the start of the
-      training. This solution is faster but consumes much more memory than
-      IN_NODE. - IN_NODE: The features are sorted just before being used in the
-      node. This solution is slow but consumes little amount of memory. .
-      Default: "PRESORT".
-    sparse_oblique_normalization: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Normalization applied on the features, before
-      applying the sparse oblique projections. - `NONE`: No normalization. -
-      `STANDARD_DEVIATION`: Normalize the feature by the estimated standard
-      deviation on the entire train dataset. Also known as Z-Score
-      normalization. - `MIN_MAX`: Normalize the feature by the range (i.e.
-      max-min) estimated on the entire train dataset. Default: None.
-    sparse_oblique_num_projections_exponent: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections
-      to test at each node as `num_features^num_projections_exponent`. Default:
-      None.
-    sparse_oblique_projection_density_factor: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections
-      to test at each node as `num_features^num_projections_exponent`. Default:
-      None.
-    sparse_oblique_weights: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Possible values: - `BINARY`: The oblique
-      weights are sampled in {-1,1} (default). - `CONTINUOUS`: The oblique
-      weights are be sampled in [-1,1]. Default: None.
-    split_axis: What structure of split to consider for numerical features. -
-      `AXIS_ALIGNED`: Axis aligned splits (i.e. one condition at a time). This
-      is the "classical" way to train a tree. Default value. - `SPARSE_OBLIQUE`:
-      Sparse oblique splits (i.e. splits one a small number of features) from
-      "Sparse Projection Oblique Random Forests", Tomita et al., 2020. Default:
-      "AXIS_ALIGNED".
-    subsample: Ratio of the dataset (sampling without replacement) used to train
-      individual trees for the random sampling method. If \\"subsample\\" is set
-      and if \\"sampling_method\\" is NOT set or set to \\"NONE\\", then
-      \\"sampling_method\\" is implicitly set to \\"RANDOM\\". In other words,
-      to enable random subsampling, you only need to set "\\"subsample\\".
-      Default: 1.0.
-    uplift_min_examples_in_treatment: For uplift models only. Minimum number of
-      examples per treatment in a node. Default: 5.
-    uplift_split_score: For uplift models only. Splitter score i.e. score
-      optimized by the splitters. The scores are introduced in "Decision trees
-      for uplift modeling with single and multiple treatments", Rzepakowski et
-      al. Notation: `p` probability / average value of the positive outcome, `q`
-      probability / average value in the control group. - `KULLBACK_LEIBLER` or
-      `KL`: - p log (p/q) - `EUCLIDEAN_DISTANCE` or `ED`: (p-q)^2 -
-      `CHI_SQUARED` or `CS`: (p-q)^2/q
-        Default: "KULLBACK_LEIBLER".
-    use_hessian_gain: Use true, uses a formulation of split gain with a hessian
-      term i.e. optimizes the splits to minimize the variance of "gradient /
-      hessian. Available for all losses except regression. Default: False.
-    validation_interval_in_trees: Evaluate the model on the validation set every
-      "validation_interval_in_trees" trees. Increasing this value reduce the
-      cost of validation and can impact the early stopping policy (as early
-      stopping is only tested during the validation). Default: 1.
-    validation_ratio: Fraction of the training dataset used for validation if
-      not validation dataset is provided. The validation dataset, whether
-      provided directly or extracted from the training dataset, is used to
-      compute the validation loss, other validation metrics, and possibly
-      trigger early stopping (if enabled). When early stopping is disabled, the
-      validation dataset is only used for monitoring and does not influence the
-      model directly. If the "validation_ratio" is set to 0, early stopping is
-      disabled (i.e., it implies setting early_stopping=NONE). Default: 0.1.
-    num_threads: Number of threads used to train the model. Different learning
-      algorithms use multi-threading differently and with different degree of
-      efficiency. If `None`, `num_threads` will be automatically set to the
-      number of processors (up to a maximum of 32; or set to 6 if the number of
-      processors is not available). Making `num_threads` significantly larger
-      than the number of processors can slow-down the training speed. The
-      default value logic might change in the future.
-    resume_training: If true, the model training resumes from the checkpoint
-      stored in the `working_dir` directory. If `working_dir` does not contain
-      any model checkpoint, the training starts from the beginning. Resuming
-      training is useful in the following situations: (1) The training was
-      interrupted by the user (e.g. ctrl+c or "stop" button in a notebook) or
-      rescheduled, or (2) the hyper-parameter of the learner was changed e.g.
-      increasing the number of trees.
-    working_dir: Path to a directory available for the learning algorithm to
-      store intermediate computation results. Depending on the learning
-      algorithm and parameters, the working_dir might be optional, required, or
-      ignored. For instance, distributed training algorithm always need a
-      "working_dir", and the gradient boosted tree and hyper-parameter tuners
-      will export artefacts to the "working_dir" if provided.
-    resume_training_snapshot_interval_seconds: Indicative number of seconds in
-      between snapshots when `resume_training=True`. Might be ignored by some
-      learners.
-    tuner: If set, automatically select the best hyperparameters using the
-      provided tuner. When using distributed training, the tuning is
-      distributed.
-    workers: If set, enable distributed training. "workers" is the list of IP
-      addresses of the workers. A worker is a process running
-      `ydf.start_worker(port)`.
-  """
-
-  def __init__(
-      self,
-      label: str,
-      task: generic_learner.Task = generic_learner.Task.CLASSIFICATION,
-      weights: Optional[str] = None,
-      ranking_group: Optional[str] = None,
-      uplift_treatment: Optional[str] = None,
-      features: dataspec.ColumnDefs = None,
-      include_all_columns: bool = False,
-      max_vocab_count: int = 2000,
-      min_vocab_frequency: int = 5,
-      discretize_numerical_columns: bool = False,
-      num_discretized_numerical_bins: int = 255,
-      max_num_scanned_rows_to_infer_semantic: int = 10000,
-      max_num_scanned_rows_to_compute_statistics: int = 10000,
-      data_spec: Optional[data_spec_pb2.DataSpecification] = None,
-      adapt_subsample_for_maximum_training_duration: Optional[bool] = False,
-      allow_na_conditions: Optional[bool] = False,
-      apply_link_function: Optional[bool] = True,
-      categorical_algorithm: Optional[str] = "CART",
-      categorical_set_split_greedy_sampling: Optional[float] = 0.1,
-      categorical_set_split_max_num_items: Optional[int] = -1,
-      categorical_set_split_min_item_frequency: Optional[int] = 1,
-      compute_permutation_variable_importance: Optional[bool] = False,
-      dart_dropout: Optional[float] = 0.01,
-      early_stopping: Optional[str] = "LOSS_INCREASE",
-      early_stopping_initial_iteration: Optional[int] = 10,
-      early_stopping_num_trees_look_ahead: Optional[int] = 30,
-      focal_loss_alpha: Optional[float] = 0.5,
-      focal_loss_gamma: Optional[float] = 2.0,
-      forest_extraction: Optional[str] = "MART",
-      goss_alpha: Optional[float] = 0.2,
-      goss_beta: Optional[float] = 0.1,
-      growing_strategy: Optional[str] = "LOCAL",
-      honest: Optional[bool] = False,
-      honest_fixed_separation: Optional[bool] = False,
-      honest_ratio_leaf_examples: Optional[float] = 0.5,
-      in_split_min_examples_check: Optional[bool] = True,
-      keep_non_leaf_label_distribution: Optional[bool] = True,
-      l1_regularization: Optional[float] = 0.0,
-      l2_categorical_regularization: Optional[float] = 1.0,
-      l2_regularization: Optional[float] = 0.0,
-      lambda_loss: Optional[float] = 1.0,
-      loss: Optional[Union[str, custom_loss.AbstractCustomLoss]] = "DEFAULT",
-      max_depth: Optional[int] = 6,
-      max_num_nodes: Optional[int] = None,
-      maximum_model_size_in_memory_in_bytes: Optional[float] = -1.0,
-      maximum_training_duration_seconds: Optional[float] = -1.0,
-      min_examples: Optional[int] = 5,
-      missing_value_policy: Optional[str] = "GLOBAL_IMPUTATION",
-      num_candidate_attributes: Optional[int] = -1,
-      num_candidate_attributes_ratio: Optional[float] = -1.0,
-      num_trees: Optional[int] = 300,
-      pure_serving_model: Optional[bool] = False,
-      random_seed: Optional[int] = 123456,
-      sampling_method: Optional[str] = "RANDOM",
-      selective_gradient_boosting_ratio: Optional[float] = 0.01,
-      shrinkage: Optional[float] = 0.1,
-      sorting_strategy: Optional[str] = "PRESORT",
-      sparse_oblique_normalization: Optional[str] = None,
-      sparse_oblique_num_projections_exponent: Optional[float] = None,
-      sparse_oblique_projection_density_factor: Optional[float] = None,
-      sparse_oblique_weights: Optional[str] = None,
-      split_axis: Optional[str] = "AXIS_ALIGNED",
-      subsample: Optional[float] = 1.0,
-      uplift_min_examples_in_treatment: Optional[int] = 5,
-      uplift_split_score: Optional[str] = "KULLBACK_LEIBLER",
-      use_hessian_gain: Optional[bool] = False,
-      validation_interval_in_trees: Optional[int] = 1,
-      validation_ratio: Optional[float] = 0.1,
-      num_threads: Optional[int] = None,
-      working_dir: Optional[str] = None,
-      resume_training: bool = False,
-      resume_training_snapshot_interval_seconds: int = 1800,
-      tuner: Optional[tuner_lib.AbstractTuner] = None,
-      workers: Optional[Sequence[str]] = None,
-  ):
-
-    hyper_parameters = {
-        "adapt_subsample_for_maximum_training_duration": (
-            adapt_subsample_for_maximum_training_duration
-        ),
-        "allow_na_conditions": allow_na_conditions,
-        "apply_link_function": apply_link_function,
-        "categorical_algorithm": categorical_algorithm,
-        "categorical_set_split_greedy_sampling": (
-            categorical_set_split_greedy_sampling
-        ),
-        "categorical_set_split_max_num_items": (
-            categorical_set_split_max_num_items
-        ),
-        "categorical_set_split_min_item_frequency": (
-            categorical_set_split_min_item_frequency
-        ),
-        "compute_permutation_variable_importance": (
-            compute_permutation_variable_importance
-        ),
-        "dart_dropout": dart_dropout,
-        "early_stopping": early_stopping,
-        "early_stopping_initial_iteration": early_stopping_initial_iteration,
-        "early_stopping_num_trees_look_ahead": (
-            early_stopping_num_trees_look_ahead
-        ),
-        "focal_loss_alpha": focal_loss_alpha,
-        "focal_loss_gamma": focal_loss_gamma,
-        "forest_extraction": forest_extraction,
-        "goss_alpha": goss_alpha,
-        "goss_beta": goss_beta,
-        "growing_strategy": growing_strategy,
-        "honest": honest,
-        "honest_fixed_separation": honest_fixed_separation,
-        "honest_ratio_leaf_examples": honest_ratio_leaf_examples,
-        "in_split_min_examples_check": in_split_min_examples_check,
-        "keep_non_leaf_label_distribution": keep_non_leaf_label_distribution,
-        "l1_regularization": l1_regularization,
-        "l2_categorical_regularization": l2_categorical_regularization,
-        "l2_regularization": l2_regularization,
-        "lambda_loss": lambda_loss,
-        "loss": loss,
-        "max_depth": max_depth,
-        "max_num_nodes": max_num_nodes,
-        "maximum_model_size_in_memory_in_bytes": (
-            maximum_model_size_in_memory_in_bytes
-        ),
-        "maximum_training_duration_seconds": maximum_training_duration_seconds,
-        "min_examples": min_examples,
-        "missing_value_policy": missing_value_policy,
-        "num_candidate_attributes": num_candidate_attributes,
-        "num_candidate_attributes_ratio": num_candidate_attributes_ratio,
-        "num_trees": num_trees,
-        "pure_serving_model": pure_serving_model,
-        "random_seed": random_seed,
-        "sampling_method": sampling_method,
-        "selective_gradient_boosting_ratio": selective_gradient_boosting_ratio,
-        "shrinkage": shrinkage,
-        "sorting_strategy": sorting_strategy,
-        "sparse_oblique_normalization": sparse_oblique_normalization,
-        "sparse_oblique_num_projections_exponent": (
-            sparse_oblique_num_projections_exponent
-        ),
-        "sparse_oblique_projection_density_factor": (
-            sparse_oblique_projection_density_factor
-        ),
-        "sparse_oblique_weights": sparse_oblique_weights,
-        "split_axis": split_axis,
-        "subsample": subsample,
-        "uplift_min_examples_in_treatment": uplift_min_examples_in_treatment,
-        "uplift_split_score": uplift_split_score,
-        "use_hessian_gain": use_hessian_gain,
-        "validation_interval_in_trees": validation_interval_in_trees,
-        "validation_ratio": validation_ratio,
-    }
-
-    data_spec_args = dataspec.DataSpecInferenceArgs(
-        columns=dataspec.normalize_column_defs(features),
-        include_all_columns=include_all_columns,
-        max_vocab_count=max_vocab_count,
-        min_vocab_frequency=min_vocab_frequency,
-        discretize_numerical_columns=discretize_numerical_columns,
-        num_discretized_numerical_bins=num_discretized_numerical_bins,
-        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
-        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
-    )
-
-    deployment_config = self._build_deployment_config(
-        num_threads=num_threads,
-        resume_training=resume_training,
-        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
-        working_dir=working_dir,
-        workers=workers,
-    )
-
-    super().__init__(
-        learner_name="GRADIENT_BOOSTED_TREES",
-        task=task,
-        label=label,
-        weights=weights,
-        ranking_group=ranking_group,
-        uplift_treatment=uplift_treatment,
-        data_spec_args=data_spec_args,
-        data_spec=data_spec,
-        hyper_parameters=hyper_parameters,
-        deployment_config=deployment_config,
-        tuner=tuner,
-    )
-
-  def train(
-      self,
-      ds: dataset.InputDataset,
-      valid: Optional[dataset.InputDataset] = None,
-  ) -> gradient_boosted_trees_model.GradientBoostedTreesModel:
-    """Trains a model on the given dataset.
-
-    Options for dataset reading are given on the learner. Consult the
-    documentation of the learner or ydf.create_vertical_dataset() for additional
-    information on dataset reading in YDF.
-
-    Usage example:
-
-    ```
-    import ydf
-    import pandas as pd
-
-    train_ds = pd.read_csv(...)
-
-    learner = ydf.GradientBoostedTreesLearner(label="label")
-    model = learner.train(train_ds)
-    print(model.summary())
-    ```
-
-    If training is interrupted (for example, by interrupting the cell execution
-    in Colab), the model will be returned to the state it was in at the moment
-    of interruption.
-
-    Args:
-      ds: Training dataset.
-      valid: Optional validation dataset. Some learners, such as Random Forest,
-        do not need validation dataset. Some learners, such as
-        GradientBoostedTrees, automatically extract a validation dataset from
-        the training dataset if the validation dataset is not provided.
-
-    Returns:
-      A trained model.
-    """
-    return super().train(ds, valid)
-
-  @classmethod
-  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
-    return abstract_learner_pb2.LearnerCapabilities(
-        support_max_training_duration=True,
-        resume_training=True,
-        support_validation_dataset=True,
-        support_partial_cache_dataset_format=False,
-        support_max_model_size_in_memory=False,
-        support_monotonic_constraints=True,
-    )
-
-  @classmethod
-  def hyperparameter_templates(
-      cls,
-  ) -> Dict[str, hyperparameters.HyperparameterTemplate]:
-    r"""Hyperparameter templates for this Learner.
-
-    Hyperparameter templates are sets of pre-defined hyperparameters for easy
-    access to different variants of the learner. Each template is a mapping to a
-    set of hyperparameters and can be applied directly on the learner.
-
-    Usage example:
-    ```python
-    templates = ydf.GradientBoostedTreesLearner.hyperparameter_templates()
-    better_defaultv1 = templates["better_defaultv1"]
-    # Print a description of the template
-    print(better_defaultv1.description)
-    # Apply the template's settings on the learner.
-    learner = ydf.GradientBoostedTreesLearner(label, **better_defaultv1)
-    ```
-
-    Returns:
-      Dictionary of the available templates
-    """
-    return {
-        "better_defaultv1": hyperparameters.HyperparameterTemplate(
-            name="better_default",
-            version=1,
-            description=(
-                "A configuration that is generally better than the default"
-                " parameters without being more expensive."
-            ),
-            parameters={"growing_strategy": "BEST_FIRST_GLOBAL"},
-        ),
-        "benchmark_rank1v1": hyperparameters.HyperparameterTemplate(
-            name="benchmark_rank1",
-            version=1,
-            description=(
-                "Top ranking hyper-parameters on our benchmark slightly"
-                " modified to run in reasonable time."
-            ),
-            parameters={
-                "growing_strategy": "BEST_FIRST_GLOBAL",
-                "categorical_algorithm": "RANDOM",
-                "split_axis": "SPARSE_OBLIQUE",
-                "sparse_oblique_normalization": "MIN_MAX",
-                "sparse_oblique_num_projections_exponent": 1.0,
-            },
-        ),
-    }
-
-
-class DistributedGradientBoostedTreesLearner(generic_learner.GenericLearner):
-  r"""Distributed Gradient Boosted Trees learning algorithm.
-
-  Exact distributed version of the Gradient Boosted Tree learning algorithm. See
-  the documentation of the non-distributed Gradient Boosted Tree learning
-  algorithm for an introduction to GBTs.
-
-  Usage example:
-
-  ```python
-  import ydf
-  import pandas as pd
-
-  dataset = pd.read_csv("project/dataset.csv")
-
-  model = ydf.DistributedGradientBoostedTreesLearner().train(dataset)
-
-  print(model.summary())
-  ```
-
-  Hyperparameters are configured to give reasonable results for typical
-  datasets. Hyperparameters can also be modified manually (see descriptions)
-  below or by applying the hyperparameter templates available with
-  `DistributedGradientBoostedTreesLearner.hyperparameter_templates()` (see this
-  function's documentation for
-  details).
-
-  Attributes:
-    label: Label of the dataset. The label column should not be identified as a
-      feature in the `features` parameter.
-    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,
-      Task.RANKING, Task.CATEGORICAL_UPLIFT, Task.NUMERICAL_UPLIFT).
-    weights: Name of a feature that identifies the weight of each example. If
-      weights are not specified, unit weights are assumed. The weight column
-      should not be identified as a feature in the `features` parameter.
-    ranking_group: Only for `task=Task.RANKING`. Name of a feature that
-      identifies queries in a query/document ranking task. The ranking group
-      should not be identified as a feature in the `features` parameter.
-    uplift_treatment: Only for `task=Task.CATEGORICAL_UPLIFT` and `task=Task`.
-      NUMERICAL_UPLIFT. Name of a numerical feature that identifies the
-      treatment in an uplift problem. The value 0 is reserved for the control
-      treatment. Currently, only 0/1 binary treatments are supported.
-    features: If None, all columns are used as features. The semantic of the
-      features is determined automatically. Otherwise, if
-      include_all_columns=False (default) only the column listed in `features`
-      are imported. If include_all_columns=True, all the columns are imported as
-      features and only the semantic of the columns NOT in `columns` is
-      determined automatically. If specified,  defines the order of the features
-      - any non-listed features are appended in-order after the specified
-      features (if include_all_columns=True). The label, weights, uplift
-      treatment and ranking_group columns should not be specified as features.
-    include_all_columns: See `features`.
-    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
-      CATEGORICAL_SET columns stored as strings. If more unique values exist,
-      only the most frequent values are kept, and the remaining values are
-      considered as out-of-vocabulary.
-    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
-      and CATEGORICAL_SET columns. Value observed less than
-      `min_vocab_frequency` are considered as out-of-vocabulary.
-    discretize_numerical_columns: If true, discretize all the numerical columns
-      before training. Discretized numerical columns are faster to train with,
-      but they can have a negative impact on the model quality. Using
-      `discretize_numerical_columns=True` is equivalent as setting the column
-      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
-      definition of DISCRETIZED_NUMERICAL for more details.
-    num_discretized_numerical_bins: Number of bins used when disretizing
-      numerical columns.
-    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
-      inferring the column's semantic if it is not explicitly specified. Only
-      used when reading from file, in-memory datasets are always read in full.
-      Setting this to a lower number will speed up dataset reading, but might
-      result in incorrect column semantics. Set to -1 to scan the entire
-      dataset.
-    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
-      computing a column's statistics. Only used when reading from file,
-      in-memory datasets are always read in full. A column's statistics include
-      the dictionary for categorical features and the mean / min / max for
-      numerical features. Setting this to a lower number will speed up dataset
-      reading, but skew statistics in the dataspec, which can hurt model quality
-      (e.g. if an important category of a categorical feature is considered
-      OOV). Set to -1 to scan the entire dataset.
-    data_spec: Dataspec to be used (advanced). If a data spec is given,
-      `columns`, `include_all_columns`, `max_vocab_count`,
-      `min_vocab_frequency`, `discretize_numerical_columns` and
-      `num_discretized_numerical_bins` will be ignored.
-    apply_link_function: If true, applies the link function (a.k.a. activation
-      function), if any, before returning the model prediction. If false,
-      returns the pre-link function model output. For example, in the case of
-      binary classification, the pre-link function output is a logic while the
-      post-link function is a probability. Default: True.
-    force_numerical_discretization: If false, only the numerical column
-      safisfying "max_unique_values_for_discretized_numerical" will be
-      discretized. If true, all the numerical columns will be discretized.
-      Columns with more than "max_unique_values_for_discretized_numerical"
-      unique values will be approximated with
-      "max_unique_values_for_discretized_numerical" bins. This parameter will
-      impact the model training. Default: False.
-    max_depth: Maximum depth of the tree. `max_depth=1` means that all trees
-      will be roots. `max_depth=-1` means that tree depth is not restricted by
-      this parameter. Values <= -2 will be ignored. Default: 6.
-    max_unique_values_for_discretized_numerical: Maximum number of unique value
-      of a numerical feature to allow its pre-discretization. In case of large
-      datasets, discretized numerical features with a small number of unique
-      values are more efficient to learn than classical / non-discretized
-      numerical features. This parameter does not impact the final model.
-      However, it can speed-up or slown the training. Default: 16000.
-    maximum_model_size_in_memory_in_bytes: Limit the size of the model when
-      stored in ram. Different algorithms can enforce this limit differently.
-      Note that when models are compiled into an inference, the size of the
-      inference engine is generally much smaller than the original model.
-      Default: -1.0.
-    maximum_training_duration_seconds: Maximum training duration of the model
-      expressed in seconds. Each learning algorithm is free to use this
-      parameter at it sees fit. Enabling maximum training duration makes the
-      model training non-deterministic. Default: -1.0.
-    min_examples: Minimum number of examples in a node. Default: 5.
-    num_candidate_attributes: Number of unique valid attributes tested for each
-      node. An attribute is valid if it has at least a valid split. If
-      `num_candidate_attributes=0`, the value is set to the classical default
-      value for Random Forest: `sqrt(number of input attributes)` in case of
-      classification and `number_of_input_attributes / 3` in case of regression.
-      If `num_candidate_attributes=-1`, all the attributes are tested. Default:
-      -1.
-    num_candidate_attributes_ratio: Ratio of attributes tested at each node. If
-      set, it is equivalent to `num_candidate_attributes =
-      number_of_input_features x num_candidate_attributes_ratio`. The possible
-      values are between ]0, and 1] as well as -1. If not set or equal to -1,
-      the `num_candidate_attributes` is used. Default: -1.0.
-    num_trees: Maximum number of decision trees. The effective number of trained
-      tree can be smaller if early stopping is enabled. Default: 300.
-    pure_serving_model: Clear the model from any information that is not
-      required for model serving. This includes debugging, model interpretation
-      and other meta-data. The size of the serialized model can be reduced
-      significatively (50% model size reduction is common). This parameter has
-      no impact on the quality, serving speed or RAM usage of model serving.
-      Default: False.
-    random_seed: Random seed for the training of the model. Learners are
-      expected to be deterministic by the random seed. Default: 123456.
-    shrinkage: Coefficient applied to each tree prediction. A small value (0.02)
-      tends to give more accurate results (assuming enough trees are trained),
-      but results in larger models. Analogous to neural network learning rate.
-      Default: 0.1.
-    use_hessian_gain: Use true, uses a formulation of split gain with a hessian
-      term i.e. optimizes the splits to minimize the variance of "gradient /
-      hessian. Available for all losses except regression. Default: False.
-    worker_logs: If true, workers will print training logs. Default: True.
-    num_threads: Number of threads used to train the model. Different learning
-      algorithms use multi-threading differently and with different degree of
-      efficiency. If `None`, `num_threads` will be automatically set to the
-      number of processors (up to a maximum of 32; or set to 6 if the number of
-      processors is not available). Making `num_threads` significantly larger
-      than the number of processors can slow-down the training speed. The
-      default value logic might change in the future.
-    resume_training: If true, the model training resumes from the checkpoint
-      stored in the `working_dir` directory. If `working_dir` does not contain
-      any model checkpoint, the training starts from the beginning. Resuming
-      training is useful in the following situations: (1) The training was
-      interrupted by the user (e.g. ctrl+c or "stop" button in a notebook) or
-      rescheduled, or (2) the hyper-parameter of the learner was changed e.g.
-      increasing the number of trees.
-    working_dir: Path to a directory available for the learning algorithm to
-      store intermediate computation results. Depending on the learning
-      algorithm and parameters, the working_dir might be optional, required, or
-      ignored. For instance, distributed training algorithm always need a
-      "working_dir", and the gradient boosted tree and hyper-parameter tuners
-      will export artefacts to the "working_dir" if provided.
-    resume_training_snapshot_interval_seconds: Indicative number of seconds in
-      between snapshots when `resume_training=True`. Might be ignored by some
-      learners.
-    tuner: If set, automatically select the best hyperparameters using the
-      provided tuner. When using distributed training, the tuning is
-      distributed.
-    workers: If set, enable distributed training. "workers" is the list of IP
-      addresses of the workers. A worker is a process running
-      `ydf.start_worker(port)`.
-  """
-
-  def __init__(
-      self,
-      label: str,
-      task: generic_learner.Task = generic_learner.Task.CLASSIFICATION,
-      weights: Optional[str] = None,
-      ranking_group: Optional[str] = None,
-      uplift_treatment: Optional[str] = None,
-      features: dataspec.ColumnDefs = None,
-      include_all_columns: bool = False,
-      max_vocab_count: int = 2000,
-      min_vocab_frequency: int = 5,
-      discretize_numerical_columns: bool = False,
-      num_discretized_numerical_bins: int = 255,
-      max_num_scanned_rows_to_infer_semantic: int = 10000,
-      max_num_scanned_rows_to_compute_statistics: int = 10000,
-      data_spec: Optional[data_spec_pb2.DataSpecification] = None,
-      apply_link_function: Optional[bool] = True,
-      force_numerical_discretization: Optional[bool] = False,
-      max_depth: Optional[int] = 6,
-      max_unique_values_for_discretized_numerical: Optional[int] = 16000,
-      maximum_model_size_in_memory_in_bytes: Optional[float] = -1.0,
-      maximum_training_duration_seconds: Optional[float] = -1.0,
-      min_examples: Optional[int] = 5,
-      num_candidate_attributes: Optional[int] = -1,
-      num_candidate_attributes_ratio: Optional[float] = -1.0,
-      num_trees: Optional[int] = 300,
-      pure_serving_model: Optional[bool] = False,
-      random_seed: Optional[int] = 123456,
-      shrinkage: Optional[float] = 0.1,
-      use_hessian_gain: Optional[bool] = False,
-      worker_logs: Optional[bool] = True,
-      num_threads: Optional[int] = None,
-      working_dir: Optional[str] = None,
-      resume_training: bool = False,
-      resume_training_snapshot_interval_seconds: int = 1800,
-      tuner: Optional[tuner_lib.AbstractTuner] = None,
-      workers: Optional[Sequence[str]] = None,
-  ):
-
-    hyper_parameters = {
-        "apply_link_function": apply_link_function,
-        "force_numerical_discretization": force_numerical_discretization,
-        "max_depth": max_depth,
-        "max_unique_values_for_discretized_numerical": (
-            max_unique_values_for_discretized_numerical
-        ),
-        "maximum_model_size_in_memory_in_bytes": (
-            maximum_model_size_in_memory_in_bytes
-        ),
-        "maximum_training_duration_seconds": maximum_training_duration_seconds,
-        "min_examples": min_examples,
-        "num_candidate_attributes": num_candidate_attributes,
-        "num_candidate_attributes_ratio": num_candidate_attributes_ratio,
-        "num_trees": num_trees,
-        "pure_serving_model": pure_serving_model,
-        "random_seed": random_seed,
-        "shrinkage": shrinkage,
-        "use_hessian_gain": use_hessian_gain,
-        "worker_logs": worker_logs,
-    }
-
-    data_spec_args = dataspec.DataSpecInferenceArgs(
-        columns=dataspec.normalize_column_defs(features),
-        include_all_columns=include_all_columns,
-        max_vocab_count=max_vocab_count,
-        min_vocab_frequency=min_vocab_frequency,
-        discretize_numerical_columns=discretize_numerical_columns,
-        num_discretized_numerical_bins=num_discretized_numerical_bins,
-        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
-        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
-    )
-
-    deployment_config = self._build_deployment_config(
-        num_threads=num_threads,
-        resume_training=resume_training,
-        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
-        working_dir=working_dir,
-        workers=workers,
-    )
-
-    super().__init__(
-        learner_name="DISTRIBUTED_GRADIENT_BOOSTED_TREES",
-        task=task,
-        label=label,
-        weights=weights,
-        ranking_group=ranking_group,
-        uplift_treatment=uplift_treatment,
-        data_spec_args=data_spec_args,
-        data_spec=data_spec,
-        hyper_parameters=hyper_parameters,
-        deployment_config=deployment_config,
-        tuner=tuner,
-    )
-
-  def train(
-      self,
-      ds: dataset.InputDataset,
-      valid: Optional[dataset.InputDataset] = None,
-  ) -> gradient_boosted_trees_model.GradientBoostedTreesModel:
-    """Trains a model on the given dataset.
-
-    Options for dataset reading are given on the learner. Consult the
-    documentation of the learner or ydf.create_vertical_dataset() for additional
-    information on dataset reading in YDF.
-
-    Usage example:
-
-    ```
-    import ydf
-    import pandas as pd
-
-    train_ds = pd.read_csv(...)
-
-    learner = ydf.DistributedGradientBoostedTreesLearner(label="label")
-    model = learner.train(train_ds)
-    print(model.summary())
-    ```
-
-    If training is interrupted (for example, by interrupting the cell execution
-    in Colab), the model will be returned to the state it was in at the moment
-    of interruption.
-
-    Args:
-      ds: Training dataset.
-      valid: Optional validation dataset. Some learners, such as Random Forest,
-        do not need validation dataset. Some learners, such as
-        GradientBoostedTrees, automatically extract a validation dataset from
-        the training dataset if the validation dataset is not provided.
-
-    Returns:
-      A trained model.
-    """
-    return super().train(ds, valid)
-
-  @classmethod
-  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
-    return abstract_learner_pb2.LearnerCapabilities(
-        support_max_training_duration=False,
-        resume_training=True,
-        support_validation_dataset=False,
-        support_partial_cache_dataset_format=True,
-        support_max_model_size_in_memory=False,
-        support_monotonic_constraints=False,
-    )
-
-  @classmethod
-  def hyperparameter_templates(
-      cls,
-  ) -> Dict[str, hyperparameters.HyperparameterTemplate]:
-    r"""Hyperparameter templates for this Learner.
-
-    This learner currently does not provide any hyperparameter templates, this
-    method is provided for consistency with other learners.
-
-    Returns:
-      Empty dictionary.
-    """
-    return {}
-
-
-class CartLearner(generic_learner.GenericLearner):
-  r"""Cart learning algorithm.
-
-  A CART (Classification and Regression Trees) a decision tree. The non-leaf
-  nodes contains conditions (also known as splits) while the leaf nodes contain
-  prediction values. The training dataset is divided in two parts. The first is
-  used to grow the tree while the second is used to prune the tree.
-
-  Usage example:
-
-  ```python
-  import ydf
-  import pandas as pd
-
-  dataset = pd.read_csv("project/dataset.csv")
-
-  model = ydf.CartLearner().train(dataset)
-
-  print(model.summary())
-  ```
-
-  Hyperparameters are configured to give reasonable results for typical
-  datasets. Hyperparameters can also be modified manually (see descriptions)
-  below or by applying the hyperparameter templates available with
-  `CartLearner.hyperparameter_templates()` (see this function's documentation
-  for
-  details).
-
-  Attributes:
-    label: Label of the dataset. The label column should not be identified as a
-      feature in the `features` parameter.
-    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,
-      Task.RANKING, Task.CATEGORICAL_UPLIFT, Task.NUMERICAL_UPLIFT).
-    weights: Name of a feature that identifies the weight of each example. If
-      weights are not specified, unit weights are assumed. The weight column
-      should not be identified as a feature in the `features` parameter.
-    ranking_group: Only for `task=Task.RANKING`. Name of a feature that
-      identifies queries in a query/document ranking task. The ranking group
-      should not be identified as a feature in the `features` parameter.
-    uplift_treatment: Only for `task=Task.CATEGORICAL_UPLIFT` and `task=Task`.
-      NUMERICAL_UPLIFT. Name of a numerical feature that identifies the
-      treatment in an uplift problem. The value 0 is reserved for the control
-      treatment. Currently, only 0/1 binary treatments are supported.
-    features: If None, all columns are used as features. The semantic of the
-      features is determined automatically. Otherwise, if
-      include_all_columns=False (default) only the column listed in `features`
-      are imported. If include_all_columns=True, all the columns are imported as
-      features and only the semantic of the columns NOT in `columns` is
-      determined automatically. If specified,  defines the order of the features
-      - any non-listed features are appended in-order after the specified
-      features (if include_all_columns=True). The label, weights, uplift
-      treatment and ranking_group columns should not be specified as features.
-    include_all_columns: See `features`.
-    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
-      CATEGORICAL_SET columns stored as strings. If more unique values exist,
-      only the most frequent values are kept, and the remaining values are
-      considered as out-of-vocabulary.
-    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
-      and CATEGORICAL_SET columns. Value observed less than
-      `min_vocab_frequency` are considered as out-of-vocabulary.
-    discretize_numerical_columns: If true, discretize all the numerical columns
-      before training. Discretized numerical columns are faster to train with,
-      but they can have a negative impact on the model quality. Using
-      `discretize_numerical_columns=True` is equivalent as setting the column
-      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
-      definition of DISCRETIZED_NUMERICAL for more details.
-    num_discretized_numerical_bins: Number of bins used when disretizing
-      numerical columns.
-    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
-      inferring the column's semantic if it is not explicitly specified. Only
-      used when reading from file, in-memory datasets are always read in full.
-      Setting this to a lower number will speed up dataset reading, but might
-      result in incorrect column semantics. Set to -1 to scan the entire
-      dataset.
-    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
-      computing a column's statistics. Only used when reading from file,
-      in-memory datasets are always read in full. A column's statistics include
-      the dictionary for categorical features and the mean / min / max for
-      numerical features. Setting this to a lower number will speed up dataset
-      reading, but skew statistics in the dataspec, which can hurt model quality
-      (e.g. if an important category of a categorical feature is considered
-      OOV). Set to -1 to scan the entire dataset.
-    data_spec: Dataspec to be used (advanced). If a data spec is given,
-      `columns`, `include_all_columns`, `max_vocab_count`,
-      `min_vocab_frequency`, `discretize_numerical_columns` and
-      `num_discretized_numerical_bins` will be ignored.
-    allow_na_conditions: If true, the tree training evaluates conditions of the
-      type `X is NA` i.e. `X is missing`. Default: False.
-    categorical_algorithm: How to learn splits on categorical attributes. -
-      `CART`: CART algorithm. Find categorical splits of the form "value \\in
-      mask". The solution is exact for binary classification, regression and
-      ranking. It is approximated for multi-class classification. This is a good
-      first algorithm to use. In case of overfitting (very small dataset, large
-      dictionary), the "random" algorithm is a good alternative. - `ONE_HOT`:
-      One-hot encoding. Find the optimal categorical split of the form
-      "attribute == param". This method is similar (but more efficient) than
-      converting converting each possible categorical value into a boolean
-      feature. This method is available for comparison purpose and generally
-      performs worse than other alternatives. - `RANDOM`: Best splits among a
-      set of random candidate. Find the a categorical split of the form "value
-      \\in mask" using a random search. This solution can be seen as an
-      approximation of the CART algorithm. This method is a strong alternative
-      to CART. This algorithm is inspired from section "5.1 Categorical
-      Variables" of "Random Forest", 2001.
-        Default: "CART".
-    categorical_set_split_greedy_sampling: For categorical set splits e.g.
-      texts. Probability for a categorical value to be a candidate for the
-      positive set. The sampling is applied once per node (i.e. not at every
-      step of the greedy optimization). Default: 0.1.
-    categorical_set_split_max_num_items: For categorical set splits e.g. texts.
-      Maximum number of items (prior to the sampling). If more items are
-      available, the least frequent items are ignored. Changing this value is
-      similar to change the "max_vocab_count" before loading the dataset, with
-      the following exception: With `max_vocab_count`, all the remaining items
-      are grouped in a special Out-of-vocabulary item. With `max_num_items`,
-      this is not the case. Default: -1.
-    categorical_set_split_min_item_frequency: For categorical set splits e.g.
-      texts. Minimum number of occurrences of an item to be considered.
-      Default: 1.
-    growing_strategy: How to grow the tree. - `LOCAL`: Each node is split
-      independently of the other nodes. In other words, as long as a node
-      satisfy the splits "constraints (e.g. maximum depth, minimum number of
-      observations), the node will be split. This is the "classical" way to grow
-      decision trees. - `BEST_FIRST_GLOBAL`: The node with the best loss
-      reduction among all the nodes of the tree is selected for splitting. This
-      method is also called "best first" or "leaf-wise growth". See "Best-first
-      decision tree learning", Shi and "Additive logistic regression : A
-      statistical view of boosting", Friedman for more details. Default:
-      "LOCAL".
-    honest: In honest trees, different training examples are used to infer the
-      structure and the leaf values. This regularization technique trades
-      examples for bias estimates. It might increase or reduce the quality of
-      the model. See "Generalized Random Forests", Athey et al. In this paper,
-      Honest trees are trained with the Random Forest algorithm with a sampling
-      without replacement. Default: False.
-    honest_fixed_separation: For honest trees only i.e. honest=true. If true, a
-      new random separation is generated for each tree. If false, the same
-      separation is used for all the trees (e.g., in Gradient Boosted Trees
-      containing multiple trees). Default: False.
-    honest_ratio_leaf_examples: For honest trees only i.e. honest=true. Ratio of
-      examples used to set the leaf values. Default: 0.5.
-    in_split_min_examples_check: Whether to check the `min_examples` constraint
-      in the split search (i.e. splits leading to one child having less than
-      `min_examples` examples are considered invalid) or before the split search
-      (i.e. a node can be derived only if it contains more than `min_examples`
-      examples). If false, there can be nodes with less than `min_examples`
-      training examples. Default: True.
-    keep_non_leaf_label_distribution: Whether to keep the node value (i.e. the
-      distribution of the labels of the training examples) of non-leaf nodes.
-      This information is not used during serving, however it can be used for
-      model interpretation as well as hyper parameter tuning. This can take lots
-      of space, sometimes accounting for half of the model size. Default: True.
-    max_depth: Maximum depth of the tree. `max_depth=1` means that all trees
-      will be roots. `max_depth=-1` means that tree depth is not restricted by
-      this parameter. Values <= -2 will be ignored. Default: 16.
-    max_num_nodes: Maximum number of nodes in the tree. Set to -1 to disable
-      this limit. Only available for `growing_strategy=BEST_FIRST_GLOBAL`.
-      Default: None.
-    maximum_model_size_in_memory_in_bytes: Limit the size of the model when
-      stored in ram. Different algorithms can enforce this limit differently.
-      Note that when models are compiled into an inference, the size of the
-      inference engine is generally much smaller than the original model.
-      Default: -1.0.
-    maximum_training_duration_seconds: Maximum training duration of the model
-      expressed in seconds. Each learning algorithm is free to use this
-      parameter at it sees fit. Enabling maximum training duration makes the
-      model training non-deterministic. Default: -1.0.
-    min_examples: Minimum number of examples in a node. Default: 5.
-    missing_value_policy: Method used to handle missing attribute values. -
-      `GLOBAL_IMPUTATION`: Missing attribute values are imputed, with the mean
-      (in case of numerical attribute) or the most-frequent-item (in case of
-      categorical attribute) computed on the entire dataset (i.e. the
-      information contained in the data spec). - `LOCAL_IMPUTATION`: Missing
-      attribute values are imputed with the mean (numerical attribute) or
-      most-frequent-item (in the case of categorical attribute) evaluated on the
-      training examples in the current node. - `RANDOM_LOCAL_IMPUTATION`:
-      Missing attribute values are imputed from randomly sampled values from the
-      training examples in the current node. This method was proposed by Clinic
-      et al. in "Random Survival Forests"
-      (https://projecteuclid.org/download/pdfview_1/euclid.aoas/1223908043).
-        Default: "GLOBAL_IMPUTATION".
-    num_candidate_attributes: Number of unique valid attributes tested for each
-      node. An attribute is valid if it has at least a valid split. If
-      `num_candidate_attributes=0`, the value is set to the classical default
-      value for Random Forest: `sqrt(number of input attributes)` in case of
-      classification and `number_of_input_attributes / 3` in case of regression.
-      If `num_candidate_attributes=-1`, all the attributes are tested. Default:
-      0.
-    num_candidate_attributes_ratio: Ratio of attributes tested at each node. If
-      set, it is equivalent to `num_candidate_attributes =
-      number_of_input_features x num_candidate_attributes_ratio`. The possible
-      values are between ]0, and 1] as well as -1. If not set or equal to -1,
-      the `num_candidate_attributes` is used. Default: -1.0.
-    pure_serving_model: Clear the model from any information that is not
-      required for model serving. This includes debugging, model interpretation
-      and other meta-data. The size of the serialized model can be reduced
-      significatively (50% model size reduction is common). This parameter has
-      no impact on the quality, serving speed or RAM usage of model serving.
-      Default: False.
-    random_seed: Random seed for the training of the model. Learners are
-      expected to be deterministic by the random seed. Default: 123456.
-    sorting_strategy: How are sorted the numerical features in order to find the
-      splits - PRESORT: The features are pre-sorted at the start of the
-      training. This solution is faster but consumes much more memory than
-      IN_NODE. - IN_NODE: The features are sorted just before being used in the
-      node. This solution is slow but consumes little amount of memory. .
-      Default: "PRESORT".
-    sparse_oblique_normalization: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Normalization applied on the features, before
-      applying the sparse oblique projections. - `NONE`: No normalization. -
-      `STANDARD_DEVIATION`: Normalize the feature by the estimated standard
-      deviation on the entire train dataset. Also known as Z-Score
-      normalization. - `MIN_MAX`: Normalize the feature by the range (i.e.
-      max-min) estimated on the entire train dataset. Default: None.
-    sparse_oblique_num_projections_exponent: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections
-      to test at each node as `num_features^num_projections_exponent`. Default:
-      None.
-    sparse_oblique_projection_density_factor: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections
-      to test at each node as `num_features^num_projections_exponent`. Default:
-      None.
-    sparse_oblique_weights: For sparse oblique splits i.e.
-      `split_axis=SPARSE_OBLIQUE`. Possible values: - `BINARY`: The oblique
-      weights are sampled in {-1,1} (default). - `CONTINUOUS`: The oblique
-      weights are be sampled in [-1,1]. Default: None.
-    split_axis: What structure of split to consider for numerical features. -
-      `AXIS_ALIGNED`: Axis aligned splits (i.e. one condition at a time). This
-      is the "classical" way to train a tree. Default value. - `SPARSE_OBLIQUE`:
-      Sparse oblique splits (i.e. splits one a small number of features) from
-      "Sparse Projection Oblique Random Forests", Tomita et al., 2020. Default:
-      "AXIS_ALIGNED".
-    uplift_min_examples_in_treatment: For uplift models only. Minimum number of
-      examples per treatment in a node. Default: 5.
-    uplift_split_score: For uplift models only. Splitter score i.e. score
-      optimized by the splitters. The scores are introduced in "Decision trees
-      for uplift modeling with single and multiple treatments", Rzepakowski et
-      al. Notation: `p` probability / average value of the positive outcome, `q`
-      probability / average value in the control group. - `KULLBACK_LEIBLER` or
-      `KL`: - p log (p/q) - `EUCLIDEAN_DISTANCE` or `ED`: (p-q)^2 -
-      `CHI_SQUARED` or `CS`: (p-q)^2/q
-        Default: "KULLBACK_LEIBLER".
-    validation_ratio: Ratio of the training dataset used to create the
-      validation dataset for pruning the tree. If set to 0, the entire dataset
-      is used for training, and the tree is not pruned. Default: 0.1.
-    num_threads: Number of threads used to train the model. Different learning
-      algorithms use multi-threading differently and with different degree of
-      efficiency. If `None`, `num_threads` will be automatically set to the
-      number of processors (up to a maximum of 32; or set to 6 if the number of
-      processors is not available). Making `num_threads` significantly larger
-      than the number of processors can slow-down the training speed. The
-      default value logic might change in the future.
-    resume_training: If true, the model training resumes from the checkpoint
-      stored in the `working_dir` directory. If `working_dir` does not contain
-      any model checkpoint, the training starts from the beginning. Resuming
-      training is useful in the following situations: (1) The training was
-      interrupted by the user (e.g. ctrl+c or "stop" button in a notebook) or
-      rescheduled, or (2) the hyper-parameter of the learner was changed e.g.
-      increasing the number of trees.
-    working_dir: Path to a directory available for the learning algorithm to
-      store intermediate computation results. Depending on the learning
-      algorithm and parameters, the working_dir might be optional, required, or
-      ignored. For instance, distributed training algorithm always need a
-      "working_dir", and the gradient boosted tree and hyper-parameter tuners
-      will export artefacts to the "working_dir" if provided.
-    resume_training_snapshot_interval_seconds: Indicative number of seconds in
-      between snapshots when `resume_training=True`. Might be ignored by some
-      learners.
-    tuner: If set, automatically select the best hyperparameters using the
-      provided tuner. When using distributed training, the tuning is
-      distributed.
-    workers: If set, enable distributed training. "workers" is the list of IP
-      addresses of the workers. A worker is a process running
-      `ydf.start_worker(port)`.
-  """
-
-  def __init__(
-      self,
-      label: str,
-      task: generic_learner.Task = generic_learner.Task.CLASSIFICATION,
-      weights: Optional[str] = None,
-      ranking_group: Optional[str] = None,
-      uplift_treatment: Optional[str] = None,
-      features: dataspec.ColumnDefs = None,
-      include_all_columns: bool = False,
-      max_vocab_count: int = 2000,
-      min_vocab_frequency: int = 5,
-      discretize_numerical_columns: bool = False,
-      num_discretized_numerical_bins: int = 255,
-      max_num_scanned_rows_to_infer_semantic: int = 10000,
-      max_num_scanned_rows_to_compute_statistics: int = 10000,
-      data_spec: Optional[data_spec_pb2.DataSpecification] = None,
-      allow_na_conditions: Optional[bool] = False,
-      categorical_algorithm: Optional[str] = "CART",
-      categorical_set_split_greedy_sampling: Optional[float] = 0.1,
-      categorical_set_split_max_num_items: Optional[int] = -1,
-      categorical_set_split_min_item_frequency: Optional[int] = 1,
-      growing_strategy: Optional[str] = "LOCAL",
-      honest: Optional[bool] = False,
-      honest_fixed_separation: Optional[bool] = False,
-      honest_ratio_leaf_examples: Optional[float] = 0.5,
-      in_split_min_examples_check: Optional[bool] = True,
-      keep_non_leaf_label_distribution: Optional[bool] = True,
-      max_depth: Optional[int] = 16,
-      max_num_nodes: Optional[int] = None,
-      maximum_model_size_in_memory_in_bytes: Optional[float] = -1.0,
-      maximum_training_duration_seconds: Optional[float] = -1.0,
-      min_examples: Optional[int] = 5,
-      missing_value_policy: Optional[str] = "GLOBAL_IMPUTATION",
-      num_candidate_attributes: Optional[int] = 0,
-      num_candidate_attributes_ratio: Optional[float] = -1.0,
-      pure_serving_model: Optional[bool] = False,
-      random_seed: Optional[int] = 123456,
-      sorting_strategy: Optional[str] = "PRESORT",
-      sparse_oblique_normalization: Optional[str] = None,
-      sparse_oblique_num_projections_exponent: Optional[float] = None,
-      sparse_oblique_projection_density_factor: Optional[float] = None,
-      sparse_oblique_weights: Optional[str] = None,
-      split_axis: Optional[str] = "AXIS_ALIGNED",
-      uplift_min_examples_in_treatment: Optional[int] = 5,
-      uplift_split_score: Optional[str] = "KULLBACK_LEIBLER",
-      validation_ratio: Optional[float] = 0.1,
-      num_threads: Optional[int] = None,
-      working_dir: Optional[str] = None,
-      resume_training: bool = False,
-      resume_training_snapshot_interval_seconds: int = 1800,
-      tuner: Optional[tuner_lib.AbstractTuner] = None,
-      workers: Optional[Sequence[str]] = None,
-  ):
-
-    hyper_parameters = {
-        "allow_na_conditions": allow_na_conditions,
-        "categorical_algorithm": categorical_algorithm,
-        "categorical_set_split_greedy_sampling": (
-            categorical_set_split_greedy_sampling
-        ),
-        "categorical_set_split_max_num_items": (
-            categorical_set_split_max_num_items
-        ),
-        "categorical_set_split_min_item_frequency": (
-            categorical_set_split_min_item_frequency
-        ),
-        "growing_strategy": growing_strategy,
-        "honest": honest,
-        "honest_fixed_separation": honest_fixed_separation,
-        "honest_ratio_leaf_examples": honest_ratio_leaf_examples,
-        "in_split_min_examples_check": in_split_min_examples_check,
-        "keep_non_leaf_label_distribution": keep_non_leaf_label_distribution,
-        "max_depth": max_depth,
-        "max_num_nodes": max_num_nodes,
-        "maximum_model_size_in_memory_in_bytes": (
-            maximum_model_size_in_memory_in_bytes
-        ),
-        "maximum_training_duration_seconds": maximum_training_duration_seconds,
-        "min_examples": min_examples,
-        "missing_value_policy": missing_value_policy,
-        "num_candidate_attributes": num_candidate_attributes,
-        "num_candidate_attributes_ratio": num_candidate_attributes_ratio,
-        "pure_serving_model": pure_serving_model,
-        "random_seed": random_seed,
-        "sorting_strategy": sorting_strategy,
-        "sparse_oblique_normalization": sparse_oblique_normalization,
-        "sparse_oblique_num_projections_exponent": (
-            sparse_oblique_num_projections_exponent
-        ),
-        "sparse_oblique_projection_density_factor": (
-            sparse_oblique_projection_density_factor
-        ),
-        "sparse_oblique_weights": sparse_oblique_weights,
-        "split_axis": split_axis,
-        "uplift_min_examples_in_treatment": uplift_min_examples_in_treatment,
-        "uplift_split_score": uplift_split_score,
-        "validation_ratio": validation_ratio,
-    }
-
-    data_spec_args = dataspec.DataSpecInferenceArgs(
-        columns=dataspec.normalize_column_defs(features),
-        include_all_columns=include_all_columns,
-        max_vocab_count=max_vocab_count,
-        min_vocab_frequency=min_vocab_frequency,
-        discretize_numerical_columns=discretize_numerical_columns,
-        num_discretized_numerical_bins=num_discretized_numerical_bins,
-        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
-        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
-    )
-
-    deployment_config = self._build_deployment_config(
-        num_threads=num_threads,
-        resume_training=resume_training,
-        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
-        working_dir=working_dir,
-        workers=workers,
-    )
-
-    super().__init__(
-        learner_name="CART",
-        task=task,
-        label=label,
-        weights=weights,
-        ranking_group=ranking_group,
-        uplift_treatment=uplift_treatment,
-        data_spec_args=data_spec_args,
-        data_spec=data_spec,
-        hyper_parameters=hyper_parameters,
-        deployment_config=deployment_config,
-        tuner=tuner,
-    )
-
-  def train(
-      self,
-      ds: dataset.InputDataset,
-      valid: Optional[dataset.InputDataset] = None,
-  ) -> random_forest_model.RandomForestModel:
-    """Trains a model on the given dataset.
-
-    Options for dataset reading are given on the learner. Consult the
-    documentation of the learner or ydf.create_vertical_dataset() for additional
-    information on dataset reading in YDF.
-
-    Usage example:
-
-    ```
-    import ydf
-    import pandas as pd
-
-    train_ds = pd.read_csv(...)
-
-    learner = ydf.CartLearner(label="label")
-    model = learner.train(train_ds)
-    print(model.summary())
-    ```
-
-    If training is interrupted (for example, by interrupting the cell execution
-    in Colab), the model will be returned to the state it was in at the moment
-    of interruption.
-
-    Args:
-      ds: Training dataset.
-      valid: Optional validation dataset. Some learners, such as Random Forest,
-        do not need validation dataset. Some learners, such as
-        GradientBoostedTrees, automatically extract a validation dataset from
-        the training dataset if the validation dataset is not provided.
-
-    Returns:
-      A trained model.
-    """
-    return super().train(ds, valid)
-
-  @classmethod
-  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
-    return abstract_learner_pb2.LearnerCapabilities(
-        support_max_training_duration=True,
-        resume_training=False,
-        support_validation_dataset=False,
-        support_partial_cache_dataset_format=False,
-        support_max_model_size_in_memory=False,
-        support_monotonic_constraints=False,
-    )
-
-  @classmethod
-  def hyperparameter_templates(
-      cls,
-  ) -> Dict[str, hyperparameters.HyperparameterTemplate]:
-    r"""Hyperparameter templates for this Learner.
-
-    This learner currently does not provide any hyperparameter templates, this
-    method is provided for consistency with other learners.
-
-    Returns:
-      Empty dictionary.
-    """
-    return {}
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+r"""Wrappers around the YDF learners.
+
+This file is generated automatically by running the following commands:
+  bazel build //external/ydf_cc/yggdrasil_decision_forests/port/python/ydf/learner:specialized_learners\
+  && bazel-bin/external/ydf_cc/yggdrasil_decision_forests/port/python/ydf/learner/specialized_learners_generator\
+  > external/ydf_cc/yggdrasil_decision_forests/port/python/ydf/learner/specialized_learners_pre_generated.py
+
+Please don't change this file directly. Instead, changes the source. The
+documentation source is contained in the "GetGenericHyperParameterSpecification"
+method of each learner e.g. GetGenericHyperParameterSpecification in
+learner/gradient_boosted_trees/gradient_boosted_trees.cc contains the
+documentation (and meta-data) used to generate this file.
+
+In particular, these pre-generated wrappers included in the source code are 
+included for reference only. The actual wrappers are re-generated during
+compilation.
+"""
+
+from typing import Dict, Optional, Sequence, Union
+
+from ydf.proto.dataset import data_spec_pb2
+from ydf.proto.learner import abstract_learner_pb2
+from ydf.dataset import dataset
+from ydf.dataset import dataspec
+from ydf.learner import custom_loss
+from ydf.learner import generic_learner
+from ydf.learner import hyperparameters
+from ydf.learner import tuner as tuner_lib
+from ydf.model import generic_model
+from ydf.model.gradient_boosted_trees_model import gradient_boosted_trees_model
+from ydf.model.random_forest_model import random_forest_model
+
+
+class RandomForestLearner(generic_learner.GenericLearner):
+  r"""Random Forest learning algorithm.
+
+  A Random Forest (https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)
+  is a collection of deep CART decision trees trained independently and without
+  pruning. Each tree is trained on a random subset of the original training
+  dataset (sampled with replacement).
+
+  The algorithm is unique in that it is robust to overfitting, even in extreme
+  cases e.g. when there are more features than training examples.
+
+  It is probably the most well-known of the Decision Forest training
+  algorithms.
+
+  Usage example:
+
+  ```python
+  import ydf
+  import pandas as pd
+
+  dataset = pd.read_csv("project/dataset.csv")
+
+  model = ydf.RandomForestLearner().train(dataset)
+
+  print(model.summary())
+  ```
+
+  Hyperparameters are configured to give reasonable results for typical
+  datasets. Hyperparameters can also be modified manually (see descriptions)
+  below or by applying the hyperparameter templates available with
+  `RandomForestLearner.hyperparameter_templates()` (see this function's
+  documentation for
+  details).
+
+  Attributes:
+    label: Label of the dataset. The label column should not be identified as a
+      feature in the `features` parameter.
+    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,
+      Task.RANKING, Task.CATEGORICAL_UPLIFT, Task.NUMERICAL_UPLIFT).
+    weights: Name of a feature that identifies the weight of each example. If
+      weights are not specified, unit weights are assumed. The weight column
+      should not be identified as a feature in the `features` parameter.
+    ranking_group: Only for `task=Task.RANKING`. Name of a feature that
+      identifies queries in a query/document ranking task. The ranking group
+      should not be identified as a feature in the `features` parameter.
+    uplift_treatment: Only for `task=Task.CATEGORICAL_UPLIFT` and `task=Task`.
+      NUMERICAL_UPLIFT. Name of a numerical feature that identifies the
+      treatment in an uplift problem. The value 0 is reserved for the control
+      treatment. Currently, only 0/1 binary treatments are supported.
+    features: If None, all columns are used as features. The semantic of the
+      features is determined automatically. Otherwise, if
+      include_all_columns=False (default) only the column listed in `features`
+      are imported. If include_all_columns=True, all the columns are imported as
+      features and only the semantic of the columns NOT in `columns` is
+      determined automatically. If specified,  defines the order of the features
+      - any non-listed features are appended in-order after the specified
+      features (if include_all_columns=True). The label, weights, uplift
+      treatment and ranking_group columns should not be specified as features.
+    include_all_columns: See `features`.
+    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
+      CATEGORICAL_SET columns stored as strings. If more unique values exist,
+      only the most frequent values are kept, and the remaining values are
+      considered as out-of-vocabulary.
+    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
+      and CATEGORICAL_SET columns. Value observed less than
+      `min_vocab_frequency` are considered as out-of-vocabulary.
+    discretize_numerical_columns: If true, discretize all the numerical columns
+      before training. Discretized numerical columns are faster to train with,
+      but they can have a negative impact on the model quality. Using
+      `discretize_numerical_columns=True` is equivalent as setting the column
+      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
+      definition of DISCRETIZED_NUMERICAL for more details.
+    num_discretized_numerical_bins: Number of bins used when disretizing
+      numerical columns.
+    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
+      inferring the column's semantic if it is not explicitly specified. Only
+      used when reading from file, in-memory datasets are always read in full.
+      Setting this to a lower number will speed up dataset reading, but might
+      result in incorrect column semantics. Set to -1 to scan the entire
+      dataset.
+    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
+      computing a column's statistics. Only used when reading from file,
+      in-memory datasets are always read in full. A column's statistics include
+      the dictionary for categorical features and the mean / min / max for
+      numerical features. Setting this to a lower number will speed up dataset
+      reading, but skew statistics in the dataspec, which can hurt model quality
+      (e.g. if an important category of a categorical feature is considered
+      OOV). Set to -1 to scan the entire dataset.
+    data_spec: Dataspec to be used (advanced). If a data spec is given,
+      `columns`, `include_all_columns`, `max_vocab_count`,
+      `min_vocab_frequency`, `discretize_numerical_columns` and
+      `num_discretized_numerical_bins` will be ignored.
+    adapt_bootstrap_size_ratio_for_maximum_training_duration: Control how the
+      maximum training duration (if set) is applied. If false, the training stop
+      when the time is used. If true, adapts the size of the sampled dataset
+      used to train each tree such that `num_trees` will train within
+      `maximum_training_duration`. Has no effect if there is no maximum training
+      duration specified. Default: False.
+    allow_na_conditions: If true, the tree training evaluates conditions of the
+      type `X is NA` i.e. `X is missing`. Default: False.
+    bootstrap_size_ratio: Number of examples used to train each trees; expressed
+      as a ratio of the training dataset size. Default: 1.0.
+    bootstrap_training_dataset: If true (default), each tree is trained on a
+      separate dataset sampled with replacement from the original dataset. If
+      false, all the trees are trained on the entire same dataset. If
+      bootstrap_training_dataset:false, OOB metrics are not available.
+        bootstrap_training_dataset=false is used in "Extremely randomized trees"
+        (https://link.springer.com/content/pdf/10.1007%2Fs10994-006-6226-1.pdf).
+      Default: True.
+    categorical_algorithm: How to learn splits on categorical attributes. -
+      `CART`: CART algorithm. Find categorical splits of the form "value \\in
+      mask". The solution is exact for binary classification, regression and
+      ranking. It is approximated for multi-class classification. This is a good
+      first algorithm to use. In case of overfitting (very small dataset, large
+      dictionary), the "random" algorithm is a good alternative. - `ONE_HOT`:
+      One-hot encoding. Find the optimal categorical split of the form
+      "attribute == param". This method is similar (but more efficient) than
+      converting converting each possible categorical value into a boolean
+      feature. This method is available for comparison purpose and generally
+      performs worse than other alternatives. - `RANDOM`: Best splits among a
+      set of random candidate. Find the a categorical split of the form "value
+      \\in mask" using a random search. This solution can be seen as an
+      approximation of the CART algorithm. This method is a strong alternative
+      to CART. This algorithm is inspired from section "5.1 Categorical
+      Variables" of "Random Forest", 2001.
+        Default: "CART".
+    categorical_set_split_greedy_sampling: For categorical set splits e.g.
+      texts. Probability for a categorical value to be a candidate for the
+      positive set. The sampling is applied once per node (i.e. not at every
+      step of the greedy optimization). Default: 0.1.
+    categorical_set_split_max_num_items: For categorical set splits e.g. texts.
+      Maximum number of items (prior to the sampling). If more items are
+      available, the least frequent items are ignored. Changing this value is
+      similar to change the "max_vocab_count" before loading the dataset, with
+      the following exception: With `max_vocab_count`, all the remaining items
+      are grouped in a special Out-of-vocabulary item. With `max_num_items`,
+      this is not the case. Default: -1.
+    categorical_set_split_min_item_frequency: For categorical set splits e.g.
+      texts. Minimum number of occurrences of an item to be considered.
+      Default: 1.
+    compute_oob_performances: If true, compute the Out-of-bag evaluation (then
+      available in the summary and model inspector). This evaluation is a cheap
+      alternative to cross-validation evaluation. Default: True.
+    compute_oob_variable_importances: If true, compute the Out-of-bag feature
+      importance (then available in the summary and model inspector). Note that
+      the OOB feature importance can be expensive to compute. Default: False.
+    growing_strategy: How to grow the tree. - `LOCAL`: Each node is split
+      independently of the other nodes. In other words, as long as a node
+      satisfy the splits "constraints (e.g. maximum depth, minimum number of
+      observations), the node will be split. This is the "classical" way to grow
+      decision trees. - `BEST_FIRST_GLOBAL`: The node with the best loss
+      reduction among all the nodes of the tree is selected for splitting. This
+      method is also called "best first" or "leaf-wise growth". See "Best-first
+      decision tree learning", Shi and "Additive logistic regression : A
+      statistical view of boosting", Friedman for more details. Default:
+      "LOCAL".
+    honest: In honest trees, different training examples are used to infer the
+      structure and the leaf values. This regularization technique trades
+      examples for bias estimates. It might increase or reduce the quality of
+      the model. See "Generalized Random Forests", Athey et al. In this paper,
+      Honest trees are trained with the Random Forest algorithm with a sampling
+      without replacement. Default: False.
+    honest_fixed_separation: For honest trees only i.e. honest=true. If true, a
+      new random separation is generated for each tree. If false, the same
+      separation is used for all the trees (e.g., in Gradient Boosted Trees
+      containing multiple trees). Default: False.
+    honest_ratio_leaf_examples: For honest trees only i.e. honest=true. Ratio of
+      examples used to set the leaf values. Default: 0.5.
+    in_split_min_examples_check: Whether to check the `min_examples` constraint
+      in the split search (i.e. splits leading to one child having less than
+      `min_examples` examples are considered invalid) or before the split search
+      (i.e. a node can be derived only if it contains more than `min_examples`
+      examples). If false, there can be nodes with less than `min_examples`
+      training examples. Default: True.
+    keep_non_leaf_label_distribution: Whether to keep the node value (i.e. the
+      distribution of the labels of the training examples) of non-leaf nodes.
+      This information is not used during serving, however it can be used for
+      model interpretation as well as hyper parameter tuning. This can take lots
+      of space, sometimes accounting for half of the model size. Default: True.
+    max_depth: Maximum depth of the tree. `max_depth=1` means that all trees
+      will be roots. `max_depth=-1` means that tree depth is not restricted by
+      this parameter. Values <= -2 will be ignored. Default: 16.
+    max_num_nodes: Maximum number of nodes in the tree. Set to -1 to disable
+      this limit. Only available for `growing_strategy=BEST_FIRST_GLOBAL`.
+      Default: None.
+    maximum_model_size_in_memory_in_bytes: Limit the size of the model when
+      stored in ram. Different algorithms can enforce this limit differently.
+      Note that when models are compiled into an inference, the size of the
+      inference engine is generally much smaller than the original model.
+      Default: -1.0.
+    maximum_training_duration_seconds: Maximum training duration of the model
+      expressed in seconds. Each learning algorithm is free to use this
+      parameter at it sees fit. Enabling maximum training duration makes the
+      model training non-deterministic. Default: -1.0.
+    min_examples: Minimum number of examples in a node. Default: 5.
+    missing_value_policy: Method used to handle missing attribute values. -
+      `GLOBAL_IMPUTATION`: Missing attribute values are imputed, with the mean
+      (in case of numerical attribute) or the most-frequent-item (in case of
+      categorical attribute) computed on the entire dataset (i.e. the
+      information contained in the data spec). - `LOCAL_IMPUTATION`: Missing
+      attribute values are imputed with the mean (numerical attribute) or
+      most-frequent-item (in the case of categorical attribute) evaluated on the
+      training examples in the current node. - `RANDOM_LOCAL_IMPUTATION`:
+      Missing attribute values are imputed from randomly sampled values from the
+      training examples in the current node. This method was proposed by Clinic
+      et al. in "Random Survival Forests"
+      (https://projecteuclid.org/download/pdfview_1/euclid.aoas/1223908043).
+        Default: "GLOBAL_IMPUTATION".
+    num_candidate_attributes: Number of unique valid attributes tested for each
+      node. An attribute is valid if it has at least a valid split. If
+      `num_candidate_attributes=0`, the value is set to the classical default
+      value for Random Forest: `sqrt(number of input attributes)` in case of
+      classification and `number_of_input_attributes / 3` in case of regression.
+      If `num_candidate_attributes=-1`, all the attributes are tested. Default:
+      0.
+    num_candidate_attributes_ratio: Ratio of attributes tested at each node. If
+      set, it is equivalent to `num_candidate_attributes =
+      number_of_input_features x num_candidate_attributes_ratio`. The possible
+      values are between ]0, and 1] as well as -1. If not set or equal to -1,
+      the `num_candidate_attributes` is used. Default: -1.0.
+    num_oob_variable_importances_permutations: Number of time the dataset is
+      re-shuffled to compute the permutation variable importances. Increasing
+      this value increase the training time (if
+      "compute_oob_variable_importances:true") as well as the stability of the
+      oob variable importance metrics. Default: 1.
+    num_trees: Number of individual decision trees. Increasing the number of
+      trees can increase the quality of the model at the expense of size,
+      training speed, and inference latency. Default: 300.
+    pure_serving_model: Clear the model from any information that is not
+      required for model serving. This includes debugging, model interpretation
+      and other meta-data. The size of the serialized model can be reduced
+      significatively (50% model size reduction is common). This parameter has
+      no impact on the quality, serving speed or RAM usage of model serving.
+      Default: False.
+    random_seed: Random seed for the training of the model. Learners are
+      expected to be deterministic by the random seed. Default: 123456.
+    sampling_with_replacement: If true, the training examples are sampled with
+      replacement. If false, the training samples are sampled without
+      replacement. Only used when "bootstrap_training_dataset=true". If false
+      (sampling without replacement) and if "bootstrap_size_ratio=1" (default),
+      all the examples are used to train all the trees (you probably do not want
+      that). Default: True.
+    sorting_strategy: How are sorted the numerical features in order to find the
+      splits - PRESORT: The features are pre-sorted at the start of the
+      training. This solution is faster but consumes much more memory than
+      IN_NODE. - IN_NODE: The features are sorted just before being used in the
+      node. This solution is slow but consumes little amount of memory. .
+      Default: "PRESORT".
+    sparse_oblique_normalization: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Normalization applied on the features, before
+      applying the sparse oblique projections. - `NONE`: No normalization. -
+      `STANDARD_DEVIATION`: Normalize the feature by the estimated standard
+      deviation on the entire train dataset. Also known as Z-Score
+      normalization. - `MIN_MAX`: Normalize the feature by the range (i.e.
+      max-min) estimated on the entire train dataset. Default: None.
+    sparse_oblique_num_projections_exponent: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections
+      to test at each node as `num_features^num_projections_exponent`. Default:
+      None.
+    sparse_oblique_projection_density_factor: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections
+      to test at each node as `num_features^num_projections_exponent`. Default:
+      None.
+    sparse_oblique_weights: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Possible values: - `BINARY`: The oblique
+      weights are sampled in {-1,1} (default). - `CONTINUOUS`: The oblique
+      weights are be sampled in [-1,1]. Default: None.
+    split_axis: What structure of split to consider for numerical features. -
+      `AXIS_ALIGNED`: Axis aligned splits (i.e. one condition at a time). This
+      is the "classical" way to train a tree. Default value. - `SPARSE_OBLIQUE`:
+      Sparse oblique splits (i.e. splits one a small number of features) from
+      "Sparse Projection Oblique Random Forests", Tomita et al., 2020. Default:
+      "AXIS_ALIGNED".
+    uplift_min_examples_in_treatment: For uplift models only. Minimum number of
+      examples per treatment in a node. Default: 5.
+    uplift_split_score: For uplift models only. Splitter score i.e. score
+      optimized by the splitters. The scores are introduced in "Decision trees
+      for uplift modeling with single and multiple treatments", Rzepakowski et
+      al. Notation: `p` probability / average value of the positive outcome, `q`
+      probability / average value in the control group. - `KULLBACK_LEIBLER` or
+      `KL`: - p log (p/q) - `EUCLIDEAN_DISTANCE` or `ED`: (p-q)^2 -
+      `CHI_SQUARED` or `CS`: (p-q)^2/q
+        Default: "KULLBACK_LEIBLER".
+    winner_take_all: Control how classification trees vote. If true, each tree
+      votes for one class. If false, each tree vote for a distribution of
+      classes. winner_take_all_inference=false is often preferable. Default:
+      True.
+    num_threads: Number of threads used to train the model. Different learning
+      algorithms use multi-threading differently and with different degree of
+      efficiency. If `None`, `num_threads` will be automatically set to the
+      number of processors (up to a maximum of 32; or set to 6 if the number of
+      processors is not available). Making `num_threads` significantly larger
+      than the number of processors can slow-down the training speed. The
+      default value logic might change in the future.
+    resume_training: If true, the model training resumes from the checkpoint
+      stored in the `working_dir` directory. If `working_dir` does not contain
+      any model checkpoint, the training starts from the beginning. Resuming
+      training is useful in the following situations: (1) The training was
+      interrupted by the user (e.g. ctrl+c or "stop" button in a notebook) or
+      rescheduled, or (2) the hyper-parameter of the learner was changed e.g.
+      increasing the number of trees.
+    working_dir: Path to a directory available for the learning algorithm to
+      store intermediate computation results. Depending on the learning
+      algorithm and parameters, the working_dir might be optional, required, or
+      ignored. For instance, distributed training algorithm always need a
+      "working_dir", and the gradient boosted tree and hyper-parameter tuners
+      will export artefacts to the "working_dir" if provided.
+    resume_training_snapshot_interval_seconds: Indicative number of seconds in
+      between snapshots when `resume_training=True`. Might be ignored by some
+      learners.
+    tuner: If set, automatically select the best hyperparameters using the
+      provided tuner. When using distributed training, the tuning is
+      distributed.
+    workers: If set, enable distributed training. "workers" is the list of IP
+      addresses of the workers. A worker is a process running
+      `ydf.start_worker(port)`.
+  """
+
+  def __init__(
+      self,
+      label: str,
+      task: generic_learner.Task = generic_learner.Task.CLASSIFICATION,
+      weights: Optional[str] = None,
+      ranking_group: Optional[str] = None,
+      uplift_treatment: Optional[str] = None,
+      features: dataspec.ColumnDefs = None,
+      include_all_columns: bool = False,
+      max_vocab_count: int = 2000,
+      min_vocab_frequency: int = 5,
+      discretize_numerical_columns: bool = False,
+      num_discretized_numerical_bins: int = 255,
+      max_num_scanned_rows_to_infer_semantic: int = 10000,
+      max_num_scanned_rows_to_compute_statistics: int = 10000,
+      data_spec: Optional[data_spec_pb2.DataSpecification] = None,
+      adapt_bootstrap_size_ratio_for_maximum_training_duration: Optional[
+          bool
+      ] = False,
+      allow_na_conditions: Optional[bool] = False,
+      bootstrap_size_ratio: Optional[float] = 1.0,
+      bootstrap_training_dataset: Optional[bool] = True,
+      categorical_algorithm: Optional[str] = "CART",
+      categorical_set_split_greedy_sampling: Optional[float] = 0.1,
+      categorical_set_split_max_num_items: Optional[int] = -1,
+      categorical_set_split_min_item_frequency: Optional[int] = 1,
+      compute_oob_performances: Optional[bool] = True,
+      compute_oob_variable_importances: Optional[bool] = False,
+      growing_strategy: Optional[str] = "LOCAL",
+      honest: Optional[bool] = False,
+      honest_fixed_separation: Optional[bool] = False,
+      honest_ratio_leaf_examples: Optional[float] = 0.5,
+      in_split_min_examples_check: Optional[bool] = True,
+      keep_non_leaf_label_distribution: Optional[bool] = True,
+      max_depth: Optional[int] = 16,
+      max_num_nodes: Optional[int] = None,
+      maximum_model_size_in_memory_in_bytes: Optional[float] = -1.0,
+      maximum_training_duration_seconds: Optional[float] = -1.0,
+      min_examples: Optional[int] = 5,
+      missing_value_policy: Optional[str] = "GLOBAL_IMPUTATION",
+      num_candidate_attributes: Optional[int] = 0,
+      num_candidate_attributes_ratio: Optional[float] = -1.0,
+      num_oob_variable_importances_permutations: Optional[int] = 1,
+      num_trees: Optional[int] = 300,
+      pure_serving_model: Optional[bool] = False,
+      random_seed: Optional[int] = 123456,
+      sampling_with_replacement: Optional[bool] = True,
+      sorting_strategy: Optional[str] = "PRESORT",
+      sparse_oblique_normalization: Optional[str] = None,
+      sparse_oblique_num_projections_exponent: Optional[float] = None,
+      sparse_oblique_projection_density_factor: Optional[float] = None,
+      sparse_oblique_weights: Optional[str] = None,
+      split_axis: Optional[str] = "AXIS_ALIGNED",
+      uplift_min_examples_in_treatment: Optional[int] = 5,
+      uplift_split_score: Optional[str] = "KULLBACK_LEIBLER",
+      winner_take_all: Optional[bool] = True,
+      num_threads: Optional[int] = None,
+      working_dir: Optional[str] = None,
+      resume_training: bool = False,
+      resume_training_snapshot_interval_seconds: int = 1800,
+      tuner: Optional[tuner_lib.AbstractTuner] = None,
+      workers: Optional[Sequence[str]] = None,
+  ):
+
+    hyper_parameters = {
+        "adapt_bootstrap_size_ratio_for_maximum_training_duration": (
+            adapt_bootstrap_size_ratio_for_maximum_training_duration
+        ),
+        "allow_na_conditions": allow_na_conditions,
+        "bootstrap_size_ratio": bootstrap_size_ratio,
+        "bootstrap_training_dataset": bootstrap_training_dataset,
+        "categorical_algorithm": categorical_algorithm,
+        "categorical_set_split_greedy_sampling": (
+            categorical_set_split_greedy_sampling
+        ),
+        "categorical_set_split_max_num_items": (
+            categorical_set_split_max_num_items
+        ),
+        "categorical_set_split_min_item_frequency": (
+            categorical_set_split_min_item_frequency
+        ),
+        "compute_oob_performances": compute_oob_performances,
+        "compute_oob_variable_importances": compute_oob_variable_importances,
+        "growing_strategy": growing_strategy,
+        "honest": honest,
+        "honest_fixed_separation": honest_fixed_separation,
+        "honest_ratio_leaf_examples": honest_ratio_leaf_examples,
+        "in_split_min_examples_check": in_split_min_examples_check,
+        "keep_non_leaf_label_distribution": keep_non_leaf_label_distribution,
+        "max_depth": max_depth,
+        "max_num_nodes": max_num_nodes,
+        "maximum_model_size_in_memory_in_bytes": (
+            maximum_model_size_in_memory_in_bytes
+        ),
+        "maximum_training_duration_seconds": maximum_training_duration_seconds,
+        "min_examples": min_examples,
+        "missing_value_policy": missing_value_policy,
+        "num_candidate_attributes": num_candidate_attributes,
+        "num_candidate_attributes_ratio": num_candidate_attributes_ratio,
+        "num_oob_variable_importances_permutations": (
+            num_oob_variable_importances_permutations
+        ),
+        "num_trees": num_trees,
+        "pure_serving_model": pure_serving_model,
+        "random_seed": random_seed,
+        "sampling_with_replacement": sampling_with_replacement,
+        "sorting_strategy": sorting_strategy,
+        "sparse_oblique_normalization": sparse_oblique_normalization,
+        "sparse_oblique_num_projections_exponent": (
+            sparse_oblique_num_projections_exponent
+        ),
+        "sparse_oblique_projection_density_factor": (
+            sparse_oblique_projection_density_factor
+        ),
+        "sparse_oblique_weights": sparse_oblique_weights,
+        "split_axis": split_axis,
+        "uplift_min_examples_in_treatment": uplift_min_examples_in_treatment,
+        "uplift_split_score": uplift_split_score,
+        "winner_take_all": winner_take_all,
+    }
+
+    data_spec_args = dataspec.DataSpecInferenceArgs(
+        columns=dataspec.normalize_column_defs(features),
+        include_all_columns=include_all_columns,
+        max_vocab_count=max_vocab_count,
+        min_vocab_frequency=min_vocab_frequency,
+        discretize_numerical_columns=discretize_numerical_columns,
+        num_discretized_numerical_bins=num_discretized_numerical_bins,
+        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
+        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
+    )
+
+    deployment_config = self._build_deployment_config(
+        num_threads=num_threads,
+        resume_training=resume_training,
+        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
+        working_dir=working_dir,
+        workers=workers,
+    )
+
+    super().__init__(
+        learner_name="RANDOM_FOREST",
+        task=task,
+        label=label,
+        weights=weights,
+        ranking_group=ranking_group,
+        uplift_treatment=uplift_treatment,
+        data_spec_args=data_spec_args,
+        data_spec=data_spec,
+        hyper_parameters=hyper_parameters,
+        deployment_config=deployment_config,
+        tuner=tuner,
+    )
+
+  def train(
+      self,
+      ds: dataset.InputDataset,
+      valid: Optional[dataset.InputDataset] = None,
+  ) -> random_forest_model.RandomForestModel:
+    """Trains a model on the given dataset.
+
+    Options for dataset reading are given on the learner. Consult the
+    documentation of the learner or ydf.create_vertical_dataset() for additional
+    information on dataset reading in YDF.
+
+    Usage example:
+
+    ```
+    import ydf
+    import pandas as pd
+
+    train_ds = pd.read_csv(...)
+
+    learner = ydf.RandomForestLearner(label="label")
+    model = learner.train(train_ds)
+    print(model.summary())
+    ```
+
+    If training is interrupted (for example, by interrupting the cell execution
+    in Colab), the model will be returned to the state it was in at the moment
+    of interruption.
+
+    Args:
+      ds: Training dataset.
+      valid: Optional validation dataset. Some learners, such as Random Forest,
+        do not need validation dataset. Some learners, such as
+        GradientBoostedTrees, automatically extract a validation dataset from
+        the training dataset if the validation dataset is not provided.
+
+    Returns:
+      A trained model.
+    """
+    return super().train(ds, valid)
+
+  @classmethod
+  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
+    return abstract_learner_pb2.LearnerCapabilities(
+        support_max_training_duration=True,
+        resume_training=False,
+        support_validation_dataset=False,
+        support_partial_cache_dataset_format=False,
+        support_max_model_size_in_memory=True,
+        support_monotonic_constraints=False,
+    )
+
+  @classmethod
+  def hyperparameter_templates(
+      cls,
+  ) -> Dict[str, hyperparameters.HyperparameterTemplate]:
+    r"""Hyperparameter templates for this Learner.
+
+    Hyperparameter templates are sets of pre-defined hyperparameters for easy
+    access to different variants of the learner. Each template is a mapping to a
+    set of hyperparameters and can be applied directly on the learner.
+
+    Usage example:
+    ```python
+    templates = ydf.RandomForestLearner.hyperparameter_templates()
+    better_defaultv1 = templates["better_defaultv1"]
+    # Print a description of the template
+    print(better_defaultv1.description)
+    # Apply the template's settings on the learner.
+    learner = ydf.RandomForestLearner(label, **better_defaultv1)
+    ```
+
+    Returns:
+      Dictionary of the available templates
+    """
+    return {
+        "better_defaultv1": hyperparameters.HyperparameterTemplate(
+            name="better_default",
+            version=1,
+            description=(
+                "A configuration that is generally better than the default"
+                " parameters without being more expensive."
+            ),
+            parameters={"winner_take_all": True},
+        ),
+        "benchmark_rank1v1": hyperparameters.HyperparameterTemplate(
+            name="benchmark_rank1",
+            version=1,
+            description=(
+                "Top ranking hyper-parameters on our benchmark slightly"
+                " modified to run in reasonable time."
+            ),
+            parameters={
+                "winner_take_all": True,
+                "categorical_algorithm": "RANDOM",
+                "split_axis": "SPARSE_OBLIQUE",
+                "sparse_oblique_normalization": "MIN_MAX",
+                "sparse_oblique_num_projections_exponent": 1.0,
+            },
+        ),
+    }
+
+
+class HyperparameterOptimizerLearner(generic_learner.GenericLearner):
+  r"""Hyperparameter Optimizer learning algorithm.
+
+  Usage example:
+
+  ```python
+  import ydf
+  import pandas as pd
+
+  dataset = pd.read_csv("project/dataset.csv")
+
+  model = ydf.HyperparameterOptimizerLearner().train(dataset)
+
+  print(model.summary())
+  ```
+
+  Hyperparameters are configured to give reasonable results for typical
+  datasets. Hyperparameters can also be modified manually (see descriptions)
+  below or by applying the hyperparameter templates available with
+  `HyperparameterOptimizerLearner.hyperparameter_templates()` (see this
+  function's documentation for
+  details).
+
+  Attributes:
+    label: Label of the dataset. The label column should not be identified as a
+      feature in the `features` parameter.
+    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,
+      Task.RANKING, Task.CATEGORICAL_UPLIFT, Task.NUMERICAL_UPLIFT).
+    weights: Name of a feature that identifies the weight of each example. If
+      weights are not specified, unit weights are assumed. The weight column
+      should not be identified as a feature in the `features` parameter.
+    ranking_group: Only for `task=Task.RANKING`. Name of a feature that
+      identifies queries in a query/document ranking task. The ranking group
+      should not be identified as a feature in the `features` parameter.
+    uplift_treatment: Only for `task=Task.CATEGORICAL_UPLIFT` and `task=Task`.
+      NUMERICAL_UPLIFT. Name of a numerical feature that identifies the
+      treatment in an uplift problem. The value 0 is reserved for the control
+      treatment. Currently, only 0/1 binary treatments are supported.
+    features: If None, all columns are used as features. The semantic of the
+      features is determined automatically. Otherwise, if
+      include_all_columns=False (default) only the column listed in `features`
+      are imported. If include_all_columns=True, all the columns are imported as
+      features and only the semantic of the columns NOT in `columns` is
+      determined automatically. If specified,  defines the order of the features
+      - any non-listed features are appended in-order after the specified
+      features (if include_all_columns=True). The label, weights, uplift
+      treatment and ranking_group columns should not be specified as features.
+    include_all_columns: See `features`.
+    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
+      CATEGORICAL_SET columns stored as strings. If more unique values exist,
+      only the most frequent values are kept, and the remaining values are
+      considered as out-of-vocabulary.
+    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
+      and CATEGORICAL_SET columns. Value observed less than
+      `min_vocab_frequency` are considered as out-of-vocabulary.
+    discretize_numerical_columns: If true, discretize all the numerical columns
+      before training. Discretized numerical columns are faster to train with,
+      but they can have a negative impact on the model quality. Using
+      `discretize_numerical_columns=True` is equivalent as setting the column
+      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
+      definition of DISCRETIZED_NUMERICAL for more details.
+    num_discretized_numerical_bins: Number of bins used when disretizing
+      numerical columns.
+    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
+      inferring the column's semantic if it is not explicitly specified. Only
+      used when reading from file, in-memory datasets are always read in full.
+      Setting this to a lower number will speed up dataset reading, but might
+      result in incorrect column semantics. Set to -1 to scan the entire
+      dataset.
+    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
+      computing a column's statistics. Only used when reading from file,
+      in-memory datasets are always read in full. A column's statistics include
+      the dictionary for categorical features and the mean / min / max for
+      numerical features. Setting this to a lower number will speed up dataset
+      reading, but skew statistics in the dataspec, which can hurt model quality
+      (e.g. if an important category of a categorical feature is considered
+      OOV). Set to -1 to scan the entire dataset.
+    data_spec: Dataspec to be used (advanced). If a data spec is given,
+      `columns`, `include_all_columns`, `max_vocab_count`,
+      `min_vocab_frequency`, `discretize_numerical_columns` and
+      `num_discretized_numerical_bins` will be ignored.
+    maximum_model_size_in_memory_in_bytes: Limit the size of the model when
+      stored in ram. Different algorithms can enforce this limit differently.
+      Note that when models are compiled into an inference, the size of the
+      inference engine is generally much smaller than the original model.
+      Default: -1.0.
+    maximum_training_duration_seconds: Maximum training duration of the model
+      expressed in seconds. Each learning algorithm is free to use this
+      parameter at it sees fit. Enabling maximum training duration makes the
+      model training non-deterministic. Default: -1.0.
+    pure_serving_model: Clear the model from any information that is not
+      required for model serving. This includes debugging, model interpretation
+      and other meta-data. The size of the serialized model can be reduced
+      significatively (50% model size reduction is common). This parameter has
+      no impact on the quality, serving speed or RAM usage of model serving.
+      Default: False.
+    random_seed: Random seed for the training of the model. Learners are
+      expected to be deterministic by the random seed. Default: 123456.
+    num_threads: Number of threads used to train the model. Different learning
+      algorithms use multi-threading differently and with different degree of
+      efficiency. If `None`, `num_threads` will be automatically set to the
+      number of processors (up to a maximum of 32; or set to 6 if the number of
+      processors is not available). Making `num_threads` significantly larger
+      than the number of processors can slow-down the training speed. The
+      default value logic might change in the future.
+    resume_training: If true, the model training resumes from the checkpoint
+      stored in the `working_dir` directory. If `working_dir` does not contain
+      any model checkpoint, the training starts from the beginning. Resuming
+      training is useful in the following situations: (1) The training was
+      interrupted by the user (e.g. ctrl+c or "stop" button in a notebook) or
+      rescheduled, or (2) the hyper-parameter of the learner was changed e.g.
+      increasing the number of trees.
+    working_dir: Path to a directory available for the learning algorithm to
+      store intermediate computation results. Depending on the learning
+      algorithm and parameters, the working_dir might be optional, required, or
+      ignored. For instance, distributed training algorithm always need a
+      "working_dir", and the gradient boosted tree and hyper-parameter tuners
+      will export artefacts to the "working_dir" if provided.
+    resume_training_snapshot_interval_seconds: Indicative number of seconds in
+      between snapshots when `resume_training=True`. Might be ignored by some
+      learners.
+    tuner: If set, automatically select the best hyperparameters using the
+      provided tuner. When using distributed training, the tuning is
+      distributed.
+    workers: If set, enable distributed training. "workers" is the list of IP
+      addresses of the workers. A worker is a process running
+      `ydf.start_worker(port)`.
+  """
+
+  def __init__(
+      self,
+      label: str,
+      task: generic_learner.Task = generic_learner.Task.CLASSIFICATION,
+      weights: Optional[str] = None,
+      ranking_group: Optional[str] = None,
+      uplift_treatment: Optional[str] = None,
+      features: dataspec.ColumnDefs = None,
+      include_all_columns: bool = False,
+      max_vocab_count: int = 2000,
+      min_vocab_frequency: int = 5,
+      discretize_numerical_columns: bool = False,
+      num_discretized_numerical_bins: int = 255,
+      max_num_scanned_rows_to_infer_semantic: int = 10000,
+      max_num_scanned_rows_to_compute_statistics: int = 10000,
+      data_spec: Optional[data_spec_pb2.DataSpecification] = None,
+      maximum_model_size_in_memory_in_bytes: Optional[float] = -1.0,
+      maximum_training_duration_seconds: Optional[float] = -1.0,
+      pure_serving_model: Optional[bool] = False,
+      random_seed: Optional[int] = 123456,
+      num_threads: Optional[int] = None,
+      working_dir: Optional[str] = None,
+      resume_training: bool = False,
+      resume_training_snapshot_interval_seconds: int = 1800,
+      tuner: Optional[tuner_lib.AbstractTuner] = None,
+      workers: Optional[Sequence[str]] = None,
+  ):
+
+    hyper_parameters = {
+        "maximum_model_size_in_memory_in_bytes": (
+            maximum_model_size_in_memory_in_bytes
+        ),
+        "maximum_training_duration_seconds": maximum_training_duration_seconds,
+        "pure_serving_model": pure_serving_model,
+        "random_seed": random_seed,
+    }
+
+    data_spec_args = dataspec.DataSpecInferenceArgs(
+        columns=dataspec.normalize_column_defs(features),
+        include_all_columns=include_all_columns,
+        max_vocab_count=max_vocab_count,
+        min_vocab_frequency=min_vocab_frequency,
+        discretize_numerical_columns=discretize_numerical_columns,
+        num_discretized_numerical_bins=num_discretized_numerical_bins,
+        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
+        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
+    )
+
+    deployment_config = self._build_deployment_config(
+        num_threads=num_threads,
+        resume_training=resume_training,
+        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
+        working_dir=working_dir,
+        workers=workers,
+    )
+
+    super().__init__(
+        learner_name="HYPERPARAMETER_OPTIMIZER",
+        task=task,
+        label=label,
+        weights=weights,
+        ranking_group=ranking_group,
+        uplift_treatment=uplift_treatment,
+        data_spec_args=data_spec_args,
+        data_spec=data_spec,
+        hyper_parameters=hyper_parameters,
+        deployment_config=deployment_config,
+        tuner=tuner,
+    )
+
+  def train(
+      self,
+      ds: dataset.InputDataset,
+      valid: Optional[dataset.InputDataset] = None,
+  ) -> generic_model.GenericModel:
+    """Trains a model on the given dataset.
+
+    Options for dataset reading are given on the learner. Consult the
+    documentation of the learner or ydf.create_vertical_dataset() for additional
+    information on dataset reading in YDF.
+
+    Usage example:
+
+    ```
+    import ydf
+    import pandas as pd
+
+    train_ds = pd.read_csv(...)
+
+    learner = ydf.HyperparameterOptimizerLearner(label="label")
+    model = learner.train(train_ds)
+    print(model.summary())
+    ```
+
+    If training is interrupted (for example, by interrupting the cell execution
+    in Colab), the model will be returned to the state it was in at the moment
+    of interruption.
+
+    Args:
+      ds: Training dataset.
+      valid: Optional validation dataset. Some learners, such as Random Forest,
+        do not need validation dataset. Some learners, such as
+        GradientBoostedTrees, automatically extract a validation dataset from
+        the training dataset if the validation dataset is not provided.
+
+    Returns:
+      A trained model.
+    """
+    return super().train(ds, valid)
+
+  @classmethod
+  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
+    return abstract_learner_pb2.LearnerCapabilities(
+        support_max_training_duration=True,
+        resume_training=False,
+        support_validation_dataset=False,
+        support_partial_cache_dataset_format=False,
+        support_max_model_size_in_memory=False,
+        support_monotonic_constraints=False,
+    )
+
+  @classmethod
+  def hyperparameter_templates(
+      cls,
+  ) -> Dict[str, hyperparameters.HyperparameterTemplate]:
+    r"""Hyperparameter templates for this Learner.
+
+    This learner currently does not provide any hyperparameter templates, this
+    method is provided for consistency with other learners.
+
+    Returns:
+      Empty dictionary.
+    """
+    return {}
+
+
+class GradientBoostedTreesLearner(generic_learner.GenericLearner):
+  r"""Gradient Boosted Trees learning algorithm.
+
+  A [Gradient Boosted Trees](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)
+  (GBT), also known as Gradient Boosted Decision Trees (GBDT) or Gradient
+  Boosted Machines (GBM),  is a set of shallow decision trees trained
+  sequentially. Each tree is trained to predict and then "correct" for the
+  errors of the previously trained trees (more precisely each tree predict the
+  gradient of the loss relative to the model output).
+
+  Usage example:
+
+  ```python
+  import ydf
+  import pandas as pd
+
+  dataset = pd.read_csv("project/dataset.csv")
+
+  model = ydf.GradientBoostedTreesLearner().train(dataset)
+
+  print(model.summary())
+  ```
+
+  Hyperparameters are configured to give reasonable results for typical
+  datasets. Hyperparameters can also be modified manually (see descriptions)
+  below or by applying the hyperparameter templates available with
+  `GradientBoostedTreesLearner.hyperparameter_templates()` (see this function's
+  documentation for
+  details).
+
+  Attributes:
+    label: Label of the dataset. The label column should not be identified as a
+      feature in the `features` parameter.
+    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,
+      Task.RANKING, Task.CATEGORICAL_UPLIFT, Task.NUMERICAL_UPLIFT).
+    weights: Name of a feature that identifies the weight of each example. If
+      weights are not specified, unit weights are assumed. The weight column
+      should not be identified as a feature in the `features` parameter.
+    ranking_group: Only for `task=Task.RANKING`. Name of a feature that
+      identifies queries in a query/document ranking task. The ranking group
+      should not be identified as a feature in the `features` parameter.
+    uplift_treatment: Only for `task=Task.CATEGORICAL_UPLIFT` and `task=Task`.
+      NUMERICAL_UPLIFT. Name of a numerical feature that identifies the
+      treatment in an uplift problem. The value 0 is reserved for the control
+      treatment. Currently, only 0/1 binary treatments are supported.
+    features: If None, all columns are used as features. The semantic of the
+      features is determined automatically. Otherwise, if
+      include_all_columns=False (default) only the column listed in `features`
+      are imported. If include_all_columns=True, all the columns are imported as
+      features and only the semantic of the columns NOT in `columns` is
+      determined automatically. If specified,  defines the order of the features
+      - any non-listed features are appended in-order after the specified
+      features (if include_all_columns=True). The label, weights, uplift
+      treatment and ranking_group columns should not be specified as features.
+    include_all_columns: See `features`.
+    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
+      CATEGORICAL_SET columns stored as strings. If more unique values exist,
+      only the most frequent values are kept, and the remaining values are
+      considered as out-of-vocabulary.
+    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
+      and CATEGORICAL_SET columns. Value observed less than
+      `min_vocab_frequency` are considered as out-of-vocabulary.
+    discretize_numerical_columns: If true, discretize all the numerical columns
+      before training. Discretized numerical columns are faster to train with,
+      but they can have a negative impact on the model quality. Using
+      `discretize_numerical_columns=True` is equivalent as setting the column
+      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
+      definition of DISCRETIZED_NUMERICAL for more details.
+    num_discretized_numerical_bins: Number of bins used when disretizing
+      numerical columns.
+    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
+      inferring the column's semantic if it is not explicitly specified. Only
+      used when reading from file, in-memory datasets are always read in full.
+      Setting this to a lower number will speed up dataset reading, but might
+      result in incorrect column semantics. Set to -1 to scan the entire
+      dataset.
+    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
+      computing a column's statistics. Only used when reading from file,
+      in-memory datasets are always read in full. A column's statistics include
+      the dictionary for categorical features and the mean / min / max for
+      numerical features. Setting this to a lower number will speed up dataset
+      reading, but skew statistics in the dataspec, which can hurt model quality
+      (e.g. if an important category of a categorical feature is considered
+      OOV). Set to -1 to scan the entire dataset.
+    data_spec: Dataspec to be used (advanced). If a data spec is given,
+      `columns`, `include_all_columns`, `max_vocab_count`,
+      `min_vocab_frequency`, `discretize_numerical_columns` and
+      `num_discretized_numerical_bins` will be ignored.
+    adapt_subsample_for_maximum_training_duration: Control how the maximum
+      training duration (if set) is applied. If false, the training stop when
+      the time is used. If true, the size of the sampled datasets used train
+      individual trees are adapted dynamically so that all the trees are trained
+      in time. Default: False.
+    allow_na_conditions: If true, the tree training evaluates conditions of the
+      type `X is NA` i.e. `X is missing`. Default: False.
+    apply_link_function: If true, applies the link function (a.k.a. activation
+      function), if any, before returning the model prediction. If false,
+      returns the pre-link function model output. For example, in the case of
+      binary classification, the pre-link function output is a logic while the
+      post-link function is a probability. Default: True.
+    categorical_algorithm: How to learn splits on categorical attributes. -
+      `CART`: CART algorithm. Find categorical splits of the form "value \\in
+      mask". The solution is exact for binary classification, regression and
+      ranking. It is approximated for multi-class classification. This is a good
+      first algorithm to use. In case of overfitting (very small dataset, large
+      dictionary), the "random" algorithm is a good alternative. - `ONE_HOT`:
+      One-hot encoding. Find the optimal categorical split of the form
+      "attribute == param". This method is similar (but more efficient) than
+      converting converting each possible categorical value into a boolean
+      feature. This method is available for comparison purpose and generally
+      performs worse than other alternatives. - `RANDOM`: Best splits among a
+      set of random candidate. Find the a categorical split of the form "value
+      \\in mask" using a random search. This solution can be seen as an
+      approximation of the CART algorithm. This method is a strong alternative
+      to CART. This algorithm is inspired from section "5.1 Categorical
+      Variables" of "Random Forest", 2001.
+        Default: "CART".
+    categorical_set_split_greedy_sampling: For categorical set splits e.g.
+      texts. Probability for a categorical value to be a candidate for the
+      positive set. The sampling is applied once per node (i.e. not at every
+      step of the greedy optimization). Default: 0.1.
+    categorical_set_split_max_num_items: For categorical set splits e.g. texts.
+      Maximum number of items (prior to the sampling). If more items are
+      available, the least frequent items are ignored. Changing this value is
+      similar to change the "max_vocab_count" before loading the dataset, with
+      the following exception: With `max_vocab_count`, all the remaining items
+      are grouped in a special Out-of-vocabulary item. With `max_num_items`,
+      this is not the case. Default: -1.
+    categorical_set_split_min_item_frequency: For categorical set splits e.g.
+      texts. Minimum number of occurrences of an item to be considered.
+      Default: 1.
+    compute_permutation_variable_importance: If true, compute the permutation
+      variable importance of the model at the end of the training using the
+      validation dataset. Enabling this feature can increase the training time
+      significantly. Default: False.
+    dart_dropout: Dropout rate applied when using the DART i.e. when
+      forest_extraction=DART. Default: 0.01.
+    early_stopping: Early stopping detects the overfitting of the model and
+      halts it training using the validation dataset. If not provided directly,
+      the validation dataset is extracted from the training dataset (see
+      "validation_ratio" parameter): - `NONE`: No early stopping. All the
+      num_trees are trained and kept. - `MIN_LOSS_FINAL`: All the num_trees are
+      trained. The model is then truncated to minimize the validation loss i.e.
+      some of the trees are discarded as to minimum the validation loss. -
+      `LOSS_INCREASE`: Classical early stopping. Stop the training when the
+      validation does not decrease for `early_stopping_num_trees_look_ahead`
+      trees. Default: "LOSS_INCREASE".
+    early_stopping_initial_iteration: 0-based index of the first iteration
+      considered for early stopping computation. Increasing this value prevents
+      too early stopping due to noisy initial iterations of the learner.
+      Default: 10.
+    early_stopping_num_trees_look_ahead: Rolling number of trees used to detect
+      validation loss increase and trigger early stopping. Default: 30.
+    focal_loss_alpha: EXPERIMENTAL. Weighting parameter for focal loss, positive
+      samples weighted by alpha, negative samples by (1-alpha). The default 0.5
+      value means no active class-level weighting. Only used with focal loss
+      i.e. `loss="BINARY_FOCAL_LOSS"` Default: 0.5.
+    focal_loss_gamma: EXPERIMENTAL. Exponent of the misprediction exponent term
+      in focal loss, corresponds to gamma parameter in
+      https://arxiv.org/pdf/1708.02002.pdf. Only used with focal loss i.e.
+        `loss="BINARY_FOCAL_LOSS"` Default: 2.0.
+    forest_extraction: How to construct the forest: - MART: For Multiple
+      Additive Regression Trees. The "classical" way to build a GBDT i.e. each
+      tree tries to "correct" the mistakes of the previous trees. - DART: For
+      Dropout Additive Regression Trees. A modification of MART proposed in
+      http://proceedings.mlr.press/v38/korlakaivinayak15.pdf. Here, each tree
+        tries to "correct" the mistakes of a random subset of the previous
+        trees.
+      Default: "MART".
+    goss_alpha: Alpha parameter for the GOSS (Gradient-based One-Side Sampling;
+      "See LightGBM: A Highly Efficient Gradient Boosting Decision Tree")
+      sampling method. Default: 0.2.
+    goss_beta: Beta parameter for the GOSS (Gradient-based One-Side Sampling)
+      sampling method. Default: 0.1.
+    growing_strategy: How to grow the tree. - `LOCAL`: Each node is split
+      independently of the other nodes. In other words, as long as a node
+      satisfy the splits "constraints (e.g. maximum depth, minimum number of
+      observations), the node will be split. This is the "classical" way to grow
+      decision trees. - `BEST_FIRST_GLOBAL`: The node with the best loss
+      reduction among all the nodes of the tree is selected for splitting. This
+      method is also called "best first" or "leaf-wise growth". See "Best-first
+      decision tree learning", Shi and "Additive logistic regression : A
+      statistical view of boosting", Friedman for more details. Default:
+      "LOCAL".
+    honest: In honest trees, different training examples are used to infer the
+      structure and the leaf values. This regularization technique trades
+      examples for bias estimates. It might increase or reduce the quality of
+      the model. See "Generalized Random Forests", Athey et al. In this paper,
+      Honest trees are trained with the Random Forest algorithm with a sampling
+      without replacement. Default: False.
+    honest_fixed_separation: For honest trees only i.e. honest=true. If true, a
+      new random separation is generated for each tree. If false, the same
+      separation is used for all the trees (e.g., in Gradient Boosted Trees
+      containing multiple trees). Default: False.
+    honest_ratio_leaf_examples: For honest trees only i.e. honest=true. Ratio of
+      examples used to set the leaf values. Default: 0.5.
+    in_split_min_examples_check: Whether to check the `min_examples` constraint
+      in the split search (i.e. splits leading to one child having less than
+      `min_examples` examples are considered invalid) or before the split search
+      (i.e. a node can be derived only if it contains more than `min_examples`
+      examples). If false, there can be nodes with less than `min_examples`
+      training examples. Default: True.
+    keep_non_leaf_label_distribution: Whether to keep the node value (i.e. the
+      distribution of the labels of the training examples) of non-leaf nodes.
+      This information is not used during serving, however it can be used for
+      model interpretation as well as hyper parameter tuning. This can take lots
+      of space, sometimes accounting for half of the model size. Default: True.
+    l1_regularization: L1 regularization applied to the training loss. Impact
+      the tree structures and lead values. Default: 0.0.
+    l2_categorical_regularization: L2 regularization applied to the training
+      loss for categorical features. Impact the tree structures and lead values.
+      Default: 1.0.
+    l2_regularization: L2 regularization applied to the training loss for all
+      features except the categorical ones. Default: 0.0.
+    lambda_loss: Lambda regularization applied to certain training loss
+      functions. Only for NDCG loss. Default: 1.0.
+    loss: The loss optimized by the model. If not specified (DEFAULT) the loss
+      is selected automatically according to the \\"task\\" and label
+      statistics. For example, if task=CLASSIFICATION and the label has two
+      possible values, the loss will be set to BINOMIAL_LOG_LIKELIHOOD. Possible
+      values are: - `DEFAULT`: Select the loss automatically according to the
+      task and label statistics. - `BINOMIAL_LOG_LIKELIHOOD`: Binomial log
+      likelihood. Only valid for binary classification. - `SQUARED_ERROR`: Least
+      square loss. Only valid for regression. - `POISSON`: Poisson log
+      likelihood loss. Mainly used for counting problems. Only valid for
+      regression. - `MULTINOMIAL_LOG_LIKELIHOOD`: Multinomial log likelihood
+      i.e. cross-entropy. Only valid for binary or multi-class classification. -
+      `LAMBDA_MART_NDCG5`: LambdaMART with NDCG5. - `XE_NDCG_MART`:  Cross
+      Entropy Loss NDCG. See arxiv.org/abs/1911.09798. - `BINARY_FOCAL_LOSS`:
+      Focal loss. Only valid for binary classification. See
+      https://arxiv.org/pdf/1708.02002.pdf. - `POISSON`: Poisson log likelihood.
+        Only valid for regression. - `MEAN_AVERAGE_ERROR`: Mean average error
+        a.k.a. MAE. For custom losses, pass the loss object here. Note that when
+        using custom losses, the link function is deactivated (aka
+        apply_link_function is always False).
+        Default: "DEFAULT".
+    max_depth: Maximum depth of the tree. `max_depth=1` means that all trees
+      will be roots. `max_depth=-1` means that tree depth is not restricted by
+      this parameter. Values <= -2 will be ignored. Default: 6.
+    max_num_nodes: Maximum number of nodes in the tree. Set to -1 to disable
+      this limit. Only available for `growing_strategy=BEST_FIRST_GLOBAL`.
+      Default: None.
+    maximum_model_size_in_memory_in_bytes: Limit the size of the model when
+      stored in ram. Different algorithms can enforce this limit differently.
+      Note that when models are compiled into an inference, the size of the
+      inference engine is generally much smaller than the original model.
+      Default: -1.0.
+    maximum_training_duration_seconds: Maximum training duration of the model
+      expressed in seconds. Each learning algorithm is free to use this
+      parameter at it sees fit. Enabling maximum training duration makes the
+      model training non-deterministic. Default: -1.0.
+    min_examples: Minimum number of examples in a node. Default: 5.
+    missing_value_policy: Method used to handle missing attribute values. -
+      `GLOBAL_IMPUTATION`: Missing attribute values are imputed, with the mean
+      (in case of numerical attribute) or the most-frequent-item (in case of
+      categorical attribute) computed on the entire dataset (i.e. the
+      information contained in the data spec). - `LOCAL_IMPUTATION`: Missing
+      attribute values are imputed with the mean (numerical attribute) or
+      most-frequent-item (in the case of categorical attribute) evaluated on the
+      training examples in the current node. - `RANDOM_LOCAL_IMPUTATION`:
+      Missing attribute values are imputed from randomly sampled values from the
+      training examples in the current node. This method was proposed by Clinic
+      et al. in "Random Survival Forests"
+      (https://projecteuclid.org/download/pdfview_1/euclid.aoas/1223908043).
+        Default: "GLOBAL_IMPUTATION".
+    num_candidate_attributes: Number of unique valid attributes tested for each
+      node. An attribute is valid if it has at least a valid split. If
+      `num_candidate_attributes=0`, the value is set to the classical default
+      value for Random Forest: `sqrt(number of input attributes)` in case of
+      classification and `number_of_input_attributes / 3` in case of regression.
+      If `num_candidate_attributes=-1`, all the attributes are tested. Default:
+      -1.
+    num_candidate_attributes_ratio: Ratio of attributes tested at each node. If
+      set, it is equivalent to `num_candidate_attributes =
+      number_of_input_features x num_candidate_attributes_ratio`. The possible
+      values are between ]0, and 1] as well as -1. If not set or equal to -1,
+      the `num_candidate_attributes` is used. Default: -1.0.
+    num_trees: Maximum number of decision trees. The effective number of trained
+      tree can be smaller if early stopping is enabled. Default: 300.
+    pure_serving_model: Clear the model from any information that is not
+      required for model serving. This includes debugging, model interpretation
+      and other meta-data. The size of the serialized model can be reduced
+      significatively (50% model size reduction is common). This parameter has
+      no impact on the quality, serving speed or RAM usage of model serving.
+      Default: False.
+    random_seed: Random seed for the training of the model. Learners are
+      expected to be deterministic by the random seed. Default: 123456.
+    sampling_method: Control the sampling of the datasets used to train
+      individual trees. - NONE: No sampling is applied. This is equivalent to
+      RANDOM sampling with \\"subsample=1\\". - RANDOM (default): Uniform random
+      sampling. Automatically selected if "subsample" is set. - GOSS:
+      Gradient-based One-Side Sampling. Automatically selected if "goss_alpha"
+      or "goss_beta" is set. - SELGB: Selective Gradient Boosting. Automatically
+      selected if "selective_gradient_boosting_ratio" is set. Only valid for
+      ranking.
+        Default: "RANDOM".
+    selective_gradient_boosting_ratio: Ratio of the dataset used to train
+      individual tree for the selective Gradient Boosting (Selective Gradient
+      Boosting for Effective Learning to Rank; Lucchese et al;
+      http://quickrank.isti.cnr.it/selective-data/selective-SIGIR2018.pdf)
+        sampling method. Default: 0.01.
+    shrinkage: Coefficient applied to each tree prediction. A small value (0.02)
+      tends to give more accurate results (assuming enough trees are trained),
+      but results in larger models. Analogous to neural network learning rate.
+      Default: 0.1.
+    sorting_strategy: How are sorted the numerical features in order to find the
+      splits - PRESORT: The features are pre-sorted at the start of the
+      training. This solution is faster but consumes much more memory than
+      IN_NODE. - IN_NODE: The features are sorted just before being used in the
+      node. This solution is slow but consumes little amount of memory. .
+      Default: "PRESORT".
+    sparse_oblique_normalization: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Normalization applied on the features, before
+      applying the sparse oblique projections. - `NONE`: No normalization. -
+      `STANDARD_DEVIATION`: Normalize the feature by the estimated standard
+      deviation on the entire train dataset. Also known as Z-Score
+      normalization. - `MIN_MAX`: Normalize the feature by the range (i.e.
+      max-min) estimated on the entire train dataset. Default: None.
+    sparse_oblique_num_projections_exponent: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections
+      to test at each node as `num_features^num_projections_exponent`. Default:
+      None.
+    sparse_oblique_projection_density_factor: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections
+      to test at each node as `num_features^num_projections_exponent`. Default:
+      None.
+    sparse_oblique_weights: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Possible values: - `BINARY`: The oblique
+      weights are sampled in {-1,1} (default). - `CONTINUOUS`: The oblique
+      weights are be sampled in [-1,1]. Default: None.
+    split_axis: What structure of split to consider for numerical features. -
+      `AXIS_ALIGNED`: Axis aligned splits (i.e. one condition at a time). This
+      is the "classical" way to train a tree. Default value. - `SPARSE_OBLIQUE`:
+      Sparse oblique splits (i.e. splits one a small number of features) from
+      "Sparse Projection Oblique Random Forests", Tomita et al., 2020. Default:
+      "AXIS_ALIGNED".
+    subsample: Ratio of the dataset (sampling without replacement) used to train
+      individual trees for the random sampling method. If \\"subsample\\" is set
+      and if \\"sampling_method\\" is NOT set or set to \\"NONE\\", then
+      \\"sampling_method\\" is implicitly set to \\"RANDOM\\". In other words,
+      to enable random subsampling, you only need to set "\\"subsample\\".
+      Default: 1.0.
+    uplift_min_examples_in_treatment: For uplift models only. Minimum number of
+      examples per treatment in a node. Default: 5.
+    uplift_split_score: For uplift models only. Splitter score i.e. score
+      optimized by the splitters. The scores are introduced in "Decision trees
+      for uplift modeling with single and multiple treatments", Rzepakowski et
+      al. Notation: `p` probability / average value of the positive outcome, `q`
+      probability / average value in the control group. - `KULLBACK_LEIBLER` or
+      `KL`: - p log (p/q) - `EUCLIDEAN_DISTANCE` or `ED`: (p-q)^2 -
+      `CHI_SQUARED` or `CS`: (p-q)^2/q
+        Default: "KULLBACK_LEIBLER".
+    use_hessian_gain: Use true, uses a formulation of split gain with a hessian
+      term i.e. optimizes the splits to minimize the variance of "gradient /
+      hessian. Available for all losses except regression. Default: False.
+    validation_interval_in_trees: Evaluate the model on the validation set every
+      "validation_interval_in_trees" trees. Increasing this value reduce the
+      cost of validation and can impact the early stopping policy (as early
+      stopping is only tested during the validation). Default: 1.
+    validation_ratio: Fraction of the training dataset used for validation if
+      not validation dataset is provided. The validation dataset, whether
+      provided directly or extracted from the training dataset, is used to
+      compute the validation loss, other validation metrics, and possibly
+      trigger early stopping (if enabled). When early stopping is disabled, the
+      validation dataset is only used for monitoring and does not influence the
+      model directly. If the "validation_ratio" is set to 0, early stopping is
+      disabled (i.e., it implies setting early_stopping=NONE). Default: 0.1.
+    num_threads: Number of threads used to train the model. Different learning
+      algorithms use multi-threading differently and with different degree of
+      efficiency. If `None`, `num_threads` will be automatically set to the
+      number of processors (up to a maximum of 32; or set to 6 if the number of
+      processors is not available). Making `num_threads` significantly larger
+      than the number of processors can slow-down the training speed. The
+      default value logic might change in the future.
+    resume_training: If true, the model training resumes from the checkpoint
+      stored in the `working_dir` directory. If `working_dir` does not contain
+      any model checkpoint, the training starts from the beginning. Resuming
+      training is useful in the following situations: (1) The training was
+      interrupted by the user (e.g. ctrl+c or "stop" button in a notebook) or
+      rescheduled, or (2) the hyper-parameter of the learner was changed e.g.
+      increasing the number of trees.
+    working_dir: Path to a directory available for the learning algorithm to
+      store intermediate computation results. Depending on the learning
+      algorithm and parameters, the working_dir might be optional, required, or
+      ignored. For instance, distributed training algorithm always need a
+      "working_dir", and the gradient boosted tree and hyper-parameter tuners
+      will export artefacts to the "working_dir" if provided.
+    resume_training_snapshot_interval_seconds: Indicative number of seconds in
+      between snapshots when `resume_training=True`. Might be ignored by some
+      learners.
+    tuner: If set, automatically select the best hyperparameters using the
+      provided tuner. When using distributed training, the tuning is
+      distributed.
+    workers: If set, enable distributed training. "workers" is the list of IP
+      addresses of the workers. A worker is a process running
+      `ydf.start_worker(port)`.
+  """
+
+  def __init__(
+      self,
+      label: str,
+      task: generic_learner.Task = generic_learner.Task.CLASSIFICATION,
+      weights: Optional[str] = None,
+      ranking_group: Optional[str] = None,
+      uplift_treatment: Optional[str] = None,
+      features: dataspec.ColumnDefs = None,
+      include_all_columns: bool = False,
+      max_vocab_count: int = 2000,
+      min_vocab_frequency: int = 5,
+      discretize_numerical_columns: bool = False,
+      num_discretized_numerical_bins: int = 255,
+      max_num_scanned_rows_to_infer_semantic: int = 10000,
+      max_num_scanned_rows_to_compute_statistics: int = 10000,
+      data_spec: Optional[data_spec_pb2.DataSpecification] = None,
+      adapt_subsample_for_maximum_training_duration: Optional[bool] = False,
+      allow_na_conditions: Optional[bool] = False,
+      apply_link_function: Optional[bool] = True,
+      categorical_algorithm: Optional[str] = "CART",
+      categorical_set_split_greedy_sampling: Optional[float] = 0.1,
+      categorical_set_split_max_num_items: Optional[int] = -1,
+      categorical_set_split_min_item_frequency: Optional[int] = 1,
+      compute_permutation_variable_importance: Optional[bool] = False,
+      dart_dropout: Optional[float] = 0.01,
+      early_stopping: Optional[str] = "LOSS_INCREASE",
+      early_stopping_initial_iteration: Optional[int] = 10,
+      early_stopping_num_trees_look_ahead: Optional[int] = 30,
+      focal_loss_alpha: Optional[float] = 0.5,
+      focal_loss_gamma: Optional[float] = 2.0,
+      forest_extraction: Optional[str] = "MART",
+      goss_alpha: Optional[float] = 0.2,
+      goss_beta: Optional[float] = 0.1,
+      growing_strategy: Optional[str] = "LOCAL",
+      honest: Optional[bool] = False,
+      honest_fixed_separation: Optional[bool] = False,
+      honest_ratio_leaf_examples: Optional[float] = 0.5,
+      in_split_min_examples_check: Optional[bool] = True,
+      keep_non_leaf_label_distribution: Optional[bool] = True,
+      l1_regularization: Optional[float] = 0.0,
+      l2_categorical_regularization: Optional[float] = 1.0,
+      l2_regularization: Optional[float] = 0.0,
+      lambda_loss: Optional[float] = 1.0,
+      loss: Optional[Union[str, custom_loss.AbstractCustomLoss]] = "DEFAULT",
+      max_depth: Optional[int] = 6,
+      max_num_nodes: Optional[int] = None,
+      maximum_model_size_in_memory_in_bytes: Optional[float] = -1.0,
+      maximum_training_duration_seconds: Optional[float] = -1.0,
+      min_examples: Optional[int] = 5,
+      missing_value_policy: Optional[str] = "GLOBAL_IMPUTATION",
+      num_candidate_attributes: Optional[int] = -1,
+      num_candidate_attributes_ratio: Optional[float] = -1.0,
+      num_trees: Optional[int] = 300,
+      pure_serving_model: Optional[bool] = False,
+      random_seed: Optional[int] = 123456,
+      sampling_method: Optional[str] = "RANDOM",
+      selective_gradient_boosting_ratio: Optional[float] = 0.01,
+      shrinkage: Optional[float] = 0.1,
+      sorting_strategy: Optional[str] = "PRESORT",
+      sparse_oblique_normalization: Optional[str] = None,
+      sparse_oblique_num_projections_exponent: Optional[float] = None,
+      sparse_oblique_projection_density_factor: Optional[float] = None,
+      sparse_oblique_weights: Optional[str] = None,
+      split_axis: Optional[str] = "AXIS_ALIGNED",
+      subsample: Optional[float] = 1.0,
+      uplift_min_examples_in_treatment: Optional[int] = 5,
+      uplift_split_score: Optional[str] = "KULLBACK_LEIBLER",
+      use_hessian_gain: Optional[bool] = False,
+      validation_interval_in_trees: Optional[int] = 1,
+      validation_ratio: Optional[float] = 0.1,
+      num_threads: Optional[int] = None,
+      working_dir: Optional[str] = None,
+      resume_training: bool = False,
+      resume_training_snapshot_interval_seconds: int = 1800,
+      tuner: Optional[tuner_lib.AbstractTuner] = None,
+      workers: Optional[Sequence[str]] = None,
+  ):
+
+    hyper_parameters = {
+        "adapt_subsample_for_maximum_training_duration": (
+            adapt_subsample_for_maximum_training_duration
+        ),
+        "allow_na_conditions": allow_na_conditions,
+        "apply_link_function": apply_link_function,
+        "categorical_algorithm": categorical_algorithm,
+        "categorical_set_split_greedy_sampling": (
+            categorical_set_split_greedy_sampling
+        ),
+        "categorical_set_split_max_num_items": (
+            categorical_set_split_max_num_items
+        ),
+        "categorical_set_split_min_item_frequency": (
+            categorical_set_split_min_item_frequency
+        ),
+        "compute_permutation_variable_importance": (
+            compute_permutation_variable_importance
+        ),
+        "dart_dropout": dart_dropout,
+        "early_stopping": early_stopping,
+        "early_stopping_initial_iteration": early_stopping_initial_iteration,
+        "early_stopping_num_trees_look_ahead": (
+            early_stopping_num_trees_look_ahead
+        ),
+        "focal_loss_alpha": focal_loss_alpha,
+        "focal_loss_gamma": focal_loss_gamma,
+        "forest_extraction": forest_extraction,
+        "goss_alpha": goss_alpha,
+        "goss_beta": goss_beta,
+        "growing_strategy": growing_strategy,
+        "honest": honest,
+        "honest_fixed_separation": honest_fixed_separation,
+        "honest_ratio_leaf_examples": honest_ratio_leaf_examples,
+        "in_split_min_examples_check": in_split_min_examples_check,
+        "keep_non_leaf_label_distribution": keep_non_leaf_label_distribution,
+        "l1_regularization": l1_regularization,
+        "l2_categorical_regularization": l2_categorical_regularization,
+        "l2_regularization": l2_regularization,
+        "lambda_loss": lambda_loss,
+        "loss": loss,
+        "max_depth": max_depth,
+        "max_num_nodes": max_num_nodes,
+        "maximum_model_size_in_memory_in_bytes": (
+            maximum_model_size_in_memory_in_bytes
+        ),
+        "maximum_training_duration_seconds": maximum_training_duration_seconds,
+        "min_examples": min_examples,
+        "missing_value_policy": missing_value_policy,
+        "num_candidate_attributes": num_candidate_attributes,
+        "num_candidate_attributes_ratio": num_candidate_attributes_ratio,
+        "num_trees": num_trees,
+        "pure_serving_model": pure_serving_model,
+        "random_seed": random_seed,
+        "sampling_method": sampling_method,
+        "selective_gradient_boosting_ratio": selective_gradient_boosting_ratio,
+        "shrinkage": shrinkage,
+        "sorting_strategy": sorting_strategy,
+        "sparse_oblique_normalization": sparse_oblique_normalization,
+        "sparse_oblique_num_projections_exponent": (
+            sparse_oblique_num_projections_exponent
+        ),
+        "sparse_oblique_projection_density_factor": (
+            sparse_oblique_projection_density_factor
+        ),
+        "sparse_oblique_weights": sparse_oblique_weights,
+        "split_axis": split_axis,
+        "subsample": subsample,
+        "uplift_min_examples_in_treatment": uplift_min_examples_in_treatment,
+        "uplift_split_score": uplift_split_score,
+        "use_hessian_gain": use_hessian_gain,
+        "validation_interval_in_trees": validation_interval_in_trees,
+        "validation_ratio": validation_ratio,
+    }
+
+    data_spec_args = dataspec.DataSpecInferenceArgs(
+        columns=dataspec.normalize_column_defs(features),
+        include_all_columns=include_all_columns,
+        max_vocab_count=max_vocab_count,
+        min_vocab_frequency=min_vocab_frequency,
+        discretize_numerical_columns=discretize_numerical_columns,
+        num_discretized_numerical_bins=num_discretized_numerical_bins,
+        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
+        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
+    )
+
+    deployment_config = self._build_deployment_config(
+        num_threads=num_threads,
+        resume_training=resume_training,
+        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
+        working_dir=working_dir,
+        workers=workers,
+    )
+
+    super().__init__(
+        learner_name="GRADIENT_BOOSTED_TREES",
+        task=task,
+        label=label,
+        weights=weights,
+        ranking_group=ranking_group,
+        uplift_treatment=uplift_treatment,
+        data_spec_args=data_spec_args,
+        data_spec=data_spec,
+        hyper_parameters=hyper_parameters,
+        deployment_config=deployment_config,
+        tuner=tuner,
+    )
+
+  def train(
+      self,
+      ds: dataset.InputDataset,
+      valid: Optional[dataset.InputDataset] = None,
+  ) -> gradient_boosted_trees_model.GradientBoostedTreesModel:
+    """Trains a model on the given dataset.
+
+    Options for dataset reading are given on the learner. Consult the
+    documentation of the learner or ydf.create_vertical_dataset() for additional
+    information on dataset reading in YDF.
+
+    Usage example:
+
+    ```
+    import ydf
+    import pandas as pd
+
+    train_ds = pd.read_csv(...)
+
+    learner = ydf.GradientBoostedTreesLearner(label="label")
+    model = learner.train(train_ds)
+    print(model.summary())
+    ```
+
+    If training is interrupted (for example, by interrupting the cell execution
+    in Colab), the model will be returned to the state it was in at the moment
+    of interruption.
+
+    Args:
+      ds: Training dataset.
+      valid: Optional validation dataset. Some learners, such as Random Forest,
+        do not need validation dataset. Some learners, such as
+        GradientBoostedTrees, automatically extract a validation dataset from
+        the training dataset if the validation dataset is not provided.
+
+    Returns:
+      A trained model.
+    """
+    return super().train(ds, valid)
+
+  @classmethod
+  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
+    return abstract_learner_pb2.LearnerCapabilities(
+        support_max_training_duration=True,
+        resume_training=True,
+        support_validation_dataset=True,
+        support_partial_cache_dataset_format=False,
+        support_max_model_size_in_memory=False,
+        support_monotonic_constraints=True,
+    )
+
+  @classmethod
+  def hyperparameter_templates(
+      cls,
+  ) -> Dict[str, hyperparameters.HyperparameterTemplate]:
+    r"""Hyperparameter templates for this Learner.
+
+    Hyperparameter templates are sets of pre-defined hyperparameters for easy
+    access to different variants of the learner. Each template is a mapping to a
+    set of hyperparameters and can be applied directly on the learner.
+
+    Usage example:
+    ```python
+    templates = ydf.GradientBoostedTreesLearner.hyperparameter_templates()
+    better_defaultv1 = templates["better_defaultv1"]
+    # Print a description of the template
+    print(better_defaultv1.description)
+    # Apply the template's settings on the learner.
+    learner = ydf.GradientBoostedTreesLearner(label, **better_defaultv1)
+    ```
+
+    Returns:
+      Dictionary of the available templates
+    """
+    return {
+        "better_defaultv1": hyperparameters.HyperparameterTemplate(
+            name="better_default",
+            version=1,
+            description=(
+                "A configuration that is generally better than the default"
+                " parameters without being more expensive."
+            ),
+            parameters={"growing_strategy": "BEST_FIRST_GLOBAL"},
+        ),
+        "benchmark_rank1v1": hyperparameters.HyperparameterTemplate(
+            name="benchmark_rank1",
+            version=1,
+            description=(
+                "Top ranking hyper-parameters on our benchmark slightly"
+                " modified to run in reasonable time."
+            ),
+            parameters={
+                "growing_strategy": "BEST_FIRST_GLOBAL",
+                "categorical_algorithm": "RANDOM",
+                "split_axis": "SPARSE_OBLIQUE",
+                "sparse_oblique_normalization": "MIN_MAX",
+                "sparse_oblique_num_projections_exponent": 1.0,
+            },
+        ),
+    }
+
+
+class DistributedGradientBoostedTreesLearner(generic_learner.GenericLearner):
+  r"""Distributed Gradient Boosted Trees learning algorithm.
+
+  Exact distributed version of the Gradient Boosted Tree learning algorithm. See
+  the documentation of the non-distributed Gradient Boosted Tree learning
+  algorithm for an introduction to GBTs.
+
+  Usage example:
+
+  ```python
+  import ydf
+  import pandas as pd
+
+  dataset = pd.read_csv("project/dataset.csv")
+
+  model = ydf.DistributedGradientBoostedTreesLearner().train(dataset)
+
+  print(model.summary())
+  ```
+
+  Hyperparameters are configured to give reasonable results for typical
+  datasets. Hyperparameters can also be modified manually (see descriptions)
+  below or by applying the hyperparameter templates available with
+  `DistributedGradientBoostedTreesLearner.hyperparameter_templates()` (see this
+  function's documentation for
+  details).
+
+  Attributes:
+    label: Label of the dataset. The label column should not be identified as a
+      feature in the `features` parameter.
+    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,
+      Task.RANKING, Task.CATEGORICAL_UPLIFT, Task.NUMERICAL_UPLIFT).
+    weights: Name of a feature that identifies the weight of each example. If
+      weights are not specified, unit weights are assumed. The weight column
+      should not be identified as a feature in the `features` parameter.
+    ranking_group: Only for `task=Task.RANKING`. Name of a feature that
+      identifies queries in a query/document ranking task. The ranking group
+      should not be identified as a feature in the `features` parameter.
+    uplift_treatment: Only for `task=Task.CATEGORICAL_UPLIFT` and `task=Task`.
+      NUMERICAL_UPLIFT. Name of a numerical feature that identifies the
+      treatment in an uplift problem. The value 0 is reserved for the control
+      treatment. Currently, only 0/1 binary treatments are supported.
+    features: If None, all columns are used as features. The semantic of the
+      features is determined automatically. Otherwise, if
+      include_all_columns=False (default) only the column listed in `features`
+      are imported. If include_all_columns=True, all the columns are imported as
+      features and only the semantic of the columns NOT in `columns` is
+      determined automatically. If specified,  defines the order of the features
+      - any non-listed features are appended in-order after the specified
+      features (if include_all_columns=True). The label, weights, uplift
+      treatment and ranking_group columns should not be specified as features.
+    include_all_columns: See `features`.
+    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
+      CATEGORICAL_SET columns stored as strings. If more unique values exist,
+      only the most frequent values are kept, and the remaining values are
+      considered as out-of-vocabulary.
+    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
+      and CATEGORICAL_SET columns. Value observed less than
+      `min_vocab_frequency` are considered as out-of-vocabulary.
+    discretize_numerical_columns: If true, discretize all the numerical columns
+      before training. Discretized numerical columns are faster to train with,
+      but they can have a negative impact on the model quality. Using
+      `discretize_numerical_columns=True` is equivalent as setting the column
+      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
+      definition of DISCRETIZED_NUMERICAL for more details.
+    num_discretized_numerical_bins: Number of bins used when disretizing
+      numerical columns.
+    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
+      inferring the column's semantic if it is not explicitly specified. Only
+      used when reading from file, in-memory datasets are always read in full.
+      Setting this to a lower number will speed up dataset reading, but might
+      result in incorrect column semantics. Set to -1 to scan the entire
+      dataset.
+    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
+      computing a column's statistics. Only used when reading from file,
+      in-memory datasets are always read in full. A column's statistics include
+      the dictionary for categorical features and the mean / min / max for
+      numerical features. Setting this to a lower number will speed up dataset
+      reading, but skew statistics in the dataspec, which can hurt model quality
+      (e.g. if an important category of a categorical feature is considered
+      OOV). Set to -1 to scan the entire dataset.
+    data_spec: Dataspec to be used (advanced). If a data spec is given,
+      `columns`, `include_all_columns`, `max_vocab_count`,
+      `min_vocab_frequency`, `discretize_numerical_columns` and
+      `num_discretized_numerical_bins` will be ignored.
+    apply_link_function: If true, applies the link function (a.k.a. activation
+      function), if any, before returning the model prediction. If false,
+      returns the pre-link function model output. For example, in the case of
+      binary classification, the pre-link function output is a logic while the
+      post-link function is a probability. Default: True.
+    force_numerical_discretization: If false, only the numerical column
+      safisfying "max_unique_values_for_discretized_numerical" will be
+      discretized. If true, all the numerical columns will be discretized.
+      Columns with more than "max_unique_values_for_discretized_numerical"
+      unique values will be approximated with
+      "max_unique_values_for_discretized_numerical" bins. This parameter will
+      impact the model training. Default: False.
+    max_depth: Maximum depth of the tree. `max_depth=1` means that all trees
+      will be roots. `max_depth=-1` means that tree depth is not restricted by
+      this parameter. Values <= -2 will be ignored. Default: 6.
+    max_unique_values_for_discretized_numerical: Maximum number of unique value
+      of a numerical feature to allow its pre-discretization. In case of large
+      datasets, discretized numerical features with a small number of unique
+      values are more efficient to learn than classical / non-discretized
+      numerical features. This parameter does not impact the final model.
+      However, it can speed-up or slown the training. Default: 16000.
+    maximum_model_size_in_memory_in_bytes: Limit the size of the model when
+      stored in ram. Different algorithms can enforce this limit differently.
+      Note that when models are compiled into an inference, the size of the
+      inference engine is generally much smaller than the original model.
+      Default: -1.0.
+    maximum_training_duration_seconds: Maximum training duration of the model
+      expressed in seconds. Each learning algorithm is free to use this
+      parameter at it sees fit. Enabling maximum training duration makes the
+      model training non-deterministic. Default: -1.0.
+    min_examples: Minimum number of examples in a node. Default: 5.
+    num_candidate_attributes: Number of unique valid attributes tested for each
+      node. An attribute is valid if it has at least a valid split. If
+      `num_candidate_attributes=0`, the value is set to the classical default
+      value for Random Forest: `sqrt(number of input attributes)` in case of
+      classification and `number_of_input_attributes / 3` in case of regression.
+      If `num_candidate_attributes=-1`, all the attributes are tested. Default:
+      -1.
+    num_candidate_attributes_ratio: Ratio of attributes tested at each node. If
+      set, it is equivalent to `num_candidate_attributes =
+      number_of_input_features x num_candidate_attributes_ratio`. The possible
+      values are between ]0, and 1] as well as -1. If not set or equal to -1,
+      the `num_candidate_attributes` is used. Default: -1.0.
+    num_trees: Maximum number of decision trees. The effective number of trained
+      tree can be smaller if early stopping is enabled. Default: 300.
+    pure_serving_model: Clear the model from any information that is not
+      required for model serving. This includes debugging, model interpretation
+      and other meta-data. The size of the serialized model can be reduced
+      significatively (50% model size reduction is common). This parameter has
+      no impact on the quality, serving speed or RAM usage of model serving.
+      Default: False.
+    random_seed: Random seed for the training of the model. Learners are
+      expected to be deterministic by the random seed. Default: 123456.
+    shrinkage: Coefficient applied to each tree prediction. A small value (0.02)
+      tends to give more accurate results (assuming enough trees are trained),
+      but results in larger models. Analogous to neural network learning rate.
+      Default: 0.1.
+    use_hessian_gain: Use true, uses a formulation of split gain with a hessian
+      term i.e. optimizes the splits to minimize the variance of "gradient /
+      hessian. Available for all losses except regression. Default: False.
+    worker_logs: If true, workers will print training logs. Default: True.
+    num_threads: Number of threads used to train the model. Different learning
+      algorithms use multi-threading differently and with different degree of
+      efficiency. If `None`, `num_threads` will be automatically set to the
+      number of processors (up to a maximum of 32; or set to 6 if the number of
+      processors is not available). Making `num_threads` significantly larger
+      than the number of processors can slow-down the training speed. The
+      default value logic might change in the future.
+    resume_training: If true, the model training resumes from the checkpoint
+      stored in the `working_dir` directory. If `working_dir` does not contain
+      any model checkpoint, the training starts from the beginning. Resuming
+      training is useful in the following situations: (1) The training was
+      interrupted by the user (e.g. ctrl+c or "stop" button in a notebook) or
+      rescheduled, or (2) the hyper-parameter of the learner was changed e.g.
+      increasing the number of trees.
+    working_dir: Path to a directory available for the learning algorithm to
+      store intermediate computation results. Depending on the learning
+      algorithm and parameters, the working_dir might be optional, required, or
+      ignored. For instance, distributed training algorithm always need a
+      "working_dir", and the gradient boosted tree and hyper-parameter tuners
+      will export artefacts to the "working_dir" if provided.
+    resume_training_snapshot_interval_seconds: Indicative number of seconds in
+      between snapshots when `resume_training=True`. Might be ignored by some
+      learners.
+    tuner: If set, automatically select the best hyperparameters using the
+      provided tuner. When using distributed training, the tuning is
+      distributed.
+    workers: If set, enable distributed training. "workers" is the list of IP
+      addresses of the workers. A worker is a process running
+      `ydf.start_worker(port)`.
+  """
+
+  def __init__(
+      self,
+      label: str,
+      task: generic_learner.Task = generic_learner.Task.CLASSIFICATION,
+      weights: Optional[str] = None,
+      ranking_group: Optional[str] = None,
+      uplift_treatment: Optional[str] = None,
+      features: dataspec.ColumnDefs = None,
+      include_all_columns: bool = False,
+      max_vocab_count: int = 2000,
+      min_vocab_frequency: int = 5,
+      discretize_numerical_columns: bool = False,
+      num_discretized_numerical_bins: int = 255,
+      max_num_scanned_rows_to_infer_semantic: int = 10000,
+      max_num_scanned_rows_to_compute_statistics: int = 10000,
+      data_spec: Optional[data_spec_pb2.DataSpecification] = None,
+      apply_link_function: Optional[bool] = True,
+      force_numerical_discretization: Optional[bool] = False,
+      max_depth: Optional[int] = 6,
+      max_unique_values_for_discretized_numerical: Optional[int] = 16000,
+      maximum_model_size_in_memory_in_bytes: Optional[float] = -1.0,
+      maximum_training_duration_seconds: Optional[float] = -1.0,
+      min_examples: Optional[int] = 5,
+      num_candidate_attributes: Optional[int] = -1,
+      num_candidate_attributes_ratio: Optional[float] = -1.0,
+      num_trees: Optional[int] = 300,
+      pure_serving_model: Optional[bool] = False,
+      random_seed: Optional[int] = 123456,
+      shrinkage: Optional[float] = 0.1,
+      use_hessian_gain: Optional[bool] = False,
+      worker_logs: Optional[bool] = True,
+      num_threads: Optional[int] = None,
+      working_dir: Optional[str] = None,
+      resume_training: bool = False,
+      resume_training_snapshot_interval_seconds: int = 1800,
+      tuner: Optional[tuner_lib.AbstractTuner] = None,
+      workers: Optional[Sequence[str]] = None,
+  ):
+
+    hyper_parameters = {
+        "apply_link_function": apply_link_function,
+        "force_numerical_discretization": force_numerical_discretization,
+        "max_depth": max_depth,
+        "max_unique_values_for_discretized_numerical": (
+            max_unique_values_for_discretized_numerical
+        ),
+        "maximum_model_size_in_memory_in_bytes": (
+            maximum_model_size_in_memory_in_bytes
+        ),
+        "maximum_training_duration_seconds": maximum_training_duration_seconds,
+        "min_examples": min_examples,
+        "num_candidate_attributes": num_candidate_attributes,
+        "num_candidate_attributes_ratio": num_candidate_attributes_ratio,
+        "num_trees": num_trees,
+        "pure_serving_model": pure_serving_model,
+        "random_seed": random_seed,
+        "shrinkage": shrinkage,
+        "use_hessian_gain": use_hessian_gain,
+        "worker_logs": worker_logs,
+    }
+
+    data_spec_args = dataspec.DataSpecInferenceArgs(
+        columns=dataspec.normalize_column_defs(features),
+        include_all_columns=include_all_columns,
+        max_vocab_count=max_vocab_count,
+        min_vocab_frequency=min_vocab_frequency,
+        discretize_numerical_columns=discretize_numerical_columns,
+        num_discretized_numerical_bins=num_discretized_numerical_bins,
+        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
+        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
+    )
+
+    deployment_config = self._build_deployment_config(
+        num_threads=num_threads,
+        resume_training=resume_training,
+        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
+        working_dir=working_dir,
+        workers=workers,
+    )
+
+    super().__init__(
+        learner_name="DISTRIBUTED_GRADIENT_BOOSTED_TREES",
+        task=task,
+        label=label,
+        weights=weights,
+        ranking_group=ranking_group,
+        uplift_treatment=uplift_treatment,
+        data_spec_args=data_spec_args,
+        data_spec=data_spec,
+        hyper_parameters=hyper_parameters,
+        deployment_config=deployment_config,
+        tuner=tuner,
+    )
+
+  def train(
+      self,
+      ds: dataset.InputDataset,
+      valid: Optional[dataset.InputDataset] = None,
+  ) -> gradient_boosted_trees_model.GradientBoostedTreesModel:
+    """Trains a model on the given dataset.
+
+    Options for dataset reading are given on the learner. Consult the
+    documentation of the learner or ydf.create_vertical_dataset() for additional
+    information on dataset reading in YDF.
+
+    Usage example:
+
+    ```
+    import ydf
+    import pandas as pd
+
+    train_ds = pd.read_csv(...)
+
+    learner = ydf.DistributedGradientBoostedTreesLearner(label="label")
+    model = learner.train(train_ds)
+    print(model.summary())
+    ```
+
+    If training is interrupted (for example, by interrupting the cell execution
+    in Colab), the model will be returned to the state it was in at the moment
+    of interruption.
+
+    Args:
+      ds: Training dataset.
+      valid: Optional validation dataset. Some learners, such as Random Forest,
+        do not need validation dataset. Some learners, such as
+        GradientBoostedTrees, automatically extract a validation dataset from
+        the training dataset if the validation dataset is not provided.
+
+    Returns:
+      A trained model.
+    """
+    return super().train(ds, valid)
+
+  @classmethod
+  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
+    return abstract_learner_pb2.LearnerCapabilities(
+        support_max_training_duration=False,
+        resume_training=True,
+        support_validation_dataset=False,
+        support_partial_cache_dataset_format=True,
+        support_max_model_size_in_memory=False,
+        support_monotonic_constraints=False,
+    )
+
+  @classmethod
+  def hyperparameter_templates(
+      cls,
+  ) -> Dict[str, hyperparameters.HyperparameterTemplate]:
+    r"""Hyperparameter templates for this Learner.
+
+    This learner currently does not provide any hyperparameter templates, this
+    method is provided for consistency with other learners.
+
+    Returns:
+      Empty dictionary.
+    """
+    return {}
+
+
+class CartLearner(generic_learner.GenericLearner):
+  r"""Cart learning algorithm.
+
+  A CART (Classification and Regression Trees) a decision tree. The non-leaf
+  nodes contains conditions (also known as splits) while the leaf nodes contain
+  prediction values. The training dataset is divided in two parts. The first is
+  used to grow the tree while the second is used to prune the tree.
+
+  Usage example:
+
+  ```python
+  import ydf
+  import pandas as pd
+
+  dataset = pd.read_csv("project/dataset.csv")
+
+  model = ydf.CartLearner().train(dataset)
+
+  print(model.summary())
+  ```
+
+  Hyperparameters are configured to give reasonable results for typical
+  datasets. Hyperparameters can also be modified manually (see descriptions)
+  below or by applying the hyperparameter templates available with
+  `CartLearner.hyperparameter_templates()` (see this function's documentation
+  for
+  details).
+
+  Attributes:
+    label: Label of the dataset. The label column should not be identified as a
+      feature in the `features` parameter.
+    task: Task to solve (e.g. Task.CLASSIFICATION, Task.REGRESSION,
+      Task.RANKING, Task.CATEGORICAL_UPLIFT, Task.NUMERICAL_UPLIFT).
+    weights: Name of a feature that identifies the weight of each example. If
+      weights are not specified, unit weights are assumed. The weight column
+      should not be identified as a feature in the `features` parameter.
+    ranking_group: Only for `task=Task.RANKING`. Name of a feature that
+      identifies queries in a query/document ranking task. The ranking group
+      should not be identified as a feature in the `features` parameter.
+    uplift_treatment: Only for `task=Task.CATEGORICAL_UPLIFT` and `task=Task`.
+      NUMERICAL_UPLIFT. Name of a numerical feature that identifies the
+      treatment in an uplift problem. The value 0 is reserved for the control
+      treatment. Currently, only 0/1 binary treatments are supported.
+    features: If None, all columns are used as features. The semantic of the
+      features is determined automatically. Otherwise, if
+      include_all_columns=False (default) only the column listed in `features`
+      are imported. If include_all_columns=True, all the columns are imported as
+      features and only the semantic of the columns NOT in `columns` is
+      determined automatically. If specified,  defines the order of the features
+      - any non-listed features are appended in-order after the specified
+      features (if include_all_columns=True). The label, weights, uplift
+      treatment and ranking_group columns should not be specified as features.
+    include_all_columns: See `features`.
+    max_vocab_count: Maximum size of the vocabulary of CATEGORICAL and
+      CATEGORICAL_SET columns stored as strings. If more unique values exist,
+      only the most frequent values are kept, and the remaining values are
+      considered as out-of-vocabulary.
+    min_vocab_frequency: Minimum number of occurrence of a value for CATEGORICAL
+      and CATEGORICAL_SET columns. Value observed less than
+      `min_vocab_frequency` are considered as out-of-vocabulary.
+    discretize_numerical_columns: If true, discretize all the numerical columns
+      before training. Discretized numerical columns are faster to train with,
+      but they can have a negative impact on the model quality. Using
+      `discretize_numerical_columns=True` is equivalent as setting the column
+      semantic DISCRETIZED_NUMERICAL in the `column` argument. See the
+      definition of DISCRETIZED_NUMERICAL for more details.
+    num_discretized_numerical_bins: Number of bins used when disretizing
+      numerical columns.
+    max_num_scanned_rows_to_infer_semantic: Number of rows to scan when
+      inferring the column's semantic if it is not explicitly specified. Only
+      used when reading from file, in-memory datasets are always read in full.
+      Setting this to a lower number will speed up dataset reading, but might
+      result in incorrect column semantics. Set to -1 to scan the entire
+      dataset.
+    max_num_scanned_rows_to_compute_statistics: Number of rows to scan when
+      computing a column's statistics. Only used when reading from file,
+      in-memory datasets are always read in full. A column's statistics include
+      the dictionary for categorical features and the mean / min / max for
+      numerical features. Setting this to a lower number will speed up dataset
+      reading, but skew statistics in the dataspec, which can hurt model quality
+      (e.g. if an important category of a categorical feature is considered
+      OOV). Set to -1 to scan the entire dataset.
+    data_spec: Dataspec to be used (advanced). If a data spec is given,
+      `columns`, `include_all_columns`, `max_vocab_count`,
+      `min_vocab_frequency`, `discretize_numerical_columns` and
+      `num_discretized_numerical_bins` will be ignored.
+    allow_na_conditions: If true, the tree training evaluates conditions of the
+      type `X is NA` i.e. `X is missing`. Default: False.
+    categorical_algorithm: How to learn splits on categorical attributes. -
+      `CART`: CART algorithm. Find categorical splits of the form "value \\in
+      mask". The solution is exact for binary classification, regression and
+      ranking. It is approximated for multi-class classification. This is a good
+      first algorithm to use. In case of overfitting (very small dataset, large
+      dictionary), the "random" algorithm is a good alternative. - `ONE_HOT`:
+      One-hot encoding. Find the optimal categorical split of the form
+      "attribute == param". This method is similar (but more efficient) than
+      converting converting each possible categorical value into a boolean
+      feature. This method is available for comparison purpose and generally
+      performs worse than other alternatives. - `RANDOM`: Best splits among a
+      set of random candidate. Find the a categorical split of the form "value
+      \\in mask" using a random search. This solution can be seen as an
+      approximation of the CART algorithm. This method is a strong alternative
+      to CART. This algorithm is inspired from section "5.1 Categorical
+      Variables" of "Random Forest", 2001.
+        Default: "CART".
+    categorical_set_split_greedy_sampling: For categorical set splits e.g.
+      texts. Probability for a categorical value to be a candidate for the
+      positive set. The sampling is applied once per node (i.e. not at every
+      step of the greedy optimization). Default: 0.1.
+    categorical_set_split_max_num_items: For categorical set splits e.g. texts.
+      Maximum number of items (prior to the sampling). If more items are
+      available, the least frequent items are ignored. Changing this value is
+      similar to change the "max_vocab_count" before loading the dataset, with
+      the following exception: With `max_vocab_count`, all the remaining items
+      are grouped in a special Out-of-vocabulary item. With `max_num_items`,
+      this is not the case. Default: -1.
+    categorical_set_split_min_item_frequency: For categorical set splits e.g.
+      texts. Minimum number of occurrences of an item to be considered.
+      Default: 1.
+    growing_strategy: How to grow the tree. - `LOCAL`: Each node is split
+      independently of the other nodes. In other words, as long as a node
+      satisfy the splits "constraints (e.g. maximum depth, minimum number of
+      observations), the node will be split. This is the "classical" way to grow
+      decision trees. - `BEST_FIRST_GLOBAL`: The node with the best loss
+      reduction among all the nodes of the tree is selected for splitting. This
+      method is also called "best first" or "leaf-wise growth". See "Best-first
+      decision tree learning", Shi and "Additive logistic regression : A
+      statistical view of boosting", Friedman for more details. Default:
+      "LOCAL".
+    honest: In honest trees, different training examples are used to infer the
+      structure and the leaf values. This regularization technique trades
+      examples for bias estimates. It might increase or reduce the quality of
+      the model. See "Generalized Random Forests", Athey et al. In this paper,
+      Honest trees are trained with the Random Forest algorithm with a sampling
+      without replacement. Default: False.
+    honest_fixed_separation: For honest trees only i.e. honest=true. If true, a
+      new random separation is generated for each tree. If false, the same
+      separation is used for all the trees (e.g., in Gradient Boosted Trees
+      containing multiple trees). Default: False.
+    honest_ratio_leaf_examples: For honest trees only i.e. honest=true. Ratio of
+      examples used to set the leaf values. Default: 0.5.
+    in_split_min_examples_check: Whether to check the `min_examples` constraint
+      in the split search (i.e. splits leading to one child having less than
+      `min_examples` examples are considered invalid) or before the split search
+      (i.e. a node can be derived only if it contains more than `min_examples`
+      examples). If false, there can be nodes with less than `min_examples`
+      training examples. Default: True.
+    keep_non_leaf_label_distribution: Whether to keep the node value (i.e. the
+      distribution of the labels of the training examples) of non-leaf nodes.
+      This information is not used during serving, however it can be used for
+      model interpretation as well as hyper parameter tuning. This can take lots
+      of space, sometimes accounting for half of the model size. Default: True.
+    max_depth: Maximum depth of the tree. `max_depth=1` means that all trees
+      will be roots. `max_depth=-1` means that tree depth is not restricted by
+      this parameter. Values <= -2 will be ignored. Default: 16.
+    max_num_nodes: Maximum number of nodes in the tree. Set to -1 to disable
+      this limit. Only available for `growing_strategy=BEST_FIRST_GLOBAL`.
+      Default: None.
+    maximum_model_size_in_memory_in_bytes: Limit the size of the model when
+      stored in ram. Different algorithms can enforce this limit differently.
+      Note that when models are compiled into an inference, the size of the
+      inference engine is generally much smaller than the original model.
+      Default: -1.0.
+    maximum_training_duration_seconds: Maximum training duration of the model
+      expressed in seconds. Each learning algorithm is free to use this
+      parameter at it sees fit. Enabling maximum training duration makes the
+      model training non-deterministic. Default: -1.0.
+    min_examples: Minimum number of examples in a node. Default: 5.
+    missing_value_policy: Method used to handle missing attribute values. -
+      `GLOBAL_IMPUTATION`: Missing attribute values are imputed, with the mean
+      (in case of numerical attribute) or the most-frequent-item (in case of
+      categorical attribute) computed on the entire dataset (i.e. the
+      information contained in the data spec). - `LOCAL_IMPUTATION`: Missing
+      attribute values are imputed with the mean (numerical attribute) or
+      most-frequent-item (in the case of categorical attribute) evaluated on the
+      training examples in the current node. - `RANDOM_LOCAL_IMPUTATION`:
+      Missing attribute values are imputed from randomly sampled values from the
+      training examples in the current node. This method was proposed by Clinic
+      et al. in "Random Survival Forests"
+      (https://projecteuclid.org/download/pdfview_1/euclid.aoas/1223908043).
+        Default: "GLOBAL_IMPUTATION".
+    num_candidate_attributes: Number of unique valid attributes tested for each
+      node. An attribute is valid if it has at least a valid split. If
+      `num_candidate_attributes=0`, the value is set to the classical default
+      value for Random Forest: `sqrt(number of input attributes)` in case of
+      classification and `number_of_input_attributes / 3` in case of regression.
+      If `num_candidate_attributes=-1`, all the attributes are tested. Default:
+      0.
+    num_candidate_attributes_ratio: Ratio of attributes tested at each node. If
+      set, it is equivalent to `num_candidate_attributes =
+      number_of_input_features x num_candidate_attributes_ratio`. The possible
+      values are between ]0, and 1] as well as -1. If not set or equal to -1,
+      the `num_candidate_attributes` is used. Default: -1.0.
+    pure_serving_model: Clear the model from any information that is not
+      required for model serving. This includes debugging, model interpretation
+      and other meta-data. The size of the serialized model can be reduced
+      significatively (50% model size reduction is common). This parameter has
+      no impact on the quality, serving speed or RAM usage of model serving.
+      Default: False.
+    random_seed: Random seed for the training of the model. Learners are
+      expected to be deterministic by the random seed. Default: 123456.
+    sorting_strategy: How are sorted the numerical features in order to find the
+      splits - PRESORT: The features are pre-sorted at the start of the
+      training. This solution is faster but consumes much more memory than
+      IN_NODE. - IN_NODE: The features are sorted just before being used in the
+      node. This solution is slow but consumes little amount of memory. .
+      Default: "PRESORT".
+    sparse_oblique_normalization: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Normalization applied on the features, before
+      applying the sparse oblique projections. - `NONE`: No normalization. -
+      `STANDARD_DEVIATION`: Normalize the feature by the estimated standard
+      deviation on the entire train dataset. Also known as Z-Score
+      normalization. - `MIN_MAX`: Normalize the feature by the range (i.e.
+      max-min) estimated on the entire train dataset. Default: None.
+    sparse_oblique_num_projections_exponent: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections
+      to test at each node as `num_features^num_projections_exponent`. Default:
+      None.
+    sparse_oblique_projection_density_factor: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Controls of the number of random projections
+      to test at each node as `num_features^num_projections_exponent`. Default:
+      None.
+    sparse_oblique_weights: For sparse oblique splits i.e.
+      `split_axis=SPARSE_OBLIQUE`. Possible values: - `BINARY`: The oblique
+      weights are sampled in {-1,1} (default). - `CONTINUOUS`: The oblique
+      weights are be sampled in [-1,1]. Default: None.
+    split_axis: What structure of split to consider for numerical features. -
+      `AXIS_ALIGNED`: Axis aligned splits (i.e. one condition at a time). This
+      is the "classical" way to train a tree. Default value. - `SPARSE_OBLIQUE`:
+      Sparse oblique splits (i.e. splits one a small number of features) from
+      "Sparse Projection Oblique Random Forests", Tomita et al., 2020. Default:
+      "AXIS_ALIGNED".
+    uplift_min_examples_in_treatment: For uplift models only. Minimum number of
+      examples per treatment in a node. Default: 5.
+    uplift_split_score: For uplift models only. Splitter score i.e. score
+      optimized by the splitters. The scores are introduced in "Decision trees
+      for uplift modeling with single and multiple treatments", Rzepakowski et
+      al. Notation: `p` probability / average value of the positive outcome, `q`
+      probability / average value in the control group. - `KULLBACK_LEIBLER` or
+      `KL`: - p log (p/q) - `EUCLIDEAN_DISTANCE` or `ED`: (p-q)^2 -
+      `CHI_SQUARED` or `CS`: (p-q)^2/q
+        Default: "KULLBACK_LEIBLER".
+    validation_ratio: Ratio of the training dataset used to create the
+      validation dataset for pruning the tree. If set to 0, the entire dataset
+      is used for training, and the tree is not pruned. Default: 0.1.
+    num_threads: Number of threads used to train the model. Different learning
+      algorithms use multi-threading differently and with different degree of
+      efficiency. If `None`, `num_threads` will be automatically set to the
+      number of processors (up to a maximum of 32; or set to 6 if the number of
+      processors is not available). Making `num_threads` significantly larger
+      than the number of processors can slow-down the training speed. The
+      default value logic might change in the future.
+    resume_training: If true, the model training resumes from the checkpoint
+      stored in the `working_dir` directory. If `working_dir` does not contain
+      any model checkpoint, the training starts from the beginning. Resuming
+      training is useful in the following situations: (1) The training was
+      interrupted by the user (e.g. ctrl+c or "stop" button in a notebook) or
+      rescheduled, or (2) the hyper-parameter of the learner was changed e.g.
+      increasing the number of trees.
+    working_dir: Path to a directory available for the learning algorithm to
+      store intermediate computation results. Depending on the learning
+      algorithm and parameters, the working_dir might be optional, required, or
+      ignored. For instance, distributed training algorithm always need a
+      "working_dir", and the gradient boosted tree and hyper-parameter tuners
+      will export artefacts to the "working_dir" if provided.
+    resume_training_snapshot_interval_seconds: Indicative number of seconds in
+      between snapshots when `resume_training=True`. Might be ignored by some
+      learners.
+    tuner: If set, automatically select the best hyperparameters using the
+      provided tuner. When using distributed training, the tuning is
+      distributed.
+    workers: If set, enable distributed training. "workers" is the list of IP
+      addresses of the workers. A worker is a process running
+      `ydf.start_worker(port)`.
+  """
+
+  def __init__(
+      self,
+      label: str,
+      task: generic_learner.Task = generic_learner.Task.CLASSIFICATION,
+      weights: Optional[str] = None,
+      ranking_group: Optional[str] = None,
+      uplift_treatment: Optional[str] = None,
+      features: dataspec.ColumnDefs = None,
+      include_all_columns: bool = False,
+      max_vocab_count: int = 2000,
+      min_vocab_frequency: int = 5,
+      discretize_numerical_columns: bool = False,
+      num_discretized_numerical_bins: int = 255,
+      max_num_scanned_rows_to_infer_semantic: int = 10000,
+      max_num_scanned_rows_to_compute_statistics: int = 10000,
+      data_spec: Optional[data_spec_pb2.DataSpecification] = None,
+      allow_na_conditions: Optional[bool] = False,
+      categorical_algorithm: Optional[str] = "CART",
+      categorical_set_split_greedy_sampling: Optional[float] = 0.1,
+      categorical_set_split_max_num_items: Optional[int] = -1,
+      categorical_set_split_min_item_frequency: Optional[int] = 1,
+      growing_strategy: Optional[str] = "LOCAL",
+      honest: Optional[bool] = False,
+      honest_fixed_separation: Optional[bool] = False,
+      honest_ratio_leaf_examples: Optional[float] = 0.5,
+      in_split_min_examples_check: Optional[bool] = True,
+      keep_non_leaf_label_distribution: Optional[bool] = True,
+      max_depth: Optional[int] = 16,
+      max_num_nodes: Optional[int] = None,
+      maximum_model_size_in_memory_in_bytes: Optional[float] = -1.0,
+      maximum_training_duration_seconds: Optional[float] = -1.0,
+      min_examples: Optional[int] = 5,
+      missing_value_policy: Optional[str] = "GLOBAL_IMPUTATION",
+      num_candidate_attributes: Optional[int] = 0,
+      num_candidate_attributes_ratio: Optional[float] = -1.0,
+      pure_serving_model: Optional[bool] = False,
+      random_seed: Optional[int] = 123456,
+      sorting_strategy: Optional[str] = "PRESORT",
+      sparse_oblique_normalization: Optional[str] = None,
+      sparse_oblique_num_projections_exponent: Optional[float] = None,
+      sparse_oblique_projection_density_factor: Optional[float] = None,
+      sparse_oblique_weights: Optional[str] = None,
+      split_axis: Optional[str] = "AXIS_ALIGNED",
+      uplift_min_examples_in_treatment: Optional[int] = 5,
+      uplift_split_score: Optional[str] = "KULLBACK_LEIBLER",
+      validation_ratio: Optional[float] = 0.1,
+      num_threads: Optional[int] = None,
+      working_dir: Optional[str] = None,
+      resume_training: bool = False,
+      resume_training_snapshot_interval_seconds: int = 1800,
+      tuner: Optional[tuner_lib.AbstractTuner] = None,
+      workers: Optional[Sequence[str]] = None,
+  ):
+
+    hyper_parameters = {
+        "allow_na_conditions": allow_na_conditions,
+        "categorical_algorithm": categorical_algorithm,
+        "categorical_set_split_greedy_sampling": (
+            categorical_set_split_greedy_sampling
+        ),
+        "categorical_set_split_max_num_items": (
+            categorical_set_split_max_num_items
+        ),
+        "categorical_set_split_min_item_frequency": (
+            categorical_set_split_min_item_frequency
+        ),
+        "growing_strategy": growing_strategy,
+        "honest": honest,
+        "honest_fixed_separation": honest_fixed_separation,
+        "honest_ratio_leaf_examples": honest_ratio_leaf_examples,
+        "in_split_min_examples_check": in_split_min_examples_check,
+        "keep_non_leaf_label_distribution": keep_non_leaf_label_distribution,
+        "max_depth": max_depth,
+        "max_num_nodes": max_num_nodes,
+        "maximum_model_size_in_memory_in_bytes": (
+            maximum_model_size_in_memory_in_bytes
+        ),
+        "maximum_training_duration_seconds": maximum_training_duration_seconds,
+        "min_examples": min_examples,
+        "missing_value_policy": missing_value_policy,
+        "num_candidate_attributes": num_candidate_attributes,
+        "num_candidate_attributes_ratio": num_candidate_attributes_ratio,
+        "pure_serving_model": pure_serving_model,
+        "random_seed": random_seed,
+        "sorting_strategy": sorting_strategy,
+        "sparse_oblique_normalization": sparse_oblique_normalization,
+        "sparse_oblique_num_projections_exponent": (
+            sparse_oblique_num_projections_exponent
+        ),
+        "sparse_oblique_projection_density_factor": (
+            sparse_oblique_projection_density_factor
+        ),
+        "sparse_oblique_weights": sparse_oblique_weights,
+        "split_axis": split_axis,
+        "uplift_min_examples_in_treatment": uplift_min_examples_in_treatment,
+        "uplift_split_score": uplift_split_score,
+        "validation_ratio": validation_ratio,
+    }
+
+    data_spec_args = dataspec.DataSpecInferenceArgs(
+        columns=dataspec.normalize_column_defs(features),
+        include_all_columns=include_all_columns,
+        max_vocab_count=max_vocab_count,
+        min_vocab_frequency=min_vocab_frequency,
+        discretize_numerical_columns=discretize_numerical_columns,
+        num_discretized_numerical_bins=num_discretized_numerical_bins,
+        max_num_scanned_rows_to_infer_semantic=max_num_scanned_rows_to_infer_semantic,
+        max_num_scanned_rows_to_compute_statistics=max_num_scanned_rows_to_compute_statistics,
+    )
+
+    deployment_config = self._build_deployment_config(
+        num_threads=num_threads,
+        resume_training=resume_training,
+        resume_training_snapshot_interval_seconds=resume_training_snapshot_interval_seconds,
+        working_dir=working_dir,
+        workers=workers,
+    )
+
+    super().__init__(
+        learner_name="CART",
+        task=task,
+        label=label,
+        weights=weights,
+        ranking_group=ranking_group,
+        uplift_treatment=uplift_treatment,
+        data_spec_args=data_spec_args,
+        data_spec=data_spec,
+        hyper_parameters=hyper_parameters,
+        deployment_config=deployment_config,
+        tuner=tuner,
+    )
+
+  def train(
+      self,
+      ds: dataset.InputDataset,
+      valid: Optional[dataset.InputDataset] = None,
+  ) -> random_forest_model.RandomForestModel:
+    """Trains a model on the given dataset.
+
+    Options for dataset reading are given on the learner. Consult the
+    documentation of the learner or ydf.create_vertical_dataset() for additional
+    information on dataset reading in YDF.
+
+    Usage example:
+
+    ```
+    import ydf
+    import pandas as pd
+
+    train_ds = pd.read_csv(...)
+
+    learner = ydf.CartLearner(label="label")
+    model = learner.train(train_ds)
+    print(model.summary())
+    ```
+
+    If training is interrupted (for example, by interrupting the cell execution
+    in Colab), the model will be returned to the state it was in at the moment
+    of interruption.
+
+    Args:
+      ds: Training dataset.
+      valid: Optional validation dataset. Some learners, such as Random Forest,
+        do not need validation dataset. Some learners, such as
+        GradientBoostedTrees, automatically extract a validation dataset from
+        the training dataset if the validation dataset is not provided.
+
+    Returns:
+      A trained model.
+    """
+    return super().train(ds, valid)
+
+  @classmethod
+  def capabilities(cls) -> abstract_learner_pb2.LearnerCapabilities:
+    return abstract_learner_pb2.LearnerCapabilities(
+        support_max_training_duration=True,
+        resume_training=False,
+        support_validation_dataset=False,
+        support_partial_cache_dataset_format=False,
+        support_max_model_size_in_memory=False,
+        support_monotonic_constraints=False,
+    )
+
+  @classmethod
+  def hyperparameter_templates(
+      cls,
+  ) -> Dict[str, hyperparameters.HyperparameterTemplate]:
+    r"""Hyperparameter templates for this Learner.
+
+    This learner currently does not provide any hyperparameter templates, this
+    method is provided for consistency with other learners.
+
+    Returns:
+      Empty dictionary.
+    """
+    return {}
```

## ydf/learner/tuner.py

```diff
@@ -1,344 +1,344 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""A tuner is a utility to configure of the C++ hyperparameter optimizer v2.
-
-Usage example:
-
-```
-import ydf
-
-# Create a tuner
-tuner = ydf.RandomSearchTuner(num_trials=20)
-tuner.choice("num_candidate_attributes_ratio", [1.0, 0.8, 0.6])
-tuner.choice("use_hessian_gain", [True, False])
-
-local_search_space = tuner.choice("growing_strategy", ["LOCAL"])
-local_search_space.choice("max_depth", [4, 5, 6, 7])
-
-global_search_space = tuner.choice(
-    "growing_strategy", ["BEST_FIRST_GLOBAL"], merge=True)
-global_search_space.choice("max_num_nodes", [16, 32, 64, 128])
-
-# Configure a learner
-learner = ydf.RandomForestLearner(tuner=tuner)
-```
-"""
-
-from typing import Optional, Sequence, Union
-
-from google.protobuf.internal import containers
-from yggdrasil_decision_forests.learner import abstract_learner_pb2
-from yggdrasil_decision_forests.learner.hyperparameters_optimizer import hyperparameters_optimizer_pb2
-from yggdrasil_decision_forests.learner.hyperparameters_optimizer.optimizers import random_pb2
-from yggdrasil_decision_forests.model import hyperparameter_pb2
-
-
-# Single hyperparameter value
-HyperParameterValue = Union[int, float, str, bool]
-
-# List of values for a given hyperparameter
-# Note: Hyperparameter types cannot be mixed.
-HyperParameterSequence = Union[
-    Sequence[int], Sequence[float], Sequence[str], Sequence[bool]
-]
-
-# Short aliases
-TrainingConfig = abstract_learner_pb2.TrainingConfig
-OptimizerConfig = (
-    hyperparameters_optimizer_pb2.HyperParametersOptimizerLearnerTrainingConfig
-)
-Field = hyperparameter_pb2.HyperParameterSpace.Field
-Value = hyperparameter_pb2.GenericHyperParameters.Value
-
-
-class AbstractTuner:
-  """Base class for tuners."""
-
-  def __init__(
-      self,
-      optimizer_key: str,
-      automatic_search_space: bool = False,
-      parallel_trials: int = 1,
-      max_trial_duration: Optional[float] = None,
-  ):
-    """Initializes tuner.
-
-    Args:
-      optimizer_key: Registered identifier of the optimizer.
-      automatic_search_space: If true, automatically define the search space of
-        hyperparameters. In this case, configuring the hyperparameters manually
-        (e.g. calling "choice(...)" on the tuner) is not necessary.
-      parallel_trials: Number of trials to evaluate in parallel. The training of
-        an individual model uses "num_threads" threads (configured in the
-        learner). Therefore, in the non-distributed training setting, the total
-        number of threads will be `parallel_trials x num_threads`. In the
-        distributed training setting, the average number of user threads per
-        worker will be `parallel_trials x num_threads // num_workers`. In this
-        case, make sure `parallel_trials` is a multiple of the number of
-        workers.
-      max_trial_duration: Maximum training duration of an individual trial
-        expressed in seconds. This parameter is different from the
-        `maximum_training_duration_seconds` learner parameter that defines the
-        maximum total training and tuning duration. Set to None for no time
-        limit.
-    """
-
-    self._automatic_search_space = automatic_search_space
-    self._parallel_trials = parallel_trials
-    self._max_trial_duration = max_trial_duration
-
-    self._train_config = TrainingConfig(learner="HYPERPARAMETER_OPTIMIZER")
-
-    optimizer_config = self._optimizer_config()
-    optimizer_config.optimizer.optimizer_key = optimizer_key
-    optimizer_config.optimizer.parallel_trials = parallel_trials
-
-    if automatic_search_space:
-      optimizer_config.predefined_search_space.SetInParent()
-
-    if max_trial_duration is not None:
-      optimizer_config.base_learner.maximum_training_duration_seconds = (
-          max_trial_duration
-      )
-
-  @property
-  def parallel_trials(self) -> int:
-    return self._parallel_trials
-
-  @property
-  def train_config(self) -> TrainingConfig:
-    """Gets the training configuration proto."""
-    return self._train_config
-
-  def _optimizer_config(self) -> OptimizerConfig:
-    """Gets the optimizer configuration proto."""
-    return self._train_config.Extensions[
-        hyperparameters_optimizer_pb2.hyperparameters_optimizer_config
-    ]
-
-  def set_base_learner(self, learner: str) -> None:
-    """Sets the base learner key."""
-    self._optimizer_config().base_learner.learner = learner
-
-  def set_base_learner_num_threads(self, num_threads: int) -> None:
-    """Sets the number of threads in the base learner."""
-    self._optimizer_config().base_learner_deployment.num_threads = num_threads
-
-  def choice(
-      self,
-      key: str,
-      values: HyperParameterSequence,
-      merge: bool = False,
-  ) -> "SearchSpace":
-    """Adds a hyperparameter with a list of possible values.
-
-    Args:
-      key: Name of the hyperparameter.
-      values: List of possible values.
-      merge: If false (default), raises an error if the hyperparameter already
-        exist. If true, and if the hyperparameter already exist, adds "values"
-        to the already configured values. If true, and if the hyperparameter
-        does not already exist, raises an error.
-
-    Returns:
-      The conditional SearchSpace corresponding to the values in "values".
-    """
-
-    sp = SearchSpace(self._optimizer_config().search_space.fields)
-    return sp.choice(key, values, merge)
-
-
-class RandomSearchTuner(AbstractTuner):
-  """Tuner using random search.
-
-  The candidate hyper-parameter can be evaluated independently and in parallel.
-
-  Attributes:
-    num_trials: Number of hyperparameter configurations to evaluate.
-    automatic_search_space: If true, automatically define the search space of
-      hyperparameters. In this case, configuring the hyperparameters manually
-      (e.g. calling "choice(...)" on the tuner) is not necessary.
-    parallel_trials: Number of trials to evaluate in parallel. The training of
-      an individual model uses "num_threads" threads (configured in the
-      learner). Therefore, in the non-distributed training setting, the total
-      number of threads will be `parallel_trials x num_threads`. In the
-      distributed training setting, the average number of user threads per
-      worker will be `parallel_trials x num_threads // num_workers`. In this
-      case, make sure `parallel_trials` is a multiple of the number of workers.
-    max_trial_duration: Maximum training duration of an individual trial
-      expressed in seconds. This parameter is different from the
-      `maximum_training_duration_seconds` learner parameter that define the
-      maximum total training and tuning duration. Set to None for no time limit.
-  """
-
-  def __init__(
-      self,
-      num_trials: int = 100,
-      automatic_search_space: bool = False,
-      parallel_trials: int = 1,
-      max_trial_duration: Optional[float] = None,
-  ):
-    super().__init__(
-        optimizer_key="RANDOM",
-        automatic_search_space=automatic_search_space,
-        parallel_trials=parallel_trials,
-        max_trial_duration=max_trial_duration,
-    )
-    self._random_optimizer_config().num_trials = num_trials
-
-  def _random_optimizer_config(self) -> random_pb2.RandomOptimizerConfig:
-    return self._optimizer_config().optimizer.Extensions[random_pb2.random]
-
-
-class VizierTuner(AbstractTuner):
-  """Tuner using Vizier.
-
-  Attributes:
-    num_trials: Number of hyperparameter configurations to evaluate.
-    automatic_search_space: If true, automatically define the search space of
-      hyperparameters. In this case, configuring the hyperparameters manually
-      (e.g. calling "choice(...)" on the tuner) is not necessary.
-    parallel_trials: Number of trials to evaluate in parallel. The training of
-      an individual model uses "num_threads" threads (configured in the
-      learner). Therefore, in the non-distributed training setting, the total
-      number of threads will be `parallel_trials x num_threads`. In the
-      distributed training setting, the average number of user threads per
-      worker will be `parallel_trials x num_threads // num_workers`. In this
-      case, make sure `parallel_trials` is a multiple of the number of workers.
-    max_trial_duration: Maximum training duration of an individual trial
-      expressed in seconds. This parameter is different from the
-      `maximum_training_duration_seconds` learner parameter that define the
-      maximum total training and tuning duration. Set to None for no time limit.
-  """
-
-  def __init__(
-      self,
-      num_trials: int = 100,
-      automatic_search_space: bool = False,
-      parallel_trials: int = 1,
-      max_trial_duration: Optional[float] = None,
-  ):
-    super().__init__(
-        optimizer_key="VIZIER",
-        automatic_search_space=automatic_search_space,
-        parallel_trials=parallel_trials,
-        max_trial_duration=max_trial_duration,
-    )
-
-
-class SearchSpace:
-  """Set of hyper-parameter values to explore.
-
-  For the users: Don't create this object directly. Instead, use a tuner e.g.
-  `tuner.choice(...)`.
-  """
-
-  def __init__(
-      self,
-      fields: "containers.RepeatedCompositeFieldContainer[Field]",
-      parent_values: Optional[
-          hyperparameter_pb2.HyperParameterSpace.DiscreteCandidates
-      ] = None,
-  ):
-    """Initializes the search space.
-
-    Args:
-      fields: Fields to populate in a hyperparameter proto.
-      parent_values: Conditionnal parent value.
-    """
-
-    self._fields = fields
-    self._parent_values = parent_values
-
-  def choice(
-      self,
-      key: str,
-      values: HyperParameterSequence,
-      merge: bool = False,
-  ) -> "SearchSpace":
-    """Adds a hyperparameter with a list of possible values.
-
-    Args:
-      key: Name of the hyperparameter.
-      values: List of possible values.
-      merge: If false (default), raises an error if the hyperparameter already
-        exist. If true, and if the hyperparameter already exist, adds "values"
-        to the already configured values. If true, and if the hyperparameter
-        does not already exist, raises an error.
-
-    Returns:
-      The conditional SearchSpace corresponding to the values in "values".
-    """
-
-    if not values:
-      raise ValueError("The list of values is empty")
-
-    field = self._find_field(key)
-    if field is None:
-      # New hyperparameter
-      if merge:
-        raise ValueError(
-            f"Using `merge=true` but the hyperparameter {key!r} does not"
-            " already exist"
-        )
-      field = self._fields.add(name=key)
-      if self._parent_values:
-        field.parent_discrete_values.MergeFrom(self._parent_values)
-    else:
-      # Existing hyperparameter
-      if not merge:
-        raise ValueError(
-            f"The hyperparameter {key!r} already exist. Use `merge=True` to add"
-            " values to an existing hyperparameter"
-        )
-
-    # Register new values
-    possible_values = [_py_value_to_hp_value(key, value) for value in values]
-    dst_values = hyperparameter_pb2.HyperParameterSpace.DiscreteCandidates(
-        possible_values=possible_values[:]
-    )
-    field.discrete_candidates.possible_values.extend(possible_values)
-    return SearchSpace(field.children, parent_values=dst_values)
-
-  def _find_field(self, key: str) -> Optional[Field]:
-    """Returns the hyperparameter with a given name.
-
-    If the hyperparameter does not exist, return None.
-
-    Args:
-      key: Hyperparameter name.
-    """
-
-    for field in self._fields:
-      if field.name == key:
-        return field
-    return None
-
-
-def _py_value_to_hp_value(key: str, value: HyperParameterValue) -> Value:
-  """Converts a user input / python primitive into a Value proto."""
-
-  if isinstance(value, bool):
-    return Value(categorical="true" if value else "false")
-  elif isinstance(value, int):
-    return Value(integer=value)
-  elif isinstance(value, float):
-    return Value(real=value)
-  elif isinstance(value, str):
-    return Value(categorical=value)
-  else:
-    raise ValueError(
-        f"Not supported value {value!r} ({type(value)}) for {key!r}"
-    )
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""A tuner is a utility to configure of the C++ hyperparameter optimizer v2.
+
+Usage example:
+
+```
+import ydf
+
+# Create a tuner
+tuner = ydf.RandomSearchTuner(num_trials=20)
+tuner.choice("num_candidate_attributes_ratio", [1.0, 0.8, 0.6])
+tuner.choice("use_hessian_gain", [True, False])
+
+local_search_space = tuner.choice("growing_strategy", ["LOCAL"])
+local_search_space.choice("max_depth", [4, 5, 6, 7])
+
+global_search_space = tuner.choice(
+    "growing_strategy", ["BEST_FIRST_GLOBAL"], merge=True)
+global_search_space.choice("max_num_nodes", [16, 32, 64, 128])
+
+# Configure a learner
+learner = ydf.RandomForestLearner(tuner=tuner)
+```
+"""
+
+from typing import Optional, Sequence, Union
+
+from google.protobuf.internal import containers
+from ydf.proto.learner import abstract_learner_pb2
+from ydf.proto.learner.hyperparameters_optimizer import hyperparameters_optimizer_pb2
+from ydf.proto.learner.hyperparameters_optimizer.optimizers import random_pb2
+from ydf.proto.model import hyperparameter_pb2
+
+
+# Single hyperparameter value
+HyperParameterValue = Union[int, float, str, bool]
+
+# List of values for a given hyperparameter
+# Note: Hyperparameter types cannot be mixed.
+HyperParameterSequence = Union[
+    Sequence[int], Sequence[float], Sequence[str], Sequence[bool]
+]
+
+# Short aliases
+TrainingConfig = abstract_learner_pb2.TrainingConfig
+OptimizerConfig = (
+    hyperparameters_optimizer_pb2.HyperParametersOptimizerLearnerTrainingConfig
+)
+Field = hyperparameter_pb2.HyperParameterSpace.Field
+Value = hyperparameter_pb2.GenericHyperParameters.Value
+
+
+class AbstractTuner:
+  """Base class for tuners."""
+
+  def __init__(
+      self,
+      optimizer_key: str,
+      automatic_search_space: bool = False,
+      parallel_trials: int = 1,
+      max_trial_duration: Optional[float] = None,
+  ):
+    """Initializes tuner.
+
+    Args:
+      optimizer_key: Registered identifier of the optimizer.
+      automatic_search_space: If true, automatically define the search space of
+        hyperparameters. In this case, configuring the hyperparameters manually
+        (e.g. calling "choice(...)" on the tuner) is not necessary.
+      parallel_trials: Number of trials to evaluate in parallel. The training of
+        an individual model uses "num_threads" threads (configured in the
+        learner). Therefore, in the non-distributed training setting, the total
+        number of threads will be `parallel_trials x num_threads`. In the
+        distributed training setting, the average number of user threads per
+        worker will be `parallel_trials x num_threads // num_workers`. In this
+        case, make sure `parallel_trials` is a multiple of the number of
+        workers.
+      max_trial_duration: Maximum training duration of an individual trial
+        expressed in seconds. This parameter is different from the
+        `maximum_training_duration_seconds` learner parameter that defines the
+        maximum total training and tuning duration. Set to None for no time
+        limit.
+    """
+
+    self._automatic_search_space = automatic_search_space
+    self._parallel_trials = parallel_trials
+    self._max_trial_duration = max_trial_duration
+
+    self._train_config = TrainingConfig(learner="HYPERPARAMETER_OPTIMIZER")
+
+    optimizer_config = self._optimizer_config()
+    optimizer_config.optimizer.optimizer_key = optimizer_key
+    optimizer_config.optimizer.parallel_trials = parallel_trials
+
+    if automatic_search_space:
+      optimizer_config.predefined_search_space.SetInParent()
+
+    if max_trial_duration is not None:
+      optimizer_config.base_learner.maximum_training_duration_seconds = (
+          max_trial_duration
+      )
+
+  @property
+  def parallel_trials(self) -> int:
+    return self._parallel_trials
+
+  @property
+  def train_config(self) -> TrainingConfig:
+    """Gets the training configuration proto."""
+    return self._train_config
+
+  def _optimizer_config(self) -> OptimizerConfig:
+    """Gets the optimizer configuration proto."""
+    return self._train_config.Extensions[
+        hyperparameters_optimizer_pb2.hyperparameters_optimizer_config
+    ]
+
+  def set_base_learner(self, learner: str) -> None:
+    """Sets the base learner key."""
+    self._optimizer_config().base_learner.learner = learner
+
+  def set_base_learner_num_threads(self, num_threads: int) -> None:
+    """Sets the number of threads in the base learner."""
+    self._optimizer_config().base_learner_deployment.num_threads = num_threads
+
+  def choice(
+      self,
+      key: str,
+      values: HyperParameterSequence,
+      merge: bool = False,
+  ) -> "SearchSpace":
+    """Adds a hyperparameter with a list of possible values.
+
+    Args:
+      key: Name of the hyperparameter.
+      values: List of possible values.
+      merge: If false (default), raises an error if the hyperparameter already
+        exist. If true, and if the hyperparameter already exist, adds "values"
+        to the already configured values. If true, and if the hyperparameter
+        does not already exist, raises an error.
+
+    Returns:
+      The conditional SearchSpace corresponding to the values in "values".
+    """
+
+    sp = SearchSpace(self._optimizer_config().search_space.fields)
+    return sp.choice(key, values, merge)
+
+
+class RandomSearchTuner(AbstractTuner):
+  """Tuner using random search.
+
+  The candidate hyper-parameter can be evaluated independently and in parallel.
+
+  Attributes:
+    num_trials: Number of hyperparameter configurations to evaluate.
+    automatic_search_space: If true, automatically define the search space of
+      hyperparameters. In this case, configuring the hyperparameters manually
+      (e.g. calling "choice(...)" on the tuner) is not necessary.
+    parallel_trials: Number of trials to evaluate in parallel. The training of
+      an individual model uses "num_threads" threads (configured in the
+      learner). Therefore, in the non-distributed training setting, the total
+      number of threads will be `parallel_trials x num_threads`. In the
+      distributed training setting, the average number of user threads per
+      worker will be `parallel_trials x num_threads // num_workers`. In this
+      case, make sure `parallel_trials` is a multiple of the number of workers.
+    max_trial_duration: Maximum training duration of an individual trial
+      expressed in seconds. This parameter is different from the
+      `maximum_training_duration_seconds` learner parameter that define the
+      maximum total training and tuning duration. Set to None for no time limit.
+  """
+
+  def __init__(
+      self,
+      num_trials: int = 100,
+      automatic_search_space: bool = False,
+      parallel_trials: int = 1,
+      max_trial_duration: Optional[float] = None,
+  ):
+    super().__init__(
+        optimizer_key="RANDOM",
+        automatic_search_space=automatic_search_space,
+        parallel_trials=parallel_trials,
+        max_trial_duration=max_trial_duration,
+    )
+    self._random_optimizer_config().num_trials = num_trials
+
+  def _random_optimizer_config(self) -> random_pb2.RandomOptimizerConfig:
+    return self._optimizer_config().optimizer.Extensions[random_pb2.random]
+
+
+class VizierTuner(AbstractTuner):
+  """Tuner using Vizier.
+
+  Attributes:
+    num_trials: Number of hyperparameter configurations to evaluate.
+    automatic_search_space: If true, automatically define the search space of
+      hyperparameters. In this case, configuring the hyperparameters manually
+      (e.g. calling "choice(...)" on the tuner) is not necessary.
+    parallel_trials: Number of trials to evaluate in parallel. The training of
+      an individual model uses "num_threads" threads (configured in the
+      learner). Therefore, in the non-distributed training setting, the total
+      number of threads will be `parallel_trials x num_threads`. In the
+      distributed training setting, the average number of user threads per
+      worker will be `parallel_trials x num_threads // num_workers`. In this
+      case, make sure `parallel_trials` is a multiple of the number of workers.
+    max_trial_duration: Maximum training duration of an individual trial
+      expressed in seconds. This parameter is different from the
+      `maximum_training_duration_seconds` learner parameter that define the
+      maximum total training and tuning duration. Set to None for no time limit.
+  """
+
+  def __init__(
+      self,
+      num_trials: int = 100,
+      automatic_search_space: bool = False,
+      parallel_trials: int = 1,
+      max_trial_duration: Optional[float] = None,
+  ):
+    super().__init__(
+        optimizer_key="VIZIER",
+        automatic_search_space=automatic_search_space,
+        parallel_trials=parallel_trials,
+        max_trial_duration=max_trial_duration,
+    )
+
+
+class SearchSpace:
+  """Set of hyper-parameter values to explore.
+
+  For the users: Don't create this object directly. Instead, use a tuner e.g.
+  `tuner.choice(...)`.
+  """
+
+  def __init__(
+      self,
+      fields: "containers.RepeatedCompositeFieldContainer[Field]",
+      parent_values: Optional[
+          hyperparameter_pb2.HyperParameterSpace.DiscreteCandidates
+      ] = None,
+  ):
+    """Initializes the search space.
+
+    Args:
+      fields: Fields to populate in a hyperparameter proto.
+      parent_values: Conditionnal parent value.
+    """
+
+    self._fields = fields
+    self._parent_values = parent_values
+
+  def choice(
+      self,
+      key: str,
+      values: HyperParameterSequence,
+      merge: bool = False,
+  ) -> "SearchSpace":
+    """Adds a hyperparameter with a list of possible values.
+
+    Args:
+      key: Name of the hyperparameter.
+      values: List of possible values.
+      merge: If false (default), raises an error if the hyperparameter already
+        exist. If true, and if the hyperparameter already exist, adds "values"
+        to the already configured values. If true, and if the hyperparameter
+        does not already exist, raises an error.
+
+    Returns:
+      The conditional SearchSpace corresponding to the values in "values".
+    """
+
+    if not values:
+      raise ValueError("The list of values is empty")
+
+    field = self._find_field(key)
+    if field is None:
+      # New hyperparameter
+      if merge:
+        raise ValueError(
+            f"Using `merge=true` but the hyperparameter {key!r} does not"
+            " already exist"
+        )
+      field = self._fields.add(name=key)
+      if self._parent_values:
+        field.parent_discrete_values.MergeFrom(self._parent_values)
+    else:
+      # Existing hyperparameter
+      if not merge:
+        raise ValueError(
+            f"The hyperparameter {key!r} already exist. Use `merge=True` to add"
+            " values to an existing hyperparameter"
+        )
+
+    # Register new values
+    possible_values = [_py_value_to_hp_value(key, value) for value in values]
+    dst_values = hyperparameter_pb2.HyperParameterSpace.DiscreteCandidates(
+        possible_values=possible_values[:]
+    )
+    field.discrete_candidates.possible_values.extend(possible_values)
+    return SearchSpace(field.children, parent_values=dst_values)
+
+  def _find_field(self, key: str) -> Optional[Field]:
+    """Returns the hyperparameter with a given name.
+
+    If the hyperparameter does not exist, return None.
+
+    Args:
+      key: Hyperparameter name.
+    """
+
+    for field in self._fields:
+      if field.name == key:
+        return field
+    return None
+
+
+def _py_value_to_hp_value(key: str, value: HyperParameterValue) -> Value:
+  """Converts a user input / python primitive into a Value proto."""
+
+  if isinstance(value, bool):
+    return Value(categorical="true" if value else "false")
+  elif isinstance(value, int):
+    return Value(integer=value)
+  elif isinstance(value, float):
+    return Value(real=value)
+  elif isinstance(value, str):
+    return Value(categorical=value)
+  else:
+    raise ValueError(
+        f"Not supported value {value!r} ({type(value)}) for {key!r}"
+    )
```

## ydf/learner/tuner_test.py

```diff
@@ -1,198 +1,198 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Unit tests of the tuner."""
-
-from absl.testing import absltest
-from absl.testing import parameterized
-
-from yggdrasil_decision_forests.learner import abstract_learner_pb2
-from yggdrasil_decision_forests.learner.hyperparameters_optimizer import hyperparameters_optimizer_pb2
-from yggdrasil_decision_forests.learner.hyperparameters_optimizer.optimizers import random_pb2
-from yggdrasil_decision_forests.model import hyperparameter_pb2
-from ydf.learner import tuner as tuner_lib
-from ydf.utils import test_utils
-
-DiscreteCandidates = hyperparameter_pb2.HyperParameterSpace.DiscreteCandidates
-Field = hyperparameter_pb2.HyperParameterSpace.Field
-Value = hyperparameter_pb2.GenericHyperParameters.Value
-HyperParametersOptimizerLearnerTrainingConfig = (
-    hyperparameters_optimizer_pb2.HyperParametersOptimizerLearnerTrainingConfig
-)
-
-
-class TunerTest(parameterized.TestCase):
-
-  def test_to_proto(self):
-    # Define toy tuner
-    tuner = tuner_lib.RandomSearchTuner(
-        num_trials=20,
-        parallel_trials=2,
-        max_trial_duration=10,
-    )
-    tuner.choice("a", [1, 2, 3])
-    tuner.choice("b", [1.0, 2.0, 3.0])
-    tuner.choice("c", ["x", "y"])
-
-    s = tuner.choice("c", ["v", "w"], merge=True)
-    s.choice("d", [1, 2, 3])
-
-    # Check internal state
-    self.assertEqual(tuner.parallel_trials, 2)
-
-    expected_proto = abstract_learner_pb2.TrainingConfig(
-        learner="HYPERPARAMETER_OPTIMIZER"
-    )
-
-    expected_optimizer = HyperParametersOptimizerLearnerTrainingConfig(
-        optimizer=hyperparameters_optimizer_pb2.Optimizer(
-            optimizer_key="RANDOM",
-            parallel_trials=2,
-        ),
-        base_learner=abstract_learner_pb2.TrainingConfig(
-            maximum_training_duration_seconds=10
-        ),
-        search_space=hyperparameter_pb2.HyperParameterSpace(
-            fields=[
-                Field(
-                    name="a",
-                    discrete_candidates=DiscreteCandidates(
-                        possible_values=[
-                            Value(integer=1),
-                            Value(integer=2),
-                            Value(integer=3),
-                        ],
-                    ),
-                ),
-                Field(
-                    name="b",
-                    discrete_candidates=DiscreteCandidates(
-                        possible_values=[
-                            Value(real=1),
-                            Value(real=2),
-                            Value(real=3),
-                        ]
-                    ),
-                ),
-                Field(
-                    name="c",
-                    discrete_candidates=DiscreteCandidates(
-                        possible_values=[
-                            Value(categorical="x"),
-                            Value(categorical="y"),
-                            Value(categorical="v"),
-                            Value(categorical="w"),
-                        ]
-                    ),
-                    children=[
-                        Field(
-                            name="d",
-                            discrete_candidates=DiscreteCandidates(
-                                possible_values=[
-                                    Value(integer=1),
-                                    Value(integer=2),
-                                    Value(integer=3),
-                                ]
-                            ),
-                            parent_discrete_values=DiscreteCandidates(
-                                possible_values=[
-                                    Value(categorical="v"),
-                                    Value(categorical="w"),
-                                ],
-                            ),
-                        ),
-                    ],
-                ),
-            ],
-        ),
-    )
-
-    expected_random_optimizer = random_pb2.RandomOptimizerConfig(num_trials=20)
-
-    # Note: Extension construction is not supported.
-    expected_optimizer.optimizer.Extensions[random_pb2.random].CopyFrom(
-        expected_random_optimizer
-    )
-    expected_proto.Extensions[
-        hyperparameters_optimizer_pb2.hyperparameters_optimizer_config
-    ].CopyFrom(expected_optimizer)
-
-    test_utils.assertProto2Equal(self, tuner.train_config, expected_proto)
-
-  def test_error_no_values(self):
-    tuner = tuner_lib.RandomSearchTuner(num_trials=20)
-    with self.assertRaisesRegex(ValueError, "The list of values is empty"):
-      tuner.choice("a", [])
-
-  def test_merging_does_not_exist(self):
-    tuner = tuner_lib.RandomSearchTuner(num_trials=20)
-    with self.assertRaisesRegex(
-        ValueError, "hyperparameter 'a' does not already exist"
-    ):
-      tuner.choice("a", [1, 2], merge=True)
-
-  def test_merging_already_exist(self):
-    tuner = tuner_lib.RandomSearchTuner(num_trials=20)
-    tuner.choice("a", [1, 2])
-
-    with self.assertRaisesRegex(
-        ValueError, "The hyperparameter 'a' already exist"
-    ):
-      tuner.choice("a", [3, 4])
-
-  def test_merging_good(self):
-    tuner = tuner_lib.RandomSearchTuner(num_trials=20)
-    tuner.choice("a", [1, 2])
-    tuner.choice("a", [3, 4], merge=True)
-
-    expected_proto = abstract_learner_pb2.TrainingConfig(
-        learner="HYPERPARAMETER_OPTIMIZER"
-    )
-
-    expected_optimizer = HyperParametersOptimizerLearnerTrainingConfig(
-        optimizer=hyperparameters_optimizer_pb2.Optimizer(
-            optimizer_key="RANDOM",
-            parallel_trials=1,
-        ),
-        search_space=hyperparameter_pb2.HyperParameterSpace(
-            fields=[
-                Field(
-                    name="a",
-                    discrete_candidates=DiscreteCandidates(
-                        possible_values=[
-                            Value(integer=1),
-                            Value(integer=2),
-                            Value(integer=3),
-                            Value(integer=4),
-                        ],
-                    ),
-                ),
-            ],
-        ),
-    )
-    expected_random_optimizer = random_pb2.RandomOptimizerConfig(num_trials=20)
-
-    # Note: Extension construction is not supported.
-    expected_optimizer.optimizer.Extensions[random_pb2.random].CopyFrom(
-        expected_random_optimizer
-    )
-    expected_proto.Extensions[
-        hyperparameters_optimizer_pb2.hyperparameters_optimizer_config
-    ].CopyFrom(expected_optimizer)
-
-    test_utils.assertProto2Equal(self, tuner.train_config, expected_proto)
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Unit tests of the tuner."""
+
+from absl.testing import absltest
+from absl.testing import parameterized
+
+from ydf.proto.learner import abstract_learner_pb2
+from ydf.proto.learner.hyperparameters_optimizer import hyperparameters_optimizer_pb2
+from ydf.proto.learner.hyperparameters_optimizer.optimizers import random_pb2
+from ydf.proto.model import hyperparameter_pb2
+from ydf.learner import tuner as tuner_lib
+from ydf.utils import test_utils
+
+DiscreteCandidates = hyperparameter_pb2.HyperParameterSpace.DiscreteCandidates
+Field = hyperparameter_pb2.HyperParameterSpace.Field
+Value = hyperparameter_pb2.GenericHyperParameters.Value
+HyperParametersOptimizerLearnerTrainingConfig = (
+    hyperparameters_optimizer_pb2.HyperParametersOptimizerLearnerTrainingConfig
+)
+
+
+class TunerTest(parameterized.TestCase):
+
+  def test_to_proto(self):
+    # Define toy tuner
+    tuner = tuner_lib.RandomSearchTuner(
+        num_trials=20,
+        parallel_trials=2,
+        max_trial_duration=10,
+    )
+    tuner.choice("a", [1, 2, 3])
+    tuner.choice("b", [1.0, 2.0, 3.0])
+    tuner.choice("c", ["x", "y"])
+
+    s = tuner.choice("c", ["v", "w"], merge=True)
+    s.choice("d", [1, 2, 3])
+
+    # Check internal state
+    self.assertEqual(tuner.parallel_trials, 2)
+
+    expected_proto = abstract_learner_pb2.TrainingConfig(
+        learner="HYPERPARAMETER_OPTIMIZER"
+    )
+
+    expected_optimizer = HyperParametersOptimizerLearnerTrainingConfig(
+        optimizer=hyperparameters_optimizer_pb2.Optimizer(
+            optimizer_key="RANDOM",
+            parallel_trials=2,
+        ),
+        base_learner=abstract_learner_pb2.TrainingConfig(
+            maximum_training_duration_seconds=10
+        ),
+        search_space=hyperparameter_pb2.HyperParameterSpace(
+            fields=[
+                Field(
+                    name="a",
+                    discrete_candidates=DiscreteCandidates(
+                        possible_values=[
+                            Value(integer=1),
+                            Value(integer=2),
+                            Value(integer=3),
+                        ],
+                    ),
+                ),
+                Field(
+                    name="b",
+                    discrete_candidates=DiscreteCandidates(
+                        possible_values=[
+                            Value(real=1),
+                            Value(real=2),
+                            Value(real=3),
+                        ]
+                    ),
+                ),
+                Field(
+                    name="c",
+                    discrete_candidates=DiscreteCandidates(
+                        possible_values=[
+                            Value(categorical="x"),
+                            Value(categorical="y"),
+                            Value(categorical="v"),
+                            Value(categorical="w"),
+                        ]
+                    ),
+                    children=[
+                        Field(
+                            name="d",
+                            discrete_candidates=DiscreteCandidates(
+                                possible_values=[
+                                    Value(integer=1),
+                                    Value(integer=2),
+                                    Value(integer=3),
+                                ]
+                            ),
+                            parent_discrete_values=DiscreteCandidates(
+                                possible_values=[
+                                    Value(categorical="v"),
+                                    Value(categorical="w"),
+                                ],
+                            ),
+                        ),
+                    ],
+                ),
+            ],
+        ),
+    )
+
+    expected_random_optimizer = random_pb2.RandomOptimizerConfig(num_trials=20)
+
+    # Note: Extension construction is not supported.
+    expected_optimizer.optimizer.Extensions[random_pb2.random].CopyFrom(
+        expected_random_optimizer
+    )
+    expected_proto.Extensions[
+        hyperparameters_optimizer_pb2.hyperparameters_optimizer_config
+    ].CopyFrom(expected_optimizer)
+
+    test_utils.assertProto2Equal(self, tuner.train_config, expected_proto)
+
+  def test_error_no_values(self):
+    tuner = tuner_lib.RandomSearchTuner(num_trials=20)
+    with self.assertRaisesRegex(ValueError, "The list of values is empty"):
+      tuner.choice("a", [])
+
+  def test_merging_does_not_exist(self):
+    tuner = tuner_lib.RandomSearchTuner(num_trials=20)
+    with self.assertRaisesRegex(
+        ValueError, "hyperparameter 'a' does not already exist"
+    ):
+      tuner.choice("a", [1, 2], merge=True)
+
+  def test_merging_already_exist(self):
+    tuner = tuner_lib.RandomSearchTuner(num_trials=20)
+    tuner.choice("a", [1, 2])
+
+    with self.assertRaisesRegex(
+        ValueError, "The hyperparameter 'a' already exist"
+    ):
+      tuner.choice("a", [3, 4])
+
+  def test_merging_good(self):
+    tuner = tuner_lib.RandomSearchTuner(num_trials=20)
+    tuner.choice("a", [1, 2])
+    tuner.choice("a", [3, 4], merge=True)
+
+    expected_proto = abstract_learner_pb2.TrainingConfig(
+        learner="HYPERPARAMETER_OPTIMIZER"
+    )
+
+    expected_optimizer = HyperParametersOptimizerLearnerTrainingConfig(
+        optimizer=hyperparameters_optimizer_pb2.Optimizer(
+            optimizer_key="RANDOM",
+            parallel_trials=1,
+        ),
+        search_space=hyperparameter_pb2.HyperParameterSpace(
+            fields=[
+                Field(
+                    name="a",
+                    discrete_candidates=DiscreteCandidates(
+                        possible_values=[
+                            Value(integer=1),
+                            Value(integer=2),
+                            Value(integer=3),
+                            Value(integer=4),
+                        ],
+                    ),
+                ),
+            ],
+        ),
+    )
+    expected_random_optimizer = random_pb2.RandomOptimizerConfig(num_trials=20)
+
+    # Note: Extension construction is not supported.
+    expected_optimizer.optimizer.Extensions[random_pb2.random].CopyFrom(
+        expected_random_optimizer
+    )
+    expected_proto.Extensions[
+        hyperparameters_optimizer_pb2.hyperparameters_optimizer_config
+    ].CopyFrom(expected_optimizer)
+
+    test_utils.assertProto2Equal(self, tuner.train_config, expected_proto)
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/learner/worker.py

 * *Ordering differences only*

```diff
@@ -1,72 +1,72 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Worker for distributed training."""
-
-from typing import Callable, Optional
-
-from ydf.cc import ydf as ydf_cc
-
-
-def start_worker(
-    port: int, blocking: bool = True
-) -> Optional[Callable[[], None]]:
-  """Starts a worker locally on the given port.
-
-  The addresses of workers are passed to learners with the `workers` argument.
-
-  Usage example:
-
-  ```python
-  # On worker machine #0 at address 192.168.0.1
-  ydf.start_worker(9000)
-
-  # On worker machine #1 at address 192.168.0.2
-  ydf.start_worker(9000)
-
-  # On manager
-  learner = ydf.DistributedGradientBoostedTreesLearner(
-        label = "my_label",
-        working_dir = "/shared/working_dir,
-        resume_training = True,
-        workers = ["192.168.0.1:9000", "192.168.0.2:9000"],
-    ).train(dataset)
-  ```
-
-  Example with non-blocking call:
-
-  ```python
-  # On worker machine
-  stop_worker = start_worker(blocking=False)
-  # Do some work with the worker
-  stop_worker() # Stops the worker
-  ```
-
-  Args:
-    port: TCP port of the worker.
-    blocking: If true (default), the function is blocking until the worker is
-      stopped (e.g., error, interruption by the manager). If false, the function
-      is non-blocking and returns a callable that, when called, will stop the
-      worker.
-
-  Returns:
-    Callable to stop the worker. Only returned if `blocking=True`.
-  """
-
-  if blocking:
-    ydf_cc.StartWorkerBlocking(port)
-    return None
-
-  uid = ydf_cc.StartWorkerNonBlocking(port)
-  return lambda: ydf_cc.StopWorkerNonBlocking(uid)
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Worker for distributed training."""
+
+from typing import Callable, Optional
+
+from ydf.cc import ydf as ydf_cc
+
+
+def start_worker(
+    port: int, blocking: bool = True
+) -> Optional[Callable[[], None]]:
+  """Starts a worker locally on the given port.
+
+  The addresses of workers are passed to learners with the `workers` argument.
+
+  Usage example:
+
+  ```python
+  # On worker machine #0 at address 192.168.0.1
+  ydf.start_worker(9000)
+
+  # On worker machine #1 at address 192.168.0.2
+  ydf.start_worker(9000)
+
+  # On manager
+  learner = ydf.DistributedGradientBoostedTreesLearner(
+        label = "my_label",
+        working_dir = "/shared/working_dir,
+        resume_training = True,
+        workers = ["192.168.0.1:9000", "192.168.0.2:9000"],
+    ).train(dataset)
+  ```
+
+  Example with non-blocking call:
+
+  ```python
+  # On worker machine
+  stop_worker = start_worker(blocking=False)
+  # Do some work with the worker
+  stop_worker() # Stops the worker
+  ```
+
+  Args:
+    port: TCP port of the worker.
+    blocking: If true (default), the function is blocking until the worker is
+      stopped (e.g., error, interruption by the manager). If false, the function
+      is non-blocking and returns a callable that, when called, will stop the
+      worker.
+
+  Returns:
+    Callable to stop the worker. Only returned if `blocking=True`.
+  """
+
+  if blocking:
+    ydf_cc.StartWorkerBlocking(port)
+    return None
+
+  uid = ydf_cc.StartWorkerNonBlocking(port)
+  return lambda: ydf_cc.StopWorkerNonBlocking(uid)
```

## ydf/learner/worker_main.py

 * *Ordering differences only*

```diff
@@ -1,29 +1,29 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""A worker for distributed training."""
-
-from absl import app
-from absl import flags
-import ydf
-
-_PORT = flags.DEFINE_integer("port", None, "IP port to run the worker")
-
-
-def main(argv) -> None:
-  ydf.start_worker(_PORT.value)
-
-
-if __name__ == "__main__":
-  app.run(main)
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""A worker for distributed training."""
+
+from absl import app
+from absl import flags
+import ydf
+
+_PORT = flags.DEFINE_integer("port", None, "IP port to run the worker")
+
+
+def main(argv) -> None:
+  ydf.start_worker(_PORT.value)
+
+
+if __name__ == "__main__":
+  app.run(main)
```

## ydf/metric/__init__.py

 * *Ordering differences only*

```diff
@@ -1,14 +1,14 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
```

## ydf/metric/display_metric.py

 * *Ordering differences only*

```diff
@@ -1,485 +1,485 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Utilities to display metrics."""
-
-import base64
-import io
-import textwrap
-from typing import Any, Optional, Tuple
-from xml.dom import minidom
-
-from ydf.cc import ydf
-from ydf.metric import metric
-from ydf.utils import documentation
-from ydf.utils import html
-from ydf.utils import string_lib
-
-
-class _UnescappedString(minidom.Text):
-  """Html element able to print unescaped string."""
-
-  def set_data(self, data: str) -> None:
-    """Sets content to print."""
-
-    self._data = data
-
-  def writexml(
-      self,
-      writer,
-      indent="",
-      addindent="",
-      newl="",
-      encoding=None,
-      standalone=None,
-  ) -> None:
-    writer.write(self._data)
-
-
-def css_style(doc: html.Doc) -> html.Elem:
-  """Css style for metric display."""
-
-  raw_style = textwrap.dedent("""\
-  .metric_box {
-  }
-
-  .metric_box a {
-    text-decoration:none;
-    color: darkblue;
-  }
-
-  .metric_box .title {
-    font-weight: bold;
-  }
-
-  .metric_box .section {
-    margin: 5px 5px 5px 5px;
-  }
-
-  .metric_box .grid {
-    display: grid;
-    grid-template-columns: max-content max-content max-content max-content;
-    grid-template-rows: auto;
-    border-bottom: 1px solid lightgray;
-    padding: 10px;
-  }
-    
-  .metric_box .grid > div {
-  }
-
-  .metric_box .grid > div:nth-child(odd) {
-    font-weight: bold;
-    padding-right: 5px;
-    text-align: right;
-  }
-
-  .metric_box .grid > div:nth-child(even) {
-    padding-right: 20px;
-  }
-
-  .metric_box .complex {
-    display: inline-block;
-    margin: 15px 20px 0px 0px;
-    vertical-align: top;
-  }
-
-  .metric_box .complex .key {
-    font-weight: bold;
-  }
-
-  .metric_box .complex .value {
-  }
-
-  .metric_box .confusion_matrix {
-    border-collapse: collapse;
-    margin: 15px 15px;
-    border: 1px solid lightgray;
-  }
-
-  .metric_box .confusion_matrix th {
-    background-color: #ededed;
-    font-weight: bold;
-    text-align: left;
-    padding: 5px;
-    border: 1px solid lightgray;
-  }
-
-  .metric_box .confusion_matrix td {
-    text-align: right;
-    padding: 3px;
-    border: 1px solid lightgray;
-  }
-  """)
-
-  style = doc.createElement("style")
-  raw_node = _UnescappedString()
-  raw_node.set_data(raw_style)
-  style.appendChild(raw_node)
-  return style
-
-
-def evaluation_to_str(e: metric.Evaluation) -> str:
-  """String representation of an evaluation."""
-
-  text = ""
-
-  # Classification
-  text += _field_to_str("accuracy", e.accuracy)
-  text += _field_to_str("confusion matrix", e.confusion_matrix)
-  if e.characteristics is not None:
-    text += "characteristics:"
-    for characteristic in e.characteristics:
-      text += "\n" + string_lib.indent(str(characteristic))
-
-  # Regression
-  text += _field_to_str("RMSE", e.rmse)
-  text += _field_to_str("RMSE 95% CI [B]", e.rmse_ci95_bootstrap)
-
-  # Ranking
-  text += _field_to_str("NDCG", e.ndcg)
-
-  # Uplifting
-  text += _field_to_str("QINI", e.qini)
-  text += _field_to_str("AUUC", e.auuc)
-
-  # Custom
-  if e.custom_metrics:
-    for k, v in e.custom_metrics.items():
-      text += _field_to_str(k, v)
-
-  # Generic
-  text += _field_to_str("loss", e.loss)
-  text += _field_to_str("num examples", e.num_examples)
-  text += _field_to_str("num examples (weighted)", e.num_examples_weighted)
-
-  if not text:
-    text = "No metrics"
-
-  return text
-
-
-def evaluation_to_html_str(e: metric.Evaluation, add_style: bool = True) -> str:
-  """Html representation of an evaluation."""
-
-  doc, root = html.create_doc()
-
-  if add_style:
-    root.appendChild(css_style(doc))
-
-  html_metric_box = doc.createElement("div")
-  html_metric_box.setAttribute("class", "metric_box")
-  root.appendChild(html_metric_box)
-
-  html_metric_grid = doc.createElement("div")
-  html_metric_grid.setAttribute("class", "grid section")
-  html_metric_box.appendChild(html_metric_grid)
-
-  # Metrics
-
-  # Classification
-  _field_to_html(doc, html_metric_grid, "accuracy", e.accuracy)
-
-  if e.characteristics is not None:
-    for characteristic in e.characteristics:
-      _field_to_html(
-          doc,
-          html_metric_grid,
-          "AUC: " + characteristic.name,
-          characteristic.roc_auc,
-      )
-
-      _field_to_html(
-          doc,
-          html_metric_grid,
-          "PR-AUC: " + characteristic.name,
-          characteristic.pr_auc,
-      )
-
-  # Regression
-  _field_to_html(doc, html_metric_grid, "RMSE", e.rmse)
-  _field_to_html(
-      doc, html_metric_grid, "RMSE 95% CI [B]", e.rmse_ci95_bootstrap
-  )
-
-  # Ranking
-  _field_to_html(doc, html_metric_grid, "NDCG", e.ndcg)
-
-  # Uplifting
-  _field_to_html(doc, html_metric_grid, "QINI", e.qini)
-  _field_to_html(doc, html_metric_grid, "AUUC", e.auuc)
-
-  # Custom
-  if e.custom_metrics:
-    for k, v in e.custom_metrics.items():
-      _field_to_html(doc, html_metric_grid, k, v)
-
-  # Generic
-  _field_to_html(doc, html_metric_grid, "loss", e.loss)
-  _field_to_html(
-      doc,
-      html_metric_grid,
-      "num examples",
-      e.num_examples,
-      documentation_url=documentation.URL_NUM_EXAMPLES,
-  )
-  _field_to_html(
-      doc,
-      html_metric_grid,
-      "num examples (weighted)",
-      e.num_examples_weighted,
-      documentation_url=documentation.URL_WEIGHTED_NUM_EXAMPLES,
-  )
-
-  if e.confusion_matrix is not None:
-    _object_to_html(
-        doc,
-        html_metric_box,
-        "Confusion matrix",
-        confusion_matrix_to_html_str(doc, e.confusion_matrix),
-        documentation_url=documentation.URL_CONFUSION_MATRIX,
-    )
-
-  # Curves
-  plot_html = ydf.EvaluationPlotToHtml(e._evaluation_proto)
-  _object_to_html(doc, html_metric_box, None, plot_html, raw_html=True)
-
-  return root.toprettyxml(indent="  ")
-
-
-def confusion_matrix_to_html_str(
-    doc: html.Doc, confusion: metric.ConfusionMatrix
-) -> html.Elem:
-  """Html representation of a confusion matrix."""
-
-  html_table = doc.createElement("table")
-  html_table.setAttribute("class", "confusion_matrix")
-
-  # First line
-  tr = doc.createElement("tr")
-  html_table.appendChild(tr)
-
-  th = doc.createElement("th")
-  tr.appendChild(th)
-  th.appendChild(doc.createTextNode("Label \\ Pred"))
-
-  for label in confusion.classes:
-    th = doc.createElement("th")
-    tr.appendChild(th)
-    th.appendChild(doc.createTextNode(label))
-
-  for prediction_idx, prediction in enumerate(confusion.classes):
-    tr = doc.createElement("tr")
-    html_table.appendChild(tr)
-
-    th = doc.createElement("th")
-    tr.appendChild(th)
-    th.appendChild(doc.createTextNode(prediction))
-
-    for label_idx in range(len(confusion.classes)):
-      value = confusion.value(
-          label_idx=label_idx, prediction_idx=prediction_idx
-      )
-      td = doc.createElement("td")
-      tr.appendChild(td)
-      td.appendChild(doc.createTextNode(f"{value:g}"))
-
-  return html_table
-
-
-def _field_value_to_str(value: Any) -> Tuple[str, bool]:
-  """Friendly text print a "value".
-
-  Operations:
-    - Remove decimal points in float e.g. 4.0 => "4".
-    - Round digits to precision e.g 2.99999999 => "3
-    - Remove any trailing line return.
-
-  Args:
-    value: The value to print.
-
-  Returns:
-    The string value, and a boolean indicating if the string value is
-    multi-lines.
-  """
-
-  if value is None:
-    return "", False
-
-  if isinstance(value, float):
-    if round(value) == value:
-      # Remove decimale point
-      str_value = str(int(value))
-    else:
-      # Compact print
-      str_value = f"{value:g}"
-  else:
-    str_value = str(value)
-
-  if "\n" not in str_value:
-    return str_value, False
-
-  # Indent the value from the key
-  str_value = string_lib.indent(str_value)
-  if str_value and str_value[-1] == "\n":
-    # Remove final line return if any.
-    str_value = str_value[:-1]
-  return str_value, True
-
-
-def _field_to_str(key: str, value: Any) -> str:
-  """Friendly text print a "key:value".
-
-  Operations:
-    - Operations from "_field_value_to_str"
-
-  Args:
-    key: Name of the field.
-    value: Value of the field.
-
-  Returns:
-    Formated key:value.
-  """
-
-  if value is None:
-    return ""
-
-  str_value, is_multi_lines = _field_value_to_str(value)
-
-  if is_multi_lines:
-    return f"{key}:\n{str_value}\n"
-  else:
-    return f"{key}: {str_value}\n"
-
-
-def _field_to_html(
-    doc: html.Doc,
-    parent,
-    key: str,
-    value: Any,
-    documentation_url: Optional[str] = None,
-) -> None:
-  """Friendly html print a "key:value".
-
-  Operations:
-    - All the operations of "_field_value_to_str"
-    - Wrap multi-line values into <pre>.
-
-  Args:
-    doc: Html document.
-    parent: Html element.
-    key: Name of the field.
-    value: Value of the field.
-    documentation_url: Url to the documentation of this field.
-  """
-
-  if value is None:
-    return
-
-  str_value, is_multi_lines = _field_value_to_str(value)
-
-  html_key = doc.createElement("div")
-  parent.appendChild(html_key)
-  html_key.setAttribute("class", "key")
-
-  if documentation_url is not None:
-    link = html.link(doc, documentation_url)
-    html_key.appendChild(link)
-    html_key = link
-
-  html_key.appendChild(doc.createTextNode(key + ":"))
-
-  html_value = doc.createElement("div")
-  html_value.setAttribute("class", "value")
-  parent.appendChild(html_value)
-
-  if is_multi_lines:
-    html_pre_value = doc.createElement("pre")
-    html_value.appendChild(html_pre_value)
-    html_pre_value.appendChild(doc.createTextNode(str_value))
-  else:
-    html_value.appendChild(doc.createTextNode(str_value))
-
-
-def _object_to_html(
-    doc: html.Doc,
-    parent,
-    key: Optional[str],
-    value: Any,
-    documentation_url: Optional[str] = None,
-    raw_html: bool = False,
-) -> None:
-  """Friendly html print a "key" and a complex element.
-
-  The complex element can be a multi-line string (or equivalent) or a Dom
-  object.
-
-  Args:
-    doc: Html document.
-    parent: Html element.
-    key: Name of the field.
-    value: Complex object to display.
-    documentation_url: Url to the documentation of this field.
-    raw_html: If true, "value" is interpreted as raw html.
-  """
-
-  if value is None:
-    return
-
-  str_value, _ = _field_value_to_str(value)
-
-  html_container = doc.createElement("div")
-  html_container.setAttribute("class", "complex")
-  parent.appendChild(html_container)
-
-  html_key = doc.createElement("div")
-  html_container.appendChild(html_key)
-  html_key.setAttribute("class", "key")
-
-  if documentation_url is not None:
-    link = html.link(doc, documentation_url)
-    html_key.appendChild(link)
-    html_key = link
-
-  if key:
-    html_key.appendChild(doc.createTextNode(key))
-
-  html_value = doc.createElement("div")
-  html_value.setAttribute("class", "value")
-  html_container.appendChild(html_value)
-
-  if isinstance(value, minidom.Element):
-    html_value.appendChild(value)
-  else:
-    if raw_html:
-      node = _RawXMLNode(value, doc)
-      html_value.appendChild(node)
-    else:
-      html_pre_value = doc.createElement("pre")
-      html_value.appendChild(html_pre_value)
-      html_pre_value.appendChild(doc.createTextNode(str_value))
-
-
-class _RawXMLNode(minidom.Node):
-  # Required by Minidom
-  nodeType = 1
-
-  def __init__(self, data, parent):
-    self.data = data
-    self.ownerDocument = parent
-
-  def writexml(self, writer, indent, addindent, newl):
-    del indent
-    del addindent
-    del newl
-    writer.write(self.data)
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Utilities to display metrics."""
+
+import base64
+import io
+import textwrap
+from typing import Any, Optional, Tuple
+from xml.dom import minidom
+
+from ydf.cc import ydf
+from ydf.metric import metric
+from ydf.utils import documentation
+from ydf.utils import html
+from ydf.utils import string_lib
+
+
+class _UnescappedString(minidom.Text):
+  """Html element able to print unescaped string."""
+
+  def set_data(self, data: str) -> None:
+    """Sets content to print."""
+
+    self._data = data
+
+  def writexml(
+      self,
+      writer,
+      indent="",
+      addindent="",
+      newl="",
+      encoding=None,
+      standalone=None,
+  ) -> None:
+    writer.write(self._data)
+
+
+def css_style(doc: html.Doc) -> html.Elem:
+  """Css style for metric display."""
+
+  raw_style = textwrap.dedent("""\
+  .metric_box {
+  }
+
+  .metric_box a {
+    text-decoration:none;
+    color: darkblue;
+  }
+
+  .metric_box .title {
+    font-weight: bold;
+  }
+
+  .metric_box .section {
+    margin: 5px 5px 5px 5px;
+  }
+
+  .metric_box .grid {
+    display: grid;
+    grid-template-columns: max-content max-content max-content max-content;
+    grid-template-rows: auto;
+    border-bottom: 1px solid lightgray;
+    padding: 10px;
+  }
+    
+  .metric_box .grid > div {
+  }
+
+  .metric_box .grid > div:nth-child(odd) {
+    font-weight: bold;
+    padding-right: 5px;
+    text-align: right;
+  }
+
+  .metric_box .grid > div:nth-child(even) {
+    padding-right: 20px;
+  }
+
+  .metric_box .complex {
+    display: inline-block;
+    margin: 15px 20px 0px 0px;
+    vertical-align: top;
+  }
+
+  .metric_box .complex .key {
+    font-weight: bold;
+  }
+
+  .metric_box .complex .value {
+  }
+
+  .metric_box .confusion_matrix {
+    border-collapse: collapse;
+    margin: 15px 15px;
+    border: 1px solid lightgray;
+  }
+
+  .metric_box .confusion_matrix th {
+    background-color: #ededed;
+    font-weight: bold;
+    text-align: left;
+    padding: 5px;
+    border: 1px solid lightgray;
+  }
+
+  .metric_box .confusion_matrix td {
+    text-align: right;
+    padding: 3px;
+    border: 1px solid lightgray;
+  }
+  """)
+
+  style = doc.createElement("style")
+  raw_node = _UnescappedString()
+  raw_node.set_data(raw_style)
+  style.appendChild(raw_node)
+  return style
+
+
+def evaluation_to_str(e: metric.Evaluation) -> str:
+  """String representation of an evaluation."""
+
+  text = ""
+
+  # Classification
+  text += _field_to_str("accuracy", e.accuracy)
+  text += _field_to_str("confusion matrix", e.confusion_matrix)
+  if e.characteristics is not None:
+    text += "characteristics:"
+    for characteristic in e.characteristics:
+      text += "\n" + string_lib.indent(str(characteristic))
+
+  # Regression
+  text += _field_to_str("RMSE", e.rmse)
+  text += _field_to_str("RMSE 95% CI [B]", e.rmse_ci95_bootstrap)
+
+  # Ranking
+  text += _field_to_str("NDCG", e.ndcg)
+
+  # Uplifting
+  text += _field_to_str("QINI", e.qini)
+  text += _field_to_str("AUUC", e.auuc)
+
+  # Custom
+  if e.custom_metrics:
+    for k, v in e.custom_metrics.items():
+      text += _field_to_str(k, v)
+
+  # Generic
+  text += _field_to_str("loss", e.loss)
+  text += _field_to_str("num examples", e.num_examples)
+  text += _field_to_str("num examples (weighted)", e.num_examples_weighted)
+
+  if not text:
+    text = "No metrics"
+
+  return text
+
+
+def evaluation_to_html_str(e: metric.Evaluation, add_style: bool = True) -> str:
+  """Html representation of an evaluation."""
+
+  doc, root = html.create_doc()
+
+  if add_style:
+    root.appendChild(css_style(doc))
+
+  html_metric_box = doc.createElement("div")
+  html_metric_box.setAttribute("class", "metric_box")
+  root.appendChild(html_metric_box)
+
+  html_metric_grid = doc.createElement("div")
+  html_metric_grid.setAttribute("class", "grid section")
+  html_metric_box.appendChild(html_metric_grid)
+
+  # Metrics
+
+  # Classification
+  _field_to_html(doc, html_metric_grid, "accuracy", e.accuracy)
+
+  if e.characteristics is not None:
+    for characteristic in e.characteristics:
+      _field_to_html(
+          doc,
+          html_metric_grid,
+          "AUC: " + characteristic.name,
+          characteristic.roc_auc,
+      )
+
+      _field_to_html(
+          doc,
+          html_metric_grid,
+          "PR-AUC: " + characteristic.name,
+          characteristic.pr_auc,
+      )
+
+  # Regression
+  _field_to_html(doc, html_metric_grid, "RMSE", e.rmse)
+  _field_to_html(
+      doc, html_metric_grid, "RMSE 95% CI [B]", e.rmse_ci95_bootstrap
+  )
+
+  # Ranking
+  _field_to_html(doc, html_metric_grid, "NDCG", e.ndcg)
+
+  # Uplifting
+  _field_to_html(doc, html_metric_grid, "QINI", e.qini)
+  _field_to_html(doc, html_metric_grid, "AUUC", e.auuc)
+
+  # Custom
+  if e.custom_metrics:
+    for k, v in e.custom_metrics.items():
+      _field_to_html(doc, html_metric_grid, k, v)
+
+  # Generic
+  _field_to_html(doc, html_metric_grid, "loss", e.loss)
+  _field_to_html(
+      doc,
+      html_metric_grid,
+      "num examples",
+      e.num_examples,
+      documentation_url=documentation.URL_NUM_EXAMPLES,
+  )
+  _field_to_html(
+      doc,
+      html_metric_grid,
+      "num examples (weighted)",
+      e.num_examples_weighted,
+      documentation_url=documentation.URL_WEIGHTED_NUM_EXAMPLES,
+  )
+
+  if e.confusion_matrix is not None:
+    _object_to_html(
+        doc,
+        html_metric_box,
+        "Confusion matrix",
+        confusion_matrix_to_html_str(doc, e.confusion_matrix),
+        documentation_url=documentation.URL_CONFUSION_MATRIX,
+    )
+
+  # Curves
+  plot_html = ydf.EvaluationPlotToHtml(e._evaluation_proto)
+  _object_to_html(doc, html_metric_box, None, plot_html, raw_html=True)
+
+  return root.toprettyxml(indent="  ")
+
+
+def confusion_matrix_to_html_str(
+    doc: html.Doc, confusion: metric.ConfusionMatrix
+) -> html.Elem:
+  """Html representation of a confusion matrix."""
+
+  html_table = doc.createElement("table")
+  html_table.setAttribute("class", "confusion_matrix")
+
+  # First line
+  tr = doc.createElement("tr")
+  html_table.appendChild(tr)
+
+  th = doc.createElement("th")
+  tr.appendChild(th)
+  th.appendChild(doc.createTextNode("Label \\ Pred"))
+
+  for label in confusion.classes:
+    th = doc.createElement("th")
+    tr.appendChild(th)
+    th.appendChild(doc.createTextNode(label))
+
+  for prediction_idx, prediction in enumerate(confusion.classes):
+    tr = doc.createElement("tr")
+    html_table.appendChild(tr)
+
+    th = doc.createElement("th")
+    tr.appendChild(th)
+    th.appendChild(doc.createTextNode(prediction))
+
+    for label_idx in range(len(confusion.classes)):
+      value = confusion.value(
+          label_idx=label_idx, prediction_idx=prediction_idx
+      )
+      td = doc.createElement("td")
+      tr.appendChild(td)
+      td.appendChild(doc.createTextNode(f"{value:g}"))
+
+  return html_table
+
+
+def _field_value_to_str(value: Any) -> Tuple[str, bool]:
+  """Friendly text print a "value".
+
+  Operations:
+    - Remove decimal points in float e.g. 4.0 => "4".
+    - Round digits to precision e.g 2.99999999 => "3
+    - Remove any trailing line return.
+
+  Args:
+    value: The value to print.
+
+  Returns:
+    The string value, and a boolean indicating if the string value is
+    multi-lines.
+  """
+
+  if value is None:
+    return "", False
+
+  if isinstance(value, float):
+    if round(value) == value:
+      # Remove decimale point
+      str_value = str(int(value))
+    else:
+      # Compact print
+      str_value = f"{value:g}"
+  else:
+    str_value = str(value)
+
+  if "\n" not in str_value:
+    return str_value, False
+
+  # Indent the value from the key
+  str_value = string_lib.indent(str_value)
+  if str_value and str_value[-1] == "\n":
+    # Remove final line return if any.
+    str_value = str_value[:-1]
+  return str_value, True
+
+
+def _field_to_str(key: str, value: Any) -> str:
+  """Friendly text print a "key:value".
+
+  Operations:
+    - Operations from "_field_value_to_str"
+
+  Args:
+    key: Name of the field.
+    value: Value of the field.
+
+  Returns:
+    Formated key:value.
+  """
+
+  if value is None:
+    return ""
+
+  str_value, is_multi_lines = _field_value_to_str(value)
+
+  if is_multi_lines:
+    return f"{key}:\n{str_value}\n"
+  else:
+    return f"{key}: {str_value}\n"
+
+
+def _field_to_html(
+    doc: html.Doc,
+    parent,
+    key: str,
+    value: Any,
+    documentation_url: Optional[str] = None,
+) -> None:
+  """Friendly html print a "key:value".
+
+  Operations:
+    - All the operations of "_field_value_to_str"
+    - Wrap multi-line values into <pre>.
+
+  Args:
+    doc: Html document.
+    parent: Html element.
+    key: Name of the field.
+    value: Value of the field.
+    documentation_url: Url to the documentation of this field.
+  """
+
+  if value is None:
+    return
+
+  str_value, is_multi_lines = _field_value_to_str(value)
+
+  html_key = doc.createElement("div")
+  parent.appendChild(html_key)
+  html_key.setAttribute("class", "key")
+
+  if documentation_url is not None:
+    link = html.link(doc, documentation_url)
+    html_key.appendChild(link)
+    html_key = link
+
+  html_key.appendChild(doc.createTextNode(key + ":"))
+
+  html_value = doc.createElement("div")
+  html_value.setAttribute("class", "value")
+  parent.appendChild(html_value)
+
+  if is_multi_lines:
+    html_pre_value = doc.createElement("pre")
+    html_value.appendChild(html_pre_value)
+    html_pre_value.appendChild(doc.createTextNode(str_value))
+  else:
+    html_value.appendChild(doc.createTextNode(str_value))
+
+
+def _object_to_html(
+    doc: html.Doc,
+    parent,
+    key: Optional[str],
+    value: Any,
+    documentation_url: Optional[str] = None,
+    raw_html: bool = False,
+) -> None:
+  """Friendly html print a "key" and a complex element.
+
+  The complex element can be a multi-line string (or equivalent) or a Dom
+  object.
+
+  Args:
+    doc: Html document.
+    parent: Html element.
+    key: Name of the field.
+    value: Complex object to display.
+    documentation_url: Url to the documentation of this field.
+    raw_html: If true, "value" is interpreted as raw html.
+  """
+
+  if value is None:
+    return
+
+  str_value, _ = _field_value_to_str(value)
+
+  html_container = doc.createElement("div")
+  html_container.setAttribute("class", "complex")
+  parent.appendChild(html_container)
+
+  html_key = doc.createElement("div")
+  html_container.appendChild(html_key)
+  html_key.setAttribute("class", "key")
+
+  if documentation_url is not None:
+    link = html.link(doc, documentation_url)
+    html_key.appendChild(link)
+    html_key = link
+
+  if key:
+    html_key.appendChild(doc.createTextNode(key))
+
+  html_value = doc.createElement("div")
+  html_value.setAttribute("class", "value")
+  html_container.appendChild(html_value)
+
+  if isinstance(value, minidom.Element):
+    html_value.appendChild(value)
+  else:
+    if raw_html:
+      node = _RawXMLNode(value, doc)
+      html_value.appendChild(node)
+    else:
+      html_pre_value = doc.createElement("pre")
+      html_value.appendChild(html_pre_value)
+      html_pre_value.appendChild(doc.createTextNode(str_value))
+
+
+class _RawXMLNode(minidom.Node):
+  # Required by Minidom
+  nodeType = 1
+
+  def __init__(self, data, parent):
+    self.data = data
+    self.ownerDocument = parent
+
+  def writexml(self, writer, indent, addindent, newl):
+    del indent
+    del addindent
+    del newl
+    writer.write(self.data)
```

## ydf/metric/metric.py

```diff
@@ -1,528 +1,528 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Metrics."""
-
-import dataclasses
-import math
-from typing import Any, Dict, List, Optional, Tuple
-
-import numpy as np
-import numpy.typing as npt
-
-from yggdrasil_decision_forests.metric import metric_pb2
-from ydf.dataset import dataspec
-from ydf.utils import string_lib
-
-# Offset added to skip the OOD item (always with index 0).
-_OUT_OF_DICTIONARY_OFFSET = 1
-
-# Confidence interval of a numerical metric.
-ConfidenceInterval = Tuple[float, float]
-
-
-@dataclasses.dataclass
-class ConfusionMatrix:
-  """A confusion matrix.
-
-  See https://developers.google.com/machine-learning/glossary#confusion-matrix
-
-
-  Attributes:
-    classes: The label classes. The number of elements should match the size of
-      `matrix`.
-    matrix: A square matrix of size `len(classes)` x `len(classes)`. For
-      unweighted evaluation, `matrix[i,j]` is the number of test examples with
-      label `classes[i]` and predicted label `classes[j]`. For weighted
-      evaluation, the confusion matrix contains sum of examples weights instead
-      number of examples.
-  """
-
-  classes: Tuple[str, ...]
-  matrix: npt.NDArray[np.float64]
-
-  def __str__(self):
-    return "label (row) \\ prediction (col)\n" + string_lib.table(
-        content=self.matrix.tolist(),
-        row_labels=self.classes,
-        column_labels=self.classes,
-    )
-
-  def value(self, prediction_idx: int, label_idx: int) -> float:
-    return self.matrix[label_idx, prediction_idx]
-
-
-@dataclasses.dataclass
-class CharacteristicPerThreshold:
-  """Model recall, precision and other metrics per a threshold value.
-
-  Only used for classification models.
-
-  Attributes:
-    threshold: Threshold.
-    true_positive: True positive.
-    false_positive: False positive.
-    true_negative: True negative.
-    false_negative: False negative.
-  """
-
-  threshold: float
-  true_positive: float
-  false_positive: float
-  true_negative: float
-  false_negative: float
-
-  @property
-  def recall(self) -> float:
-    """Recall."""
-
-    return _safe_div(
-        self.true_positive, self.true_positive + self.false_negative
-    )
-
-  @property
-  def specificity(self) -> float:
-    """Specificity."""
-
-    return _safe_div(
-        self.true_negative, self.true_negative + self.false_positive
-    )
-
-  @property
-  def false_positive_rate(self) -> float:
-    """False positie rate."""
-
-    return _safe_div(
-        self.false_positive, self.false_positive + self.true_negative
-    )
-
-  @property
-  def precision(self) -> float:
-    """Precision."""
-
-    return _safe_div(
-        self.true_positive, self.true_positive + self.false_positive
-    )
-
-  @property
-  def accuracy(self) -> float:
-    """Accuracy."""
-
-    return _safe_div(
-        self.true_positive + self.true_negative,
-        self.true_positive
-        + self.false_positive
-        + self.true_negative
-        + self.false_negative,
-    )
-
-
-def _safe_div(a: float, b: float) -> float:
-  """Returns a/b. If a==b==0, returns 0."""
-
-  if b == 0:
-    assert a == 0
-    if a != 0:
-      raise ValueError(
-          f"Cannot divide a={a} by b={b}. If b==0, then a should be zero."
-      )
-    return 0
-  return a / b
-
-
-@dataclasses.dataclass
-class Characteristic:
-  """Model recall, precision and other metrics per threshold values.
-
-  Only used for classification models.
-
-  Attributes:
-    name: Identifier of the characteristic.
-    roc_auc: Area under the curve (AUC) of the Receiver operating characteristic
-      (ROC) curve.
-    pr_auc: Area under the curve (AUC) of the Precision-Recall curve.
-    per_threshold: Model characteristics per thresholds.
-    true_positives: List of true positives.
-    false_positives: List of false positives.
-    true_negatives: List of true negatives.
-    false_negatives: List of false negatives.
-    thresholds: List of threhsolds.
-  """
-
-  name: str
-  roc_auc: float
-  pr_auc: float
-  per_threshold: List[CharacteristicPerThreshold]
-
-  def __str__(self):
-    return f"""name: {self.name}
-ROC AUC: {self.roc_auc:g}
-PR AUC: {self.pr_auc:g}
-Num thresholds: {len(self.per_threshold)}
-"""
-
-  def __repr__(self):
-    return self.__str__()
-
-  @property
-  def auc(self) -> float:
-    """Alias for `roc_auc` i.e. the AUC of the ROC curve."""
-
-    return self.roc_auc
-
-  @property
-  def true_positives(self) -> npt.NDArray[np.float32]:
-    """List of true positives."""
-
-    return np.array([t.true_positive for t in self.per_threshold], np.float32)
-
-  @property
-  def false_positives(self) -> npt.NDArray[np.float32]:
-    """List of false positives."""
-
-    return np.array([t.false_positive for t in self.per_threshold], np.float32)
-
-  @property
-  def true_negatives(self) -> npt.NDArray[np.float32]:
-    """List of true negatives."""
-    return np.array([t.true_negative for t in self.per_threshold], np.float32)
-
-  @property
-  def false_negatives(self) -> npt.NDArray[np.float32]:
-    """List of true negatives."""
-
-    return np.array([t.false_negative for t in self.per_threshold], np.float32)
-
-  @property
-  def thresholds(self) -> npt.NDArray[np.float32]:
-    """List of thresholds."""
-
-    return np.array([t.threshold for t in self.per_threshold], np.float32)
-
-  @property
-  def recalls(self) -> npt.NDArray[np.float32]:
-    """List of recall."""
-
-    return np.array(
-        [t.recall for t in self.per_threshold],
-        np.float32,
-    )
-
-  @property
-  def specificities(self) -> npt.NDArray[np.float32]:
-    """List of specificity."""
-
-    return np.array(
-        [t.specificity for t in self.per_threshold],
-        np.float32,
-    )
-
-  @property
-  def precisions(self) -> npt.NDArray[np.float32]:
-    """List of precisions."""
-
-    return np.array(
-        [t.precision for t in self.per_threshold],
-        np.float32,
-    )
-
-  @property
-  def false_positive_rates(self) -> npt.NDArray[np.float32]:
-    """List of false positive rates."""
-
-    return np.array(
-        [t.false_positive_rate for t in self.per_threshold],
-        np.float32,
-    )
-
-  @property
-  def accuracies(self) -> npt.NDArray[np.float32]:
-    """List of accuracies."""
-
-    return np.array(
-        [t.accuracy for t in self.per_threshold],
-        np.float32,
-    )
-
-
-@dataclasses.dataclass
-class Evaluation:
-  """A collection of metrics, plots and tables about the quality of a model.
-
-  Basic usage example:
-
-  ```python
-  evaluation = model.evaluate(test_ds)
-  print(evaluation)
-  print(evaluation.accuracy)
-  evaluation  # Html evaluation in notebook
-  ```
-
-  Attributes:
-    loss: Model loss. The loss definition is model dependent.
-    num_examples: Number of examples (non weighted).
-    num_examples_weighted: Number of examples (with weight).
-    accuracy:
-    confusion_matrix:
-    characteristics:
-    rmse: Root Mean Square Error. Only available for regression task.
-    rmse_ci95_bootstrap: 95% confidence interval of the RMSE computed using
-      bootstrapping. Only available for regression task.
-    ndcg: Normalized Discounted Cumulative Gain. For Ranking.
-    qini: For uplifting.
-    auuc: For uplifting.
-    custom_metrics: User custom metrics dictionary.
-  """
-
-  def __init__(
-      self,
-      evaluation_proto: metric_pb2.EvaluationResults,
-  ):
-    self._evaluation_proto = evaluation_proto
-
-  def __str__(self) -> str:
-    """Returns the string representation of an evaluation."""
-
-    from ydf.metric import display_metric  # pylint: disable=g-import-not-at-top
-
-    return display_metric.evaluation_to_str(self)
-
-  def _repr_html_(self) -> str:
-    """Html representation of the metrics."""
-
-    from ydf.metric import display_metric  # pylint: disable=g-import-not-at-top
-
-    return display_metric.evaluation_to_html_str(self)
-
-  def html(self) -> str:
-    """Html representation of the metrics."""
-
-    return self._repr_html_()
-
-  def _get_proto_field_float(self, key: str) -> Optional[float]:
-    if self._evaluation_proto.HasField(key):
-      return getattr(self._evaluation_proto, key)
-    return None
-
-  def _get_proto_field_int(self, key: str) -> Optional[int]:
-    if self._evaluation_proto.HasField(key):
-      return getattr(self._evaluation_proto, key)
-    return None
-
-  @property
-  def loss(self) -> Optional[float]:
-    if self._evaluation_proto.HasField("loss_value"):
-      return self._evaluation_proto.loss_value
-
-    if self._evaluation_proto.HasField("classification"):
-      clas = self._evaluation_proto.classification
-      if clas.HasField("sum_log_loss"):
-        return clas.sum_log_loss / self._evaluation_proto.count_predictions
-
-    return None
-
-  @property
-  def num_examples(self) -> Optional[float]:
-    return self._get_proto_field_int("count_predictions_no_weight")
-
-  @property
-  def num_examples_weighted(self) -> Optional[float]:
-    return self._get_proto_field_float("count_predictions")
-
-  @property
-  def custom_metrics(self) -> Dict[str, Any]:
-    return {k: v for k, v in self._evaluation_proto.user_metrics.items()}
-
-  @property
-  def accuracy(self) -> Optional[float]:
-    if self._evaluation_proto.HasField("classification"):
-      clas = self._evaluation_proto.classification
-      classes = dataspec.categorical_column_dictionary_to_list(
-          self._evaluation_proto.label_column
-      )
-
-      if clas.HasField("confusion"):
-        confusion = clas.confusion
-        assert confusion.nrow == confusion.ncol, "Invalid confusion matrix"
-        assert confusion.nrow == len(classes), "Invalid confusion matrix"
-        assert confusion.nrow >= 1, "Invalid confusion matrix"
-        # YDF confusing matrices are stored column major.
-        raw_confusion = (
-            np.array(confusion.counts)
-            .reshape(confusion.nrow, confusion.nrow)
-            .transpose()
-        )
-
-        return safe_div(np.trace(raw_confusion), np.sum(raw_confusion))
-    return None
-
-  @property
-  def confusion_matrix(self) -> Optional[ConfusionMatrix]:
-    if self._evaluation_proto.HasField("classification"):
-      clas = self._evaluation_proto.classification
-      classes = dataspec.categorical_column_dictionary_to_list(
-          self._evaluation_proto.label_column
-      )
-      classes_wo_oov = classes[_OUT_OF_DICTIONARY_OFFSET:]
-
-      if clas.HasField("confusion"):
-        confusion = clas.confusion
-        assert confusion.nrow == confusion.ncol, "Invalid confusion matrix"
-        assert confusion.nrow == len(classes), "Invalid confusion matrix"
-        assert confusion.nrow >= 1, "Invalid confusion matrix"
-        raw_confusion = (
-            np.array(confusion.counts)
-            .reshape(confusion.nrow, confusion.nrow)
-            .transpose()
-        )
-
-        return ConfusionMatrix(
-            classes=tuple(classes_wo_oov),
-            matrix=raw_confusion[
-                _OUT_OF_DICTIONARY_OFFSET:, _OUT_OF_DICTIONARY_OFFSET:
-            ],
-        )
-    return None
-
-  @property
-  def characteristics(self) -> Optional[List[Characteristic]]:
-    if self._evaluation_proto.HasField("classification"):
-      clas = self._evaluation_proto.classification
-      classes = dataspec.categorical_column_dictionary_to_list(
-          self._evaluation_proto.label_column
-      )
-      if clas.rocs:
-        characteristics = []
-        for roc_idx, roc in enumerate(clas.rocs):
-          if roc_idx == 0:
-            # Skip the OOV item
-            continue
-          if roc_idx == 1 and len(clas.rocs) == 3:
-            # In case of binary classification, skip the negative class
-            continue
-          name = f"'{classes[roc_idx]}' vs others"
-          characteristics.append(
-              Characteristic(
-                  name=name,
-                  roc_auc=roc.auc,
-                  pr_auc=roc.pr_auc,
-                  per_threshold=[
-                      CharacteristicPerThreshold(
-                          true_positive=x.tp,
-                          false_positive=x.fp,
-                          true_negative=x.tn,
-                          false_negative=x.fn,
-                          threshold=x.threshold,
-                      )
-                      for x in roc.curve
-                  ],
-              )
-          )
-        return characteristics
-    return None
-
-  @property
-  def rmse(self) -> Optional[float]:
-    if self._evaluation_proto.HasField("regression"):
-      reg = self._evaluation_proto.regression
-      if reg.HasField("sum_square_error"):
-        return math.sqrt(
-            safe_div(
-                reg.sum_square_error, self._evaluation_proto.count_predictions
-            )
-        )
-    return None
-
-  @property
-  def rmse_ci95_bootstrap(self) -> Optional[ConfidenceInterval]:
-    if self._evaluation_proto.HasField("regression"):
-      reg = self._evaluation_proto.regression
-      if reg.HasField("bootstrap_rmse_lower_bounds_95p") and reg.HasField(
-          "bootstrap_rmse_upper_bounds_95p"
-      ):
-        return (
-            reg.bootstrap_rmse_lower_bounds_95p,
-            reg.bootstrap_rmse_upper_bounds_95p,
-        )
-    return None
-
-  @property
-  def ndcg(self) -> Optional[float]:
-    if self._evaluation_proto.HasField("ranking"):
-      rank = self._evaluation_proto.ranking
-      if rank.HasField("ndcg"):
-        return rank.ndcg.value
-
-  @property
-  def qini(self) -> Optional[float]:
-    if self._evaluation_proto.HasField("uplift"):
-      uplift = self._evaluation_proto.uplift
-      if uplift.HasField("qini"):
-        return uplift.qini
-
-  @property
-  def auuc(self) -> Optional[float]:
-    if self._evaluation_proto.HasField("uplift"):
-      uplift = self._evaluation_proto.uplift
-      if uplift.HasField("auuc"):
-        return uplift.auuc
-
-  def to_dict(self) -> Dict[str, Any]:
-    """Metrics in a dictionary."""
-
-    output = {**self.custom_metrics}
-
-    def add_item(key, value):
-      if value is not None:
-        output[key] = value
-
-    add_item("loss", self.loss)
-    add_item("num_examples", self.num_examples)
-    add_item("num_examples_weighted", self.num_examples_weighted)
-    add_item("accuracy", self.accuracy)
-    add_item("confusion_matrix", self.confusion_matrix)
-
-    if self.characteristics is not None:
-      for idx, characteristic in enumerate(self.characteristics):
-        base_name = f"characteristic_{idx}"
-        add_item(f"{base_name}:name", characteristic.name)
-        add_item(f"{base_name}:roc_auc", characteristic.roc_auc)
-        add_item(f"{base_name}:pr_auc", characteristic.pr_auc)
-
-    add_item("rmse", self.rmse)
-    add_item("rmse_ci95_bootstrap", self.rmse_ci95_bootstrap)
-    add_item("ndcg", self.ndcg)
-    add_item("qini", self.qini)
-    add_item("auuc", self.auuc)
-    return output
-
-
-def safe_div(a: float, b: float) -> float:
-  """Returns a/b. If a==b==0, returns 0.
-
-  If b==0 and a!=0, raises an exception.
-
-  Args:
-    a: Numerator.
-    b: Denominator.
-  """
-
-  if b == 0.0:
-    if a != 0.0:
-      raise ValueError(
-          f"Cannot divide `a={a}` by `b={b}`. If `b==0`, then `a` should be"
-          " zero."
-      )
-    return 0.0
-  return a / b
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Metrics."""
+
+import dataclasses
+import math
+from typing import Any, Dict, List, Optional, Tuple
+
+import numpy as np
+import numpy.typing as npt
+
+from ydf.proto.metric import metric_pb2
+from ydf.dataset import dataspec
+from ydf.utils import string_lib
+
+# Offset added to skip the OOD item (always with index 0).
+_OUT_OF_DICTIONARY_OFFSET = 1
+
+# Confidence interval of a numerical metric.
+ConfidenceInterval = Tuple[float, float]
+
+
+@dataclasses.dataclass
+class ConfusionMatrix:
+  """A confusion matrix.
+
+  See https://developers.google.com/machine-learning/glossary#confusion-matrix
+
+
+  Attributes:
+    classes: The label classes. The number of elements should match the size of
+      `matrix`.
+    matrix: A square matrix of size `len(classes)` x `len(classes)`. For
+      unweighted evaluation, `matrix[i,j]` is the number of test examples with
+      label `classes[i]` and predicted label `classes[j]`. For weighted
+      evaluation, the confusion matrix contains sum of examples weights instead
+      number of examples.
+  """
+
+  classes: Tuple[str, ...]
+  matrix: npt.NDArray[np.float64]
+
+  def __str__(self):
+    return "label (row) \\ prediction (col)\n" + string_lib.table(
+        content=self.matrix.tolist(),
+        row_labels=self.classes,
+        column_labels=self.classes,
+    )
+
+  def value(self, prediction_idx: int, label_idx: int) -> float:
+    return self.matrix[label_idx, prediction_idx]
+
+
+@dataclasses.dataclass
+class CharacteristicPerThreshold:
+  """Model recall, precision and other metrics per a threshold value.
+
+  Only used for classification models.
+
+  Attributes:
+    threshold: Threshold.
+    true_positive: True positive.
+    false_positive: False positive.
+    true_negative: True negative.
+    false_negative: False negative.
+  """
+
+  threshold: float
+  true_positive: float
+  false_positive: float
+  true_negative: float
+  false_negative: float
+
+  @property
+  def recall(self) -> float:
+    """Recall."""
+
+    return _safe_div(
+        self.true_positive, self.true_positive + self.false_negative
+    )
+
+  @property
+  def specificity(self) -> float:
+    """Specificity."""
+
+    return _safe_div(
+        self.true_negative, self.true_negative + self.false_positive
+    )
+
+  @property
+  def false_positive_rate(self) -> float:
+    """False positie rate."""
+
+    return _safe_div(
+        self.false_positive, self.false_positive + self.true_negative
+    )
+
+  @property
+  def precision(self) -> float:
+    """Precision."""
+
+    return _safe_div(
+        self.true_positive, self.true_positive + self.false_positive
+    )
+
+  @property
+  def accuracy(self) -> float:
+    """Accuracy."""
+
+    return _safe_div(
+        self.true_positive + self.true_negative,
+        self.true_positive
+        + self.false_positive
+        + self.true_negative
+        + self.false_negative,
+    )
+
+
+def _safe_div(a: float, b: float) -> float:
+  """Returns a/b. If a==b==0, returns 0."""
+
+  if b == 0:
+    assert a == 0
+    if a != 0:
+      raise ValueError(
+          f"Cannot divide a={a} by b={b}. If b==0, then a should be zero."
+      )
+    return 0
+  return a / b
+
+
+@dataclasses.dataclass
+class Characteristic:
+  """Model recall, precision and other metrics per threshold values.
+
+  Only used for classification models.
+
+  Attributes:
+    name: Identifier of the characteristic.
+    roc_auc: Area under the curve (AUC) of the Receiver operating characteristic
+      (ROC) curve.
+    pr_auc: Area under the curve (AUC) of the Precision-Recall curve.
+    per_threshold: Model characteristics per thresholds.
+    true_positives: List of true positives.
+    false_positives: List of false positives.
+    true_negatives: List of true negatives.
+    false_negatives: List of false negatives.
+    thresholds: List of threhsolds.
+  """
+
+  name: str
+  roc_auc: float
+  pr_auc: float
+  per_threshold: List[CharacteristicPerThreshold]
+
+  def __str__(self):
+    return f"""name: {self.name}
+ROC AUC: {self.roc_auc:g}
+PR AUC: {self.pr_auc:g}
+Num thresholds: {len(self.per_threshold)}
+"""
+
+  def __repr__(self):
+    return self.__str__()
+
+  @property
+  def auc(self) -> float:
+    """Alias for `roc_auc` i.e. the AUC of the ROC curve."""
+
+    return self.roc_auc
+
+  @property
+  def true_positives(self) -> npt.NDArray[np.float32]:
+    """List of true positives."""
+
+    return np.array([t.true_positive for t in self.per_threshold], np.float32)
+
+  @property
+  def false_positives(self) -> npt.NDArray[np.float32]:
+    """List of false positives."""
+
+    return np.array([t.false_positive for t in self.per_threshold], np.float32)
+
+  @property
+  def true_negatives(self) -> npt.NDArray[np.float32]:
+    """List of true negatives."""
+    return np.array([t.true_negative for t in self.per_threshold], np.float32)
+
+  @property
+  def false_negatives(self) -> npt.NDArray[np.float32]:
+    """List of true negatives."""
+
+    return np.array([t.false_negative for t in self.per_threshold], np.float32)
+
+  @property
+  def thresholds(self) -> npt.NDArray[np.float32]:
+    """List of thresholds."""
+
+    return np.array([t.threshold for t in self.per_threshold], np.float32)
+
+  @property
+  def recalls(self) -> npt.NDArray[np.float32]:
+    """List of recall."""
+
+    return np.array(
+        [t.recall for t in self.per_threshold],
+        np.float32,
+    )
+
+  @property
+  def specificities(self) -> npt.NDArray[np.float32]:
+    """List of specificity."""
+
+    return np.array(
+        [t.specificity for t in self.per_threshold],
+        np.float32,
+    )
+
+  @property
+  def precisions(self) -> npt.NDArray[np.float32]:
+    """List of precisions."""
+
+    return np.array(
+        [t.precision for t in self.per_threshold],
+        np.float32,
+    )
+
+  @property
+  def false_positive_rates(self) -> npt.NDArray[np.float32]:
+    """List of false positive rates."""
+
+    return np.array(
+        [t.false_positive_rate for t in self.per_threshold],
+        np.float32,
+    )
+
+  @property
+  def accuracies(self) -> npt.NDArray[np.float32]:
+    """List of accuracies."""
+
+    return np.array(
+        [t.accuracy for t in self.per_threshold],
+        np.float32,
+    )
+
+
+@dataclasses.dataclass
+class Evaluation:
+  """A collection of metrics, plots and tables about the quality of a model.
+
+  Basic usage example:
+
+  ```python
+  evaluation = model.evaluate(test_ds)
+  print(evaluation)
+  print(evaluation.accuracy)
+  evaluation  # Html evaluation in notebook
+  ```
+
+  Attributes:
+    loss: Model loss. The loss definition is model dependent.
+    num_examples: Number of examples (non weighted).
+    num_examples_weighted: Number of examples (with weight).
+    accuracy:
+    confusion_matrix:
+    characteristics:
+    rmse: Root Mean Square Error. Only available for regression task.
+    rmse_ci95_bootstrap: 95% confidence interval of the RMSE computed using
+      bootstrapping. Only available for regression task.
+    ndcg: Normalized Discounted Cumulative Gain. For Ranking.
+    qini: For uplifting.
+    auuc: For uplifting.
+    custom_metrics: User custom metrics dictionary.
+  """
+
+  def __init__(
+      self,
+      evaluation_proto: metric_pb2.EvaluationResults,
+  ):
+    self._evaluation_proto = evaluation_proto
+
+  def __str__(self) -> str:
+    """Returns the string representation of an evaluation."""
+
+    from ydf.metric import display_metric  # pylint: disable=g-import-not-at-top
+
+    return display_metric.evaluation_to_str(self)
+
+  def _repr_html_(self) -> str:
+    """Html representation of the metrics."""
+
+    from ydf.metric import display_metric  # pylint: disable=g-import-not-at-top
+
+    return display_metric.evaluation_to_html_str(self)
+
+  def html(self) -> str:
+    """Html representation of the metrics."""
+
+    return self._repr_html_()
+
+  def _get_proto_field_float(self, key: str) -> Optional[float]:
+    if self._evaluation_proto.HasField(key):
+      return getattr(self._evaluation_proto, key)
+    return None
+
+  def _get_proto_field_int(self, key: str) -> Optional[int]:
+    if self._evaluation_proto.HasField(key):
+      return getattr(self._evaluation_proto, key)
+    return None
+
+  @property
+  def loss(self) -> Optional[float]:
+    if self._evaluation_proto.HasField("loss_value"):
+      return self._evaluation_proto.loss_value
+
+    if self._evaluation_proto.HasField("classification"):
+      clas = self._evaluation_proto.classification
+      if clas.HasField("sum_log_loss"):
+        return clas.sum_log_loss / self._evaluation_proto.count_predictions
+
+    return None
+
+  @property
+  def num_examples(self) -> Optional[float]:
+    return self._get_proto_field_int("count_predictions_no_weight")
+
+  @property
+  def num_examples_weighted(self) -> Optional[float]:
+    return self._get_proto_field_float("count_predictions")
+
+  @property
+  def custom_metrics(self) -> Dict[str, Any]:
+    return {k: v for k, v in self._evaluation_proto.user_metrics.items()}
+
+  @property
+  def accuracy(self) -> Optional[float]:
+    if self._evaluation_proto.HasField("classification"):
+      clas = self._evaluation_proto.classification
+      classes = dataspec.categorical_column_dictionary_to_list(
+          self._evaluation_proto.label_column
+      )
+
+      if clas.HasField("confusion"):
+        confusion = clas.confusion
+        assert confusion.nrow == confusion.ncol, "Invalid confusion matrix"
+        assert confusion.nrow == len(classes), "Invalid confusion matrix"
+        assert confusion.nrow >= 1, "Invalid confusion matrix"
+        # YDF confusing matrices are stored column major.
+        raw_confusion = (
+            np.array(confusion.counts)
+            .reshape(confusion.nrow, confusion.nrow)
+            .transpose()
+        )
+
+        return safe_div(np.trace(raw_confusion), np.sum(raw_confusion))
+    return None
+
+  @property
+  def confusion_matrix(self) -> Optional[ConfusionMatrix]:
+    if self._evaluation_proto.HasField("classification"):
+      clas = self._evaluation_proto.classification
+      classes = dataspec.categorical_column_dictionary_to_list(
+          self._evaluation_proto.label_column
+      )
+      classes_wo_oov = classes[_OUT_OF_DICTIONARY_OFFSET:]
+
+      if clas.HasField("confusion"):
+        confusion = clas.confusion
+        assert confusion.nrow == confusion.ncol, "Invalid confusion matrix"
+        assert confusion.nrow == len(classes), "Invalid confusion matrix"
+        assert confusion.nrow >= 1, "Invalid confusion matrix"
+        raw_confusion = (
+            np.array(confusion.counts)
+            .reshape(confusion.nrow, confusion.nrow)
+            .transpose()
+        )
+
+        return ConfusionMatrix(
+            classes=tuple(classes_wo_oov),
+            matrix=raw_confusion[
+                _OUT_OF_DICTIONARY_OFFSET:, _OUT_OF_DICTIONARY_OFFSET:
+            ],
+        )
+    return None
+
+  @property
+  def characteristics(self) -> Optional[List[Characteristic]]:
+    if self._evaluation_proto.HasField("classification"):
+      clas = self._evaluation_proto.classification
+      classes = dataspec.categorical_column_dictionary_to_list(
+          self._evaluation_proto.label_column
+      )
+      if clas.rocs:
+        characteristics = []
+        for roc_idx, roc in enumerate(clas.rocs):
+          if roc_idx == 0:
+            # Skip the OOV item
+            continue
+          if roc_idx == 1 and len(clas.rocs) == 3:
+            # In case of binary classification, skip the negative class
+            continue
+          name = f"'{classes[roc_idx]}' vs others"
+          characteristics.append(
+              Characteristic(
+                  name=name,
+                  roc_auc=roc.auc,
+                  pr_auc=roc.pr_auc,
+                  per_threshold=[
+                      CharacteristicPerThreshold(
+                          true_positive=x.tp,
+                          false_positive=x.fp,
+                          true_negative=x.tn,
+                          false_negative=x.fn,
+                          threshold=x.threshold,
+                      )
+                      for x in roc.curve
+                  ],
+              )
+          )
+        return characteristics
+    return None
+
+  @property
+  def rmse(self) -> Optional[float]:
+    if self._evaluation_proto.HasField("regression"):
+      reg = self._evaluation_proto.regression
+      if reg.HasField("sum_square_error"):
+        return math.sqrt(
+            safe_div(
+                reg.sum_square_error, self._evaluation_proto.count_predictions
+            )
+        )
+    return None
+
+  @property
+  def rmse_ci95_bootstrap(self) -> Optional[ConfidenceInterval]:
+    if self._evaluation_proto.HasField("regression"):
+      reg = self._evaluation_proto.regression
+      if reg.HasField("bootstrap_rmse_lower_bounds_95p") and reg.HasField(
+          "bootstrap_rmse_upper_bounds_95p"
+      ):
+        return (
+            reg.bootstrap_rmse_lower_bounds_95p,
+            reg.bootstrap_rmse_upper_bounds_95p,
+        )
+    return None
+
+  @property
+  def ndcg(self) -> Optional[float]:
+    if self._evaluation_proto.HasField("ranking"):
+      rank = self._evaluation_proto.ranking
+      if rank.HasField("ndcg"):
+        return rank.ndcg.value
+
+  @property
+  def qini(self) -> Optional[float]:
+    if self._evaluation_proto.HasField("uplift"):
+      uplift = self._evaluation_proto.uplift
+      if uplift.HasField("qini"):
+        return uplift.qini
+
+  @property
+  def auuc(self) -> Optional[float]:
+    if self._evaluation_proto.HasField("uplift"):
+      uplift = self._evaluation_proto.uplift
+      if uplift.HasField("auuc"):
+        return uplift.auuc
+
+  def to_dict(self) -> Dict[str, Any]:
+    """Metrics in a dictionary."""
+
+    output = {**self.custom_metrics}
+
+    def add_item(key, value):
+      if value is not None:
+        output[key] = value
+
+    add_item("loss", self.loss)
+    add_item("num_examples", self.num_examples)
+    add_item("num_examples_weighted", self.num_examples_weighted)
+    add_item("accuracy", self.accuracy)
+    add_item("confusion_matrix", self.confusion_matrix)
+
+    if self.characteristics is not None:
+      for idx, characteristic in enumerate(self.characteristics):
+        base_name = f"characteristic_{idx}"
+        add_item(f"{base_name}:name", characteristic.name)
+        add_item(f"{base_name}:roc_auc", characteristic.roc_auc)
+        add_item(f"{base_name}:pr_auc", characteristic.pr_auc)
+
+    add_item("rmse", self.rmse)
+    add_item("rmse_ci95_bootstrap", self.rmse_ci95_bootstrap)
+    add_item("ndcg", self.ndcg)
+    add_item("qini", self.qini)
+    add_item("auuc", self.auuc)
+    return output
+
+
+def safe_div(a: float, b: float) -> float:
+  """Returns a/b. If a==b==0, returns 0.
+
+  If b==0 and a!=0, raises an exception.
+
+  Args:
+    a: Numerator.
+    b: Denominator.
+  """
+
+  if b == 0.0:
+    if a != 0.0:
+      raise ValueError(
+          f"Cannot divide `a={a}` by `b={b}`. If `b==0`, then `a` should be"
+          " zero."
+      )
+    return 0.0
+  return a / b
```

## ydf/metric/metric_test.py

```diff
@@ -1,297 +1,297 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Testing Metrics."""
-
-import textwrap
-
-from absl.testing import absltest
-import numpy as np
-from numpy import testing as npt
-
-from yggdrasil_decision_forests.dataset import data_spec_pb2 as ds_pb
-from yggdrasil_decision_forests.metric import metric_pb2
-from ydf.metric import metric
-from yggdrasil_decision_forests.utils import distribution_pb2
-
-
-class ConfusionTest(absltest.TestCase):
-
-  def test_str(self):
-    c = metric.ConfusionMatrix(
-        classes=(
-            "a",
-            "b",
-        ),
-        matrix=np.array([[1, 2], [3, 4]]),
-    )
-    self.assertEqual(
-        str(c),
-        textwrap.dedent("""\
-        label (row) \\ prediction (col)
-        +---+---+---+
-        |   | a | b |
-        +---+---+---+
-        | a | 1 | 2 |
-        +---+---+---+
-        | b | 3 | 4 |
-        +---+---+---+
-        """),
-    )
-
-
-class SafeDivTest(absltest.TestCase):
-
-  def test_base(self):
-    self.assertEqual(metric.safe_div(4.0, 2.0), 2.0)
-
-  def test_zero(self):
-    self.assertEqual(metric.safe_div(0.0, 0.0), 0.0)
-
-  def test_error(self):
-    with self.assertRaisesRegex(ValueError, "Cannot divide"):
-      metric.safe_div(1.0, 0.0)
-
-
-class CharacteristicTest(absltest.TestCase):
-
-  def test_base(self):
-    c = metric.Characteristic(
-        name="name",
-        roc_auc=0.1,
-        pr_auc=0.2,
-        per_threshold=[
-            metric.CharacteristicPerThreshold(
-                true_positive=1,
-                false_positive=2,
-                true_negative=3,
-                false_negative=4,
-                threshold=5,
-            ),
-            metric.CharacteristicPerThreshold(
-                true_positive=10,
-                false_positive=20,
-                true_negative=30,
-                false_negative=40,
-                threshold=50,
-            ),
-        ],
-    )
-    self.assertEqual(c.name, "name")
-
-    npt.assert_array_almost_equal(c.recalls, [(1) / (1 + 4), (10) / (10 + 40)])
-    npt.assert_array_almost_equal(
-        c.specificities, [3 / (3 + 2), 30 / (30 + 20)]
-    )
-    npt.assert_array_almost_equal(
-        c.false_positive_rates, [2 / (2 + 3), 20 / (20 + 30)]
-    )
-    npt.assert_array_almost_equal(c.precisions, [1 / (1 + 2), 10 / (10 + 20)])
-    npt.assert_array_almost_equal(
-        c.accuracies,
-        [(1 + 3) / (1 + 2 + 3 + 4), (10 + 30) / (10 + 20 + 30 + 40)],
-    )
-
-
-class EvaluationTest(absltest.TestCase):
-
-  def test_empty(self):
-    proto_eval = metric_pb2.EvaluationResults()
-    self.assertEqual(metric.Evaluation(proto_eval).to_dict(), {})
-
-  def test_classification(self):
-    proto_eval = metric_pb2.EvaluationResults(
-        count_predictions_no_weight=1,
-        count_predictions=1,
-        label_column=ds_pb.Column(
-            name="my_label",
-            categorical=ds_pb.CategoricalSpec(
-                number_of_unique_values=3, is_already_integerized=True
-            ),
-        ),
-        classification=metric_pb2.EvaluationResults.Classification(
-            confusion=distribution_pb2.IntegersConfusionMatrixDouble(
-                counts=list([0, 0, 0, 0, 1, 2, 0, 3, 4]),
-                sum=1 + 2 + 3 + 4,
-                nrow=3,
-                ncol=3,
-            ),
-            sum_log_loss=2,
-            rocs=[
-                metric_pb2.Roc(),
-                metric_pb2.Roc(),
-                metric_pb2.Roc(
-                    count_predictions=10,
-                    auc=0.8,
-                    pr_auc=0.7,
-                    curve=[
-                        metric_pb2.Roc.Point(
-                            threshold=1, tp=2, fp=3, tn=4, fn=6
-                        ),
-                        metric_pb2.Roc.Point(
-                            threshold=2, tp=1, fp=2, tn=3, fn=4
-                        ),
-                    ],
-                ),
-            ],
-        ),
-    )
-    evaluation = metric.Evaluation(proto_eval)
-    print(evaluation)
-    dict_eval = evaluation.to_dict()
-    self.assertDictContainsSubset(
-        {"accuracy": (1 + 4) / (1 + 2 + 3 + 4), "loss": 2.0, "num_examples": 1},
-        dict_eval,
-    )
-
-    self.assertEqual(dict_eval["confusion_matrix"].classes, ("1", "2"))
-    npt.assert_array_almost_equal(
-        dict_eval["confusion_matrix"].matrix, [[1, 3], [2, 4]]
-    )
-
-    self.assertEqual(
-        str(evaluation),
-        textwrap.dedent("""\
-        accuracy: 0.5
-        confusion matrix:
-            label (row) \\ prediction (col)
-            +---+---+---+
-            |   | 1 | 2 |
-            +---+---+---+
-            | 1 | 1 | 3 |
-            +---+---+---+
-            | 2 | 2 | 4 |
-            +---+---+---+
-        characteristics:
-            name: '2' vs others
-            ROC AUC: 0.8
-            PR AUC: 0.7
-            Num thresholds: 2
-        loss: 2
-        num examples: 1
-        num examples (weighted): 1
-        """),
-    )
-
-    _ = evaluation.html()
-
-  def test_regression(self):
-    proto_eval = metric_pb2.EvaluationResults(
-        count_predictions_no_weight=1,
-        loss_value=2,
-        count_predictions=2,
-        label_column=ds_pb.Column(name="my_label"),
-        regression=metric_pb2.EvaluationResults.Regression(
-            sum_square_error=8,
-            bootstrap_rmse_lower_bounds_95p=9,
-            bootstrap_rmse_upper_bounds_95p=10,
-        ),
-    )
-    evaluation = metric.Evaluation(proto_eval)
-    print(evaluation)
-    self.assertDictEqual(
-        evaluation.to_dict(),
-        {
-            "loss": 2.0,
-            "num_examples": 1,
-            "rmse": 2.0,
-            "rmse_ci95_bootstrap": (9.0, 10.0),
-            "num_examples_weighted": 2,
-        },
-    )
-
-    self.assertEqual(
-        str(evaluation),
-        textwrap.dedent("""\
-        RMSE: 2
-        RMSE 95% CI [B]: (9.0, 10.0)
-        loss: 2
-        num examples: 1
-        num examples (weighted): 2
-        """),
-    )
-
-    _ = evaluation.html()
-
-  def test_ranking(self):
-    proto_eval = metric_pb2.EvaluationResults(
-        count_predictions_no_weight=1,
-        loss_value=2,
-        count_predictions=3,
-        label_column=ds_pb.Column(name="my_label"),
-        ranking=metric_pb2.EvaluationResults.Ranking(
-            ndcg=metric_pb2.MetricEstimate(value=5)
-        ),
-    )
-    evaluation = metric.Evaluation(proto_eval)
-    print(evaluation)
-    self.assertDictEqual(
-        evaluation.to_dict(),
-        {
-            "loss": 2.0,
-            "ndcg": 5.0,
-            "num_examples": 1,
-            "num_examples_weighted": 3,
-        },
-    )
-
-    self.assertEqual(
-        str(evaluation),
-        textwrap.dedent("""\
-        NDCG: 5
-        loss: 2
-        num examples: 1
-        num examples (weighted): 3
-        """),
-    )
-
-    _ = evaluation.html()
-
-  def test_uplift(self):
-    proto_eval = metric_pb2.EvaluationResults(
-        count_predictions_no_weight=1,
-        loss_value=2,
-        count_predictions=3,
-        label_column=ds_pb.Column(name="my_label"),
-        uplift=metric_pb2.EvaluationResults.Uplift(qini=6, auuc=7),
-    )
-    evaluation = metric.Evaluation(proto_eval)
-    print(evaluation)
-    self.assertDictEqual(
-        evaluation.to_dict(),
-        {
-            "auuc": 7.0,
-            "loss": 2.0,
-            "num_examples": 1,
-            "qini": 6.0,
-            "num_examples_weighted": 3,
-        },
-    )
-
-    self.assertEqual(
-        str(evaluation),
-        textwrap.dedent("""\
-        QINI: 6
-        AUUC: 7
-        loss: 2
-        num examples: 1
-        num examples (weighted): 3
-        """),
-    )
-
-    _ = evaluation.html()
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Testing Metrics."""
+
+import textwrap
+
+from absl.testing import absltest
+import numpy as np
+from numpy import testing as npt
+
+from ydf.proto.dataset import data_spec_pb2 as ds_pb
+from ydf.proto.metric import metric_pb2
+from ydf.metric import metric
+from ydf.proto.utils import distribution_pb2
+
+
+class ConfusionTest(absltest.TestCase):
+
+  def test_str(self):
+    c = metric.ConfusionMatrix(
+        classes=(
+            "a",
+            "b",
+        ),
+        matrix=np.array([[1, 2], [3, 4]]),
+    )
+    self.assertEqual(
+        str(c),
+        textwrap.dedent("""\
+        label (row) \\ prediction (col)
+        +---+---+---+
+        |   | a | b |
+        +---+---+---+
+        | a | 1 | 2 |
+        +---+---+---+
+        | b | 3 | 4 |
+        +---+---+---+
+        """),
+    )
+
+
+class SafeDivTest(absltest.TestCase):
+
+  def test_base(self):
+    self.assertEqual(metric.safe_div(4.0, 2.0), 2.0)
+
+  def test_zero(self):
+    self.assertEqual(metric.safe_div(0.0, 0.0), 0.0)
+
+  def test_error(self):
+    with self.assertRaisesRegex(ValueError, "Cannot divide"):
+      metric.safe_div(1.0, 0.0)
+
+
+class CharacteristicTest(absltest.TestCase):
+
+  def test_base(self):
+    c = metric.Characteristic(
+        name="name",
+        roc_auc=0.1,
+        pr_auc=0.2,
+        per_threshold=[
+            metric.CharacteristicPerThreshold(
+                true_positive=1,
+                false_positive=2,
+                true_negative=3,
+                false_negative=4,
+                threshold=5,
+            ),
+            metric.CharacteristicPerThreshold(
+                true_positive=10,
+                false_positive=20,
+                true_negative=30,
+                false_negative=40,
+                threshold=50,
+            ),
+        ],
+    )
+    self.assertEqual(c.name, "name")
+
+    npt.assert_array_almost_equal(c.recalls, [(1) / (1 + 4), (10) / (10 + 40)])
+    npt.assert_array_almost_equal(
+        c.specificities, [3 / (3 + 2), 30 / (30 + 20)]
+    )
+    npt.assert_array_almost_equal(
+        c.false_positive_rates, [2 / (2 + 3), 20 / (20 + 30)]
+    )
+    npt.assert_array_almost_equal(c.precisions, [1 / (1 + 2), 10 / (10 + 20)])
+    npt.assert_array_almost_equal(
+        c.accuracies,
+        [(1 + 3) / (1 + 2 + 3 + 4), (10 + 30) / (10 + 20 + 30 + 40)],
+    )
+
+
+class EvaluationTest(absltest.TestCase):
+
+  def test_empty(self):
+    proto_eval = metric_pb2.EvaluationResults()
+    self.assertEqual(metric.Evaluation(proto_eval).to_dict(), {})
+
+  def test_classification(self):
+    proto_eval = metric_pb2.EvaluationResults(
+        count_predictions_no_weight=1,
+        count_predictions=1,
+        label_column=ds_pb.Column(
+            name="my_label",
+            categorical=ds_pb.CategoricalSpec(
+                number_of_unique_values=3, is_already_integerized=True
+            ),
+        ),
+        classification=metric_pb2.EvaluationResults.Classification(
+            confusion=distribution_pb2.IntegersConfusionMatrixDouble(
+                counts=list([0, 0, 0, 0, 1, 2, 0, 3, 4]),
+                sum=1 + 2 + 3 + 4,
+                nrow=3,
+                ncol=3,
+            ),
+            sum_log_loss=2,
+            rocs=[
+                metric_pb2.Roc(),
+                metric_pb2.Roc(),
+                metric_pb2.Roc(
+                    count_predictions=10,
+                    auc=0.8,
+                    pr_auc=0.7,
+                    curve=[
+                        metric_pb2.Roc.Point(
+                            threshold=1, tp=2, fp=3, tn=4, fn=6
+                        ),
+                        metric_pb2.Roc.Point(
+                            threshold=2, tp=1, fp=2, tn=3, fn=4
+                        ),
+                    ],
+                ),
+            ],
+        ),
+    )
+    evaluation = metric.Evaluation(proto_eval)
+    print(evaluation)
+    dict_eval = evaluation.to_dict()
+    self.assertDictContainsSubset(
+        {"accuracy": (1 + 4) / (1 + 2 + 3 + 4), "loss": 2.0, "num_examples": 1},
+        dict_eval,
+    )
+
+    self.assertEqual(dict_eval["confusion_matrix"].classes, ("1", "2"))
+    npt.assert_array_almost_equal(
+        dict_eval["confusion_matrix"].matrix, [[1, 3], [2, 4]]
+    )
+
+    self.assertEqual(
+        str(evaluation),
+        textwrap.dedent("""\
+        accuracy: 0.5
+        confusion matrix:
+            label (row) \\ prediction (col)
+            +---+---+---+
+            |   | 1 | 2 |
+            +---+---+---+
+            | 1 | 1 | 3 |
+            +---+---+---+
+            | 2 | 2 | 4 |
+            +---+---+---+
+        characteristics:
+            name: '2' vs others
+            ROC AUC: 0.8
+            PR AUC: 0.7
+            Num thresholds: 2
+        loss: 2
+        num examples: 1
+        num examples (weighted): 1
+        """),
+    )
+
+    _ = evaluation.html()
+
+  def test_regression(self):
+    proto_eval = metric_pb2.EvaluationResults(
+        count_predictions_no_weight=1,
+        loss_value=2,
+        count_predictions=2,
+        label_column=ds_pb.Column(name="my_label"),
+        regression=metric_pb2.EvaluationResults.Regression(
+            sum_square_error=8,
+            bootstrap_rmse_lower_bounds_95p=9,
+            bootstrap_rmse_upper_bounds_95p=10,
+        ),
+    )
+    evaluation = metric.Evaluation(proto_eval)
+    print(evaluation)
+    self.assertDictEqual(
+        evaluation.to_dict(),
+        {
+            "loss": 2.0,
+            "num_examples": 1,
+            "rmse": 2.0,
+            "rmse_ci95_bootstrap": (9.0, 10.0),
+            "num_examples_weighted": 2,
+        },
+    )
+
+    self.assertEqual(
+        str(evaluation),
+        textwrap.dedent("""\
+        RMSE: 2
+        RMSE 95% CI [B]: (9.0, 10.0)
+        loss: 2
+        num examples: 1
+        num examples (weighted): 2
+        """),
+    )
+
+    _ = evaluation.html()
+
+  def test_ranking(self):
+    proto_eval = metric_pb2.EvaluationResults(
+        count_predictions_no_weight=1,
+        loss_value=2,
+        count_predictions=3,
+        label_column=ds_pb.Column(name="my_label"),
+        ranking=metric_pb2.EvaluationResults.Ranking(
+            ndcg=metric_pb2.MetricEstimate(value=5)
+        ),
+    )
+    evaluation = metric.Evaluation(proto_eval)
+    print(evaluation)
+    self.assertDictEqual(
+        evaluation.to_dict(),
+        {
+            "loss": 2.0,
+            "ndcg": 5.0,
+            "num_examples": 1,
+            "num_examples_weighted": 3,
+        },
+    )
+
+    self.assertEqual(
+        str(evaluation),
+        textwrap.dedent("""\
+        NDCG: 5
+        loss: 2
+        num examples: 1
+        num examples (weighted): 3
+        """),
+    )
+
+    _ = evaluation.html()
+
+  def test_uplift(self):
+    proto_eval = metric_pb2.EvaluationResults(
+        count_predictions_no_weight=1,
+        loss_value=2,
+        count_predictions=3,
+        label_column=ds_pb.Column(name="my_label"),
+        uplift=metric_pb2.EvaluationResults.Uplift(qini=6, auuc=7),
+    )
+    evaluation = metric.Evaluation(proto_eval)
+    print(evaluation)
+    self.assertDictEqual(
+        evaluation.to_dict(),
+        {
+            "auuc": 7.0,
+            "loss": 2.0,
+            "num_examples": 1,
+            "qini": 6.0,
+            "num_examples_weighted": 3,
+        },
+    )
+
+    self.assertEqual(
+        str(evaluation),
+        textwrap.dedent("""\
+        QINI: 6
+        AUUC: 7
+        loss: 2
+        num examples: 1
+        num examples (weighted): 3
+        """),
+    )
+
+    _ = evaluation.html()
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/model/__init__.py

 * *Ordering differences only*

```diff
@@ -1,14 +1,14 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
```

## ydf/model/analysis.py

```diff
@@ -1,297 +1,297 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""An analysis contains information about a model."""
-
-import copy
-import dataclasses
-from typing import Dict, List, Optional, Sequence, Tuple, Union
-
-import numpy as np
-
-from yggdrasil_decision_forests.dataset import data_spec_pb2
-from ydf.cc import ydf
-from ydf.dataset import dataspec
-from yggdrasil_decision_forests.utils import model_analysis_pb2
-from yggdrasil_decision_forests.utils import partial_dependence_plot_pb2
-
-Bin = (
-    partial_dependence_plot_pb2.PartialDependencePlotSet.PartialDependencePlot.Bin
-)
-AttributeInfo = (
-    partial_dependence_plot_pb2.PartialDependencePlotSet.PartialDependencePlot.AttributeInfo
-)
-
-
-@dataclasses.dataclass(frozen=True)
-class PartialDependencePlot:
-  """A partial dependence plot (PDP).
-
-  Attributes:
-    predictions: PDP value for each of the bins. If the model output is
-      multi-dimensional, `predictions` is of shape [num bins, num dimensions].
-      If the model output is single-dimensional, `predictions` is of shape [num
-      bins].
-    feature_names: Input feature names.
-    feature_values: Input feature values. `feature_values[i][j]` is the value of
-      feature `feature_names[i]` for the prediction `predictions[j]`.
-  """
-
-  feature_names: Sequence[str]
-  feature_values: Sequence[np.ndarray]
-  predictions: np.ndarray
-
-
-class Analysis:
-  """A model analysis.
-
-  An analysis contains information about a model (e.g., variable
-  importance, training logs), a dataset (e.g., column statistics), and the
-  application of the model on this dataset (e.g. partial dependence plots).
-  """
-
-  def __init__(
-      self,
-      analysis_proto: model_analysis_pb2.StandaloneAnalysisResult,
-      options_proto: model_analysis_pb2.Options,
-  ):
-    self._analysis_proto = analysis_proto
-    self._options_proto = options_proto
-
-  def __str__(self) -> str:
-    """Returns the string representation of the analysis."""
-    return (
-        "A model analysis. Use a notebook cell to display the analysis."
-        " Alternatively, export the analysis with"
-        ' `analysis.to_file("analysis.html")`.'
-    )
-
-  def html(self) -> str:
-    """Html representation of the analysis."""
-
-    return self._repr_html_()
-
-  def _repr_html_(self) -> str:
-    """Returns the Html representation of the analysis."""
-
-    effective_options = copy.deepcopy(self._options_proto)
-    effective_options.report_header.enabled = False
-    effective_options.report_setup.enabled = False
-    effective_options.model_description.enabled = False
-    effective_options.plot_width = 400
-    effective_options.plot_height = 300
-    effective_options.figure_width = effective_options.plot_width * 3
-
-    return ydf.ModelAnalysisCreateHtmlReport(
-        self._analysis_proto, effective_options
-    )
-
-  def to_file(self, path: str) -> None:
-    """Exports the analysis to a html file."""
-    with open(path, "w") as f:
-      f.write(self.html())
-
-  def partial_dependence_plots(self) -> Sequence[PartialDependencePlot]:
-    """Programmatic access to the Partial Dependence Plots (PDP).
-
-    A PDP is a powerful tool to interpret a model.
-    See https://christophm.github.io/interpretable-ml-book/pdp.html
-
-    Note: PDPs can be plotted automatically with `model.analyze`.
-
-    Usage example:
-
-    ```python
-    import ydf
-    import matplotlib.pyplot as plt
-
-    # Get the model PDPs
-    pdps = ydf.load_model(...).analyze(...).partial_dependence_plots()
-
-    # Select the first PDP
-    pdp = pdps[0]
-
-    # Check the PDP is single dimensional.
-    assert len(pdp.feature_names) == 1, "PDP not single dimensional"
-
-    plt.plot(pdp.feature_values[0], pdp.predictions)
-    plt.xlabel(pdp.feature_names[0])
-    plt.ylabel("prediction")
-    plt.show()
-    ```
-    """
-
-    # Unfold the PDP protos into a python structure.
-    dst_pdps = []
-    for src_pdp in self._analysis_proto.core_analysis.pdp_set.pdps:
-
-      # Note: If "_pdp_prediction_value" returns a numpy array, the following
-      # "np.array" acts as a "np.stack".
-      predictions = np.array([
-          _pdp_prediction_value(bin, src_pdp.num_observations)
-          for bin in src_pdp.pdp_bins
-      ])
-
-      feature_names = [
-          _pdp_feature_name(attr, data_spec=self._analysis_proto.data_spec)
-          for attr in src_pdp.attribute_info
-      ]
-
-      feature_values = []
-      for local_attribute_idx, attr in enumerate(src_pdp.attribute_info):
-        column_spec = self._analysis_proto.data_spec.columns[attr.attribute_idx]
-        if column_spec.HasField("categorical"):
-          categorical_dictionary = (
-              dataspec.categorical_column_dictionary_to_list(column_spec)
-          )
-        else:
-          categorical_dictionary = None
-
-        feature_values.append(
-            np.array([
-                _pdp_feature_value(
-                    bin, local_attribute_idx, categorical_dictionary
-                )
-                for bin in src_pdp.pdp_bins
-            ])
-        )
-
-      dst_pdps.append(
-          PartialDependencePlot(
-              predictions=predictions,
-              feature_values=feature_values,
-              feature_names=feature_names,
-          )
-      )
-    return dst_pdps
-
-  def variable_importances(self) -> Dict[str, List[Tuple[float, str]]]:
-    """Programmatic access to the Variable importances.
-
-    Note: Variable importances can be plotted automatically with
-    `model.analyze`.
-
-    Usage example:
-
-    ```python
-
-    # Get the variable importances
-    vis = ydf.load_model(...).analyze(...).variable_importances()
-
-    # Print the variable importances
-    print(vis)
-    ```
-
-    Returns:
-      Variable importances.
-    """
-
-    def feature_name(attribute_idx: int) -> str:
-      return self._analysis_proto.data_spec.columns[attribute_idx].name
-
-    # Unfold the VIs protos into a python structure.
-    variable_importances = {}
-    for (
-        name,
-        importance_set,
-    ) in self._analysis_proto.core_analysis.variable_importances.items():
-      variable_importances[name] = [
-          (src.importance, feature_name(src.attribute_idx))
-          for src in importance_set.variable_importances
-      ]
-    return variable_importances
-
-
-class PredictionAnalysis:
-  """A prediction analysis.
-
-  A prediction analysis explains why a model made a prediction.
-  """
-
-  def __init__(
-      self,
-      analysis_proto: model_analysis_pb2.PredictionAnalysisResult,
-      options_proto: model_analysis_pb2.PredictionAnalysisOptions,
-  ):
-    self._analysis_proto = analysis_proto
-    self._options_proto = options_proto
-
-  def __str__(self) -> str:
-    """Returns the string representation of the analysis."""
-    return (
-        "A prediction analysis. Use a notebook cell to display the analysis."
-        " Alternatively, export the analysis with"
-        ' `analysis.to_file("analysis.html")`.'
-    )
-
-  def html(self) -> str:
-    """Html representation of the analysis."""
-
-    return self._repr_html_()
-
-  def _repr_html_(self) -> str:
-    """Returns the Html representation of the analysis."""
-
-    effective_options = copy.deepcopy(self._options_proto)
-    return ydf.PredictionAnalysisCreateHtmlReport(
-        self._analysis_proto, effective_options
-    )
-
-  def to_file(self, path: str) -> None:
-    """Exports the analysis to a html file."""
-    with open(path, "w") as f:
-      f.write(self.html())
-
-
-def _pdp_prediction_value(
-    bin: Bin, num_observations: float
-) -> Union[float, np.ndarray]:
-  """Extracts a uniform numerical prediction value from a bin."""
-  if bin.prediction.HasField("sum_of_regression_predictions"):
-    return bin.prediction.sum_of_regression_predictions / num_observations
-  elif bin.prediction.HasField("sum_of_ranking_predictions"):
-    return bin.prediction.sum_of_ranking_predictions / num_observations
-  elif bin.prediction.HasField("classification_class_distribution"):
-    # Skip OOV item.
-    return (
-        np.array(bin.prediction.classification_class_distribution.counts[1:])
-        / bin.prediction.classification_class_distribution.sum
-    )
-  else:
-    raise ValueError(f"Unsupported prediction type: {bin}")
-
-
-def _pdp_feature_name(
-    attr: AttributeInfo, data_spec: data_spec_pb2.DataSpecification
-) -> str:
-  """Name of a feature."""
-  return data_spec.columns[attr.attribute_idx].name
-
-
-def _pdp_feature_value(
-    bin: Bin,
-    local_attribute_idx: int,
-    categorical_dictionary: Optional[List[str]],
-) -> Union[bool, float, int, bool, str]:
-  """Value of a feature."""
-  value = bin.center_input_feature_values[local_attribute_idx]
-  if value.HasField("boolean"):
-    return value.boolean
-  elif value.HasField("numerical"):
-    return value.numerical
-  elif value.HasField("categorical"):
-    assert categorical_dictionary is not None
-    return categorical_dictionary[value.categorical]
-  else:
-    raise ValueError(f"Unsupported value: {value}")
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""An analysis contains information about a model."""
+
+import copy
+import dataclasses
+from typing import Dict, List, Optional, Sequence, Tuple, Union
+
+import numpy as np
+
+from ydf.proto.dataset import data_spec_pb2
+from ydf.cc import ydf
+from ydf.dataset import dataspec
+from ydf.proto.utils import model_analysis_pb2
+from ydf.proto.utils import partial_dependence_plot_pb2
+
+Bin = (
+    partial_dependence_plot_pb2.PartialDependencePlotSet.PartialDependencePlot.Bin
+)
+AttributeInfo = (
+    partial_dependence_plot_pb2.PartialDependencePlotSet.PartialDependencePlot.AttributeInfo
+)
+
+
+@dataclasses.dataclass(frozen=True)
+class PartialDependencePlot:
+  """A partial dependence plot (PDP).
+
+  Attributes:
+    predictions: PDP value for each of the bins. If the model output is
+      multi-dimensional, `predictions` is of shape [num bins, num dimensions].
+      If the model output is single-dimensional, `predictions` is of shape [num
+      bins].
+    feature_names: Input feature names.
+    feature_values: Input feature values. `feature_values[i][j]` is the value of
+      feature `feature_names[i]` for the prediction `predictions[j]`.
+  """
+
+  feature_names: Sequence[str]
+  feature_values: Sequence[np.ndarray]
+  predictions: np.ndarray
+
+
+class Analysis:
+  """A model analysis.
+
+  An analysis contains information about a model (e.g., variable
+  importance, training logs), a dataset (e.g., column statistics), and the
+  application of the model on this dataset (e.g. partial dependence plots).
+  """
+
+  def __init__(
+      self,
+      analysis_proto: model_analysis_pb2.StandaloneAnalysisResult,
+      options_proto: model_analysis_pb2.Options,
+  ):
+    self._analysis_proto = analysis_proto
+    self._options_proto = options_proto
+
+  def __str__(self) -> str:
+    """Returns the string representation of the analysis."""
+    return (
+        "A model analysis. Use a notebook cell to display the analysis."
+        " Alternatively, export the analysis with"
+        ' `analysis.to_file("analysis.html")`.'
+    )
+
+  def html(self) -> str:
+    """Html representation of the analysis."""
+
+    return self._repr_html_()
+
+  def _repr_html_(self) -> str:
+    """Returns the Html representation of the analysis."""
+
+    effective_options = copy.deepcopy(self._options_proto)
+    effective_options.report_header.enabled = False
+    effective_options.report_setup.enabled = False
+    effective_options.model_description.enabled = False
+    effective_options.plot_width = 400
+    effective_options.plot_height = 300
+    effective_options.figure_width = effective_options.plot_width * 3
+
+    return ydf.ModelAnalysisCreateHtmlReport(
+        self._analysis_proto, effective_options
+    )
+
+  def to_file(self, path: str) -> None:
+    """Exports the analysis to a html file."""
+    with open(path, "w") as f:
+      f.write(self.html())
+
+  def partial_dependence_plots(self) -> Sequence[PartialDependencePlot]:
+    """Programmatic access to the Partial Dependence Plots (PDP).
+
+    A PDP is a powerful tool to interpret a model.
+    See https://christophm.github.io/interpretable-ml-book/pdp.html
+
+    Note: PDPs can be plotted automatically with `model.analyze`.
+
+    Usage example:
+
+    ```python
+    import ydf
+    import matplotlib.pyplot as plt
+
+    # Get the model PDPs
+    pdps = ydf.load_model(...).analyze(...).partial_dependence_plots()
+
+    # Select the first PDP
+    pdp = pdps[0]
+
+    # Check the PDP is single dimensional.
+    assert len(pdp.feature_names) == 1, "PDP not single dimensional"
+
+    plt.plot(pdp.feature_values[0], pdp.predictions)
+    plt.xlabel(pdp.feature_names[0])
+    plt.ylabel("prediction")
+    plt.show()
+    ```
+    """
+
+    # Unfold the PDP protos into a python structure.
+    dst_pdps = []
+    for src_pdp in self._analysis_proto.core_analysis.pdp_set.pdps:
+
+      # Note: If "_pdp_prediction_value" returns a numpy array, the following
+      # "np.array" acts as a "np.stack".
+      predictions = np.array([
+          _pdp_prediction_value(bin, src_pdp.num_observations)
+          for bin in src_pdp.pdp_bins
+      ])
+
+      feature_names = [
+          _pdp_feature_name(attr, data_spec=self._analysis_proto.data_spec)
+          for attr in src_pdp.attribute_info
+      ]
+
+      feature_values = []
+      for local_attribute_idx, attr in enumerate(src_pdp.attribute_info):
+        column_spec = self._analysis_proto.data_spec.columns[attr.attribute_idx]
+        if column_spec.HasField("categorical"):
+          categorical_dictionary = (
+              dataspec.categorical_column_dictionary_to_list(column_spec)
+          )
+        else:
+          categorical_dictionary = None
+
+        feature_values.append(
+            np.array([
+                _pdp_feature_value(
+                    bin, local_attribute_idx, categorical_dictionary
+                )
+                for bin in src_pdp.pdp_bins
+            ])
+        )
+
+      dst_pdps.append(
+          PartialDependencePlot(
+              predictions=predictions,
+              feature_values=feature_values,
+              feature_names=feature_names,
+          )
+      )
+    return dst_pdps
+
+  def variable_importances(self) -> Dict[str, List[Tuple[float, str]]]:
+    """Programmatic access to the Variable importances.
+
+    Note: Variable importances can be plotted automatically with
+    `model.analyze`.
+
+    Usage example:
+
+    ```python
+
+    # Get the variable importances
+    vis = ydf.load_model(...).analyze(...).variable_importances()
+
+    # Print the variable importances
+    print(vis)
+    ```
+
+    Returns:
+      Variable importances.
+    """
+
+    def feature_name(attribute_idx: int) -> str:
+      return self._analysis_proto.data_spec.columns[attribute_idx].name
+
+    # Unfold the VIs protos into a python structure.
+    variable_importances = {}
+    for (
+        name,
+        importance_set,
+    ) in self._analysis_proto.core_analysis.variable_importances.items():
+      variable_importances[name] = [
+          (src.importance, feature_name(src.attribute_idx))
+          for src in importance_set.variable_importances
+      ]
+    return variable_importances
+
+
+class PredictionAnalysis:
+  """A prediction analysis.
+
+  A prediction analysis explains why a model made a prediction.
+  """
+
+  def __init__(
+      self,
+      analysis_proto: model_analysis_pb2.PredictionAnalysisResult,
+      options_proto: model_analysis_pb2.PredictionAnalysisOptions,
+  ):
+    self._analysis_proto = analysis_proto
+    self._options_proto = options_proto
+
+  def __str__(self) -> str:
+    """Returns the string representation of the analysis."""
+    return (
+        "A prediction analysis. Use a notebook cell to display the analysis."
+        " Alternatively, export the analysis with"
+        ' `analysis.to_file("analysis.html")`.'
+    )
+
+  def html(self) -> str:
+    """Html representation of the analysis."""
+
+    return self._repr_html_()
+
+  def _repr_html_(self) -> str:
+    """Returns the Html representation of the analysis."""
+
+    effective_options = copy.deepcopy(self._options_proto)
+    return ydf.PredictionAnalysisCreateHtmlReport(
+        self._analysis_proto, effective_options
+    )
+
+  def to_file(self, path: str) -> None:
+    """Exports the analysis to a html file."""
+    with open(path, "w") as f:
+      f.write(self.html())
+
+
+def _pdp_prediction_value(
+    bin: Bin, num_observations: float
+) -> Union[float, np.ndarray]:
+  """Extracts a uniform numerical prediction value from a bin."""
+  if bin.prediction.HasField("sum_of_regression_predictions"):
+    return bin.prediction.sum_of_regression_predictions / num_observations
+  elif bin.prediction.HasField("sum_of_ranking_predictions"):
+    return bin.prediction.sum_of_ranking_predictions / num_observations
+  elif bin.prediction.HasField("classification_class_distribution"):
+    # Skip OOV item.
+    return (
+        np.array(bin.prediction.classification_class_distribution.counts[1:])
+        / bin.prediction.classification_class_distribution.sum
+    )
+  else:
+    raise ValueError(f"Unsupported prediction type: {bin}")
+
+
+def _pdp_feature_name(
+    attr: AttributeInfo, data_spec: data_spec_pb2.DataSpecification
+) -> str:
+  """Name of a feature."""
+  return data_spec.columns[attr.attribute_idx].name
+
+
+def _pdp_feature_value(
+    bin: Bin,
+    local_attribute_idx: int,
+    categorical_dictionary: Optional[List[str]],
+) -> Union[bool, float, int, bool, str]:
+  """Value of a feature."""
+  value = bin.center_input_feature_values[local_attribute_idx]
+  if value.HasField("boolean"):
+    return value.boolean
+  elif value.HasField("numerical"):
+    return value.numerical
+  elif value.HasField("categorical"):
+    assert categorical_dictionary is not None
+    return categorical_dictionary[value.categorical]
+  else:
+    raise ValueError(f"Unsupported value: {value}")
```

## ydf/model/export_cc_generator.py

 * *Ordering differences only*

```diff
@@ -1,50 +1,50 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Generates the c++ code to run a model for a unit-test."""
-
-
-from collections.abc import Sequence
-
-from absl import app
-from absl import flags
-from absl import logging
-
-import ydf
-
-_INPUT_MODEL = flags.DEFINE_string("input_model", None, "Path to input model")
-
-_OUTPUT_CODE = flags.DEFINE_string(
-    "output_code", None, "Path to generated c++ file."
-)
-
-
-def process(input_model: str, output_code: str) -> None:
-  logging.info(
-      "Loading model %s and generating cc code in %s", input_model, output_code
-  )
-
-  model = ydf.load_model(input_model)
-  with open(output_code, "w") as f:
-    f.write(model.to_cpp("123"))
-
-
-def main(argv: Sequence[str]) -> None:
-  if len(argv) > 1:
-    raise app.UsageError("Too many command-line arguments.")
-  process(_INPUT_MODEL.value, _OUTPUT_CODE.value)
-
-
-if __name__ == "__main__":
-  app.run(main)
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Generates the c++ code to run a model for a unit-test."""
+
+
+from collections.abc import Sequence
+
+from absl import app
+from absl import flags
+from absl import logging
+
+import ydf
+
+_INPUT_MODEL = flags.DEFINE_string("input_model", None, "Path to input model")
+
+_OUTPUT_CODE = flags.DEFINE_string(
+    "output_code", None, "Path to generated c++ file."
+)
+
+
+def process(input_model: str, output_code: str) -> None:
+  logging.info(
+      "Loading model %s and generating cc code in %s", input_model, output_code
+  )
+
+  model = ydf.load_model(input_model)
+  with open(output_code, "w") as f:
+    f.write(model.to_cpp("123"))
+
+
+def main(argv: Sequence[str]) -> None:
+  if len(argv) > 1:
+    raise app.UsageError("Too many command-line arguments.")
+  process(_INPUT_MODEL.value, _OUTPUT_CODE.value)
+
+
+if __name__ == "__main__":
+  app.run(main)
```

## ydf/model/export_tf.py

```diff
@@ -1,673 +1,673 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Utilities to export TF models."""
-
-import math
-import shutil
-import tempfile
-from typing import Any, Callable, Dict, Literal, Optional, Sequence
-import uuid
-
-from yggdrasil_decision_forests.dataset import data_spec_pb2 as ds_pb
-from ydf.dataset import dataspec
-from ydf.dataset.io import dataset_io
-from ydf.model import generic_model
-from ydf.utils import log
-
-_ERROR_MESSAGE_MISSING_TF = (
-    '"tensorflow" is needed by this function. Make sure it'
-    " installed and try again. If using pip, run `pip install"
-    " tensorflow`. If using Bazel/Blaze, add a dependency to"
-    " TensorFlow."
-)
-
-_ERROR_MESSAGE_MISSING_TFDF = (
-    '"tensorflow_decision_forests" is needed by this function. Make sure it'
-    " installed and try again. If using pip, run `pip install"
-    " tensorflow_decision_forests`. If using Bazel/Blaze, add a dependency to"
-    " TensorFlow Decision Forests."
-)
-
-TFDType = Any  # TensorFlow DType e.g. tf.float32
-TFTensor = Any  # A TensorFlow Tensor i.e. tensorflow.Tensor
-
-# Mapping between YDF dtype and TF dtypes.
-_YDF_DTYPE_TO_TF_DTYPE: Dict["ds_pb.DType", TFDType] = None
-
-# Mapping TF dtypes to the TF dtype compatible with tensorflow example.
-# Note that tensorflow example proto only support tf.int64, tf.float32,
-# and tf.string dtypes.
-_TF_DTYPE_TO_TF_EXAMPLE_DTYPE: Dict[TFDType, TFDType] = None
-
-
-def mapping_ydf_dtype_to_tf_dtype() -> Dict["ds_pb.DType", TFDType]:
-  """Mapping between YDF dtype and TF dtypes."""
-
-  global _YDF_DTYPE_TO_TF_DTYPE
-  tf = import_tensorflow()
-  if _YDF_DTYPE_TO_TF_DTYPE is None:
-    _YDF_DTYPE_TO_TF_DTYPE = {
-        ds_pb.DType.DTYPE_INT8: tf.int8,
-        ds_pb.DType.DTYPE_INT16: tf.int16,
-        ds_pb.DType.DTYPE_INT32: tf.int32,
-        ds_pb.DType.DTYPE_INT64: tf.int64,
-        ds_pb.DType.DTYPE_UINT8: tf.uint8,
-        ds_pb.DType.DTYPE_UINT16: tf.uint16,
-        ds_pb.DType.DTYPE_UINT32: tf.uint32,
-        ds_pb.DType.DTYPE_UINT64: tf.uint64,
-        ds_pb.DType.DTYPE_FLOAT16: tf.float16,
-        ds_pb.DType.DTYPE_FLOAT32: tf.float32,
-        ds_pb.DType.DTYPE_FLOAT64: tf.float64,
-        ds_pb.DType.DTYPE_BOOL: tf.bool,
-        ds_pb.DType.DTYPE_BYTES: tf.string,
-    }
-  return _YDF_DTYPE_TO_TF_DTYPE
-
-
-def mapping_tf_dtype_to_tf_example_dtype() -> Dict[TFDType, TFDType]:
-  """Mapping TF dtypes to the TF dtype compatible with tensorflow example."""
-
-  global _TF_DTYPE_TO_TF_EXAMPLE_DTYPE
-  tf = import_tensorflow()
-  if _TF_DTYPE_TO_TF_EXAMPLE_DTYPE is None:
-    _TF_DTYPE_TO_TF_EXAMPLE_DTYPE = {
-        tf.int8: tf.int64,
-        tf.int16: tf.int64,
-        tf.int32: tf.int64,
-        tf.int64: tf.int64,
-        tf.uint8: tf.int64,
-        tf.uint16: tf.int64,
-        tf.uint32: tf.int64,
-        tf.uint64: tf.int64,
-        tf.float16: tf.float32,
-        tf.float32: tf.float32,
-        tf.float64: tf.float32,
-        tf.bool: tf.int64,
-        tf.string: tf.string,
-    }
-  return _TF_DTYPE_TO_TF_EXAMPLE_DTYPE
-
-
-def ydf_model_to_tensorflow_saved_model(
-    ydf_model: "generic_model.GenericModel",
-    path: str,
-    input_model_signature_fn: Any,
-    mode: Literal["keras", "tf"],
-    feature_dtypes: Dict[str, TFDType],
-    servo_api: bool,
-    feed_example_proto: bool,
-    pre_processing: Optional[Callable],  # pylint: disable=g-bare-generic
-    post_processing: Optional[Callable],  # pylint: disable=g-bare-generic
-    temp_dir: Optional[str],
-):  # pylint: disable=g-doc-args
-  """Exports the model as a TensorFlow Saved model.
-
-  See GenericModel.to_tensorflow_saved_model for the documentation.
-  """
-  if mode == "keras":
-    for value, name, expected in [
-        (feature_dtypes, "feature_dtypes", {}),
-        (servo_api, "servo_api", False),
-        (feed_example_proto, "feed_example_proto", False),
-        (pre_processing, "pre_processing", None),
-        (post_processing, "post_processing", None),
-    ]:
-      if value != expected:
-        raise ValueError(f"{name!r} is not supported for `keras` mode.")
-    ydf_model_to_tensorflow_saved_model_keras_mode(
-        ydf_model=ydf_model,
-        path=path,
-        input_model_signature_fn=input_model_signature_fn,
-        temp_dir=temp_dir,
-    )
-
-  elif mode == "tf":
-    if input_model_signature_fn is not None:
-      raise ValueError(
-          "input_model_signature_fn is not supported for `tf` mode."
-      )
-    ydf_model_to_tensorflow_saved_model_tf_mode(
-        ydf_model=ydf_model,
-        path=path,
-        feature_dtypes=feature_dtypes,
-        servo_api=servo_api,
-        feed_example_proto=feed_example_proto,
-        pre_processing=pre_processing,
-        post_processing=post_processing,
-        temp_dir=temp_dir,
-    )
-  else:
-    raise ValueError(f"Invalid mode: {mode}")
-
-
-def ydf_model_to_tensorflow_saved_model_keras_mode(
-    ydf_model: "generic_model.GenericModel",
-    path: str,
-    input_model_signature_fn: Any,
-    temp_dir: Optional[str],
-):  # pylint: disable=g-doc-args
-  tfdf = import_tensorflow_decision_forests()
-
-  # Do not pass input_model_signature_fn if it is None.
-  not_none_params = {}
-  if input_model_signature_fn is not None:
-    not_none_params["input_model_signature_fn"] = input_model_signature_fn
-  with tempfile.TemporaryDirectory(dir=temp_dir) as tmpdirname:
-    ydf_model.save(tmpdirname)
-    tfdf.keras.yggdrasil_model_to_keras_model(
-        src_path=tmpdirname,
-        dst_path=path,
-        verbose=log.current_log_level(),
-        **not_none_params,
-    )
-
-
-def ydf_model_to_tensorflow_saved_model_tf_mode(
-    ydf_model: "generic_model.GenericModel",
-    path: str,
-    feature_dtypes: Dict[str, TFDType],
-    servo_api: bool,
-    feed_example_proto: bool,
-    pre_processing: Optional[Callable],  # pylint: disable=g-bare-generic
-    post_processing: Optional[Callable],  # pylint: disable=g-bare-generic
-    temp_dir: Optional[str],
-):  # pylint: disable=g-doc-args
-
-  tf = import_tensorflow()
-
-  # The temporary files should remain available until the call to
-  # "tf.saved_model.save"
-  with tempfile.TemporaryDirectory(dir=temp_dir) as effective_temp_dir:
-
-    tf_module = ydf_model.to_tensorflow_function(
-        temp_dir=effective_temp_dir,
-        squeeze_binary_classification=not servo_api,
-    )
-
-    # Store pre / post processing operations
-    # Note: Storing the raw variable allows for pre/post-processing to be
-    # TensorFlow modules with resources.
-    tf_module.raw_pre_processing = pre_processing
-    tf_module.raw_post_processing = post_processing
-    tf_module.pre_processing = tf.function(pre_processing or (lambda x: x))
-    tf_module.post_processing = tf.function(post_processing or (lambda x: x))
-
-    # Apply pre / post processing to call.
-    def call(features):
-      return tf_module.post_processing(
-          tf_module.raw_call(tf_module.pre_processing(features))
-      )
-
-    tf_module.raw_call = tf_module.call
-    tf_module.call = call
-
-    # Trace the call function.
-    # Note: "tf.saved_model.save" can only export functions that have been
-    # traced.
-    raw_input_signature = tensorflow_raw_input_signature(
-        ydf_model, feature_dtypes
-    )
-    tf_module.__call__.get_concrete_function(raw_input_signature)
-
-    # Output format
-    if servo_api:
-
-      if ydf_model.task() == generic_model.Task.CLASSIFICATION:
-        label_classes = ydf_model.label_classes()
-
-        # "classify" Servo API
-        def predict_output_format(features, batch_size):
-          raw_output = tf_module(features)
-          batched_label_classes = tf.broadcast_to(
-              input=tf.constant(label_classes),
-              shape=(batch_size, len(label_classes)),
-          )
-          return {"classes": batched_label_classes, "scores": raw_output}
-
-      elif ydf_model.task() == generic_model.Task.REGRESSION:
-
-        # "regress" Servo API
-        def predict_output_format(features, batch_size):
-          del batch_size
-          raw_output = tf_module(features)
-          return {"outputs": raw_output}
-
-      else:
-        raise ValueError(
-            f"servo_api=True non supported for task {ydf_model.task()!r}"
-        )
-
-    else:
-
-      # "predict" Servo API i.e. raw output
-      # Note: "signature" outputs need to be dictionaries.
-      def predict_output_format(features, batch_size):
-        del batch_size
-        return {"output": tf_module(features)}
-
-    # Input feature formats
-    if not feed_example_proto:
-
-      # Feed raw feature values.
-
-      @tf.function
-      def predict_input_format(features):
-        any_feature = next(iter(features.values()))
-        batch_size = tf.shape(any_feature)[0]
-        return predict_output_format(features, batch_size)
-
-      signatures = {
-          "serving_default": predict_input_format.get_concrete_function(
-              raw_input_signature
-          )
-      }
-
-    else:
-      # Feed binary serialized TensorFlow Example protos.
-      feature_spec = tensorflow_feature_spec(ydf_model, feature_dtypes)
-
-      @tf.function(
-          input_signature=[
-              tf.TensorSpec([None], dtype=tf.string, name="inputs")
-          ]
-      )
-      def predict_input_format(
-          serialized_examples: tf.Tensor,
-      ):
-        batch_size = tf.shape(serialized_examples)[0]
-        features = tf.io.parse_example(serialized_examples, feature_spec)
-        return predict_output_format(features, batch_size)
-
-      signatures = {"serving_default": predict_input_format}
-
-    tf.saved_model.save(tf_module, path, signatures=signatures)
-
-
-def ydf_model_to_tf_function(  # pytype: disable=name-error
-    ydf_model: "generic_model.GenericModel",
-    temp_dir: Optional[str],
-    can_be_saved: bool,
-    squeeze_binary_classification: bool,
-) -> "tensorflow.Module":  # pylint: disable=g-doc-args
-  """Converts a YDF model to a TensorFlow function.
-
-  See GenericModel.to_tensorflow_function for the documentation.
-  """
-
-  tf = import_tensorflow()
-  tfdf = import_tensorflow_decision_forests()
-  tf_op = tfdf.keras.core.tf_op
-
-  # Using prefixes ensure multiple models can be combined in a single
-  # SavedModel.
-  file_prefix = uuid.uuid4().hex[:8] + "_"
-
-  # Save the model to disk and load it as a TensorFlow resource.
-  tmp_dir = tempfile.mkdtemp(dir=temp_dir)
-  try:
-    ydf_model.save(
-        tmp_dir,
-        advanced_options=generic_model.ModelIOOptions(file_prefix=file_prefix),
-    )
-    op_model = tf_op.ModelV2(tmp_dir, verbose=False, file_prefix=file_prefix)
-  finally:
-
-    if not can_be_saved:
-      shutil.rmtree(tmp_dir)
-
-  # If "extract_dim" is not None, the model returns the "extract_dim
-  # dimension of the output of the TF-DF Predict Op.
-  if ydf_model.task() == generic_model.Task.CLASSIFICATION:
-    if squeeze_binary_classification and len(ydf_model.label_classes()) == 2:
-      extract_dim = 1
-    else:
-      extract_dim = None
-  else:
-    # Single dimension outputs (e.g. regression, ranking) is always squeezed.
-    extract_dim = 0
-
-  model_dataspec = ydf_model.data_spec()
-  input_features = ydf_model.input_feature_names()
-
-  # Wrap the model into a tf module.
-  class CallableModule(tf.Module):
-
-    @tf.function
-    def __call__(self, features):
-      return self.call(features)
-
-  callable_module = CallableModule()
-
-  @tf.function
-  def call(features):
-    unrolled_features = _unroll_dict(features, model_dataspec, input_features)
-    dense_predictions = op_model.apply(unrolled_features).dense_predictions
-    assert len(dense_predictions.shape) == 2
-    if extract_dim is not None:
-      return dense_predictions[:, extract_dim]
-    else:
-      return dense_predictions
-
-  callable_module.call = call
-  callable_module.op_model = op_model  # Link model resources
-  return callable_module
-
-
-def import_tensorflow():
-  """Imports the tensorflow module."""
-  try:
-    import tensorflow  # pylint: disable=g-import-not-at-top,import-outside-toplevel # pytype: disable=import-error
-
-    return tensorflow
-  except ImportError as exc:
-    raise ValueError(_ERROR_MESSAGE_MISSING_TF) from exc
-
-
-def import_tensorflow_decision_forests():
-  """Imports the tensorflow decision forests module."""
-  try:
-    import tensorflow_decision_forests as tfdf  # pylint: disable=g-import-not-at-top,import-outside-toplevel # pytype: disable=import-error
-
-    return tfdf
-  except ImportError as exc:
-    raise ValueError(_ERROR_MESSAGE_MISSING_TFDF) from exc
-
-
-def tf_feature_dtype(
-    feature: "generic_model.InputFeature",
-    model_dataspec: ds_pb.DataSpecification,
-    user_dtypes: Dict[str, TFDType],
-) -> TFDType:
-  """Determines the TF Dtype of a feature."""
-
-  return tf_feature_dtype_manual(
-      feature.name,
-      feature.column_idx,
-      model_dataspec,
-      user_dtypes,
-  )
-
-
-def tf_feature_dtype_manual(
-    feature_name: str,
-    column_idx: int,
-    model_dataspec: ds_pb.DataSpecification,
-    user_dtypes: Dict[str, TFDType],
-) -> TFDType:
-  """Determines the TF Dtype of a feature."""
-
-  # User specified dtype.
-  user_dtype = user_dtypes.get(feature_name)
-  if user_dtype is not None:
-    return user_dtype
-
-  tf = import_tensorflow()
-
-  # DType from training dataset
-  column_spec = model_dataspec.columns[column_idx]
-  if column_spec.HasField("dtype"):
-    tf_dtype = mapping_ydf_dtype_to_tf_dtype().get(column_spec.dtype)
-    if tf_dtype is None:
-      raise ValueError(f"Unsupported dtype: {column_spec.dtype}")
-    return tf_dtype
-
-  # DType from feature semantic
-  if column_spec.type == ds_pb.NUMERICAL:
-    return tf.float32
-  elif column_spec.type == ds_pb.CATEGORICAL:
-    return tf.string
-  elif column_spec.type == ds_pb.BOOLEAN:
-    return tf.int64
-  elif column_spec.type == ds_pb.CATEGORICAL_SET:
-    return tf.string
-  else:
-    raise ValueError(f"Unsupported semantic: {column_spec.type}")
-
-
-def tensorflow_raw_input_signature(
-    model: "generic_model.GenericModel",
-    feature_dtypes: Dict[str, TFDType],
-) -> Dict[str, Any]:
-  """A TF input_signature to feed raw feature values into the model."""
-  tf = import_tensorflow()
-
-  model_dataspec = model.data_spec()
-  input_features = model.input_features()
-  input_feature_names_set = set(f.name for f in input_features)
-
-  input_signature = {}
-
-  # Multi-dim features
-  for unstacked in model_dataspec.unstackeds:
-    if unstacked.size == 0:
-      raise RuntimeError("Empty unstacked")
-    sub_names = dataset_io.unrolled_feature_names(
-        unstacked.original_name, unstacked.size
-    )
-    # Note: The "input_features" contain unrolled feature names.
-    if sub_names[0] not in input_feature_names_set:
-      continue
-
-    tf_dtype = tf_feature_dtype_manual(
-        unstacked.original_name,
-        unstacked.begin_column_idx,
-        model_dataspec,
-        feature_dtypes,
-    )
-
-    if unstacked.type in [
-        ds_pb.ColumnType.NUMERICAL,
-        ds_pb.ColumnType.DISCRETIZED_NUMERICAL,
-        ds_pb.ColumnType.CATEGORICAL,
-        ds_pb.ColumnType.BOOLEAN,
-    ]:
-      dst_feature = tf.TensorSpec(
-          shape=[None, unstacked.size],
-          dtype=tf_dtype,
-          name=unstacked.original_name,
-      )
-    else:
-      raise ValueError(
-          f"Unsupported semantic {unstacked.type} for multi-dim feature"
-          f" {unstacked.original_name!r}"
-      )
-    input_signature[unstacked.original_name] = dst_feature
-
-  # Single-dim features
-  for src_feature in input_features:
-    column = model_dataspec.columns[src_feature.column_idx]
-    if column.is_unstacked:
-      continue
-
-    tf_dtype = tf_feature_dtype(src_feature, model_dataspec, feature_dtypes)
-
-    if column.type in [
-        ds_pb.ColumnType.NUMERICAL,
-        ds_pb.ColumnType.DISCRETIZED_NUMERICAL,
-        ds_pb.ColumnType.CATEGORICAL,
-        ds_pb.ColumnType.BOOLEAN,
-    ]:
-      dst_feature = tf.TensorSpec(
-          shape=[None], dtype=tf_dtype, name=src_feature.name
-      )
-    elif column.type == ds_pb.ColumnType.CATEGORICAL_SET:
-      dst_feature = tf.RaggedTensorSpec(
-          shape=[None, None],
-          dtype=tf_dtype,
-      )
-    else:
-      raise ValueError(
-          f"Unsupported semantic {column.type} for single-dim feature"
-          f" {src_feature.name!r}"
-      )
-    input_signature[src_feature.name] = dst_feature
-
-  return input_signature
-
-
-def tensorflow_feature_spec(
-    model: "generic_model.GenericModel",
-    feature_dtypes: Dict[str, TFDType],
-) -> Dict[str, Any]:
-  """A TF feature spec used to deserialize tf example protos."""
-  tf = import_tensorflow()
-
-  def missing_value(tf_dtype):
-    """Representation of a missing value; if possible."""
-    # Follow the missing value representation described in ydf.Semantic.
-    if tf_dtype.is_floating:
-      return math.nan
-    elif tf_dtype == tf.string:
-      return ""
-    else:
-      return None  # Missing value not allowed
-
-  model_dataspec = model.data_spec()
-  input_features = model.input_features()
-  input_feature_names_set = set(f.name for f in input_features)
-
-  feature_spec = {}
-
-  # Multi-dim features
-  for unstacked in model_dataspec.unstackeds:
-    if unstacked.size == 0:
-      raise RuntimeError("Empty unstacked")
-    sub_names = dataset_io.unrolled_feature_names(
-        unstacked.original_name, unstacked.size
-    )
-    # Note: The "input_features" contain unrolled feature names.
-    if sub_names[0] not in input_feature_names_set:
-      continue
-
-    tf_dtype = tf_feature_dtype_manual(
-        unstacked.original_name,
-        unstacked.begin_column_idx,
-        model_dataspec,
-        feature_dtypes,
-    )
-
-    tfe_dtype = mapping_tf_dtype_to_tf_example_dtype().get(tf_dtype)
-    if tfe_dtype is None:
-      raise ValueError(f"Unsupported dtype: {tf_dtype}")
-
-    if unstacked.type in [
-        ds_pb.ColumnType.NUMERICAL,
-        ds_pb.ColumnType.DISCRETIZED_NUMERICAL,
-        ds_pb.ColumnType.CATEGORICAL,
-        ds_pb.ColumnType.BOOLEAN,
-    ]:
-      effective_dtype = tfe_dtype
-      effective_missing_value = missing_value(effective_dtype)
-      if effective_missing_value is None:
-        multidim_missing_value = None
-      else:
-        multidim_missing_value = [
-            missing_value(effective_dtype)
-        ] * unstacked.size
-
-      dst_feature = tf.io.FixedLenFeature(
-          shape=[unstacked.size],
-          dtype=effective_dtype,
-          default_value=multidim_missing_value,
-      )
-    else:
-      raise ValueError(
-          f"Unsupported semantic {unstacked.type} for multi-dim feature"
-          f" {unstacked.original_name!r}"
-      )
-    feature_spec[unstacked.original_name] = dst_feature
-
-  # Single-dim features
-  for src_feature in input_features:
-    column = model_dataspec.columns[src_feature.column_idx]
-    if column.is_unstacked:
-      continue
-
-    tf_dtype = tf_feature_dtype(src_feature, model_dataspec, feature_dtypes)
-
-    tfe_dtype = mapping_tf_dtype_to_tf_example_dtype().get(tf_dtype)
-    if tfe_dtype is None:
-      raise ValueError(f"Unsupported dtype: {tf_dtype}")
-
-    if column.type in [
-        ds_pb.ColumnType.NUMERICAL,
-        ds_pb.ColumnType.DISCRETIZED_NUMERICAL,
-        ds_pb.ColumnType.CATEGORICAL,
-        ds_pb.ColumnType.BOOLEAN,
-    ]:
-      effective_dtype = tfe_dtype
-      dst_feature = tf.io.FixedLenFeature(
-          shape=[],
-          dtype=effective_dtype,
-          default_value=missing_value(effective_dtype),
-      )
-    elif src_feature.semantic == dataspec.Semantic.CATEGORICAL_SET:
-      dst_feature = tf.io.RaggedFeature(
-          dtype=tfe_dtype, row_splits_dtype=tf.dtypes.int64
-      )
-    else:
-      raise ValueError(f"Unsupported semantic: {src_feature.semantic}")
-    feature_spec[src_feature.name] = dst_feature
-
-  return feature_spec
-
-
-def _unroll_dict(
-    src: Dict[str, TFTensor],
-    data_spec: ds_pb.DataSpecification,
-    input_features: Sequence[str],
-) -> Dict[str, TFTensor]:
-  """Unrolls multi-dimensional features.
-
-  This function mirrors "_unroll_dict" in "dataset/io/dataset_io.py" which
-  unrolls numpy arrays automatically (i.e., without a dataspec).
-
-  Args:
-    src: Dictionary of single and multi-dimensional values.
-    data_spec: A dataspec.
-    input_features: Input features to unroll.
-
-  Returns:
-    Dictionary containing only single-dimensional values.
-  """
-
-  try:
-    dst = {}
-    # Unroll multi-dim features.
-    input_features_set = set(input_features)
-    for unstacked in data_spec.unstackeds:
-      sub_names = dataset_io.unrolled_feature_names(
-          unstacked.original_name, unstacked.size
-      )
-      if sub_names[0] not in input_features_set:
-        continue
-      value = src[unstacked.original_name]
-      for dim_idx, sub_name in enumerate(sub_names):
-        dst[sub_name] = value[:, dim_idx]
-
-    # Copy single-dim features
-    for column in data_spec.columns:
-      if column.is_unstacked:
-        continue
-      if column.name not in input_features_set:
-        continue
-      dst[column.name] = src[column.name]
-    return dst
-  except (KeyError, ValueError) as exc:
-    exc.add_note(
-        f"While looking for unrolled features {input_features!r} in the tensor"
-        f" dictionary {src!r}"
-    )
-    raise exc
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Utilities to export TF models."""
+
+import math
+import shutil
+import tempfile
+from typing import Any, Callable, Dict, Literal, Optional, Sequence
+import uuid
+
+from ydf.proto.dataset import data_spec_pb2 as ds_pb
+from ydf.dataset import dataspec
+from ydf.dataset.io import dataset_io
+from ydf.model import generic_model
+from ydf.utils import log
+
+_ERROR_MESSAGE_MISSING_TF = (
+    '"tensorflow" is needed by this function. Make sure it'
+    " installed and try again. If using pip, run `pip install"
+    " tensorflow`. If using Bazel/Blaze, add a dependency to"
+    " TensorFlow."
+)
+
+_ERROR_MESSAGE_MISSING_TFDF = (
+    '"tensorflow_decision_forests" is needed by this function. Make sure it'
+    " installed and try again. If using pip, run `pip install"
+    " tensorflow_decision_forests`. If using Bazel/Blaze, add a dependency to"
+    " TensorFlow Decision Forests."
+)
+
+TFDType = Any  # TensorFlow DType e.g. tf.float32
+TFTensor = Any  # A TensorFlow Tensor i.e. tensorflow.Tensor
+
+# Mapping between YDF dtype and TF dtypes.
+_YDF_DTYPE_TO_TF_DTYPE: Dict["ds_pb.DType", TFDType] = None
+
+# Mapping TF dtypes to the TF dtype compatible with tensorflow example.
+# Note that tensorflow example proto only support tf.int64, tf.float32,
+# and tf.string dtypes.
+_TF_DTYPE_TO_TF_EXAMPLE_DTYPE: Dict[TFDType, TFDType] = None
+
+
+def mapping_ydf_dtype_to_tf_dtype() -> Dict["ds_pb.DType", TFDType]:
+  """Mapping between YDF dtype and TF dtypes."""
+
+  global _YDF_DTYPE_TO_TF_DTYPE
+  tf = import_tensorflow()
+  if _YDF_DTYPE_TO_TF_DTYPE is None:
+    _YDF_DTYPE_TO_TF_DTYPE = {
+        ds_pb.DType.DTYPE_INT8: tf.int8,
+        ds_pb.DType.DTYPE_INT16: tf.int16,
+        ds_pb.DType.DTYPE_INT32: tf.int32,
+        ds_pb.DType.DTYPE_INT64: tf.int64,
+        ds_pb.DType.DTYPE_UINT8: tf.uint8,
+        ds_pb.DType.DTYPE_UINT16: tf.uint16,
+        ds_pb.DType.DTYPE_UINT32: tf.uint32,
+        ds_pb.DType.DTYPE_UINT64: tf.uint64,
+        ds_pb.DType.DTYPE_FLOAT16: tf.float16,
+        ds_pb.DType.DTYPE_FLOAT32: tf.float32,
+        ds_pb.DType.DTYPE_FLOAT64: tf.float64,
+        ds_pb.DType.DTYPE_BOOL: tf.bool,
+        ds_pb.DType.DTYPE_BYTES: tf.string,
+    }
+  return _YDF_DTYPE_TO_TF_DTYPE
+
+
+def mapping_tf_dtype_to_tf_example_dtype() -> Dict[TFDType, TFDType]:
+  """Mapping TF dtypes to the TF dtype compatible with tensorflow example."""
+
+  global _TF_DTYPE_TO_TF_EXAMPLE_DTYPE
+  tf = import_tensorflow()
+  if _TF_DTYPE_TO_TF_EXAMPLE_DTYPE is None:
+    _TF_DTYPE_TO_TF_EXAMPLE_DTYPE = {
+        tf.int8: tf.int64,
+        tf.int16: tf.int64,
+        tf.int32: tf.int64,
+        tf.int64: tf.int64,
+        tf.uint8: tf.int64,
+        tf.uint16: tf.int64,
+        tf.uint32: tf.int64,
+        tf.uint64: tf.int64,
+        tf.float16: tf.float32,
+        tf.float32: tf.float32,
+        tf.float64: tf.float32,
+        tf.bool: tf.int64,
+        tf.string: tf.string,
+    }
+  return _TF_DTYPE_TO_TF_EXAMPLE_DTYPE
+
+
+def ydf_model_to_tensorflow_saved_model(
+    ydf_model: "generic_model.GenericModel",
+    path: str,
+    input_model_signature_fn: Any,
+    mode: Literal["keras", "tf"],
+    feature_dtypes: Dict[str, TFDType],
+    servo_api: bool,
+    feed_example_proto: bool,
+    pre_processing: Optional[Callable],  # pylint: disable=g-bare-generic
+    post_processing: Optional[Callable],  # pylint: disable=g-bare-generic
+    temp_dir: Optional[str],
+):  # pylint: disable=g-doc-args
+  """Exports the model as a TensorFlow Saved model.
+
+  See GenericModel.to_tensorflow_saved_model for the documentation.
+  """
+  if mode == "keras":
+    for value, name, expected in [
+        (feature_dtypes, "feature_dtypes", {}),
+        (servo_api, "servo_api", False),
+        (feed_example_proto, "feed_example_proto", False),
+        (pre_processing, "pre_processing", None),
+        (post_processing, "post_processing", None),
+    ]:
+      if value != expected:
+        raise ValueError(f"{name!r} is not supported for `keras` mode.")
+    ydf_model_to_tensorflow_saved_model_keras_mode(
+        ydf_model=ydf_model,
+        path=path,
+        input_model_signature_fn=input_model_signature_fn,
+        temp_dir=temp_dir,
+    )
+
+  elif mode == "tf":
+    if input_model_signature_fn is not None:
+      raise ValueError(
+          "input_model_signature_fn is not supported for `tf` mode."
+      )
+    ydf_model_to_tensorflow_saved_model_tf_mode(
+        ydf_model=ydf_model,
+        path=path,
+        feature_dtypes=feature_dtypes,
+        servo_api=servo_api,
+        feed_example_proto=feed_example_proto,
+        pre_processing=pre_processing,
+        post_processing=post_processing,
+        temp_dir=temp_dir,
+    )
+  else:
+    raise ValueError(f"Invalid mode: {mode}")
+
+
+def ydf_model_to_tensorflow_saved_model_keras_mode(
+    ydf_model: "generic_model.GenericModel",
+    path: str,
+    input_model_signature_fn: Any,
+    temp_dir: Optional[str],
+):  # pylint: disable=g-doc-args
+  tfdf = import_tensorflow_decision_forests()
+
+  # Do not pass input_model_signature_fn if it is None.
+  not_none_params = {}
+  if input_model_signature_fn is not None:
+    not_none_params["input_model_signature_fn"] = input_model_signature_fn
+  with tempfile.TemporaryDirectory(dir=temp_dir) as tmpdirname:
+    ydf_model.save(tmpdirname)
+    tfdf.keras.yggdrasil_model_to_keras_model(
+        src_path=tmpdirname,
+        dst_path=path,
+        verbose=log.current_log_level(),
+        **not_none_params,
+    )
+
+
+def ydf_model_to_tensorflow_saved_model_tf_mode(
+    ydf_model: "generic_model.GenericModel",
+    path: str,
+    feature_dtypes: Dict[str, TFDType],
+    servo_api: bool,
+    feed_example_proto: bool,
+    pre_processing: Optional[Callable],  # pylint: disable=g-bare-generic
+    post_processing: Optional[Callable],  # pylint: disable=g-bare-generic
+    temp_dir: Optional[str],
+):  # pylint: disable=g-doc-args
+
+  tf = import_tensorflow()
+
+  # The temporary files should remain available until the call to
+  # "tf.saved_model.save"
+  with tempfile.TemporaryDirectory(dir=temp_dir) as effective_temp_dir:
+
+    tf_module = ydf_model.to_tensorflow_function(
+        temp_dir=effective_temp_dir,
+        squeeze_binary_classification=not servo_api,
+    )
+
+    # Store pre / post processing operations
+    # Note: Storing the raw variable allows for pre/post-processing to be
+    # TensorFlow modules with resources.
+    tf_module.raw_pre_processing = pre_processing
+    tf_module.raw_post_processing = post_processing
+    tf_module.pre_processing = tf.function(pre_processing or (lambda x: x))
+    tf_module.post_processing = tf.function(post_processing or (lambda x: x))
+
+    # Apply pre / post processing to call.
+    def call(features):
+      return tf_module.post_processing(
+          tf_module.raw_call(tf_module.pre_processing(features))
+      )
+
+    tf_module.raw_call = tf_module.call
+    tf_module.call = call
+
+    # Trace the call function.
+    # Note: "tf.saved_model.save" can only export functions that have been
+    # traced.
+    raw_input_signature = tensorflow_raw_input_signature(
+        ydf_model, feature_dtypes
+    )
+    tf_module.__call__.get_concrete_function(raw_input_signature)
+
+    # Output format
+    if servo_api:
+
+      if ydf_model.task() == generic_model.Task.CLASSIFICATION:
+        label_classes = ydf_model.label_classes()
+
+        # "classify" Servo API
+        def predict_output_format(features, batch_size):
+          raw_output = tf_module(features)
+          batched_label_classes = tf.broadcast_to(
+              input=tf.constant(label_classes),
+              shape=(batch_size, len(label_classes)),
+          )
+          return {"classes": batched_label_classes, "scores": raw_output}
+
+      elif ydf_model.task() == generic_model.Task.REGRESSION:
+
+        # "regress" Servo API
+        def predict_output_format(features, batch_size):
+          del batch_size
+          raw_output = tf_module(features)
+          return {"outputs": raw_output}
+
+      else:
+        raise ValueError(
+            f"servo_api=True non supported for task {ydf_model.task()!r}"
+        )
+
+    else:
+
+      # "predict" Servo API i.e. raw output
+      # Note: "signature" outputs need to be dictionaries.
+      def predict_output_format(features, batch_size):
+        del batch_size
+        return {"output": tf_module(features)}
+
+    # Input feature formats
+    if not feed_example_proto:
+
+      # Feed raw feature values.
+
+      @tf.function
+      def predict_input_format(features):
+        any_feature = next(iter(features.values()))
+        batch_size = tf.shape(any_feature)[0]
+        return predict_output_format(features, batch_size)
+
+      signatures = {
+          "serving_default": predict_input_format.get_concrete_function(
+              raw_input_signature
+          )
+      }
+
+    else:
+      # Feed binary serialized TensorFlow Example protos.
+      feature_spec = tensorflow_feature_spec(ydf_model, feature_dtypes)
+
+      @tf.function(
+          input_signature=[
+              tf.TensorSpec([None], dtype=tf.string, name="inputs")
+          ]
+      )
+      def predict_input_format(
+          serialized_examples: tf.Tensor,
+      ):
+        batch_size = tf.shape(serialized_examples)[0]
+        features = tf.io.parse_example(serialized_examples, feature_spec)
+        return predict_output_format(features, batch_size)
+
+      signatures = {"serving_default": predict_input_format}
+
+    tf.saved_model.save(tf_module, path, signatures=signatures)
+
+
+def ydf_model_to_tf_function(  # pytype: disable=name-error
+    ydf_model: "generic_model.GenericModel",
+    temp_dir: Optional[str],
+    can_be_saved: bool,
+    squeeze_binary_classification: bool,
+) -> "tensorflow.Module":  # pylint: disable=g-doc-args
+  """Converts a YDF model to a TensorFlow function.
+
+  See GenericModel.to_tensorflow_function for the documentation.
+  """
+
+  tf = import_tensorflow()
+  tfdf = import_tensorflow_decision_forests()
+  tf_op = tfdf.keras.core.tf_op
+
+  # Using prefixes ensure multiple models can be combined in a single
+  # SavedModel.
+  file_prefix = uuid.uuid4().hex[:8] + "_"
+
+  # Save the model to disk and load it as a TensorFlow resource.
+  tmp_dir = tempfile.mkdtemp(dir=temp_dir)
+  try:
+    ydf_model.save(
+        tmp_dir,
+        advanced_options=generic_model.ModelIOOptions(file_prefix=file_prefix),
+    )
+    op_model = tf_op.ModelV2(tmp_dir, verbose=False, file_prefix=file_prefix)
+  finally:
+
+    if not can_be_saved:
+      shutil.rmtree(tmp_dir)
+
+  # If "extract_dim" is not None, the model returns the "extract_dim
+  # dimension of the output of the TF-DF Predict Op.
+  if ydf_model.task() == generic_model.Task.CLASSIFICATION:
+    if squeeze_binary_classification and len(ydf_model.label_classes()) == 2:
+      extract_dim = 1
+    else:
+      extract_dim = None
+  else:
+    # Single dimension outputs (e.g. regression, ranking) is always squeezed.
+    extract_dim = 0
+
+  model_dataspec = ydf_model.data_spec()
+  input_features = ydf_model.input_feature_names()
+
+  # Wrap the model into a tf module.
+  class CallableModule(tf.Module):
+
+    @tf.function
+    def __call__(self, features):
+      return self.call(features)
+
+  callable_module = CallableModule()
+
+  @tf.function
+  def call(features):
+    unrolled_features = _unroll_dict(features, model_dataspec, input_features)
+    dense_predictions = op_model.apply(unrolled_features).dense_predictions
+    assert len(dense_predictions.shape) == 2
+    if extract_dim is not None:
+      return dense_predictions[:, extract_dim]
+    else:
+      return dense_predictions
+
+  callable_module.call = call
+  callable_module.op_model = op_model  # Link model resources
+  return callable_module
+
+
+def import_tensorflow():
+  """Imports the tensorflow module."""
+  try:
+    import tensorflow  # pylint: disable=g-import-not-at-top,import-outside-toplevel # pytype: disable=import-error
+
+    return tensorflow
+  except ImportError as exc:
+    raise ValueError(_ERROR_MESSAGE_MISSING_TF) from exc
+
+
+def import_tensorflow_decision_forests():
+  """Imports the tensorflow decision forests module."""
+  try:
+    import tensorflow_decision_forests as tfdf  # pylint: disable=g-import-not-at-top,import-outside-toplevel # pytype: disable=import-error
+
+    return tfdf
+  except ImportError as exc:
+    raise ValueError(_ERROR_MESSAGE_MISSING_TFDF) from exc
+
+
+def tf_feature_dtype(
+    feature: "generic_model.InputFeature",
+    model_dataspec: ds_pb.DataSpecification,
+    user_dtypes: Dict[str, TFDType],
+) -> TFDType:
+  """Determines the TF Dtype of a feature."""
+
+  return tf_feature_dtype_manual(
+      feature.name,
+      feature.column_idx,
+      model_dataspec,
+      user_dtypes,
+  )
+
+
+def tf_feature_dtype_manual(
+    feature_name: str,
+    column_idx: int,
+    model_dataspec: ds_pb.DataSpecification,
+    user_dtypes: Dict[str, TFDType],
+) -> TFDType:
+  """Determines the TF Dtype of a feature."""
+
+  # User specified dtype.
+  user_dtype = user_dtypes.get(feature_name)
+  if user_dtype is not None:
+    return user_dtype
+
+  tf = import_tensorflow()
+
+  # DType from training dataset
+  column_spec = model_dataspec.columns[column_idx]
+  if column_spec.HasField("dtype"):
+    tf_dtype = mapping_ydf_dtype_to_tf_dtype().get(column_spec.dtype)
+    if tf_dtype is None:
+      raise ValueError(f"Unsupported dtype: {column_spec.dtype}")
+    return tf_dtype
+
+  # DType from feature semantic
+  if column_spec.type == ds_pb.NUMERICAL:
+    return tf.float32
+  elif column_spec.type == ds_pb.CATEGORICAL:
+    return tf.string
+  elif column_spec.type == ds_pb.BOOLEAN:
+    return tf.int64
+  elif column_spec.type == ds_pb.CATEGORICAL_SET:
+    return tf.string
+  else:
+    raise ValueError(f"Unsupported semantic: {column_spec.type}")
+
+
+def tensorflow_raw_input_signature(
+    model: "generic_model.GenericModel",
+    feature_dtypes: Dict[str, TFDType],
+) -> Dict[str, Any]:
+  """A TF input_signature to feed raw feature values into the model."""
+  tf = import_tensorflow()
+
+  model_dataspec = model.data_spec()
+  input_features = model.input_features()
+  input_feature_names_set = set(f.name for f in input_features)
+
+  input_signature = {}
+
+  # Multi-dim features
+  for unstacked in model_dataspec.unstackeds:
+    if unstacked.size == 0:
+      raise RuntimeError("Empty unstacked")
+    sub_names = dataset_io.unrolled_feature_names(
+        unstacked.original_name, unstacked.size
+    )
+    # Note: The "input_features" contain unrolled feature names.
+    if sub_names[0] not in input_feature_names_set:
+      continue
+
+    tf_dtype = tf_feature_dtype_manual(
+        unstacked.original_name,
+        unstacked.begin_column_idx,
+        model_dataspec,
+        feature_dtypes,
+    )
+
+    if unstacked.type in [
+        ds_pb.ColumnType.NUMERICAL,
+        ds_pb.ColumnType.DISCRETIZED_NUMERICAL,
+        ds_pb.ColumnType.CATEGORICAL,
+        ds_pb.ColumnType.BOOLEAN,
+    ]:
+      dst_feature = tf.TensorSpec(
+          shape=[None, unstacked.size],
+          dtype=tf_dtype,
+          name=unstacked.original_name,
+      )
+    else:
+      raise ValueError(
+          f"Unsupported semantic {unstacked.type} for multi-dim feature"
+          f" {unstacked.original_name!r}"
+      )
+    input_signature[unstacked.original_name] = dst_feature
+
+  # Single-dim features
+  for src_feature in input_features:
+    column = model_dataspec.columns[src_feature.column_idx]
+    if column.is_unstacked:
+      continue
+
+    tf_dtype = tf_feature_dtype(src_feature, model_dataspec, feature_dtypes)
+
+    if column.type in [
+        ds_pb.ColumnType.NUMERICAL,
+        ds_pb.ColumnType.DISCRETIZED_NUMERICAL,
+        ds_pb.ColumnType.CATEGORICAL,
+        ds_pb.ColumnType.BOOLEAN,
+    ]:
+      dst_feature = tf.TensorSpec(
+          shape=[None], dtype=tf_dtype, name=src_feature.name
+      )
+    elif column.type == ds_pb.ColumnType.CATEGORICAL_SET:
+      dst_feature = tf.RaggedTensorSpec(
+          shape=[None, None],
+          dtype=tf_dtype,
+      )
+    else:
+      raise ValueError(
+          f"Unsupported semantic {column.type} for single-dim feature"
+          f" {src_feature.name!r}"
+      )
+    input_signature[src_feature.name] = dst_feature
+
+  return input_signature
+
+
+def tensorflow_feature_spec(
+    model: "generic_model.GenericModel",
+    feature_dtypes: Dict[str, TFDType],
+) -> Dict[str, Any]:
+  """A TF feature spec used to deserialize tf example protos."""
+  tf = import_tensorflow()
+
+  def missing_value(tf_dtype):
+    """Representation of a missing value; if possible."""
+    # Follow the missing value representation described in ydf.Semantic.
+    if tf_dtype.is_floating:
+      return math.nan
+    elif tf_dtype == tf.string:
+      return ""
+    else:
+      return None  # Missing value not allowed
+
+  model_dataspec = model.data_spec()
+  input_features = model.input_features()
+  input_feature_names_set = set(f.name for f in input_features)
+
+  feature_spec = {}
+
+  # Multi-dim features
+  for unstacked in model_dataspec.unstackeds:
+    if unstacked.size == 0:
+      raise RuntimeError("Empty unstacked")
+    sub_names = dataset_io.unrolled_feature_names(
+        unstacked.original_name, unstacked.size
+    )
+    # Note: The "input_features" contain unrolled feature names.
+    if sub_names[0] not in input_feature_names_set:
+      continue
+
+    tf_dtype = tf_feature_dtype_manual(
+        unstacked.original_name,
+        unstacked.begin_column_idx,
+        model_dataspec,
+        feature_dtypes,
+    )
+
+    tfe_dtype = mapping_tf_dtype_to_tf_example_dtype().get(tf_dtype)
+    if tfe_dtype is None:
+      raise ValueError(f"Unsupported dtype: {tf_dtype}")
+
+    if unstacked.type in [
+        ds_pb.ColumnType.NUMERICAL,
+        ds_pb.ColumnType.DISCRETIZED_NUMERICAL,
+        ds_pb.ColumnType.CATEGORICAL,
+        ds_pb.ColumnType.BOOLEAN,
+    ]:
+      effective_dtype = tfe_dtype
+      effective_missing_value = missing_value(effective_dtype)
+      if effective_missing_value is None:
+        multidim_missing_value = None
+      else:
+        multidim_missing_value = [
+            missing_value(effective_dtype)
+        ] * unstacked.size
+
+      dst_feature = tf.io.FixedLenFeature(
+          shape=[unstacked.size],
+          dtype=effective_dtype,
+          default_value=multidim_missing_value,
+      )
+    else:
+      raise ValueError(
+          f"Unsupported semantic {unstacked.type} for multi-dim feature"
+          f" {unstacked.original_name!r}"
+      )
+    feature_spec[unstacked.original_name] = dst_feature
+
+  # Single-dim features
+  for src_feature in input_features:
+    column = model_dataspec.columns[src_feature.column_idx]
+    if column.is_unstacked:
+      continue
+
+    tf_dtype = tf_feature_dtype(src_feature, model_dataspec, feature_dtypes)
+
+    tfe_dtype = mapping_tf_dtype_to_tf_example_dtype().get(tf_dtype)
+    if tfe_dtype is None:
+      raise ValueError(f"Unsupported dtype: {tf_dtype}")
+
+    if column.type in [
+        ds_pb.ColumnType.NUMERICAL,
+        ds_pb.ColumnType.DISCRETIZED_NUMERICAL,
+        ds_pb.ColumnType.CATEGORICAL,
+        ds_pb.ColumnType.BOOLEAN,
+    ]:
+      effective_dtype = tfe_dtype
+      dst_feature = tf.io.FixedLenFeature(
+          shape=[],
+          dtype=effective_dtype,
+          default_value=missing_value(effective_dtype),
+      )
+    elif src_feature.semantic == dataspec.Semantic.CATEGORICAL_SET:
+      dst_feature = tf.io.RaggedFeature(
+          dtype=tfe_dtype, row_splits_dtype=tf.dtypes.int64
+      )
+    else:
+      raise ValueError(f"Unsupported semantic: {src_feature.semantic}")
+    feature_spec[src_feature.name] = dst_feature
+
+  return feature_spec
+
+
+def _unroll_dict(
+    src: Dict[str, TFTensor],
+    data_spec: ds_pb.DataSpecification,
+    input_features: Sequence[str],
+) -> Dict[str, TFTensor]:
+  """Unrolls multi-dimensional features.
+
+  This function mirrors "_unroll_dict" in "dataset/io/dataset_io.py" which
+  unrolls numpy arrays automatically (i.e., without a dataspec).
+
+  Args:
+    src: Dictionary of single and multi-dimensional values.
+    data_spec: A dataspec.
+    input_features: Input features to unroll.
+
+  Returns:
+    Dictionary containing only single-dimensional values.
+  """
+
+  try:
+    dst = {}
+    # Unroll multi-dim features.
+    input_features_set = set(input_features)
+    for unstacked in data_spec.unstackeds:
+      sub_names = dataset_io.unrolled_feature_names(
+          unstacked.original_name, unstacked.size
+      )
+      if sub_names[0] not in input_features_set:
+        continue
+      value = src[unstacked.original_name]
+      for dim_idx, sub_name in enumerate(sub_names):
+        dst[sub_name] = value[:, dim_idx]
+
+    # Copy single-dim features
+    for column in data_spec.columns:
+      if column.is_unstacked:
+        continue
+      if column.name not in input_features_set:
+        continue
+      dst[column.name] = src[column.name]
+    return dst
+  except (KeyError, ValueError) as exc:
+    exc.add_note(
+        f"While looking for unrolled features {input_features!r} in the tensor"
+        f" dictionary {src!r}"
+    )
+    raise exc
```

## ydf/model/generic_model.py

```diff
@@ -1,987 +1,987 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Generic YDF model definition."""
-
-import dataclasses
-import enum
-import os
-from typing import Any, Callable, Dict, List, Literal, Optional, Sequence, Tuple, TypeVar, Union
-
-from absl import logging
-import numpy as np
-
-from yggdrasil_decision_forests.dataset import data_spec_pb2
-from yggdrasil_decision_forests.metric import metric_pb2
-from yggdrasil_decision_forests.model import abstract_model_pb2
-from ydf.cc import ydf
-from ydf.dataset import dataset
-from ydf.dataset import dataspec
-from ydf.metric import metric
-from ydf.model import analysis
-from ydf.model import export_tf
-from ydf.model import model_metadata
-from ydf.model import optimizer_logs
-from ydf.model import template_cpp_export
-from ydf.utils import html
-from ydf.utils import log
-from yggdrasil_decision_forests.utils import model_analysis_pb2
-
-
-@enum.unique
-class Task(enum.Enum):
-  """Task solved by a model.
-
-  Usage example:
-
-  ```python
-  learner = ydf.RandomForestLearner(label="income",
-                                    task=ydf.Task.CLASSIFICATION)
-  model = learner.train(dataset)
-  assert model.task() == ydf.Task.CLASSIFICATION
-  ```
-  Not all tasks are compatible with all learners and/or hyperparameters. For
-  more information, please see the documentation for tutorials on the individual
-  tasks.
-
-
-  Attributes:
-    CLASSIFICATION: Predict a categorical label i.e., an item of an enumeration.
-    REGRESSION: Predict a numerical label i.e., a quantity.
-    RANKING: Rank items by label values. The label is expected to be between 0
-      and 4 with NDCG semantic (0: completely unrelated, 4: perfect match).
-    CATEGORICAL_UPLIFT: Predicts the incremental impact of a treatment on a
-      categorical outcome.
-    NUMERICAL_UPLIFT: Predicts the incremental impact of a treatment on a
-      numerical outcome.
-  """
-
-  CLASSIFICATION = "CLASSIFICATION"
-  REGRESSION = "REGRESSION"
-  RANKING = "RANKING"
-  CATEGORICAL_UPLIFT = "CATEGORICAL_UPLIFT"
-  NUMERICAL_UPLIFT = "NUMERICAL_UPLIFT"
-
-  def _to_proto_type(self) -> abstract_model_pb2.Task:
-    if self in TASK_TO_PROTO:
-      return TASK_TO_PROTO[self]
-    else:
-      raise NotImplementedError(f"Unsupported task {self}")
-
-  @classmethod
-  def _from_proto_type(cls, task: abstract_model_pb2.Task):
-    task = PROTO_TO_TASK.get(task)
-    if task is None:
-      raise NotImplementedError(f"Unsupported task {task}")
-    return task
-
-
-# Mappings between task enum in python and in protobuffer and vice versa.
-TASK_TO_PROTO = {
-    Task.CLASSIFICATION: abstract_model_pb2.CLASSIFICATION,
-    Task.REGRESSION: abstract_model_pb2.REGRESSION,
-    Task.RANKING: abstract_model_pb2.RANKING,
-    Task.CATEGORICAL_UPLIFT: abstract_model_pb2.CATEGORICAL_UPLIFT,
-    Task.NUMERICAL_UPLIFT: abstract_model_pb2.NUMERICAL_UPLIFT,
-}
-PROTO_TO_TASK = {v: k for k, v in TASK_TO_PROTO.items()}
-
-
-@dataclasses.dataclass(frozen=True)
-class ModelIOOptions:
-  """Advanced options for saving and loading YDF models.
-
-  Attributes:
-      file_prefix: Optional prefix for the model. File prefixes allow multiple
-        models to exist in the same folder. Doing so is heavily DISCOURAGED
-        outside of edge cases. When loading a model, the prefix, if not
-        specified, is auto-detected if possible. When saving a model, the empty
-        string is used as file prefix unless it is explicitly specified.
-  """
-
-  file_prefix: Optional[str] = None
-
-
-@enum.unique
-class NodeFormat(enum.Enum):
-  # pyformat: disable
-  """Serialization format for a model.
-
-  Determines the storage format for nodes.
-
-  Attributes:
-    BLOB_SEQUENCE: Default format for the public version of YDF.
-  """
-  # pyformat: enable
-
-  BLOB_SEQUENCE = enum.auto()
-
-
-@dataclasses.dataclass
-class InputFeature:
-  """An input feature of a model.
-
-  Attributes:
-    name: Feature name. Unique for a model.
-    semantic: Semantic of the feature.
-    column_idx: Index of the column corresponding to the feature in the
-      dataspec.
-  """
-
-  name: str
-  semantic: dataspec.Semantic
-  column_idx: int
-
-
-class GenericModel:
-  """Abstract superclass for all YDF models."""
-
-  def __init__(self, raw_model: ydf.GenericCCModel):
-    self._model = raw_model
-
-  def name(self) -> str:
-    """Returns the name of the model type."""
-    return self._model.name()
-
-  def task(self) -> Task:
-    """Task solved by the model."""
-    return Task._from_proto_type(self._model.task())  # pylint: disable=protected-access
-
-  def metadata(self) -> model_metadata.ModelMetadata:
-    """Metadata associated with the model.
-
-    A model's metadata contains information stored with the model that does not
-    influence the model's predictions (e.g. data created). When distributing a
-    model for wide release, it may be useful to clear / modify the model
-    metadata with `model.set_metadata(ydf.ModelMetadata())`.
-
-    Returns:
-      The model's metadata.
-    """
-    return model_metadata.ModelMetadata._from_proto_type(self._model.metadata())  # pylint:disable=protected-access
-
-  def set_metadata(self, metadata: model_metadata.ModelMetadata):
-    """Sets the model metadata."""
-    self._model.set_metadata(metadata._to_proto_type())  # pylint:disable=protected-access
-
-  def describe(
-      self,
-      output_format: Literal["auto", "text", "notebook", "html"] = "auto",
-      full_details: bool = False,
-  ) -> Union[str, html.HtmlNotebookDisplay]:
-    """Description of the model.
-
-    Args:
-      output_format: Format of the display: - auto: Use the "notebook" format if
-        executed in an IPython notebook / Colab. Otherwise, use the "text"
-        format. - text: Text description of the model. - html: Html description
-        of the model. - notebook: Html description of the model displayed in a
-        notebook cell.
-      full_details: Should the full model be printed. This can be large.
-
-    Returns:
-      The model description.
-    """
-
-    if output_format == "auto":
-      output_format = "text" if log.is_direct_output() else "notebook"
-
-    with log.cc_log_context():
-      description = self._model.Describe(full_details, output_format == "text")
-      if output_format == "notebook":
-        return html.HtmlNotebookDisplay(description)
-      else:
-        return description
-
-  def data_spec(self) -> data_spec_pb2.DataSpecification:
-    """Returns the data spec used for train the model."""
-    return self._model.data_spec()
-
-  def __str__(self) -> str:
-    return f"""\
-Model: {self.name()}
-Task: {self.task().name}
-Class: ydf.{self.__class__.__name__}
-Use `model.describe()` for more details
-"""
-
-  def benchmark(
-      self,
-      ds: dataset.InputDataset,
-      benchmark_duration: float = 3,
-      warmup_duration: float = 1,
-      batch_size: int = 100,
-  ) -> ydf.BenchmarkInferenceCCResult:
-    """Benchmark the inference speed of the model on the given dataset.
-
-    This benchmark creates batched predictions on the given dataset using the
-    C++ API of Yggdrasil Decision Forests. Note that inference times using other
-    APIs or on different machines will be different. A serving template for the
-    C++ API can be generated with `model.to_cpp()`.
-
-    Args:
-      ds: Dataset to perform the benchmark on.
-      benchmark_duration: Total duration of the benchmark in seconds. Note that
-        this number is only indicative and the actual duration of the benchmark
-        may be shorter or longer. This parameter must be > 0.
-      warmup_duration: Total duration of the warmup runs before the benchmark in
-        seconds. During the warmup phase, the benchmark is run without being
-        timed. This allows warming up caches. The benchmark will always run at
-        least one batch for warmup. This parameter must be > 0.
-       batch_size: Size of batches when feeding examples to the inference
-         engines. The impact of this parameter on the results depends on the
-         architecture running the benchmark (notably, cache sizes).
-
-    Returns:
-      Benchmark results.
-    """
-    if benchmark_duration <= 0:
-      raise ValueError(
-          "The duration of the benchmark must be positive, got"
-          f" {benchmark_duration}"
-      )
-    if warmup_duration <= 0:
-      raise ValueError(
-          "The duration of the warmup phase must be positive, got"
-          f" {warmup_duration}."
-      )
-    if batch_size <= 0:
-      raise ValueError(
-          f"The batch size of the benchmark must be positive, got {batch_size}."
-      )
-
-    with log.cc_log_context():
-      vds = dataset.create_vertical_dataset(
-          ds,
-          data_spec=self._model.data_spec(),
-          required_columns=self.input_feature_names(),
-      )
-      result = self._model.Benchmark(
-          vds._dataset, benchmark_duration, warmup_duration, batch_size  # pylint: disable=protected-access
-      )
-    return result
-
-  def save(self, path, advanced_options=ModelIOOptions()) -> None:
-    """Save the model to disk.
-
-    YDF uses a proprietary model format for saving models. A model consists of
-    multiple files located in the same directory.
-    A directory should only contain a single YDF model. See `advanced_options`
-    for more information.
-
-    YDF models can also be exported to other formats, see
-    `to_tensorflow_saved_model()` and `to_cpp()` for details.
-
-    YDF saves some metadata inside the model, see `model.metadata()` for
-    details. Before distributing a model to the world, consider removing
-    metadata with `model.set_metadata(ydf.ModelMetadata())`.
-
-    Usage example:
-
-    ```python
-    import pandas as pd
-    import ydf
-
-    # Train a Random Forest model
-    df = pd.read_csv("my_dataset.csv")
-    model = ydf.RandomForestLearner().train(df)
-
-    # Save the model to disk
-    model.save("/models/my_model")
-    ```
-
-    Args:
-      path: Path to directory to store the model in.
-      advanced_options: Advanced options for saving models.
-    """
-    # Warn if the user is trying to save to a nonempty directory without
-    # prefixing the model.
-    if advanced_options.file_prefix is not None:
-      if os.path.exists(path):
-        if os.path.isdir(path):
-          with os.scandir(path) as it:
-            if any(it):
-              logging.warning(
-                  "The directory %s to save the model to is not empty,"
-                  " which can lead to model corruption. Specify an empty or"
-                  " non-existing directory to save the model to, or use"
-                  " `advanced_options` to specify a file prefix for the model.",
-                  path,
-              )
-
-    with log.cc_log_context():
-      self._model.Save(path, advanced_options.file_prefix)
-
-  def predict(self, data: dataset.InputDataset) -> np.ndarray:
-    """Returns the predictions of the model on the given dataset.
-
-    Usage example:
-
-    ```python
-    import pandas as pd
-    import ydf
-
-    # Train model
-    train_ds = pd.read_csv("train.csv")
-    model = ydf.RandomForestLearner(label="label").train(train_ds)
-
-    test_ds = pd.read_csv("test.csv")
-    predictions = model.predict(test_ds)
-    ```
-
-    Args:
-      data: Dataset. Can be a dictionary of list or numpy array of values,
-        Pandas DataFrame, or a VerticalDataset. If the dataset contains the
-        label column, that column is ignored.
-    """
-    with log.cc_log_context():
-      # The data spec contains the label / weights /  ranking group / uplift
-      # treatment column, but those are not required for making predictions.
-      ds = dataset.create_vertical_dataset(
-          data,
-          data_spec=self._model.data_spec(),
-          required_columns=self.input_feature_names(),
-      )
-      result = self._model.Predict(ds._dataset)  # pylint: disable=protected-access
-    return result
-
-  def evaluate(
-      self,
-      data: dataset.InputDataset,
-      bootstrapping: Union[bool, int] = False,
-  ) -> metric.Evaluation:
-    """Evaluates the quality of a model on a dataset.
-
-    Usage example:
-
-    ```python
-    import pandas as pd
-    import ydf
-
-    # Train model
-    train_ds = pd.read_csv("train.csv")
-    model = ydf.RandomForestLearner(label="label").train(train_ds)
-
-    test_ds = pd.read_csv("test.csv")
-    evaluation = model.evaluates(test_ds)
-    ```
-
-    In a notebook, if a cell returns an evaluation object, this evaluation will
-    be as a rich html with plots:
-
-    ```
-    evaluation = model.evaluate(test_ds)
-    evaluation
-    ```
-
-    Args:
-      data: Dataset. Can be a dictionary of list or numpy array of values,
-        Pandas DataFrame, or a VerticalDataset.
-      bootstrapping: Controls whether bootstrapping is used to evaluate the
-        confidence intervals and statistical tests (i.e., all the metrics ending
-        with "[B]"). If set to false, bootstrapping is disabled. If set to true,
-        bootstrapping is enabled and 2000 bootstrapping samples are used. If set
-        to an integer, it specifies the number of bootstrapping samples to use.
-        In this case, if the number is less than 100, an error is raised as
-        bootstrapping will not yield useful results.
-
-    Returns:
-      Model evaluation.
-    """
-
-    with log.cc_log_context():
-      ds = dataset.create_vertical_dataset(
-          data, data_spec=self._model.data_spec()
-      )
-
-      if isinstance(bootstrapping, bool):
-        bootstrapping_samples = 2000 if bootstrapping else -1
-      elif isinstance(bootstrapping, int) and bootstrapping >= 100:
-        bootstrapping_samples = bootstrapping
-      else:
-        raise ValueError(
-            "bootstrapping argument should be boolean or an integer greater"
-            " than 100 as bootstrapping will not yield useful results. Got"
-            f" {bootstrapping!r} instead"
-        )
-
-      options_proto = metric_pb2.EvaluationOptions(
-          bootstrapping_samples=bootstrapping_samples,
-          task=self.task()._to_proto_type(),  # pylint: disable=protected-access
-      )
-
-      evaluation_proto = self._model.Evaluate(ds._dataset, options_proto)  # pylint: disable=protected-access
-    return metric.Evaluation(evaluation_proto)
-
-  def analyze_prediction(
-      self,
-      single_example: dataset.InputDataset,
-  ) -> analysis.PredictionAnalysis:
-    """Understands a single prediction of the model.
-
-    Note: To explain the model as a whole, use `model.analyze` instead.
-
-    Usage example:
-
-    ```python
-    import pandas as pd
-    import ydf
-
-    # Train model
-    train_ds = pd.read_csv("train.csv")
-    model = ydf.RandomForestLearner(label="label").train(train_ds)
-
-    test_ds = pd.read_csv("test.csv")
-
-    # We want to explain the model prediction on the first test example.
-    selected_example = test_ds.iloc[:1]
-
-    analysis = model.analyze_prediction(selected_example, test_ds)
-
-    # Display the analysis in a notebook.
-    analysis
-    ```
-
-    Args:
-      single_example: Example to explain. Can be a dictionary of lists or numpy
-        arrays of values, Pandas DataFrame, or a VerticalDataset.
-
-    Returns:
-      Prediction explanation.
-    """
-
-    with log.cc_log_context():
-      ds = dataset.create_vertical_dataset(
-          single_example, data_spec=self._model.data_spec()
-      )
-
-      options_proto = model_analysis_pb2.PredictionAnalysisOptions()
-      analysis_proto = self._model.AnalyzePrediction(ds._dataset, options_proto)  # pylint: disable=protected-access
-      return analysis.PredictionAnalysis(analysis_proto, options_proto)
-
-  def analyze(
-      self,
-      data: dataset.InputDataset,
-      sampling: float = 1.0,
-      num_bins: int = 50,
-      partial_depepence_plot: bool = True,
-      conditional_expectation_plot: bool = True,
-      permutation_variable_importance_rounds: int = 1,
-      num_threads: int = 6,
-  ) -> analysis.Analysis:
-    """Analyzes a model on a test dataset.
-
-    An analysis contains structual information about the model (e.g., variable
-    importances), and the information about the application of the model on the
-    given dataset (e.g. partial dependence plots).
-
-    For a large dataset (many examples and / or features), computing the
-    analysis can take significant time.
-
-    While some information might be valid, it is generatly not recommended to
-    analyze a model on its training dataset.
-
-    Usage example:
-
-    ```python
-    import pandas as pd
-    import ydf
-
-    # Train model
-    train_ds = pd.read_csv("train.csv")
-    model = ydf.RandomForestLearner(label="label").train(train_ds)
-
-    test_ds = pd.read_csv("test.csv")
-    analysis = model.analyze(test_ds)
-
-    # Display the analysis in a notebook.
-    analysis
-    ```
-
-    Args:
-      data: Dataset. Can be a dictionary of list or numpy array of values,
-        Pandas DataFrame, or a VerticalDataset.
-      sampling: Ratio of examples to use for the analysis. The analysis can be
-        expensive to compute. On large datasets, use a small sampling value e.g.
-        0.01.
-      num_bins: Number of bins used to accumulate statistics. A large value
-        increase the resolution of the plots but takes more time to compute.
-      partial_depepence_plot: Compute partial dependency plots a.k.a PDPs.
-        Expensive to compute.
-      conditional_expectation_plot: Compute the conditional expectation plots
-        a.k.a. CEP. Cheap to compute.
-      permutation_variable_importance_rounds: If >1, computes permutation
-        variable importances using "permutation_variable_importance_rounds"
-        rounds. The most rounds the more accurate the results. Using a single
-        round is often acceptable i.e. permutation_variable_importance_rounds=1.
-        If permutation_variable_importance_rounds=0, disables the computation of
-        permutation variable importances.
-      num_threads: Number of threads to use to compute the analysis.
-
-    Returns:
-      Model analysis.
-    """
-
-    with log.cc_log_context():
-      ds = dataset.create_vertical_dataset(
-          data, data_spec=self._model.data_spec()
-      )
-
-      options_proto = model_analysis_pb2.Options(
-          num_threads=num_threads,
-          pdp=model_analysis_pb2.Options.PlotConfig(
-              enabled=partial_depepence_plot,
-              example_sampling=sampling,
-              num_numerical_bins=num_bins,
-          ),
-          cep=model_analysis_pb2.Options.PlotConfig(
-              enabled=conditional_expectation_plot,
-              example_sampling=sampling,
-              num_numerical_bins=num_bins,
-          ),
-          permuted_variable_importance=model_analysis_pb2.Options.PermutedVariableImportance(
-              enabled=permutation_variable_importance_rounds > 0,
-              num_rounds=permutation_variable_importance_rounds,
-          ),
-          include_model_structural_variable_importances=True,
-      )
-
-      analysis_proto = self._model.Analyze(ds._dataset, options_proto)  # pylint: disable=protected-access
-      return analysis.Analysis(analysis_proto, options_proto)
-
-  def to_cpp(self, key: str = "my_model") -> str:
-    """Generates the code of a .h file to run the model in C++.
-
-    How to use this function:
-
-    1. Copy the output of this function in a new .h file.
-      open("model.h", "w").write(model.to_cpp())
-    2. If you use Bazel/Blaze, create a rule with the dependencies:
-      //third_party/absl/status:statusor
-      //third_party/absl/strings
-      //external/ydf_cc/yggdrasil_decision_forests/api:serving
-    3. In your C++ code, include the .h file and call the model with:
-      // Load the model (to do only once).
-      namespace ydf = yggdrasil_decision_forests;
-      const auto model = ydf::exported_model_123::Load(<path to model>);
-      // Run the model
-      predictions = model.Predict();
-    4. The generated "Predict" function takes no inputs. Instead, it fills the
-      input features with placeholder values. Therefore, you will want to add
-      your input as arguments to the "Predict" function, and use it to populate
-      the "examples->Set..." section accordingly.
-    5. (Bonus) You can further optimize the inference speed by pre-allocating
-      and re-using the examples and predictions for each thread running the
-      model.
-
-    This documentation is also available in the header of the generated content
-    for more details.
-
-    Args:
-      key: Name of the model. Used to define the c++ namespace of the model.
-
-    Returns:
-      String containing an example header for running the model in C++.
-    """
-    return template_cpp_export.template(
-        key, self._model.data_spec(), self._model.input_features()
-    )
-
-  # TODO: Change default value of "mode" before 1.0 release.
-  def to_tensorflow_saved_model(  # pylint: disable=dangerous-default-value
-      self,
-      path: str,
-      input_model_signature_fn: Any = None,
-      *,
-      mode: Literal["keras", "tf"] = "keras",
-      feature_dtypes: Dict[str, export_tf.TFDType] = {},
-      servo_api: bool = False,
-      feed_example_proto: bool = False,
-      pre_processing: Optional[Callable] = None,  # pylint: disable=g-bare-generic
-      post_processing: Optional[Callable] = None,  # pylint: disable=g-bare-generic
-      temp_dir: Optional[str] = None,
-  ) -> None:
-    """Exports the model as a TensorFlow Saved model.
-
-    This function requires TensorFlow and TensorFlow Decision Forests to be
-    installed. Install them by running the command `pip install
-    tensorflow_decision_forests`. The generated SavedModel model relies on the
-    TensorFlow Decision Forests Custom Inference Op. This Op is available by
-    default in various platforms such as Servomatic, TensorFlow Serving, Vertex
-    AI, and TensorFlow.js.
-
-    Usage example:
-
-    ```python
-    !pip install tensorflow_decision_forests
-
-    import ydf
-    import numpy as np
-    import tensorflow as tf
-
-    # Train a model.
-    model = ydf.RandomForestLearner(label="l").train({
-        "f1": np.random.random(size=100),
-        "f2": np.random.random(size=100).astype(dtype=np.float32),
-        "l": np.random.randint(2, size=100),
-    })
-
-    # Export the model to the TensorFlow SavedModel format.
-    # The model can be executed with Servomatic, TensorFlow Serving and
-    # Vertex AI.
-    model.to_tensorflow_saved_model(path="/tmp/my_model", mode="tf")
-
-    # The model can also be loaded in TensorFlow and executed locally.
-
-    # Load the TensorFlow Saved model.
-    tf_model = tf.saved_model.load("/tmp/my_model")
-
-    # Make predictions
-    tf_predictions = tf_model({
-        "f1": tf.constant(np.random.random(size=10)),
-        "f2": tf.constant(np.random.random(size=10), dtype=tf.float32),
-    })
-    ```
-
-    TensorFlow SavedModel do not cast automatically feature values. For
-    instance, a model trained with a dtype=float32 semantic=numerical feature,
-    will require for this feature to be fed as float32 numbers during inference.
-    You can override the dtype of a feature with the `feature_dtypes` argument:
-
-    ```python
-    model.to_tensorflow_saved_model(
-        path="/tmp/my_model",
-        mode="tf",
-        # "f1" is fed as an tf.int64 instead of tf.float64
-        feature_dtypes={"f1": tf.int64},
-    )
-    ```
-
-    The SavedModel format allows for custom preprocessing and postprocessing
-    computation in addition to the model inference. Such computation can be
-    specified with the `pre_processing` and `post_processing` arguments:
-
-    ```python
-    def pre_processing(features):
-      features = features.copy()
-      features["f1"] = features["f1"] * 2
-      return features
-
-    model.to_tensorflow_saved_model(
-        path="/tmp/my_model",
-        mode="tf",
-        pre_processing=pre_processing,
-    )
-    ```
-
-    For more complex combinations, such as composing multiple models, use the
-    method `to_tensorflow_function` instead of `to_tensorflow_saved_model`.
-
-    Args:
-      path: Path to store the Tensorflow Decision Forests model.
-      input_model_signature_fn: A lambda that returns the
-        (Dense,Sparse,Ragged)TensorSpec (or structure of TensorSpec e.g.
-        dictionary, list) corresponding to input signature of the model. If not
-        specified, the input model signature is created by
-        `tfdf.keras.build_default_input_model_signature`. For example, specify
-        `input_model_signature_fn` if an numerical input feature (which is
-        consumed as DenseTensorSpec(float32) by default) will be feed
-        differently (e.g. RaggedTensor(int64)). Only compatible with
-        mode="keras".
-      mode: How is the YDF converted into a TensorFlow SavedModel. 1) mode =
-        "keras" (default): Turn the model into a Keras 2 model using TensorFlow
-        Decision Forests, and then save it with `tf_keras.models.save_model`. 2)
-        mode = "tf" (recommended; will become default): Turn the model into a
-        TensorFlow Module, and save it with `tf.saved_model.save`.
-      feature_dtypes: Mapping from feature name to TensorFlow dtype. Use this
-        mapping to feature dtype. For instance, numerical features are encoded
-        with tf.float32 by default. If you plan on feeding tf.float64 or
-        tf.int32, use `feature_dtype` to specify it. Only compatible with
-        mode="tf".
-      servo_api: If true, adds a SavedModel signature to make the model
-        compatible with the `Classify` or `Regress` servo APIs. Only compatible
-        with mode="tf". If false, outputs the raw model predictions.
-      feed_example_proto: If false, the model expects for the input features to
-        be provided as TensorFlow values. This is most efficient way to make
-        predictions. If true, the model expects for the input featurs to be
-        provided as a binary serialized TensorFlow Example proto. This is the
-        format expected by VertexAI and most TensorFlow Serving pipelines.
-      pre_processing: Optional TensorFlow function or module to apply on the
-        input features before applying the model. Only compatible with
-        mode="tf".
-      post_processing: Optional TensorFlow function or module to apply on the
-        model predictions. Only compatible with mode="tf".
-      temp_dir: Temporary directory used during the conversion. If None
-        (default), uses `tempfile.mkdtemp` default temporary directory.
-    """
-
-    export_tf.ydf_model_to_tensorflow_saved_model(
-        ydf_model=self,
-        path=path,
-        input_model_signature_fn=input_model_signature_fn,
-        mode=mode,
-        feature_dtypes=feature_dtypes,
-        servo_api=servo_api,
-        feed_example_proto=feed_example_proto,
-        pre_processing=pre_processing,
-        post_processing=post_processing,
-        temp_dir=temp_dir,
-    )
-
-  def to_tensorflow_function(  # pytype: disable=name-error
-      self,
-      temp_dir: Optional[str] = None,
-      can_be_saved: bool = True,
-      squeeze_binary_classification: bool = True,
-  ) -> "tensorflow.Module":
-    """Converts the YDF model into a @tf.function callable TensorFlow Module.
-
-    The output module can be composed with other TensorFlow operations,
-    including other models serialized with `to_tensorflow_function`.
-
-    This function requires TensorFlow and TensorFlow Decision Forests to be
-    installed. You can install them using the command `pip install
-    tensorflow_decision_forests`. The generated SavedModel model relies on the
-    TensorFlow Decision Forests Custom Inference Op. This Op is available by
-    default in various platforms such as Servomatic, TensorFlow Serving, Vertex
-    AI, and TensorFlow.js.
-
-    Usage example:
-
-    ```python
-    !pip install tensorflow_decision_forests
-
-    import ydf
-    import numpy as np
-    import tensorflow as tf
-
-    # Train a model.
-    model = ydf.RandomForestLearner(label="l").train({
-        "f1": np.random.random(size=100),
-        "f2": np.random.random(size=100),
-        "l": np.random.randint(2, size=100),
-    })
-
-    # Convert model to a TF module.
-    tf_model = model.to_tensorflow_function()
-
-    # Make predictions with the TF module.
-    tf_predictions = tf_model({
-        "f1": tf.constant([0, 0.5, 1]),
-        "f2": tf.constant([1, 0, 0.5]),
-    })
-    ```
-
-    Args:
-      temp_dir: Temporary directory used during the conversion. If None
-        (default), uses `tempfile.mkdtemp` default temporary directory.
-      can_be_saved: If can_be_saved = True (default), the returned module can be
-        saved using `tf.saved_model.save`. In this case, files created in
-        temporary directory during the conversion are not removed when
-        `to_tensorflow_function` exit, and those files should still be present
-        when calling `tf.saved_model.save`. If can_be_saved = False, the files
-        created in the temporary directory during conversion are immediately
-        removed, and the returned object cannot be serialized with
-        `tf.saved_model.save`.
-      squeeze_binary_classification: If true (default), in case of binary
-        classification, outputs a tensor of shape [num examples] containing the
-        probability of the positive class. If false, in case of binary
-        classification, outputs a tensorflow of shape [num examples, 2]
-        containing the probability of both the negative and positive classes.
-        Has no effect on non-binary classification models.
-
-    Returns:
-      A TensorFlow @tf.function.
-    """
-
-    return export_tf.ydf_model_to_tf_function(
-        ydf_model=self,
-        temp_dir=temp_dir,
-        can_be_saved=can_be_saved,
-        squeeze_binary_classification=squeeze_binary_classification,
-    )
-
-  def hyperparameter_optimizer_logs(
-      self,
-  ) -> Optional[optimizer_logs.OptimizerLogs]:
-    """Returns the logs of the hyper-parameter tuning.
-
-    If the model is not trained with hyper-parameter tuning, returns None.
-    """
-    proto_logs = self._model.hyperparameter_optimizer_logs()
-    if proto_logs is None:
-      return None
-    return optimizer_logs.proto_optimizer_logs_to_optimizer_logs(proto_logs)
-
-  def variable_importances(self) -> Dict[str, List[Tuple[float, str]]]:
-    """Variable importances to measure the impact of features on the model.
-
-    Variable importances generally indicates how much a variable (feature)
-    contributes to the model predictions or quality. Different Variable
-    importances have different semantics and are generally not comparable.
-
-    The variable importances returned by `variable_importances()` depends on the
-    learning algorithm and its hyper-parameters. For example, the hyperparameter
-    `compute_oob_variable_importances=True` of the Random Forest learner enables
-    the computation of permutation out-of-bag variable importances.
-
-    # TODO: Add variable importances to documentation.
-
-    Features are sorted by decreasing importance.
-
-    Usage example:
-
-    ```python
-    # Train a Random Forest. Enable the computation of OOB (out-of-bag) variable
-    # importances.
-    model = ydf.RandomForestModel(compute_oob_variable_importances=True,
-                                  label=...).train(ds)
-    # List the available variable importances.
-    print(model.variable_importances().keys())
-
-    # Show a specific variable importance.
-    model.variable_importances()["MEAN_DECREASE_IN_ACCURACY"]
-    >> [("bill_length_mm", 0.0713061951754389),
-        ("island", 0.007298519736842035),
-        ("flipper_length_mm", 0.004505893640351366),
-    ...
-    ```
-
-    Returns:
-      Variable importances.
-    """
-    variable_importances = {}
-    # Collect the variable importances stored in the model.
-    for (
-        name,
-        importance_set,
-    ) in self._model.VariableImportances().items():
-      variable_importances[name] = [
-          (src.importance, self.data_spec().columns[src.attribute_idx].name)
-          for src in importance_set.variable_importances
-      ]
-    return variable_importances
-
-  def label_col_idx(self) -> int:
-    return self._model.label_col_idx()
-
-  def label(self) -> str:
-    """Name of the label column."""
-    return self.data_spec().columns[self.label_col_idx()].name
-
-  def label_classes(self) -> List[str]:
-    """Returns the label classes for classification tasks, None otherwise."""
-    if self.task() != Task.CLASSIFICATION:
-      raise ValueError(
-          "Label classes are only available for classification models. This"
-          f" model has type {self.task().name}"
-      )
-    label_column = self.data_spec().columns[self._model.label_col_idx()]
-    if label_column.type != data_spec_pb2.CATEGORICAL:
-      semantic = dataspec.Semantic.from_proto_type(label_column.type)
-      raise ValueError(
-          "Categorical type expected for classification label."
-          f" Got {semantic} instead."
-      )
-
-    if label_column.categorical.is_already_integerized:
-      log.info(
-          "The label column is integerized. This is expected for models trained"
-          " with TensorFlow Decision Forests."
-      )
-
-    # The first element is the "out-of-vocabulary" that is not used in labels.
-    return dataspec.categorical_column_dictionary_to_list(label_column)[1:]
-
-  def input_feature_names(self) -> List[str]:
-    """Returns the names of the input features.
-
-    The features are sorted in increasing order of column_idx.
-    """
-
-    dataspec_columns = self.data_spec().columns
-    return [dataspec_columns[idx].name for idx in self._model.input_features()]
-
-  def input_features(self) -> Sequence[InputFeature]:
-    """Returns the input features of the model.
-
-    The features are sorted in increasing order of column_idx.
-    """
-    dataspec_columns = self.data_spec().columns
-    return [
-        InputFeature(
-            name=dataspec_columns[column_idx].name,
-            semantic=dataspec.Semantic.from_proto_type(
-                dataspec_columns[column_idx].type
-            ),
-            column_idx=column_idx,
-        )
-        for column_idx in self._model.input_features()
-    ]
-
-  def self_evaluation(self) -> metric.Evaluation:
-    """Returns the model's self-evaluation.
-
-    Different models use different methods for self-evaluation. Notably, Random
-    Forests use OOB evaluation and Gradient Boosted Trees use evaluation on the
-    validation dataset. Therefore, self-evaluations are not comparable between
-    different model types.
-
-    Usage example:
-
-    ```python
-    import pandas as pd
-    import ydf
-
-    # Train model
-    train_ds = pd.read_csv("train.csv")
-    model = ydf.GradientBoostedTreesLearner(label="label").train(train_ds)
-
-    self_evaluation = model.self_evaluation()
-    # In an interactive Python environment, print a rich evaluation report.
-    self_evaluation
-    ```
-    """
-    raise NotImplementedError(
-        "Self-evaluation is not available for this model type."
-    )
-
-  def list_compatible_engines(self) -> Sequence[str]:
-    """Lists the inference engines compatible with the model.
-
-    The engines are sorted to likely-fastest to  likely-slowest.
-
-    Returns:
-      List of compatible engines.
-    """
-    return self._model.ListCompatibleEngines()
-
-  def force_engine(self, engine_name: Optional[str]) -> None:
-    """Forces the engines used by the model.
-
-    If not specified (i.e., None; default value), the fastest compatible engine
-    (i.e., the first value returned from "list_compatible_engines") is used for
-    all model inferences (e.g., model.predict, model.evaluate).
-
-    If passing a non-existing or non-compatible engine, the next model inference
-    (e.g., model.predict, model.evaluate) will fail.
-
-    Args:
-      engine_name: Name of a compatible engine or None to automatically select
-        the fastest engine.
-    """
-    self._model.ForceEngine(engine_name)
-
-
-ModelType = TypeVar("ModelType", bound=GenericModel)
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Generic YDF model definition."""
+
+import dataclasses
+import enum
+import os
+from typing import Any, Callable, Dict, List, Literal, Optional, Sequence, Tuple, TypeVar, Union
+
+from absl import logging
+import numpy as np
+
+from ydf.proto.dataset import data_spec_pb2
+from ydf.proto.metric import metric_pb2
+from ydf.proto.model import abstract_model_pb2
+from ydf.cc import ydf
+from ydf.dataset import dataset
+from ydf.dataset import dataspec
+from ydf.metric import metric
+from ydf.model import analysis
+from ydf.model import export_tf
+from ydf.model import model_metadata
+from ydf.model import optimizer_logs
+from ydf.model import template_cpp_export
+from ydf.utils import html
+from ydf.utils import log
+from ydf.proto.utils import model_analysis_pb2
+
+
+@enum.unique
+class Task(enum.Enum):
+  """Task solved by a model.
+
+  Usage example:
+
+  ```python
+  learner = ydf.RandomForestLearner(label="income",
+                                    task=ydf.Task.CLASSIFICATION)
+  model = learner.train(dataset)
+  assert model.task() == ydf.Task.CLASSIFICATION
+  ```
+  Not all tasks are compatible with all learners and/or hyperparameters. For
+  more information, please see the documentation for tutorials on the individual
+  tasks.
+
+
+  Attributes:
+    CLASSIFICATION: Predict a categorical label i.e., an item of an enumeration.
+    REGRESSION: Predict a numerical label i.e., a quantity.
+    RANKING: Rank items by label values. The label is expected to be between 0
+      and 4 with NDCG semantic (0: completely unrelated, 4: perfect match).
+    CATEGORICAL_UPLIFT: Predicts the incremental impact of a treatment on a
+      categorical outcome.
+    NUMERICAL_UPLIFT: Predicts the incremental impact of a treatment on a
+      numerical outcome.
+  """
+
+  CLASSIFICATION = "CLASSIFICATION"
+  REGRESSION = "REGRESSION"
+  RANKING = "RANKING"
+  CATEGORICAL_UPLIFT = "CATEGORICAL_UPLIFT"
+  NUMERICAL_UPLIFT = "NUMERICAL_UPLIFT"
+
+  def _to_proto_type(self) -> abstract_model_pb2.Task:
+    if self in TASK_TO_PROTO:
+      return TASK_TO_PROTO[self]
+    else:
+      raise NotImplementedError(f"Unsupported task {self}")
+
+  @classmethod
+  def _from_proto_type(cls, task: abstract_model_pb2.Task):
+    task = PROTO_TO_TASK.get(task)
+    if task is None:
+      raise NotImplementedError(f"Unsupported task {task}")
+    return task
+
+
+# Mappings between task enum in python and in protobuffer and vice versa.
+TASK_TO_PROTO = {
+    Task.CLASSIFICATION: abstract_model_pb2.CLASSIFICATION,
+    Task.REGRESSION: abstract_model_pb2.REGRESSION,
+    Task.RANKING: abstract_model_pb2.RANKING,
+    Task.CATEGORICAL_UPLIFT: abstract_model_pb2.CATEGORICAL_UPLIFT,
+    Task.NUMERICAL_UPLIFT: abstract_model_pb2.NUMERICAL_UPLIFT,
+}
+PROTO_TO_TASK = {v: k for k, v in TASK_TO_PROTO.items()}
+
+
+@dataclasses.dataclass(frozen=True)
+class ModelIOOptions:
+  """Advanced options for saving and loading YDF models.
+
+  Attributes:
+      file_prefix: Optional prefix for the model. File prefixes allow multiple
+        models to exist in the same folder. Doing so is heavily DISCOURAGED
+        outside of edge cases. When loading a model, the prefix, if not
+        specified, is auto-detected if possible. When saving a model, the empty
+        string is used as file prefix unless it is explicitly specified.
+  """
+
+  file_prefix: Optional[str] = None
+
+
+@enum.unique
+class NodeFormat(enum.Enum):
+  # pyformat: disable
+  """Serialization format for a model.
+
+  Determines the storage format for nodes.
+
+  Attributes:
+    BLOB_SEQUENCE: Default format for the public version of YDF.
+  """
+  # pyformat: enable
+
+  BLOB_SEQUENCE = enum.auto()
+
+
+@dataclasses.dataclass
+class InputFeature:
+  """An input feature of a model.
+
+  Attributes:
+    name: Feature name. Unique for a model.
+    semantic: Semantic of the feature.
+    column_idx: Index of the column corresponding to the feature in the
+      dataspec.
+  """
+
+  name: str
+  semantic: dataspec.Semantic
+  column_idx: int
+
+
+class GenericModel:
+  """Abstract superclass for all YDF models."""
+
+  def __init__(self, raw_model: ydf.GenericCCModel):
+    self._model = raw_model
+
+  def name(self) -> str:
+    """Returns the name of the model type."""
+    return self._model.name()
+
+  def task(self) -> Task:
+    """Task solved by the model."""
+    return Task._from_proto_type(self._model.task())  # pylint: disable=protected-access
+
+  def metadata(self) -> model_metadata.ModelMetadata:
+    """Metadata associated with the model.
+
+    A model's metadata contains information stored with the model that does not
+    influence the model's predictions (e.g. data created). When distributing a
+    model for wide release, it may be useful to clear / modify the model
+    metadata with `model.set_metadata(ydf.ModelMetadata())`.
+
+    Returns:
+      The model's metadata.
+    """
+    return model_metadata.ModelMetadata._from_proto_type(self._model.metadata())  # pylint:disable=protected-access
+
+  def set_metadata(self, metadata: model_metadata.ModelMetadata):
+    """Sets the model metadata."""
+    self._model.set_metadata(metadata._to_proto_type())  # pylint:disable=protected-access
+
+  def describe(
+      self,
+      output_format: Literal["auto", "text", "notebook", "html"] = "auto",
+      full_details: bool = False,
+  ) -> Union[str, html.HtmlNotebookDisplay]:
+    """Description of the model.
+
+    Args:
+      output_format: Format of the display: - auto: Use the "notebook" format if
+        executed in an IPython notebook / Colab. Otherwise, use the "text"
+        format. - text: Text description of the model. - html: Html description
+        of the model. - notebook: Html description of the model displayed in a
+        notebook cell.
+      full_details: Should the full model be printed. This can be large.
+
+    Returns:
+      The model description.
+    """
+
+    if output_format == "auto":
+      output_format = "text" if log.is_direct_output() else "notebook"
+
+    with log.cc_log_context():
+      description = self._model.Describe(full_details, output_format == "text")
+      if output_format == "notebook":
+        return html.HtmlNotebookDisplay(description)
+      else:
+        return description
+
+  def data_spec(self) -> data_spec_pb2.DataSpecification:
+    """Returns the data spec used for train the model."""
+    return self._model.data_spec()
+
+  def __str__(self) -> str:
+    return f"""\
+Model: {self.name()}
+Task: {self.task().name}
+Class: ydf.{self.__class__.__name__}
+Use `model.describe()` for more details
+"""
+
+  def benchmark(
+      self,
+      ds: dataset.InputDataset,
+      benchmark_duration: float = 3,
+      warmup_duration: float = 1,
+      batch_size: int = 100,
+  ) -> ydf.BenchmarkInferenceCCResult:
+    """Benchmark the inference speed of the model on the given dataset.
+
+    This benchmark creates batched predictions on the given dataset using the
+    C++ API of Yggdrasil Decision Forests. Note that inference times using other
+    APIs or on different machines will be different. A serving template for the
+    C++ API can be generated with `model.to_cpp()`.
+
+    Args:
+      ds: Dataset to perform the benchmark on.
+      benchmark_duration: Total duration of the benchmark in seconds. Note that
+        this number is only indicative and the actual duration of the benchmark
+        may be shorter or longer. This parameter must be > 0.
+      warmup_duration: Total duration of the warmup runs before the benchmark in
+        seconds. During the warmup phase, the benchmark is run without being
+        timed. This allows warming up caches. The benchmark will always run at
+        least one batch for warmup. This parameter must be > 0.
+       batch_size: Size of batches when feeding examples to the inference
+         engines. The impact of this parameter on the results depends on the
+         architecture running the benchmark (notably, cache sizes).
+
+    Returns:
+      Benchmark results.
+    """
+    if benchmark_duration <= 0:
+      raise ValueError(
+          "The duration of the benchmark must be positive, got"
+          f" {benchmark_duration}"
+      )
+    if warmup_duration <= 0:
+      raise ValueError(
+          "The duration of the warmup phase must be positive, got"
+          f" {warmup_duration}."
+      )
+    if batch_size <= 0:
+      raise ValueError(
+          f"The batch size of the benchmark must be positive, got {batch_size}."
+      )
+
+    with log.cc_log_context():
+      vds = dataset.create_vertical_dataset(
+          ds,
+          data_spec=self._model.data_spec(),
+          required_columns=self.input_feature_names(),
+      )
+      result = self._model.Benchmark(
+          vds._dataset, benchmark_duration, warmup_duration, batch_size  # pylint: disable=protected-access
+      )
+    return result
+
+  def save(self, path, advanced_options=ModelIOOptions()) -> None:
+    """Save the model to disk.
+
+    YDF uses a proprietary model format for saving models. A model consists of
+    multiple files located in the same directory.
+    A directory should only contain a single YDF model. See `advanced_options`
+    for more information.
+
+    YDF models can also be exported to other formats, see
+    `to_tensorflow_saved_model()` and `to_cpp()` for details.
+
+    YDF saves some metadata inside the model, see `model.metadata()` for
+    details. Before distributing a model to the world, consider removing
+    metadata with `model.set_metadata(ydf.ModelMetadata())`.
+
+    Usage example:
+
+    ```python
+    import pandas as pd
+    import ydf
+
+    # Train a Random Forest model
+    df = pd.read_csv("my_dataset.csv")
+    model = ydf.RandomForestLearner().train(df)
+
+    # Save the model to disk
+    model.save("/models/my_model")
+    ```
+
+    Args:
+      path: Path to directory to store the model in.
+      advanced_options: Advanced options for saving models.
+    """
+    # Warn if the user is trying to save to a nonempty directory without
+    # prefixing the model.
+    if advanced_options.file_prefix is not None:
+      if os.path.exists(path):
+        if os.path.isdir(path):
+          with os.scandir(path) as it:
+            if any(it):
+              logging.warning(
+                  "The directory %s to save the model to is not empty,"
+                  " which can lead to model corruption. Specify an empty or"
+                  " non-existing directory to save the model to, or use"
+                  " `advanced_options` to specify a file prefix for the model.",
+                  path,
+              )
+
+    with log.cc_log_context():
+      self._model.Save(path, advanced_options.file_prefix)
+
+  def predict(self, data: dataset.InputDataset) -> np.ndarray:
+    """Returns the predictions of the model on the given dataset.
+
+    Usage example:
+
+    ```python
+    import pandas as pd
+    import ydf
+
+    # Train model
+    train_ds = pd.read_csv("train.csv")
+    model = ydf.RandomForestLearner(label="label").train(train_ds)
+
+    test_ds = pd.read_csv("test.csv")
+    predictions = model.predict(test_ds)
+    ```
+
+    Args:
+      data: Dataset. Can be a dictionary of list or numpy array of values,
+        Pandas DataFrame, or a VerticalDataset. If the dataset contains the
+        label column, that column is ignored.
+    """
+    with log.cc_log_context():
+      # The data spec contains the label / weights /  ranking group / uplift
+      # treatment column, but those are not required for making predictions.
+      ds = dataset.create_vertical_dataset(
+          data,
+          data_spec=self._model.data_spec(),
+          required_columns=self.input_feature_names(),
+      )
+      result = self._model.Predict(ds._dataset)  # pylint: disable=protected-access
+    return result
+
+  def evaluate(
+      self,
+      data: dataset.InputDataset,
+      bootstrapping: Union[bool, int] = False,
+  ) -> metric.Evaluation:
+    """Evaluates the quality of a model on a dataset.
+
+    Usage example:
+
+    ```python
+    import pandas as pd
+    import ydf
+
+    # Train model
+    train_ds = pd.read_csv("train.csv")
+    model = ydf.RandomForestLearner(label="label").train(train_ds)
+
+    test_ds = pd.read_csv("test.csv")
+    evaluation = model.evaluates(test_ds)
+    ```
+
+    In a notebook, if a cell returns an evaluation object, this evaluation will
+    be as a rich html with plots:
+
+    ```
+    evaluation = model.evaluate(test_ds)
+    evaluation
+    ```
+
+    Args:
+      data: Dataset. Can be a dictionary of list or numpy array of values,
+        Pandas DataFrame, or a VerticalDataset.
+      bootstrapping: Controls whether bootstrapping is used to evaluate the
+        confidence intervals and statistical tests (i.e., all the metrics ending
+        with "[B]"). If set to false, bootstrapping is disabled. If set to true,
+        bootstrapping is enabled and 2000 bootstrapping samples are used. If set
+        to an integer, it specifies the number of bootstrapping samples to use.
+        In this case, if the number is less than 100, an error is raised as
+        bootstrapping will not yield useful results.
+
+    Returns:
+      Model evaluation.
+    """
+
+    with log.cc_log_context():
+      ds = dataset.create_vertical_dataset(
+          data, data_spec=self._model.data_spec()
+      )
+
+      if isinstance(bootstrapping, bool):
+        bootstrapping_samples = 2000 if bootstrapping else -1
+      elif isinstance(bootstrapping, int) and bootstrapping >= 100:
+        bootstrapping_samples = bootstrapping
+      else:
+        raise ValueError(
+            "bootstrapping argument should be boolean or an integer greater"
+            " than 100 as bootstrapping will not yield useful results. Got"
+            f" {bootstrapping!r} instead"
+        )
+
+      options_proto = metric_pb2.EvaluationOptions(
+          bootstrapping_samples=bootstrapping_samples,
+          task=self.task()._to_proto_type(),  # pylint: disable=protected-access
+      )
+
+      evaluation_proto = self._model.Evaluate(ds._dataset, options_proto)  # pylint: disable=protected-access
+    return metric.Evaluation(evaluation_proto)
+
+  def analyze_prediction(
+      self,
+      single_example: dataset.InputDataset,
+  ) -> analysis.PredictionAnalysis:
+    """Understands a single prediction of the model.
+
+    Note: To explain the model as a whole, use `model.analyze` instead.
+
+    Usage example:
+
+    ```python
+    import pandas as pd
+    import ydf
+
+    # Train model
+    train_ds = pd.read_csv("train.csv")
+    model = ydf.RandomForestLearner(label="label").train(train_ds)
+
+    test_ds = pd.read_csv("test.csv")
+
+    # We want to explain the model prediction on the first test example.
+    selected_example = test_ds.iloc[:1]
+
+    analysis = model.analyze_prediction(selected_example, test_ds)
+
+    # Display the analysis in a notebook.
+    analysis
+    ```
+
+    Args:
+      single_example: Example to explain. Can be a dictionary of lists or numpy
+        arrays of values, Pandas DataFrame, or a VerticalDataset.
+
+    Returns:
+      Prediction explanation.
+    """
+
+    with log.cc_log_context():
+      ds = dataset.create_vertical_dataset(
+          single_example, data_spec=self._model.data_spec()
+      )
+
+      options_proto = model_analysis_pb2.PredictionAnalysisOptions()
+      analysis_proto = self._model.AnalyzePrediction(ds._dataset, options_proto)  # pylint: disable=protected-access
+      return analysis.PredictionAnalysis(analysis_proto, options_proto)
+
+  def analyze(
+      self,
+      data: dataset.InputDataset,
+      sampling: float = 1.0,
+      num_bins: int = 50,
+      partial_depepence_plot: bool = True,
+      conditional_expectation_plot: bool = True,
+      permutation_variable_importance_rounds: int = 1,
+      num_threads: int = 6,
+  ) -> analysis.Analysis:
+    """Analyzes a model on a test dataset.
+
+    An analysis contains structual information about the model (e.g., variable
+    importances), and the information about the application of the model on the
+    given dataset (e.g. partial dependence plots).
+
+    For a large dataset (many examples and / or features), computing the
+    analysis can take significant time.
+
+    While some information might be valid, it is generatly not recommended to
+    analyze a model on its training dataset.
+
+    Usage example:
+
+    ```python
+    import pandas as pd
+    import ydf
+
+    # Train model
+    train_ds = pd.read_csv("train.csv")
+    model = ydf.RandomForestLearner(label="label").train(train_ds)
+
+    test_ds = pd.read_csv("test.csv")
+    analysis = model.analyze(test_ds)
+
+    # Display the analysis in a notebook.
+    analysis
+    ```
+
+    Args:
+      data: Dataset. Can be a dictionary of list or numpy array of values,
+        Pandas DataFrame, or a VerticalDataset.
+      sampling: Ratio of examples to use for the analysis. The analysis can be
+        expensive to compute. On large datasets, use a small sampling value e.g.
+        0.01.
+      num_bins: Number of bins used to accumulate statistics. A large value
+        increase the resolution of the plots but takes more time to compute.
+      partial_depepence_plot: Compute partial dependency plots a.k.a PDPs.
+        Expensive to compute.
+      conditional_expectation_plot: Compute the conditional expectation plots
+        a.k.a. CEP. Cheap to compute.
+      permutation_variable_importance_rounds: If >1, computes permutation
+        variable importances using "permutation_variable_importance_rounds"
+        rounds. The most rounds the more accurate the results. Using a single
+        round is often acceptable i.e. permutation_variable_importance_rounds=1.
+        If permutation_variable_importance_rounds=0, disables the computation of
+        permutation variable importances.
+      num_threads: Number of threads to use to compute the analysis.
+
+    Returns:
+      Model analysis.
+    """
+
+    with log.cc_log_context():
+      ds = dataset.create_vertical_dataset(
+          data, data_spec=self._model.data_spec()
+      )
+
+      options_proto = model_analysis_pb2.Options(
+          num_threads=num_threads,
+          pdp=model_analysis_pb2.Options.PlotConfig(
+              enabled=partial_depepence_plot,
+              example_sampling=sampling,
+              num_numerical_bins=num_bins,
+          ),
+          cep=model_analysis_pb2.Options.PlotConfig(
+              enabled=conditional_expectation_plot,
+              example_sampling=sampling,
+              num_numerical_bins=num_bins,
+          ),
+          permuted_variable_importance=model_analysis_pb2.Options.PermutedVariableImportance(
+              enabled=permutation_variable_importance_rounds > 0,
+              num_rounds=permutation_variable_importance_rounds,
+          ),
+          include_model_structural_variable_importances=True,
+      )
+
+      analysis_proto = self._model.Analyze(ds._dataset, options_proto)  # pylint: disable=protected-access
+      return analysis.Analysis(analysis_proto, options_proto)
+
+  def to_cpp(self, key: str = "my_model") -> str:
+    """Generates the code of a .h file to run the model in C++.
+
+    How to use this function:
+
+    1. Copy the output of this function in a new .h file.
+      open("model.h", "w").write(model.to_cpp())
+    2. If you use Bazel/Blaze, create a rule with the dependencies:
+      //third_party/absl/status:statusor
+      //third_party/absl/strings
+      //external/ydf_cc/yggdrasil_decision_forests/api:serving
+    3. In your C++ code, include the .h file and call the model with:
+      // Load the model (to do only once).
+      namespace ydf = yggdrasil_decision_forests;
+      const auto model = ydf::exported_model_123::Load(<path to model>);
+      // Run the model
+      predictions = model.Predict();
+    4. The generated "Predict" function takes no inputs. Instead, it fills the
+      input features with placeholder values. Therefore, you will want to add
+      your input as arguments to the "Predict" function, and use it to populate
+      the "examples->Set..." section accordingly.
+    5. (Bonus) You can further optimize the inference speed by pre-allocating
+      and re-using the examples and predictions for each thread running the
+      model.
+
+    This documentation is also available in the header of the generated content
+    for more details.
+
+    Args:
+      key: Name of the model. Used to define the c++ namespace of the model.
+
+    Returns:
+      String containing an example header for running the model in C++.
+    """
+    return template_cpp_export.template(
+        key, self._model.data_spec(), self._model.input_features()
+    )
+
+  # TODO: Change default value of "mode" before 1.0 release.
+  def to_tensorflow_saved_model(  # pylint: disable=dangerous-default-value
+      self,
+      path: str,
+      input_model_signature_fn: Any = None,
+      *,
+      mode: Literal["keras", "tf"] = "keras",
+      feature_dtypes: Dict[str, export_tf.TFDType] = {},
+      servo_api: bool = False,
+      feed_example_proto: bool = False,
+      pre_processing: Optional[Callable] = None,  # pylint: disable=g-bare-generic
+      post_processing: Optional[Callable] = None,  # pylint: disable=g-bare-generic
+      temp_dir: Optional[str] = None,
+  ) -> None:
+    """Exports the model as a TensorFlow Saved model.
+
+    This function requires TensorFlow and TensorFlow Decision Forests to be
+    installed. Install them by running the command `pip install
+    tensorflow_decision_forests`. The generated SavedModel model relies on the
+    TensorFlow Decision Forests Custom Inference Op. This Op is available by
+    default in various platforms such as Servomatic, TensorFlow Serving, Vertex
+    AI, and TensorFlow.js.
+
+    Usage example:
+
+    ```python
+    !pip install tensorflow_decision_forests
+
+    import ydf
+    import numpy as np
+    import tensorflow as tf
+
+    # Train a model.
+    model = ydf.RandomForestLearner(label="l").train({
+        "f1": np.random.random(size=100),
+        "f2": np.random.random(size=100).astype(dtype=np.float32),
+        "l": np.random.randint(2, size=100),
+    })
+
+    # Export the model to the TensorFlow SavedModel format.
+    # The model can be executed with Servomatic, TensorFlow Serving and
+    # Vertex AI.
+    model.to_tensorflow_saved_model(path="/tmp/my_model", mode="tf")
+
+    # The model can also be loaded in TensorFlow and executed locally.
+
+    # Load the TensorFlow Saved model.
+    tf_model = tf.saved_model.load("/tmp/my_model")
+
+    # Make predictions
+    tf_predictions = tf_model({
+        "f1": tf.constant(np.random.random(size=10)),
+        "f2": tf.constant(np.random.random(size=10), dtype=tf.float32),
+    })
+    ```
+
+    TensorFlow SavedModel do not cast automatically feature values. For
+    instance, a model trained with a dtype=float32 semantic=numerical feature,
+    will require for this feature to be fed as float32 numbers during inference.
+    You can override the dtype of a feature with the `feature_dtypes` argument:
+
+    ```python
+    model.to_tensorflow_saved_model(
+        path="/tmp/my_model",
+        mode="tf",
+        # "f1" is fed as an tf.int64 instead of tf.float64
+        feature_dtypes={"f1": tf.int64},
+    )
+    ```
+
+    The SavedModel format allows for custom preprocessing and postprocessing
+    computation in addition to the model inference. Such computation can be
+    specified with the `pre_processing` and `post_processing` arguments:
+
+    ```python
+    def pre_processing(features):
+      features = features.copy()
+      features["f1"] = features["f1"] * 2
+      return features
+
+    model.to_tensorflow_saved_model(
+        path="/tmp/my_model",
+        mode="tf",
+        pre_processing=pre_processing,
+    )
+    ```
+
+    For more complex combinations, such as composing multiple models, use the
+    method `to_tensorflow_function` instead of `to_tensorflow_saved_model`.
+
+    Args:
+      path: Path to store the Tensorflow Decision Forests model.
+      input_model_signature_fn: A lambda that returns the
+        (Dense,Sparse,Ragged)TensorSpec (or structure of TensorSpec e.g.
+        dictionary, list) corresponding to input signature of the model. If not
+        specified, the input model signature is created by
+        `tfdf.keras.build_default_input_model_signature`. For example, specify
+        `input_model_signature_fn` if an numerical input feature (which is
+        consumed as DenseTensorSpec(float32) by default) will be feed
+        differently (e.g. RaggedTensor(int64)). Only compatible with
+        mode="keras".
+      mode: How is the YDF converted into a TensorFlow SavedModel. 1) mode =
+        "keras" (default): Turn the model into a Keras 2 model using TensorFlow
+        Decision Forests, and then save it with `tf_keras.models.save_model`. 2)
+        mode = "tf" (recommended; will become default): Turn the model into a
+        TensorFlow Module, and save it with `tf.saved_model.save`.
+      feature_dtypes: Mapping from feature name to TensorFlow dtype. Use this
+        mapping to feature dtype. For instance, numerical features are encoded
+        with tf.float32 by default. If you plan on feeding tf.float64 or
+        tf.int32, use `feature_dtype` to specify it. Only compatible with
+        mode="tf".
+      servo_api: If true, adds a SavedModel signature to make the model
+        compatible with the `Classify` or `Regress` servo APIs. Only compatible
+        with mode="tf". If false, outputs the raw model predictions.
+      feed_example_proto: If false, the model expects for the input features to
+        be provided as TensorFlow values. This is most efficient way to make
+        predictions. If true, the model expects for the input featurs to be
+        provided as a binary serialized TensorFlow Example proto. This is the
+        format expected by VertexAI and most TensorFlow Serving pipelines.
+      pre_processing: Optional TensorFlow function or module to apply on the
+        input features before applying the model. Only compatible with
+        mode="tf".
+      post_processing: Optional TensorFlow function or module to apply on the
+        model predictions. Only compatible with mode="tf".
+      temp_dir: Temporary directory used during the conversion. If None
+        (default), uses `tempfile.mkdtemp` default temporary directory.
+    """
+
+    export_tf.ydf_model_to_tensorflow_saved_model(
+        ydf_model=self,
+        path=path,
+        input_model_signature_fn=input_model_signature_fn,
+        mode=mode,
+        feature_dtypes=feature_dtypes,
+        servo_api=servo_api,
+        feed_example_proto=feed_example_proto,
+        pre_processing=pre_processing,
+        post_processing=post_processing,
+        temp_dir=temp_dir,
+    )
+
+  def to_tensorflow_function(  # pytype: disable=name-error
+      self,
+      temp_dir: Optional[str] = None,
+      can_be_saved: bool = True,
+      squeeze_binary_classification: bool = True,
+  ) -> "tensorflow.Module":
+    """Converts the YDF model into a @tf.function callable TensorFlow Module.
+
+    The output module can be composed with other TensorFlow operations,
+    including other models serialized with `to_tensorflow_function`.
+
+    This function requires TensorFlow and TensorFlow Decision Forests to be
+    installed. You can install them using the command `pip install
+    tensorflow_decision_forests`. The generated SavedModel model relies on the
+    TensorFlow Decision Forests Custom Inference Op. This Op is available by
+    default in various platforms such as Servomatic, TensorFlow Serving, Vertex
+    AI, and TensorFlow.js.
+
+    Usage example:
+
+    ```python
+    !pip install tensorflow_decision_forests
+
+    import ydf
+    import numpy as np
+    import tensorflow as tf
+
+    # Train a model.
+    model = ydf.RandomForestLearner(label="l").train({
+        "f1": np.random.random(size=100),
+        "f2": np.random.random(size=100),
+        "l": np.random.randint(2, size=100),
+    })
+
+    # Convert model to a TF module.
+    tf_model = model.to_tensorflow_function()
+
+    # Make predictions with the TF module.
+    tf_predictions = tf_model({
+        "f1": tf.constant([0, 0.5, 1]),
+        "f2": tf.constant([1, 0, 0.5]),
+    })
+    ```
+
+    Args:
+      temp_dir: Temporary directory used during the conversion. If None
+        (default), uses `tempfile.mkdtemp` default temporary directory.
+      can_be_saved: If can_be_saved = True (default), the returned module can be
+        saved using `tf.saved_model.save`. In this case, files created in
+        temporary directory during the conversion are not removed when
+        `to_tensorflow_function` exit, and those files should still be present
+        when calling `tf.saved_model.save`. If can_be_saved = False, the files
+        created in the temporary directory during conversion are immediately
+        removed, and the returned object cannot be serialized with
+        `tf.saved_model.save`.
+      squeeze_binary_classification: If true (default), in case of binary
+        classification, outputs a tensor of shape [num examples] containing the
+        probability of the positive class. If false, in case of binary
+        classification, outputs a tensorflow of shape [num examples, 2]
+        containing the probability of both the negative and positive classes.
+        Has no effect on non-binary classification models.
+
+    Returns:
+      A TensorFlow @tf.function.
+    """
+
+    return export_tf.ydf_model_to_tf_function(
+        ydf_model=self,
+        temp_dir=temp_dir,
+        can_be_saved=can_be_saved,
+        squeeze_binary_classification=squeeze_binary_classification,
+    )
+
+  def hyperparameter_optimizer_logs(
+      self,
+  ) -> Optional[optimizer_logs.OptimizerLogs]:
+    """Returns the logs of the hyper-parameter tuning.
+
+    If the model is not trained with hyper-parameter tuning, returns None.
+    """
+    proto_logs = self._model.hyperparameter_optimizer_logs()
+    if proto_logs is None:
+      return None
+    return optimizer_logs.proto_optimizer_logs_to_optimizer_logs(proto_logs)
+
+  def variable_importances(self) -> Dict[str, List[Tuple[float, str]]]:
+    """Variable importances to measure the impact of features on the model.
+
+    Variable importances generally indicates how much a variable (feature)
+    contributes to the model predictions or quality. Different Variable
+    importances have different semantics and are generally not comparable.
+
+    The variable importances returned by `variable_importances()` depends on the
+    learning algorithm and its hyper-parameters. For example, the hyperparameter
+    `compute_oob_variable_importances=True` of the Random Forest learner enables
+    the computation of permutation out-of-bag variable importances.
+
+    # TODO: Add variable importances to documentation.
+
+    Features are sorted by decreasing importance.
+
+    Usage example:
+
+    ```python
+    # Train a Random Forest. Enable the computation of OOB (out-of-bag) variable
+    # importances.
+    model = ydf.RandomForestModel(compute_oob_variable_importances=True,
+                                  label=...).train(ds)
+    # List the available variable importances.
+    print(model.variable_importances().keys())
+
+    # Show a specific variable importance.
+    model.variable_importances()["MEAN_DECREASE_IN_ACCURACY"]
+    >> [("bill_length_mm", 0.0713061951754389),
+        ("island", 0.007298519736842035),
+        ("flipper_length_mm", 0.004505893640351366),
+    ...
+    ```
+
+    Returns:
+      Variable importances.
+    """
+    variable_importances = {}
+    # Collect the variable importances stored in the model.
+    for (
+        name,
+        importance_set,
+    ) in self._model.VariableImportances().items():
+      variable_importances[name] = [
+          (src.importance, self.data_spec().columns[src.attribute_idx].name)
+          for src in importance_set.variable_importances
+      ]
+    return variable_importances
+
+  def label_col_idx(self) -> int:
+    return self._model.label_col_idx()
+
+  def label(self) -> str:
+    """Name of the label column."""
+    return self.data_spec().columns[self.label_col_idx()].name
+
+  def label_classes(self) -> List[str]:
+    """Returns the label classes for classification tasks, None otherwise."""
+    if self.task() != Task.CLASSIFICATION:
+      raise ValueError(
+          "Label classes are only available for classification models. This"
+          f" model has type {self.task().name}"
+      )
+    label_column = self.data_spec().columns[self._model.label_col_idx()]
+    if label_column.type != data_spec_pb2.CATEGORICAL:
+      semantic = dataspec.Semantic.from_proto_type(label_column.type)
+      raise ValueError(
+          "Categorical type expected for classification label."
+          f" Got {semantic} instead."
+      )
+
+    if label_column.categorical.is_already_integerized:
+      log.info(
+          "The label column is integerized. This is expected for models trained"
+          " with TensorFlow Decision Forests."
+      )
+
+    # The first element is the "out-of-vocabulary" that is not used in labels.
+    return dataspec.categorical_column_dictionary_to_list(label_column)[1:]
+
+  def input_feature_names(self) -> List[str]:
+    """Returns the names of the input features.
+
+    The features are sorted in increasing order of column_idx.
+    """
+
+    dataspec_columns = self.data_spec().columns
+    return [dataspec_columns[idx].name for idx in self._model.input_features()]
+
+  def input_features(self) -> Sequence[InputFeature]:
+    """Returns the input features of the model.
+
+    The features are sorted in increasing order of column_idx.
+    """
+    dataspec_columns = self.data_spec().columns
+    return [
+        InputFeature(
+            name=dataspec_columns[column_idx].name,
+            semantic=dataspec.Semantic.from_proto_type(
+                dataspec_columns[column_idx].type
+            ),
+            column_idx=column_idx,
+        )
+        for column_idx in self._model.input_features()
+    ]
+
+  def self_evaluation(self) -> metric.Evaluation:
+    """Returns the model's self-evaluation.
+
+    Different models use different methods for self-evaluation. Notably, Random
+    Forests use OOB evaluation and Gradient Boosted Trees use evaluation on the
+    validation dataset. Therefore, self-evaluations are not comparable between
+    different model types.
+
+    Usage example:
+
+    ```python
+    import pandas as pd
+    import ydf
+
+    # Train model
+    train_ds = pd.read_csv("train.csv")
+    model = ydf.GradientBoostedTreesLearner(label="label").train(train_ds)
+
+    self_evaluation = model.self_evaluation()
+    # In an interactive Python environment, print a rich evaluation report.
+    self_evaluation
+    ```
+    """
+    raise NotImplementedError(
+        "Self-evaluation is not available for this model type."
+    )
+
+  def list_compatible_engines(self) -> Sequence[str]:
+    """Lists the inference engines compatible with the model.
+
+    The engines are sorted to likely-fastest to  likely-slowest.
+
+    Returns:
+      List of compatible engines.
+    """
+    return self._model.ListCompatibleEngines()
+
+  def force_engine(self, engine_name: Optional[str]) -> None:
+    """Forces the engines used by the model.
+
+    If not specified (i.e., None; default value), the fastest compatible engine
+    (i.e., the first value returned from "list_compatible_engines") is used for
+    all model inferences (e.g., model.predict, model.evaluate).
+
+    If passing a non-existing or non-compatible engine, the next model inference
+    (e.g., model.predict, model.evaluate) will fail.
+
+    Args:
+      engine_name: Name of a compatible engine or None to automatically select
+        the fastest engine.
+    """
+    self._model.ForceEngine(engine_name)
+
+
+ModelType = TypeVar("ModelType", bound=GenericModel)
```

## ydf/model/model_lib.py

```diff
@@ -1,216 +1,216 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Utilities for loading YDF models."""
-
-import os
-
-from absl import logging
-
-from yggdrasil_decision_forests.dataset import data_spec_pb2
-from ydf.cc import ydf
-from ydf.dataset import dataspec
-from ydf.model import generic_model
-from ydf.model.gradient_boosted_trees_model import gradient_boosted_trees_model
-from ydf.model.random_forest_model import random_forest_model
-from ydf.utils import log
-
-
-TF_SAVED_MODEL_FILE_NAME = "saved_model.pb"
-TF_ASSETS_DIRECTORY_NAME = "assets"
-
-
-def is_saved_model(directory: str) -> bool:
-  """Return True if the given directory is a TensorFlow SavedModel."""
-  return os.path.isfile(os.path.join(directory, TF_SAVED_MODEL_FILE_NAME))
-
-
-def transform_tfdf_categorical_columns(
-    data_spec: data_spec_pb2.DataSpecification,
-) -> None:
-  """Add a vocabulary to integerized columns in the dataspec.
-
-  Tensorflow Decision Forests consumes categorical columns a pre-integerized
-  columns shifted by one for compliance with the Keras API. This is not
-  supported by PYDF since it makes it very hard to feed data without using
-  data converters. This function therefore adds an artificial dictionary to
-  these columns in the dataspec.
-
-  Args:
-    data_spec: The dataspec of a TF-DF model, will be modified.
-
-  Raises:
-    ValueError: If an integerized category has a vocabulary or if an integerized
-      column has a negative number of unique values in its column spec.
-  """
-  fixed = False
-  for column in data_spec.columns:
-    if (
-        column.type == data_spec_pb2.ColumnType.CATEGORICAL
-        and column.categorical.is_already_integerized
-    ):
-      if column.categorical.items:
-        raise ValueError("Integerized categories should not have a vocabulary.")
-      if column.categorical.number_of_unique_values < 0:
-        raise ValueError(
-            "The number of unique values in a dataspec must not be negative"
-        )
-      column.categorical.is_already_integerized = False
-      # TF-DF columns are shifted by 1.
-      column.categorical.items[dataspec.YDF_OOD].index = 0
-      for i in range(1, column.categorical.number_of_unique_values):
-        column.categorical.items[str(i - 1)].index = i
-      fixed = True
-  if fixed:
-    log.info(
-        "The model was created by Tensorflow Decision Forests and it contains"
-        " integerized columns. A dictionary has automatically been added to the"
-        " model's dataspec."
-    )
-
-
-def load_model(
-    directory: str,
-    advanced_options: generic_model.ModelIOOptions = generic_model.ModelIOOptions(),
-) -> generic_model.ModelType:
-  """Load a YDF model from disk.
-
-  Usage example:
-
-  ```python
-  import pandas as pd
-  import ydf
-
-  # Create a model
-  dataset = pd.DataFrame({"feature": [0, 1], "label": [0, 1]})
-  learner = ydf.RandomForestLearner(label="label")
-  model = learner.train(dataset)
-
-  # Save model
-  model.save("/tmp/my_model")
-
-  # Load model
-  loaded_model = ydf.load_model("/tmp/my_model")
-
-  # Make predictions
-  model.predict(dataset)
-  loaded_model.predict(dataset)
-  ```
-
-  If a directory contains multiple YDF models, the models are uniquely
-  identified by their prefix. The prefix to use can be specified in the advanced
-  options. If the directory only contains a single model, the correct prefix is
-  detected automatically.
-
-  Args:
-    directory: Directory containing the model.
-    advanced_options: Advanced options for model loading.
-
-  Returns:
-    Model to use for inference, evaluation or inspection
-  """
-  if advanced_options.file_prefix is not None:
-    logging.info(
-        "Loading model with prefix %s from %s",
-        directory,
-        advanced_options.file_prefix,
-    )
-  else:
-    logging.info("Loading model from %s", directory)
-  model_is_tfdf = is_saved_model(directory)
-
-  if model_is_tfdf:
-    raise ValueError(
-        f"The model in directory {directory} is a TensorFlow Decision Forests"
-        " model. For loading such models, use"
-        " ydf.from_tensorflow_decision_forests()."
-    )
-  cc_model: ydf.GenericCCModel = ydf.LoadModel(
-      directory, advanced_options.file_prefix
-  )
-  return load_cc_model(cc_model)
-
-
-def load_cc_model(cc_model: ydf.GenericCCModel) -> generic_model.ModelType:
-  """Convert a C++ model into the correct Python-wrapped model.
-
-  Args:
-    cc_model: Generic C++ model.
-
-  Returns:
-    Python-wrapped model
-  """
-  model_name = cc_model.name()
-  if model_name == ydf.RandomForestCCModel.kRegisteredName:
-    return random_forest_model.RandomForestModel(cc_model)
-  if model_name == ydf.GradientBoostedTreesCCModel.kRegisteredName:
-    return gradient_boosted_trees_model.GradientBoostedTreesModel(cc_model)
-  logging.info(
-      "This model has type %s, which is not fully supported. Only generic model"
-      " tasks (e.g. inference) are possible",
-      model_name,
-  )
-  return generic_model.GenericModel(cc_model)
-
-
-def from_tensorflow_decision_forests(directory: str) -> generic_model.ModelType:
-  """Load a TensorFlow Decision Forests model from disk.
-
-  Usage example:
-
-  ```python
-  import pandas as pd
-  import ydf
-
-  # Import TF-DF model
-  loaded_model = ydf.from_tensorflow_decision_forests("/tmp/my_tfdf_model")
-
-  # Make predictions
-  dataset = pd.read_csv("my_dataset.csv")
-  model.predict(dataset)
-
-  # Show details about the model
-  model.describe()
-  ```
-
-  The imported model creates the same predictions as the original TF-DF model.
-
-  Only TensorFlow Decision Forests models containing a single Decision Forest
-  and nothing else are supported. That is, combined neural network / decision
-  forest models cannot be imported. Unfortunately, importing such models may
-  succeed but result in incorrect predictions, so check for prediction equality
-  after importing.
-
-  Args:
-    directory: Directory containing the TF-DF model.
-
-  Returns:
-    Model to use for inference, evaluation or inspection
-  """
-  model_is_tfdf = is_saved_model(directory)
-
-  if not model_is_tfdf:
-    raise ValueError(
-        "The given model is not a TensorFlow Decision Forests since it is"
-        " missing a `saved_model.pb` file. Make sure the given directory"
-        f" {directory} refers to the full TensorFlow Decision Forest model (not"
-        " just one of its subdirectories)."
-    )
-  directory = os.path.join(directory, TF_ASSETS_DIRECTORY_NAME)
-  cc_model: ydf.GenericCCModel = ydf.LoadModel(directory, file_prefix=None)
-
-  data_spec = cc_model.data_spec()
-  transform_tfdf_categorical_columns(data_spec)
-  cc_model.set_data_spec(data_spec)
-  return load_cc_model(cc_model)
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Utilities for loading YDF models."""
+
+import os
+
+from absl import logging
+
+from ydf.proto.dataset import data_spec_pb2
+from ydf.cc import ydf
+from ydf.dataset import dataspec
+from ydf.model import generic_model
+from ydf.model.gradient_boosted_trees_model import gradient_boosted_trees_model
+from ydf.model.random_forest_model import random_forest_model
+from ydf.utils import log
+
+
+TF_SAVED_MODEL_FILE_NAME = "saved_model.pb"
+TF_ASSETS_DIRECTORY_NAME = "assets"
+
+
+def is_saved_model(directory: str) -> bool:
+  """Return True if the given directory is a TensorFlow SavedModel."""
+  return os.path.isfile(os.path.join(directory, TF_SAVED_MODEL_FILE_NAME))
+
+
+def transform_tfdf_categorical_columns(
+    data_spec: data_spec_pb2.DataSpecification,
+) -> None:
+  """Add a vocabulary to integerized columns in the dataspec.
+
+  Tensorflow Decision Forests consumes categorical columns a pre-integerized
+  columns shifted by one for compliance with the Keras API. This is not
+  supported by PYDF since it makes it very hard to feed data without using
+  data converters. This function therefore adds an artificial dictionary to
+  these columns in the dataspec.
+
+  Args:
+    data_spec: The dataspec of a TF-DF model, will be modified.
+
+  Raises:
+    ValueError: If an integerized category has a vocabulary or if an integerized
+      column has a negative number of unique values in its column spec.
+  """
+  fixed = False
+  for column in data_spec.columns:
+    if (
+        column.type == data_spec_pb2.ColumnType.CATEGORICAL
+        and column.categorical.is_already_integerized
+    ):
+      if column.categorical.items:
+        raise ValueError("Integerized categories should not have a vocabulary.")
+      if column.categorical.number_of_unique_values < 0:
+        raise ValueError(
+            "The number of unique values in a dataspec must not be negative"
+        )
+      column.categorical.is_already_integerized = False
+      # TF-DF columns are shifted by 1.
+      column.categorical.items[dataspec.YDF_OOD].index = 0
+      for i in range(1, column.categorical.number_of_unique_values):
+        column.categorical.items[str(i - 1)].index = i
+      fixed = True
+  if fixed:
+    log.info(
+        "The model was created by Tensorflow Decision Forests and it contains"
+        " integerized columns. A dictionary has automatically been added to the"
+        " model's dataspec."
+    )
+
+
+def load_model(
+    directory: str,
+    advanced_options: generic_model.ModelIOOptions = generic_model.ModelIOOptions(),
+) -> generic_model.ModelType:
+  """Load a YDF model from disk.
+
+  Usage example:
+
+  ```python
+  import pandas as pd
+  import ydf
+
+  # Create a model
+  dataset = pd.DataFrame({"feature": [0, 1], "label": [0, 1]})
+  learner = ydf.RandomForestLearner(label="label")
+  model = learner.train(dataset)
+
+  # Save model
+  model.save("/tmp/my_model")
+
+  # Load model
+  loaded_model = ydf.load_model("/tmp/my_model")
+
+  # Make predictions
+  model.predict(dataset)
+  loaded_model.predict(dataset)
+  ```
+
+  If a directory contains multiple YDF models, the models are uniquely
+  identified by their prefix. The prefix to use can be specified in the advanced
+  options. If the directory only contains a single model, the correct prefix is
+  detected automatically.
+
+  Args:
+    directory: Directory containing the model.
+    advanced_options: Advanced options for model loading.
+
+  Returns:
+    Model to use for inference, evaluation or inspection
+  """
+  if advanced_options.file_prefix is not None:
+    logging.info(
+        "Loading model with prefix %s from %s",
+        directory,
+        advanced_options.file_prefix,
+    )
+  else:
+    logging.info("Loading model from %s", directory)
+  model_is_tfdf = is_saved_model(directory)
+
+  if model_is_tfdf:
+    raise ValueError(
+        f"The model in directory {directory} is a TensorFlow Decision Forests"
+        " model. For loading such models, use"
+        " ydf.from_tensorflow_decision_forests()."
+    )
+  cc_model: ydf.GenericCCModel = ydf.LoadModel(
+      directory, advanced_options.file_prefix
+  )
+  return load_cc_model(cc_model)
+
+
+def load_cc_model(cc_model: ydf.GenericCCModel) -> generic_model.ModelType:
+  """Convert a C++ model into the correct Python-wrapped model.
+
+  Args:
+    cc_model: Generic C++ model.
+
+  Returns:
+    Python-wrapped model
+  """
+  model_name = cc_model.name()
+  if model_name == ydf.RandomForestCCModel.kRegisteredName:
+    return random_forest_model.RandomForestModel(cc_model)
+  if model_name == ydf.GradientBoostedTreesCCModel.kRegisteredName:
+    return gradient_boosted_trees_model.GradientBoostedTreesModel(cc_model)
+  logging.info(
+      "This model has type %s, which is not fully supported. Only generic model"
+      " tasks (e.g. inference) are possible",
+      model_name,
+  )
+  return generic_model.GenericModel(cc_model)
+
+
+def from_tensorflow_decision_forests(directory: str) -> generic_model.ModelType:
+  """Load a TensorFlow Decision Forests model from disk.
+
+  Usage example:
+
+  ```python
+  import pandas as pd
+  import ydf
+
+  # Import TF-DF model
+  loaded_model = ydf.from_tensorflow_decision_forests("/tmp/my_tfdf_model")
+
+  # Make predictions
+  dataset = pd.read_csv("my_dataset.csv")
+  model.predict(dataset)
+
+  # Show details about the model
+  model.describe()
+  ```
+
+  The imported model creates the same predictions as the original TF-DF model.
+
+  Only TensorFlow Decision Forests models containing a single Decision Forest
+  and nothing else are supported. That is, combined neural network / decision
+  forest models cannot be imported. Unfortunately, importing such models may
+  succeed but result in incorrect predictions, so check for prediction equality
+  after importing.
+
+  Args:
+    directory: Directory containing the TF-DF model.
+
+  Returns:
+    Model to use for inference, evaluation or inspection
+  """
+  model_is_tfdf = is_saved_model(directory)
+
+  if not model_is_tfdf:
+    raise ValueError(
+        "The given model is not a TensorFlow Decision Forests since it is"
+        " missing a `saved_model.pb` file. Make sure the given directory"
+        f" {directory} refers to the full TensorFlow Decision Forest model (not"
+        " just one of its subdirectories)."
+    )
+  directory = os.path.join(directory, TF_ASSETS_DIRECTORY_NAME)
+  cc_model: ydf.GenericCCModel = ydf.LoadModel(directory, file_prefix=None)
+
+  data_spec = cc_model.data_spec()
+  transform_tfdf_categorical_columns(data_spec)
+  cc_model.set_data_spec(data_spec)
+  return load_cc_model(cc_model)
```

## ydf/model/model_metadata.py

```diff
@@ -1,58 +1,58 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Python wrapper for the model metadata."""
-
-import dataclasses
-from typing import Optional
-
-from yggdrasil_decision_forests.model import abstract_model_pb2
-
-
-@dataclasses.dataclass
-class ModelMetadata:
-  """Metadata information stored in the model.
-
-  Attributes:
-    owner: Owner of the model, defaults to empty string for the open-source
-      build of YDF.
-    created_date: Unix timestamp of the model training (in seconds).
-    uid: Unique identifier of the model.
-    framework: Framework used to create the model. Defaults to "Python YDF" for
-      models trained with the Python API.
-  """
-
-  owner: Optional[str] = None
-  created_date: Optional[int] = None
-  uid: Optional[int] = None
-  framework: Optional[str] = None
-
-  def _to_proto_type(self) -> abstract_model_pb2.Metadata:
-    return abstract_model_pb2.Metadata(
-        owner=self.owner,
-        created_date=self.created_date,
-        uid=self.uid,
-        framework=self.framework,
-    )
-
-  @classmethod
-  def _from_proto_type(cls, proto: abstract_model_pb2.Metadata):
-    return ModelMetadata(
-        owner=proto.owner if proto.HasField("owner") else None,
-        created_date=proto.created_date
-        if proto.HasField("created_date")
-        else None,
-        uid=proto.uid if proto.HasField("uid") else None,
-        framework=proto.framework if proto.HasField("framework") else None,
-    )
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Python wrapper for the model metadata."""
+
+import dataclasses
+from typing import Optional
+
+from ydf.proto.model import abstract_model_pb2
+
+
+@dataclasses.dataclass
+class ModelMetadata:
+  """Metadata information stored in the model.
+
+  Attributes:
+    owner: Owner of the model, defaults to empty string for the open-source
+      build of YDF.
+    created_date: Unix timestamp of the model training (in seconds).
+    uid: Unique identifier of the model.
+    framework: Framework used to create the model. Defaults to "Python YDF" for
+      models trained with the Python API.
+  """
+
+  owner: Optional[str] = None
+  created_date: Optional[int] = None
+  uid: Optional[int] = None
+  framework: Optional[str] = None
+
+  def _to_proto_type(self) -> abstract_model_pb2.Metadata:
+    return abstract_model_pb2.Metadata(
+        owner=self.owner,
+        created_date=self.created_date,
+        uid=self.uid,
+        framework=self.framework,
+    )
+
+  @classmethod
+  def _from_proto_type(cls, proto: abstract_model_pb2.Metadata):
+    return ModelMetadata(
+        owner=proto.owner if proto.HasField("owner") else None,
+        created_date=proto.created_date
+        if proto.HasField("created_date")
+        else None,
+        uid=proto.uid if proto.HasField("uid") else None,
+        framework=proto.framework if proto.HasField("framework") else None,
+    )
```

## ydf/model/model_test.py

 * *Ordering differences only*

```diff
@@ -1,622 +1,622 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Tests for the YDF models."""
-
-import concurrent.futures
-import logging
-import os
-import tempfile
-import textwrap
-import time
-
-from absl.testing import absltest
-from absl.testing import parameterized
-import numpy as np
-import numpy.testing as npt
-import pandas as pd
-
-from ydf.dataset import dataset
-from ydf.model import analysis as analysis_lib
-from ydf.model import generic_model
-from ydf.model import model_lib
-from ydf.model import model_metadata
-from ydf.model.gradient_boosted_trees_model import gradient_boosted_trees_model
-from ydf.model.random_forest_model import random_forest_model
-from ydf.utils import test_utils
-
-
-class GenericModelTest(parameterized.TestCase):
-
-  maxDiff = None
-
-  @classmethod
-  def setUpClass(cls):
-    super().setUpClass()
-    # Loading models needed in many unittests.
-    model_dir = os.path.join(test_utils.ydf_test_data_path(), "model")
-    # This model is a Random Forest classification model without training logs.
-    cls.adult_binary_class_rf = model_lib.load_model(
-        os.path.join(model_dir, "adult_binary_class_rf")
-    )
-    # This model is a GBDT classification model without training logs.
-    cls.adult_binary_class_gbdt = model_lib.load_model(
-        os.path.join(model_dir, "adult_binary_class_gbdt")
-    )
-    # This model is a GBDT regression model without training logs.
-    cls.abalone_regression_gbdt = model_lib.load_model(
-        os.path.join(model_dir, "abalone_regression_gbdt")
-    )
-    # This model is a GBDT ranking model.
-    cls.synthetic_ranking_gbdt = model_lib.load_model(
-        os.path.join(model_dir, "synthetic_ranking_gbdt")
-    )
-
-  def test_rf_instance(self):
-    self.assertIsInstance(
-        self.adult_binary_class_rf,
-        random_forest_model.RandomForestModel,
-    )
-    self.assertEqual(self.adult_binary_class_rf.name(), "RANDOM_FOREST")
-
-  def test_gbt_instance(self):
-    self.assertIsInstance(
-        self.adult_binary_class_gbdt,
-        gradient_boosted_trees_model.GradientBoostedTreesModel,
-    )
-    self.assertEqual(
-        self.adult_binary_class_gbdt.name(), "GRADIENT_BOOSTED_TREES"
-    )
-
-  def test_predict_adult_rf(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
-    )
-    predictions_path = os.path.join(
-        test_utils.ydf_test_data_path(),
-        "prediction",
-        "adult_test_binary_class_rf.csv",
-    )
-
-    test_df = pd.read_csv(dataset_path)
-    predictions = self.adult_binary_class_rf.predict(test_df)
-    predictions_df = pd.read_csv(predictions_path)
-
-    expected_predictions = predictions_df[">50K"].to_numpy()
-    npt.assert_almost_equal(predictions, expected_predictions, decimal=5)
-
-  def test_predict_adult_gbt(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
-    )
-    predictions_path = os.path.join(
-        test_utils.ydf_test_data_path(),
-        "prediction",
-        "adult_test_binary_class_gbdt.csv",
-    )
-
-    test_df = pd.read_csv(dataset_path)
-    predictions = self.adult_binary_class_gbdt.predict(test_df)
-    predictions_df = pd.read_csv(predictions_path)
-
-    expected_predictions = predictions_df[">50K"].to_numpy()
-    npt.assert_almost_equal(predictions, expected_predictions, decimal=5)
-
-  def test_predict_without_label_column(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
-    )
-    predictions_path = os.path.join(
-        test_utils.ydf_test_data_path(),
-        "prediction",
-        "adult_test_binary_class_rf.csv",
-    )
-
-    test_df = pd.read_csv(dataset_path).drop(columns=["income"])
-    predictions = self.adult_binary_class_rf.predict(test_df)
-    predictions_df = pd.read_csv(predictions_path)
-
-    expected_predictions = predictions_df[">50K"].to_numpy()
-    npt.assert_almost_equal(predictions, expected_predictions, decimal=5)
-
-  def test_predict_fails_with_missing_feature_columns(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
-    )
-
-    test_df = pd.read_csv(dataset_path).drop(columns=["age"])
-    with self.assertRaises(ValueError):
-      _ = self.adult_binary_class_rf.predict(test_df)
-
-  def test_evaluate_fails_with_missing_label_columns(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
-    )
-
-    test_df = pd.read_csv(dataset_path).drop(columns=["income"])
-    with self.assertRaises(ValueError):
-      _ = self.adult_binary_class_rf.evaluate(test_df)
-
-  def test_evaluate_adult_gbt(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
-    )
-
-    test_df = pd.read_csv(dataset_path)
-    evaluation = self.adult_binary_class_gbdt.evaluate(test_df)
-
-    self.assertEqual(
-        str(evaluation),
-        textwrap.dedent("""\
-        accuracy: 0.872351
-        confusion matrix:
-            label (row) \\ prediction (col)
-            +-------+-------+-------+
-            |       | <=50K |  >50K |
-            +-------+-------+-------+
-            | <=50K |  6987 |   425 |
-            +-------+-------+-------+
-            |  >50K |   822 |  1535 |
-            +-------+-------+-------+
-        characteristics:
-            name: '>50K' vs others
-            ROC AUC: 0.927459
-            PR AUC: 0.828393
-            Num thresholds: 9491
-        loss: 0.279777
-        num examples: 9769
-        num examples (weighted): 9769
-        """),
-    )
-
-  def test_analyze_adult_gbt(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
-    )
-
-    test_df = pd.read_csv(dataset_path)
-    analysis = self.adult_binary_class_gbdt.analyze(
-        test_df, permutation_variable_importance_rounds=5
-    )
-
-    self.assertEqual(
-        str(analysis),
-        "A model analysis. Use a notebook cell to display the analysis."
-        " Alternatively, export the analysis with"
-        ' `analysis.to_file("analysis.html")`.',
-    )
-
-    # Note: The analysis computation is not deterministic.
-    analysis_html = analysis._repr_html_()
-    self.assertIn("Partial Dependence Plot", analysis_html)
-    self.assertIn("Conditional Expectation Plot", analysis_html)
-    self.assertIn("Variable Importance", analysis_html)
-
-  def test_analyze_programmatic_data_access_classification(self):
-    """Test programmatic access to analysis data."""
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
-    )
-    test_df = pd.read_csv(dataset_path)
-    analysis = self.adult_binary_class_gbdt.analyze(test_df, num_bins=4)
-
-    # Checked against report.
-    self.assertSetEqual(
-        set(analysis.variable_importances()),
-        set([
-            "MEAN_DECREASE_IN_ACCURACY",
-            "MEAN_DECREASE_IN_AP_>50K_VS_OTHERS",
-            "MEAN_DECREASE_IN_AUC_>50K_VS_OTHERS",
-            "MEAN_DECREASE_IN_PRAUC_>50K_VS_OTHERS",
-            "[In model] NUM_NODES",
-            "[In model] NUM_AS_ROOT",
-            "[In model] SUM_SCORE",
-            "[In model] INV_MEAN_MIN_DEPTH",
-        ]),
-    )
-    self.assertEqual(
-        analysis.variable_importances()["[In model] NUM_AS_ROOT"],
-        [
-            (31.0, "age"),
-            (22.0, "marital_status"),
-            (8.0, "capital_gain"),
-            (3.0, "occupation"),
-            (2.0, "education_num"),
-            (1.0, "education"),
-            (1.0, "capital_loss"),
-        ],
-    )
-    pdps = analysis.partial_dependence_plots()
-    self.assertLen(pdps, 14)
-    test_utils.assert_almost_equal(
-        pdps[:2],
-        [
-            analysis_lib.PartialDependencePlot(
-                feature_names=["age"],
-                feature_values=[np.array([22.25, 32.5, 43.0, 69.25])],
-                predictions=np.array([
-                    [0.89487603, 0.10512417],
-                    [0.77957238, 0.22042732],
-                    [0.73085017, 0.26914773],
-                    [0.79168959, 0.20831237],
-                ]),
-            ),
-            analysis_lib.PartialDependencePlot(
-                feature_names=["workclass"],
-                feature_values=[
-                    np.array(
-                        [
-                            "<OOD>",
-                            "Private",
-                            "Self-emp-not-inc",
-                            "Local-gov",
-                            "State-gov",
-                            "Self-emp-inc",
-                            "Federal-gov",
-                            "Without-pay",
-                        ],
-                    )
-                ],
-                predictions=np.array([
-                    [0.75371707, 0.24627779],
-                    [0.75371707, 0.24627779],
-                    [0.78163852, 0.21835641],
-                    [0.77356506, 0.2264321],
-                    [0.76878546, 0.23121109],
-                    [0.73476332, 0.26523371],
-                    [0.72042147, 0.27957545],
-                    [0.72042147, 0.27957545],
-                ]),
-            ),
-        ],
-    )
-
-  def test_analyze_programmatic_data_access_regression(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "abalone.csv"
-    )
-    test_df = pd.read_csv(dataset_path)
-    analysis = self.abalone_regression_gbdt.analyze(test_df, num_bins=4)
-
-    # Checked against report.
-    self.assertSetEqual(
-        set(analysis.variable_importances()),
-        set([
-            "MEAN_INCREASE_IN_RMSE",
-            "[In model] INV_MEAN_MIN_DEPTH",
-            "[In model] SUM_SCORE",
-            "[In model] NUM_NODES",
-            "[In model] NUM_AS_ROOT",
-        ]),
-    )
-    self.assertEqual(
-        analysis.variable_importances()["[In model] NUM_AS_ROOT"],
-        [
-            (20.0, "ShellWeight"),
-            (10.0, "Height"),
-            (4.0, "Type"),
-            (3.0, "Diameter"),
-            (1.0, "LongestShell"),
-        ],
-    )
-    pdps = analysis.partial_dependence_plots()
-    self.assertLen(pdps, 8)
-    test_utils.assert_almost_equal(
-        pdps[:1],
-        [
-            analysis_lib.PartialDependencePlot(
-                feature_names=["Type"],
-                feature_values=[np.array(["<OOD>", "M", "I", "F"])],
-                predictions=np.array([
-                    10.20926439,
-                    10.22119849,
-                    9.8006166,
-                    10.19440551,
-                ]),
-            ),
-        ],
-    )
-
-  def test_analyze_programmatic_data_access_ranking(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "synthetic_ranking_test.csv"
-    )
-    test_df = pd.read_csv(dataset_path)
-    analysis = self.synthetic_ranking_gbdt.analyze(test_df, num_bins=4)
-
-    # Checked against report.
-    self.assertSetEqual(
-        set(analysis.variable_importances()),
-        set([
-            "MEAN_DECREASE_IN_NDCG",
-            "[In model] SUM_SCORE",
-            "[In model] NUM_NODES",
-            "[In model] INV_MEAN_MIN_DEPTH",
-            "[In model] NUM_AS_ROOT",
-        ]),
-    )
-    self.assertEqual(
-        analysis.variable_importances()["[In model] NUM_AS_ROOT"],
-        [(11.0, "cat_str_0"), (2.0, "num_0"), (1.0, "num_2")],
-    )
-    pdps = analysis.partial_dependence_plots()
-    self.assertLen(pdps, 8)
-    test_utils.assert_almost_equal(
-        pdps[:1],
-        [
-            analysis_lib.PartialDependencePlot(
-                feature_names=["cat_int_0"],
-                feature_values=[np.array([4.25, 11.5, 19.5, 26.75])],
-                predictions=np.array(
-                    [-0.2301757, -0.20255686, -0.2001005, -0.19919406]
-                ),
-            ),
-        ],
-    )
-
-  def test_explain_prediction_adult_gbt(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
-    )
-
-    test_df = pd.read_csv(dataset_path, nrows=1)
-    analysis = self.adult_binary_class_gbdt.analyze_prediction(test_df)
-
-    self.assertEqual(
-        str(analysis),
-        "A prediction analysis. Use a notebook cell to display the analysis."
-        " Alternatively, export the analysis with"
-        ' `analysis.to_file("analysis.html")`.',
-    )
-
-    analysis_html = analysis._repr_html_()
-    with open("/tmp/analysis.html", "w") as f:
-      f.write(analysis_html)
-    self.assertIn("Feature Variation", analysis_html)
-
-  def test_explain_prediction_adult_gbt_with_wrong_selection(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
-    )
-    test_df = pd.read_csv(dataset_path, nrows=3)
-    with self.assertRaises(ValueError):
-      _ = self.adult_binary_class_gbdt.analyze_prediction(test_df)
-    with self.assertRaises(ValueError):
-      _ = self.adult_binary_class_gbdt.analyze_prediction(test_df.iloc[:0])
-
-  def test_evaluate_bootstrapping_default(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "abalone.csv"
-    )
-    test_df = pd.read_csv(dataset_path)
-    evaluation = self.abalone_regression_gbdt.evaluate(test_df)
-    self.assertIsNone(evaluation.rmse_ci95_bootstrap)
-
-  def test_evaluate_bootstrapping_bool(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "abalone.csv"
-    )
-    test_df = pd.read_csv(dataset_path)
-    evaluation = self.abalone_regression_gbdt.evaluate(
-        test_df, bootstrapping=True
-    )
-    self.assertIsNotNone(evaluation.rmse_ci95_bootstrap)
-    self.assertAlmostEqual(evaluation.rmse_ci95_bootstrap[0], 1.723, 2)
-    self.assertAlmostEqual(evaluation.rmse_ci95_bootstrap[1], 1.866, 2)
-
-  def test_evaluate_bootstrapping_integer(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "abalone.csv"
-    )
-    test_df = pd.read_csv(dataset_path)
-    evaluation = self.abalone_regression_gbdt.evaluate(
-        test_df, bootstrapping=599
-    )
-    self.assertIsNotNone(evaluation.rmse_ci95_bootstrap)
-    self.assertAlmostEqual(evaluation.rmse_ci95_bootstrap[0], 1.723, 1)
-    self.assertAlmostEqual(evaluation.rmse_ci95_bootstrap[1], 1.866, 1)
-
-  def test_evaluate_bootstrapping_error(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "abalone.csv"
-    )
-    test_df = pd.read_csv(dataset_path)
-    with self.assertRaisesRegex(ValueError, "an integer greater than 100"):
-      self.abalone_regression_gbdt.evaluate(test_df, bootstrapping=1)
-
-  def test_prefixed_model_loading_autodetection(self):
-    model_path = os.path.join(
-        test_utils.ydf_test_data_path(),
-        "model",
-        "prefixed_adult_binary_class_gbdt",
-    )
-    model = model_lib.load_model(model_path)
-    self.assertEqual(model.name(), "GRADIENT_BOOSTED_TREES")
-
-  def test_prefixed_model_loading_explicit(self):
-    model_path = os.path.join(
-        test_utils.ydf_test_data_path(),
-        "model",
-        "prefixed_adult_binary_class_gbdt",
-    )
-    model = model_lib.load_model(
-        model_path, generic_model.ModelIOOptions(file_prefix="prefixed_")
-    )
-    self.assertEqual(model.name(), "GRADIENT_BOOSTED_TREES")
-
-  def test_prefixed_model_loading_fails_when_incorrect(self):
-    model_path = os.path.join(
-        test_utils.ydf_test_data_path(),
-        "model",
-        "prefixed_adult_binary_class_gbdt",
-    )
-    with self.assertRaises(test_utils.AbslInvalidArgumentError):
-      model_lib.load_model(
-          model_path, generic_model.ModelIOOptions(file_prefix="wrong_prefix_")
-      )
-
-  def test_model_load_and_save(self):
-    model_path = os.path.join(
-        test_utils.ydf_test_data_path(),
-        "model",
-        "prefixed_adult_binary_class_gbdt",
-    )
-    model = model_lib.load_model(
-        model_path, generic_model.ModelIOOptions(file_prefix="prefixed_")
-    )
-    with tempfile.TemporaryDirectory() as tempdir:
-      model.save(tempdir, generic_model.ModelIOOptions(file_prefix="my_prefix"))
-      self.assertTrue(os.path.exists(os.path.join(tempdir, "my_prefixdone")))
-
-  def test_model_str(self):
-    self.assertEqual(
-        str(self.adult_binary_class_gbdt),
-        """\
-Model: GRADIENT_BOOSTED_TREES
-Task: CLASSIFICATION
-Class: ydf.GradientBoostedTreesModel
-Use `model.describe()` for more details
-""",
-    )
-
-  def test_model_describe_text(self):
-    self.assertIn(
-        'Type: "GRADIENT_BOOSTED_TREES"',
-        self.adult_binary_class_gbdt.describe("text"),
-    )
-
-  def test_model_describe_html(self):
-    html_description = self.adult_binary_class_gbdt.describe("html")
-    self.assertIn("GRADIENT_BOOSTED_TREES", html_description)
-
-  def test_model_to_cpp(self):
-    cc = self.adult_binary_class_gbdt.to_cpp()
-    logging.info("cc:\n%s", cc)
-
-  def test_benchmark(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
-    )
-    test_df = pd.read_csv(dataset_path)
-    benchmark_result = self.adult_binary_class_gbdt.benchmark(test_df)
-    print(benchmark_result)
-
-  def test_model_metadata(self):
-    metadata = model_metadata.ModelMetadata(
-        owner="TestOwner",
-        created_date=31415,
-        uid=271828,
-        framework="TestFramework",
-    )
-    self.adult_binary_class_gbdt.set_metadata(metadata)
-    self.assertEqual(metadata, self.adult_binary_class_gbdt.metadata())
-
-  def test_label_col_idx(self):
-    self.assertEqual(self.adult_binary_class_gbdt.label_col_idx(), 14)
-
-  def test_label_classes(self):
-    label_classes = self.adult_binary_class_gbdt.label_classes()
-    self.assertEqual(label_classes, ["<=50K", ">50K"])
-
-  def test_model_with_catset(self):
-    model_path = os.path.join(
-        test_utils.ydf_test_data_path(), "model", "sst_binary_class_gbdt"
-    )
-    model = model_lib.load_model(model_path)
-    test_ds_path = "csv:" + os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "sst_binary_test.csv"
-    )
-    evaluation = model.evaluate(test_ds_path)
-    self.assertAlmostEqual(evaluation.accuracy, 0.80011, places=5)
-
-  def test_multi_thread_predict(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
-    )
-    test_df = pd.read_csv(dataset_path)
-    test_ds = dataset.create_vertical_dataset(
-        test_df, data_spec=self.adult_binary_class_gbdt.data_spec()
-    )
-    for num_workers in range(1, 10 + 1):
-      with concurrent.futures.ThreadPoolExecutor(num_workers) as executor:
-        begin = time.time()
-        _ = list(
-            executor.map(self.adult_binary_class_gbdt.predict, [test_ds] * 10)
-        )
-        end = time.time()
-        logging.info("Runtime for %s workers: %s", num_workers, end - begin)
-
-  def test_self_evaluation_gbt(self):
-    # This model is a classification model with full training logs.
-    gbt_adult_base_with_na_path = os.path.join(
-        test_utils.ydf_test_data_path(), "golden", "gbt_adult_base_with_na"
-    )
-    gbt_adult_base_with_na = model_lib.load_model(gbt_adult_base_with_na_path)
-    self_evaluation = gbt_adult_base_with_na.self_evaluation()
-    self.assertAlmostEqual(self_evaluation.accuracy, 0.8498403)
-
-  def test_self_evaluation_rf(self):
-    self_evaluation = self.adult_binary_class_rf.self_evaluation()
-    self.assertAlmostEqual(self_evaluation.loss, 0.31474323732)
-
-  def test_empty_self_evaluation_rf(self):
-    # Uplift models do not have OOB evaluations.
-    model_path = os.path.join(
-        test_utils.ydf_test_data_path(),
-        "model",
-        "sim_pte_categorical_uplift_rf",
-    )
-    model = model_lib.load_model(model_path)
-    self.assertIsNone(model.self_evaluation())
-
-  def test_gbt_list_compatible_engines(self):
-    self.assertContainsSubsequence(
-        self.adult_binary_class_gbdt.list_compatible_engines(),
-        ["GradientBoostedTreesGeneric"],
-    )
-
-  def test_rf_list_compatible_engines(self):
-    self.assertContainsSubsequence(
-        self.adult_binary_class_rf.list_compatible_engines(),
-        ["RandomForestGeneric"],
-    )
-
-  def test_gbt_force_compatible_engines(self):
-    test_df = pd.read_csv(
-        os.path.join(
-            test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
-        )
-    )
-    p1 = self.adult_binary_class_gbdt.predict(test_df)
-    self.adult_binary_class_gbdt.force_engine("GradientBoostedTreesGeneric")
-    p2 = self.adult_binary_class_gbdt.predict(test_df)
-    self.adult_binary_class_gbdt.force_engine(None)
-    p3 = self.adult_binary_class_gbdt.predict(test_df)
-
-    np.testing.assert_allclose(
-        p1,
-        p2,
-        rtol=1e-5,
-        atol=1e-5,
-    )
-    np.testing.assert_allclose(
-        p1,
-        p3,
-        rtol=1e-5,
-        atol=1e-5,
-    )
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Tests for the YDF models."""
+
+import concurrent.futures
+import logging
+import os
+import tempfile
+import textwrap
+import time
+
+from absl.testing import absltest
+from absl.testing import parameterized
+import numpy as np
+import numpy.testing as npt
+import pandas as pd
+
+from ydf.dataset import dataset
+from ydf.model import analysis as analysis_lib
+from ydf.model import generic_model
+from ydf.model import model_lib
+from ydf.model import model_metadata
+from ydf.model.gradient_boosted_trees_model import gradient_boosted_trees_model
+from ydf.model.random_forest_model import random_forest_model
+from ydf.utils import test_utils
+
+
+class GenericModelTest(parameterized.TestCase):
+
+  maxDiff = None
+
+  @classmethod
+  def setUpClass(cls):
+    super().setUpClass()
+    # Loading models needed in many unittests.
+    model_dir = os.path.join(test_utils.ydf_test_data_path(), "model")
+    # This model is a Random Forest classification model without training logs.
+    cls.adult_binary_class_rf = model_lib.load_model(
+        os.path.join(model_dir, "adult_binary_class_rf")
+    )
+    # This model is a GBDT classification model without training logs.
+    cls.adult_binary_class_gbdt = model_lib.load_model(
+        os.path.join(model_dir, "adult_binary_class_gbdt")
+    )
+    # This model is a GBDT regression model without training logs.
+    cls.abalone_regression_gbdt = model_lib.load_model(
+        os.path.join(model_dir, "abalone_regression_gbdt")
+    )
+    # This model is a GBDT ranking model.
+    cls.synthetic_ranking_gbdt = model_lib.load_model(
+        os.path.join(model_dir, "synthetic_ranking_gbdt")
+    )
+
+  def test_rf_instance(self):
+    self.assertIsInstance(
+        self.adult_binary_class_rf,
+        random_forest_model.RandomForestModel,
+    )
+    self.assertEqual(self.adult_binary_class_rf.name(), "RANDOM_FOREST")
+
+  def test_gbt_instance(self):
+    self.assertIsInstance(
+        self.adult_binary_class_gbdt,
+        gradient_boosted_trees_model.GradientBoostedTreesModel,
+    )
+    self.assertEqual(
+        self.adult_binary_class_gbdt.name(), "GRADIENT_BOOSTED_TREES"
+    )
+
+  def test_predict_adult_rf(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
+    )
+    predictions_path = os.path.join(
+        test_utils.ydf_test_data_path(),
+        "prediction",
+        "adult_test_binary_class_rf.csv",
+    )
+
+    test_df = pd.read_csv(dataset_path)
+    predictions = self.adult_binary_class_rf.predict(test_df)
+    predictions_df = pd.read_csv(predictions_path)
+
+    expected_predictions = predictions_df[">50K"].to_numpy()
+    npt.assert_almost_equal(predictions, expected_predictions, decimal=5)
+
+  def test_predict_adult_gbt(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
+    )
+    predictions_path = os.path.join(
+        test_utils.ydf_test_data_path(),
+        "prediction",
+        "adult_test_binary_class_gbdt.csv",
+    )
+
+    test_df = pd.read_csv(dataset_path)
+    predictions = self.adult_binary_class_gbdt.predict(test_df)
+    predictions_df = pd.read_csv(predictions_path)
+
+    expected_predictions = predictions_df[">50K"].to_numpy()
+    npt.assert_almost_equal(predictions, expected_predictions, decimal=5)
+
+  def test_predict_without_label_column(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
+    )
+    predictions_path = os.path.join(
+        test_utils.ydf_test_data_path(),
+        "prediction",
+        "adult_test_binary_class_rf.csv",
+    )
+
+    test_df = pd.read_csv(dataset_path).drop(columns=["income"])
+    predictions = self.adult_binary_class_rf.predict(test_df)
+    predictions_df = pd.read_csv(predictions_path)
+
+    expected_predictions = predictions_df[">50K"].to_numpy()
+    npt.assert_almost_equal(predictions, expected_predictions, decimal=5)
+
+  def test_predict_fails_with_missing_feature_columns(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
+    )
+
+    test_df = pd.read_csv(dataset_path).drop(columns=["age"])
+    with self.assertRaises(ValueError):
+      _ = self.adult_binary_class_rf.predict(test_df)
+
+  def test_evaluate_fails_with_missing_label_columns(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
+    )
+
+    test_df = pd.read_csv(dataset_path).drop(columns=["income"])
+    with self.assertRaises(ValueError):
+      _ = self.adult_binary_class_rf.evaluate(test_df)
+
+  def test_evaluate_adult_gbt(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
+    )
+
+    test_df = pd.read_csv(dataset_path)
+    evaluation = self.adult_binary_class_gbdt.evaluate(test_df)
+
+    self.assertEqual(
+        str(evaluation),
+        textwrap.dedent("""\
+        accuracy: 0.872351
+        confusion matrix:
+            label (row) \\ prediction (col)
+            +-------+-------+-------+
+            |       | <=50K |  >50K |
+            +-------+-------+-------+
+            | <=50K |  6987 |   425 |
+            +-------+-------+-------+
+            |  >50K |   822 |  1535 |
+            +-------+-------+-------+
+        characteristics:
+            name: '>50K' vs others
+            ROC AUC: 0.927459
+            PR AUC: 0.828393
+            Num thresholds: 9491
+        loss: 0.279777
+        num examples: 9769
+        num examples (weighted): 9769
+        """),
+    )
+
+  def test_analyze_adult_gbt(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
+    )
+
+    test_df = pd.read_csv(dataset_path)
+    analysis = self.adult_binary_class_gbdt.analyze(
+        test_df, permutation_variable_importance_rounds=5
+    )
+
+    self.assertEqual(
+        str(analysis),
+        "A model analysis. Use a notebook cell to display the analysis."
+        " Alternatively, export the analysis with"
+        ' `analysis.to_file("analysis.html")`.',
+    )
+
+    # Note: The analysis computation is not deterministic.
+    analysis_html = analysis._repr_html_()
+    self.assertIn("Partial Dependence Plot", analysis_html)
+    self.assertIn("Conditional Expectation Plot", analysis_html)
+    self.assertIn("Variable Importance", analysis_html)
+
+  def test_analyze_programmatic_data_access_classification(self):
+    """Test programmatic access to analysis data."""
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
+    )
+    test_df = pd.read_csv(dataset_path)
+    analysis = self.adult_binary_class_gbdt.analyze(test_df, num_bins=4)
+
+    # Checked against report.
+    self.assertSetEqual(
+        set(analysis.variable_importances()),
+        set([
+            "MEAN_DECREASE_IN_ACCURACY",
+            "MEAN_DECREASE_IN_AP_>50K_VS_OTHERS",
+            "MEAN_DECREASE_IN_AUC_>50K_VS_OTHERS",
+            "MEAN_DECREASE_IN_PRAUC_>50K_VS_OTHERS",
+            "[In model] NUM_NODES",
+            "[In model] NUM_AS_ROOT",
+            "[In model] SUM_SCORE",
+            "[In model] INV_MEAN_MIN_DEPTH",
+        ]),
+    )
+    self.assertEqual(
+        analysis.variable_importances()["[In model] NUM_AS_ROOT"],
+        [
+            (31.0, "age"),
+            (22.0, "marital_status"),
+            (8.0, "capital_gain"),
+            (3.0, "occupation"),
+            (2.0, "education_num"),
+            (1.0, "education"),
+            (1.0, "capital_loss"),
+        ],
+    )
+    pdps = analysis.partial_dependence_plots()
+    self.assertLen(pdps, 14)
+    test_utils.assert_almost_equal(
+        pdps[:2],
+        [
+            analysis_lib.PartialDependencePlot(
+                feature_names=["age"],
+                feature_values=[np.array([22.25, 32.5, 43.0, 69.25])],
+                predictions=np.array([
+                    [0.89487603, 0.10512417],
+                    [0.77957238, 0.22042732],
+                    [0.73085017, 0.26914773],
+                    [0.79168959, 0.20831237],
+                ]),
+            ),
+            analysis_lib.PartialDependencePlot(
+                feature_names=["workclass"],
+                feature_values=[
+                    np.array(
+                        [
+                            "<OOD>",
+                            "Private",
+                            "Self-emp-not-inc",
+                            "Local-gov",
+                            "State-gov",
+                            "Self-emp-inc",
+                            "Federal-gov",
+                            "Without-pay",
+                        ],
+                    )
+                ],
+                predictions=np.array([
+                    [0.75371707, 0.24627779],
+                    [0.75371707, 0.24627779],
+                    [0.78163852, 0.21835641],
+                    [0.77356506, 0.2264321],
+                    [0.76878546, 0.23121109],
+                    [0.73476332, 0.26523371],
+                    [0.72042147, 0.27957545],
+                    [0.72042147, 0.27957545],
+                ]),
+            ),
+        ],
+    )
+
+  def test_analyze_programmatic_data_access_regression(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "abalone.csv"
+    )
+    test_df = pd.read_csv(dataset_path)
+    analysis = self.abalone_regression_gbdt.analyze(test_df, num_bins=4)
+
+    # Checked against report.
+    self.assertSetEqual(
+        set(analysis.variable_importances()),
+        set([
+            "MEAN_INCREASE_IN_RMSE",
+            "[In model] INV_MEAN_MIN_DEPTH",
+            "[In model] SUM_SCORE",
+            "[In model] NUM_NODES",
+            "[In model] NUM_AS_ROOT",
+        ]),
+    )
+    self.assertEqual(
+        analysis.variable_importances()["[In model] NUM_AS_ROOT"],
+        [
+            (20.0, "ShellWeight"),
+            (10.0, "Height"),
+            (4.0, "Type"),
+            (3.0, "Diameter"),
+            (1.0, "LongestShell"),
+        ],
+    )
+    pdps = analysis.partial_dependence_plots()
+    self.assertLen(pdps, 8)
+    test_utils.assert_almost_equal(
+        pdps[:1],
+        [
+            analysis_lib.PartialDependencePlot(
+                feature_names=["Type"],
+                feature_values=[np.array(["<OOD>", "M", "I", "F"])],
+                predictions=np.array([
+                    10.20926439,
+                    10.22119849,
+                    9.8006166,
+                    10.19440551,
+                ]),
+            ),
+        ],
+    )
+
+  def test_analyze_programmatic_data_access_ranking(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "synthetic_ranking_test.csv"
+    )
+    test_df = pd.read_csv(dataset_path)
+    analysis = self.synthetic_ranking_gbdt.analyze(test_df, num_bins=4)
+
+    # Checked against report.
+    self.assertSetEqual(
+        set(analysis.variable_importances()),
+        set([
+            "MEAN_DECREASE_IN_NDCG",
+            "[In model] SUM_SCORE",
+            "[In model] NUM_NODES",
+            "[In model] INV_MEAN_MIN_DEPTH",
+            "[In model] NUM_AS_ROOT",
+        ]),
+    )
+    self.assertEqual(
+        analysis.variable_importances()["[In model] NUM_AS_ROOT"],
+        [(11.0, "cat_str_0"), (2.0, "num_0"), (1.0, "num_2")],
+    )
+    pdps = analysis.partial_dependence_plots()
+    self.assertLen(pdps, 8)
+    test_utils.assert_almost_equal(
+        pdps[:1],
+        [
+            analysis_lib.PartialDependencePlot(
+                feature_names=["cat_int_0"],
+                feature_values=[np.array([4.25, 11.5, 19.5, 26.75])],
+                predictions=np.array(
+                    [-0.2301757, -0.20255686, -0.2001005, -0.19919406]
+                ),
+            ),
+        ],
+    )
+
+  def test_explain_prediction_adult_gbt(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
+    )
+
+    test_df = pd.read_csv(dataset_path, nrows=1)
+    analysis = self.adult_binary_class_gbdt.analyze_prediction(test_df)
+
+    self.assertEqual(
+        str(analysis),
+        "A prediction analysis. Use a notebook cell to display the analysis."
+        " Alternatively, export the analysis with"
+        ' `analysis.to_file("analysis.html")`.',
+    )
+
+    analysis_html = analysis._repr_html_()
+    with open("/tmp/analysis.html", "w") as f:
+      f.write(analysis_html)
+    self.assertIn("Feature Variation", analysis_html)
+
+  def test_explain_prediction_adult_gbt_with_wrong_selection(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
+    )
+    test_df = pd.read_csv(dataset_path, nrows=3)
+    with self.assertRaises(ValueError):
+      _ = self.adult_binary_class_gbdt.analyze_prediction(test_df)
+    with self.assertRaises(ValueError):
+      _ = self.adult_binary_class_gbdt.analyze_prediction(test_df.iloc[:0])
+
+  def test_evaluate_bootstrapping_default(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "abalone.csv"
+    )
+    test_df = pd.read_csv(dataset_path)
+    evaluation = self.abalone_regression_gbdt.evaluate(test_df)
+    self.assertIsNone(evaluation.rmse_ci95_bootstrap)
+
+  def test_evaluate_bootstrapping_bool(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "abalone.csv"
+    )
+    test_df = pd.read_csv(dataset_path)
+    evaluation = self.abalone_regression_gbdt.evaluate(
+        test_df, bootstrapping=True
+    )
+    self.assertIsNotNone(evaluation.rmse_ci95_bootstrap)
+    self.assertAlmostEqual(evaluation.rmse_ci95_bootstrap[0], 1.723, 2)
+    self.assertAlmostEqual(evaluation.rmse_ci95_bootstrap[1], 1.866, 2)
+
+  def test_evaluate_bootstrapping_integer(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "abalone.csv"
+    )
+    test_df = pd.read_csv(dataset_path)
+    evaluation = self.abalone_regression_gbdt.evaluate(
+        test_df, bootstrapping=599
+    )
+    self.assertIsNotNone(evaluation.rmse_ci95_bootstrap)
+    self.assertAlmostEqual(evaluation.rmse_ci95_bootstrap[0], 1.723, 1)
+    self.assertAlmostEqual(evaluation.rmse_ci95_bootstrap[1], 1.866, 1)
+
+  def test_evaluate_bootstrapping_error(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "abalone.csv"
+    )
+    test_df = pd.read_csv(dataset_path)
+    with self.assertRaisesRegex(ValueError, "an integer greater than 100"):
+      self.abalone_regression_gbdt.evaluate(test_df, bootstrapping=1)
+
+  def test_prefixed_model_loading_autodetection(self):
+    model_path = os.path.join(
+        test_utils.ydf_test_data_path(),
+        "model",
+        "prefixed_adult_binary_class_gbdt",
+    )
+    model = model_lib.load_model(model_path)
+    self.assertEqual(model.name(), "GRADIENT_BOOSTED_TREES")
+
+  def test_prefixed_model_loading_explicit(self):
+    model_path = os.path.join(
+        test_utils.ydf_test_data_path(),
+        "model",
+        "prefixed_adult_binary_class_gbdt",
+    )
+    model = model_lib.load_model(
+        model_path, generic_model.ModelIOOptions(file_prefix="prefixed_")
+    )
+    self.assertEqual(model.name(), "GRADIENT_BOOSTED_TREES")
+
+  def test_prefixed_model_loading_fails_when_incorrect(self):
+    model_path = os.path.join(
+        test_utils.ydf_test_data_path(),
+        "model",
+        "prefixed_adult_binary_class_gbdt",
+    )
+    with self.assertRaises(test_utils.AbslInvalidArgumentError):
+      model_lib.load_model(
+          model_path, generic_model.ModelIOOptions(file_prefix="wrong_prefix_")
+      )
+
+  def test_model_load_and_save(self):
+    model_path = os.path.join(
+        test_utils.ydf_test_data_path(),
+        "model",
+        "prefixed_adult_binary_class_gbdt",
+    )
+    model = model_lib.load_model(
+        model_path, generic_model.ModelIOOptions(file_prefix="prefixed_")
+    )
+    with tempfile.TemporaryDirectory() as tempdir:
+      model.save(tempdir, generic_model.ModelIOOptions(file_prefix="my_prefix"))
+      self.assertTrue(os.path.exists(os.path.join(tempdir, "my_prefixdone")))
+
+  def test_model_str(self):
+    self.assertEqual(
+        str(self.adult_binary_class_gbdt),
+        """\
+Model: GRADIENT_BOOSTED_TREES
+Task: CLASSIFICATION
+Class: ydf.GradientBoostedTreesModel
+Use `model.describe()` for more details
+""",
+    )
+
+  def test_model_describe_text(self):
+    self.assertIn(
+        'Type: "GRADIENT_BOOSTED_TREES"',
+        self.adult_binary_class_gbdt.describe("text"),
+    )
+
+  def test_model_describe_html(self):
+    html_description = self.adult_binary_class_gbdt.describe("html")
+    self.assertIn("GRADIENT_BOOSTED_TREES", html_description)
+
+  def test_model_to_cpp(self):
+    cc = self.adult_binary_class_gbdt.to_cpp()
+    logging.info("cc:\n%s", cc)
+
+  def test_benchmark(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
+    )
+    test_df = pd.read_csv(dataset_path)
+    benchmark_result = self.adult_binary_class_gbdt.benchmark(test_df)
+    print(benchmark_result)
+
+  def test_model_metadata(self):
+    metadata = model_metadata.ModelMetadata(
+        owner="TestOwner",
+        created_date=31415,
+        uid=271828,
+        framework="TestFramework",
+    )
+    self.adult_binary_class_gbdt.set_metadata(metadata)
+    self.assertEqual(metadata, self.adult_binary_class_gbdt.metadata())
+
+  def test_label_col_idx(self):
+    self.assertEqual(self.adult_binary_class_gbdt.label_col_idx(), 14)
+
+  def test_label_classes(self):
+    label_classes = self.adult_binary_class_gbdt.label_classes()
+    self.assertEqual(label_classes, ["<=50K", ">50K"])
+
+  def test_model_with_catset(self):
+    model_path = os.path.join(
+        test_utils.ydf_test_data_path(), "model", "sst_binary_class_gbdt"
+    )
+    model = model_lib.load_model(model_path)
+    test_ds_path = "csv:" + os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "sst_binary_test.csv"
+    )
+    evaluation = model.evaluate(test_ds_path)
+    self.assertAlmostEqual(evaluation.accuracy, 0.80011, places=5)
+
+  def test_multi_thread_predict(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
+    )
+    test_df = pd.read_csv(dataset_path)
+    test_ds = dataset.create_vertical_dataset(
+        test_df, data_spec=self.adult_binary_class_gbdt.data_spec()
+    )
+    for num_workers in range(1, 10 + 1):
+      with concurrent.futures.ThreadPoolExecutor(num_workers) as executor:
+        begin = time.time()
+        _ = list(
+            executor.map(self.adult_binary_class_gbdt.predict, [test_ds] * 10)
+        )
+        end = time.time()
+        logging.info("Runtime for %s workers: %s", num_workers, end - begin)
+
+  def test_self_evaluation_gbt(self):
+    # This model is a classification model with full training logs.
+    gbt_adult_base_with_na_path = os.path.join(
+        test_utils.ydf_test_data_path(), "golden", "gbt_adult_base_with_na"
+    )
+    gbt_adult_base_with_na = model_lib.load_model(gbt_adult_base_with_na_path)
+    self_evaluation = gbt_adult_base_with_na.self_evaluation()
+    self.assertAlmostEqual(self_evaluation.accuracy, 0.8498403)
+
+  def test_self_evaluation_rf(self):
+    self_evaluation = self.adult_binary_class_rf.self_evaluation()
+    self.assertAlmostEqual(self_evaluation.loss, 0.31474323732)
+
+  def test_empty_self_evaluation_rf(self):
+    # Uplift models do not have OOB evaluations.
+    model_path = os.path.join(
+        test_utils.ydf_test_data_path(),
+        "model",
+        "sim_pte_categorical_uplift_rf",
+    )
+    model = model_lib.load_model(model_path)
+    self.assertIsNone(model.self_evaluation())
+
+  def test_gbt_list_compatible_engines(self):
+    self.assertContainsSubsequence(
+        self.adult_binary_class_gbdt.list_compatible_engines(),
+        ["GradientBoostedTreesGeneric"],
+    )
+
+  def test_rf_list_compatible_engines(self):
+    self.assertContainsSubsequence(
+        self.adult_binary_class_rf.list_compatible_engines(),
+        ["RandomForestGeneric"],
+    )
+
+  def test_gbt_force_compatible_engines(self):
+    test_df = pd.read_csv(
+        os.path.join(
+            test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
+        )
+    )
+    p1 = self.adult_binary_class_gbdt.predict(test_df)
+    self.adult_binary_class_gbdt.force_engine("GradientBoostedTreesGeneric")
+    p2 = self.adult_binary_class_gbdt.predict(test_df)
+    self.adult_binary_class_gbdt.force_engine(None)
+    p3 = self.adult_binary_class_gbdt.predict(test_df)
+
+    np.testing.assert_allclose(
+        p1,
+        p2,
+        rtol=1e-5,
+        atol=1e-5,
+    )
+    np.testing.assert_allclose(
+        p1,
+        p3,
+        rtol=1e-5,
+        atol=1e-5,
+    )
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/model/optimizer_logs.py

```diff
@@ -1,85 +1,85 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Python object wrapper around optimizer logs."""
-
-import dataclasses
-from typing import Dict, Optional, Sequence, Union
-from yggdrasil_decision_forests.model import abstract_model_pb2
-from yggdrasil_decision_forests.model import hyperparameter_pb2
-
-# An hyper-parameter value
-HyperParameterValue = Union[str, int, float, Sequence[str]]
-
-
-@dataclasses.dataclass(frozen=True)
-class Trial:
-  """Results of a single trial.
-
-  Attributes:
-    score: Score of the trial. The semantic depends on the optimizer and the
-      learner.
-    params: Hyper-parameters of the learner.
-  """
-
-  score: Optional[float]
-  params: Dict[str, HyperParameterValue]
-
-
-@dataclasses.dataclass(frozen=True)
-class OptimizerLogs:
-  """Logs of an optimizer run.
-
-  Attributes:
-    trials: Collection of trials.
-  """
-
-  trials: Sequence[Trial]
-
-
-def proto_optimizer_logs_to_optimizer_logs(
-    proto: abstract_model_pb2.HyperparametersOptimizerLogs,
-) -> OptimizerLogs:
-  """Converts proto optimizer logs into user-facing optimizer logs."""
-
-  return OptimizerLogs(trials=[_trial_from_proto(step) for step in proto.steps])
-
-
-def _trial_from_proto(
-    step: abstract_model_pb2.HyperparametersOptimizerLogs.Step,
-) -> Trial:
-  return Trial(
-      score=step.score,
-      params={
-          field.name: value_from_proto(field.value)
-          for field in step.hyperparameters.fields
-      },
-  )
-
-
-def value_from_proto(
-    value: hyperparameter_pb2.GenericHyperParameters.Value,
-) -> HyperParameterValue:
-  """Converts a proto value into a Python object value."""
-
-  if value.HasField("categorical"):
-    return value.categorical
-  elif value.HasField("integer"):
-    return value.integer
-  elif value.HasField("real"):
-    return value.real
-  elif value.HasField("categorical_list"):
-    return value.categorical_list.values
-  else:
-    raise ValueError(f"Unsupported value {value}")
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Python object wrapper around optimizer logs."""
+
+import dataclasses
+from typing import Dict, Optional, Sequence, Union
+from ydf.proto.model import abstract_model_pb2
+from ydf.proto.model import hyperparameter_pb2
+
+# An hyper-parameter value
+HyperParameterValue = Union[str, int, float, Sequence[str]]
+
+
+@dataclasses.dataclass(frozen=True)
+class Trial:
+  """Results of a single trial.
+
+  Attributes:
+    score: Score of the trial. The semantic depends on the optimizer and the
+      learner.
+    params: Hyper-parameters of the learner.
+  """
+
+  score: Optional[float]
+  params: Dict[str, HyperParameterValue]
+
+
+@dataclasses.dataclass(frozen=True)
+class OptimizerLogs:
+  """Logs of an optimizer run.
+
+  Attributes:
+    trials: Collection of trials.
+  """
+
+  trials: Sequence[Trial]
+
+
+def proto_optimizer_logs_to_optimizer_logs(
+    proto: abstract_model_pb2.HyperparametersOptimizerLogs,
+) -> OptimizerLogs:
+  """Converts proto optimizer logs into user-facing optimizer logs."""
+
+  return OptimizerLogs(trials=[_trial_from_proto(step) for step in proto.steps])
+
+
+def _trial_from_proto(
+    step: abstract_model_pb2.HyperparametersOptimizerLogs.Step,
+) -> Trial:
+  return Trial(
+      score=step.score,
+      params={
+          field.name: value_from_proto(field.value)
+          for field in step.hyperparameters.fields
+      },
+  )
+
+
+def value_from_proto(
+    value: hyperparameter_pb2.GenericHyperParameters.Value,
+) -> HyperParameterValue:
+  """Converts a proto value into a Python object value."""
+
+  if value.HasField("categorical"):
+    return value.categorical
+  elif value.HasField("integer"):
+    return value.integer
+  elif value.HasField("real"):
+    return value.real
+  elif value.HasField("categorical_list"):
+    return value.categorical_list.values
+  else:
+    raise ValueError(f"Unsupported value {value}")
```

## ydf/model/optimizer_logs_test.py

```diff
@@ -1,76 +1,76 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from absl.testing import absltest
-from absl.testing import parameterized
-from yggdrasil_decision_forests.model import abstract_model_pb2
-from yggdrasil_decision_forests.model import hyperparameter_pb2
-from ydf.model import optimizer_logs
-
-Value = hyperparameter_pb2.GenericHyperParameters.Value
-HyperparametersOptimizerLogs = abstract_model_pb2.HyperparametersOptimizerLogs
-Step = abstract_model_pb2.HyperparametersOptimizerLogs.Step
-Field = hyperparameter_pb2.GenericHyperParameters.Field
-
-
-class OptimizerLogsTest(parameterized.TestCase):
-
-  @parameterized.parameters(
-      (Value(categorical="hello"), "hello"),
-      (Value(integer=1), 1),
-      (Value(real=1), 1.0),
-      (
-          Value(categorical_list=Value.CategoricalList(values=["a", "b"])),
-          ["a", "b"],
-      ),
-  )
-  def test_valid_value(self, proto, expected_value):
-    self.assertEqual(optimizer_logs.value_from_proto(proto), expected_value)
-
-  def test_convert_valid_hyperparameters(self):
-    proto = HyperparametersOptimizerLogs(
-        steps=[
-            Step(
-                score=1,
-                hyperparameters=hyperparameter_pb2.GenericHyperParameters(
-                    fields=[
-                        Field(
-                            name="a",
-                            value=Value(categorical="x"),
-                        ),
-                        Field(
-                            name="b",
-                            value=Value(
-                                integer=5,
-                            ),
-                        ),
-                    ]
-                ),
-            ),
-            Step(score=2),
-        ]
-    )
-    self.assertEqual(
-        optimizer_logs.proto_optimizer_logs_to_optimizer_logs(proto),
-        optimizer_logs.OptimizerLogs(
-            trials=[
-                optimizer_logs.Trial(score=1, params={"a": "x", "b": 5}),
-                optimizer_logs.Trial(score=2, params={}),
-            ]
-        ),
-    )
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from absl.testing import absltest
+from absl.testing import parameterized
+from ydf.proto.model import abstract_model_pb2
+from ydf.proto.model import hyperparameter_pb2
+from ydf.model import optimizer_logs
+
+Value = hyperparameter_pb2.GenericHyperParameters.Value
+HyperparametersOptimizerLogs = abstract_model_pb2.HyperparametersOptimizerLogs
+Step = abstract_model_pb2.HyperparametersOptimizerLogs.Step
+Field = hyperparameter_pb2.GenericHyperParameters.Field
+
+
+class OptimizerLogsTest(parameterized.TestCase):
+
+  @parameterized.parameters(
+      (Value(categorical="hello"), "hello"),
+      (Value(integer=1), 1),
+      (Value(real=1), 1.0),
+      (
+          Value(categorical_list=Value.CategoricalList(values=["a", "b"])),
+          ["a", "b"],
+      ),
+  )
+  def test_valid_value(self, proto, expected_value):
+    self.assertEqual(optimizer_logs.value_from_proto(proto), expected_value)
+
+  def test_convert_valid_hyperparameters(self):
+    proto = HyperparametersOptimizerLogs(
+        steps=[
+            Step(
+                score=1,
+                hyperparameters=hyperparameter_pb2.GenericHyperParameters(
+                    fields=[
+                        Field(
+                            name="a",
+                            value=Value(categorical="x"),
+                        ),
+                        Field(
+                            name="b",
+                            value=Value(
+                                integer=5,
+                            ),
+                        ),
+                    ]
+                ),
+            ),
+            Step(score=2),
+        ]
+    )
+    self.assertEqual(
+        optimizer_logs.proto_optimizer_logs_to_optimizer_logs(proto),
+        optimizer_logs.OptimizerLogs(
+            trials=[
+                optimizer_logs.Trial(score=1, params={"a": "x", "b": 5}),
+                optimizer_logs.Trial(score=2, params={}),
+            ]
+        ),
+    )
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/model/template_cpp_export.py

```diff
@@ -1,240 +1,240 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Template to export a model to c++."""
-
-import datetime
-import re
-from typing import List
-from yggdrasil_decision_forests.dataset import data_spec_pb2
-from ydf import version
-
-
-def _feature_name_to_variable(name: str, prefix: str = "feature_") -> str:
-  """Convert a feature into a variable name."""
-
-  # Replace spaces and non-printable characters with "_".
-  name = re.sub(r"[\W ]", "_", name)
-  return f"{prefix}{name}"
-
-
-def _cc_string(value: str) -> str:
-  """Escapes a c++ string. Does not include the quotes."""
-  return value.replace('"', '\\"')
-
-
-def template(
-    key: str,
-    data_spec: data_spec_pb2.DataSpecification,
-    input_features: List[int],
-) -> str:
-  """Returns the c++ template of exported model.
-
-  Args:
-    key: Identifier of the model. Used to create the namespace and define
-      protection.
-    data_spec: Dataspec of the model.
-    input_features: Input features of the model.
-  """
-
-  feature_vars = []
-  feature_index = []
-  feature_sets_1 = []
-  feature_sets_2 = []
-
-  for feature_idx in input_features:
-    col_spec = data_spec.columns[feature_idx]
-    variable = _feature_name_to_variable(col_spec.name)
-
-    if col_spec.type == data_spec_pb2.ColumnType.NUMERICAL:
-      feature_vars.append(f"  serving_api::NumericalFeatureId {variable};")
-      feature_index.append(
-          f"  ASSIGN_OR_RETURN(m.{variable},"
-          f' m.features->GetNumericalFeatureId("{_cc_string(col_spec.name)}"));'
-      )
-      feature_sets_1.append(
-          f"  examples->SetNumerical(/*example_idx=*/0, {variable}, 1.f,"
-          " *features);"
-      )
-      feature_sets_2.append(
-          f"  examples->SetNumerical(/*example_idx=*/1, {variable}, 2.f,"
-          " *features);"
-      )
-    elif col_spec.type == data_spec_pb2.ColumnType.CATEGORICAL:
-      feature_vars.append(f"  serving_api::CategoricalFeatureId {variable};")
-      feature_index.append(
-          f"  ASSIGN_OR_RETURN(m.{variable},"
-          f' m.features->GetCategoricalFeatureId("{_cc_string(col_spec.name)}"));'
-      )
-      feature_sets_1.append(
-          f'  examples->SetCategorical(/*example_idx=*/0, {variable}, "A",'
-          " *features);"
-      )
-      feature_sets_2.append(
-          f'  examples->SetCategorical(/*example_idx=*/1, {variable}, "B",'
-          " *features);"
-      )
-    elif col_spec.type == data_spec_pb2.ColumnType.BOOLEAN:
-      feature_vars.append(f"  serving_api::BooleanFeatureId {variable};")
-      feature_index.append(
-          f"  ASSIGN_OR_RETURN(m.{variable},"
-          f' m.features->GetBooleanFeatureId("{_cc_string(col_spec.name)}"));'
-      )
-      feature_sets_1.append(
-          f"  examples->SetBoolean(/*example_idx=*/0, {variable}, true,"
-          " *features);"
-      )
-      feature_sets_2.append(
-          f"  examples->SetBoolean(/*example_idx=*/1, {variable}, false,"
-          " *features);"
-      )
-    elif col_spec.type == data_spec_pb2.ColumnType.CATEGORICAL_SET:
-      feature_vars.append(f"  serving_api::CategoricalSetFeatureId {variable};")
-      feature_index.append(
-          f"  ASSIGN_OR_RETURN(m.{variable},"
-          f' m.features->GetCategoricalFeatureId("{_cc_string(col_spec.name)}"));'
-      )
-      feature_sets_1.append(
-          f"  examples->SetCategoricalSet(/*example_idx=*/0, {variable},"
-          ' std::vector<std::string>{"hello", "world"}, *features);'
-      )
-      feature_sets_2.append(
-          f"  examples->SetCategoricalSet(/*example_idx=*/1, {variable},"
-          ' std::vector<std::string> {"blue", "dog"}, *features);'
-      )
-    else:
-      raise ValueError(
-          "The automatic c++ exporter does not support yet feature"
-          f" {col_spec.name!r} with semantic"
-          f" {data_spec_pb2.ColumnType.Name(col_spec.type)}. Use the serving"
-          " API manually instead."
-      )
-
-  str_feature_vars = "\n".join(feature_vars)
-  str_feature_index = "\n".join(feature_index)
-  str_feature_sets_1 = "\n".join(feature_sets_1)
-  str_feature_sets_2 = "\n".join(feature_sets_2)
-
-  return f"""\
-// Automatically generated code running an Yggdrasil Decision Forests model in
-// C++. This code was generated with "model.to_cpp()".
-//
-// Date of generation: {datetime.datetime.now()}
-// YDF Version: {version.version}
-//
-// How to use this code:
-//
-// 1. Copy this code in a new .h file.
-// 2. If you use Bazel/Blaze, use the following dependencies:
-//      //third_party/absl/status:statusor
-//      //third_party/absl/strings
-//      //external/ydf_cc/yggdrasil_decision_forests/api:serving
-// 3. In your existing code, include the .h file and do:
-//   // Load the model (to do only once).
-//   namespace ydf = yggdrasil_decision_forests;
-//   const auto model = ydf::exported_model_123::Load(<path to model>);
-//   // Run the model
-//   predictions = model.Predict();
-// 4. By default, the "Predict" function takes no inputs and creates fake
-//   examples. In practice, you want to add your input data as arguments to
-//   "Predict" and call "examples->Set..." functions accordingly.
-// 4. (Bonus)
-//   Allocate one "examples" and "predictions" per thread and reuse them to
-//   speed-up the inference.
-//
-#ifndef YGGDRASIL_DECISION_FORESTS_GENERATED_MODEL_{key}
-#define YGGDRASIL_DECISION_FORESTS_GENERATED_MODEL_{key}
-
-#include <memory>
-#include <vector>
-
-#include "third_party/absl/status/statusor.h"
-#include "third_party/absl/strings/string_view.h"
-#include "external/ydf_cc/yggdrasil_decision_forests/api/serving.h"
-
-namespace yggdrasil_decision_forests {{
-namespace exported_model_{key} {{
-
-struct ServingModel {{
-  std::vector<float> Predict() const;
-
-  // Compiled model.
-  std::unique_ptr<serving_api::FastEngine> engine;
-
-  // Index of the input features of the model.
-  //
-  // Non-owning pointer. The data is owned by the engine.
-  const serving_api::FeaturesDefinition* features;
-
-  // Number of output predictions for each example.
-  // Equal to 1 for regression, ranking and binary classification with compact
-  // format. Equal to the number of classes for classification.
-  int NumPredictionDimension() const {{
-    return engine->NumPredictionDimension();
-  }}
-
-  // Indexes of the input features.
-{str_feature_vars}
-}};
-
-// TODO: Pass input feature values to "Predict".
-inline std::vector<float> ServingModel::Predict() const {{
-  // Allocate memory for 2 examples. Alternatively, for speed-sensitive code,
-  // an "examples" object can be allocated for each thread and reused. It is
-  // okay to allocate more examples than needed.
-  const int num_examples = 2;
-  auto examples = engine->AllocateExamples(num_examples);
-
-  // Set all the values to be missing. The values may then be overridden by the
-  // "Set*" methods. If all the values are set with "Set*" methods,
-  // "FillMissing" can be skipped.
-  examples->FillMissing(*features);
-
-  // Example #0
-{str_feature_sets_1}
-
-  // Example #1
-{str_feature_sets_2}
-
-  // Run the model on the two examples.
-  //
-  // For speed-sensitive code, reuse the same predictions.
-  std::vector<float> predictions;
-  engine->Predict(*examples, num_examples, &predictions);
-  return predictions;
-}}
-
-inline absl::StatusOr<ServingModel> Load(absl::string_view path) {{
-  ServingModel m;
-
-  // Load the model
-  ASSIGN_OR_RETURN(auto model, serving_api::LoadModel(path));
-
-  // Compile the model into an inference engine.
-  ASSIGN_OR_RETURN(m.engine, model->BuildFastEngine());
-
-  // Index the input features of the model.
-  m.features = &m.engine->features();
-
-  // Index the input features.
-{str_feature_index}
-
-  return m;
-}}
-
-}}  // namespace exported_model_{key}
-}}  // namespace yggdrasil_decision_forests
-
-#endif  // YGGDRASIL_DECISION_FORESTS_GENERATED_MODEL_{key}
-"""
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Template to export a model to c++."""
+
+import datetime
+import re
+from typing import List
+from ydf.proto.dataset import data_spec_pb2
+from ydf import version
+
+
+def _feature_name_to_variable(name: str, prefix: str = "feature_") -> str:
+  """Convert a feature into a variable name."""
+
+  # Replace spaces and non-printable characters with "_".
+  name = re.sub(r"[\W ]", "_", name)
+  return f"{prefix}{name}"
+
+
+def _cc_string(value: str) -> str:
+  """Escapes a c++ string. Does not include the quotes."""
+  return value.replace('"', '\\"')
+
+
+def template(
+    key: str,
+    data_spec: data_spec_pb2.DataSpecification,
+    input_features: List[int],
+) -> str:
+  """Returns the c++ template of exported model.
+
+  Args:
+    key: Identifier of the model. Used to create the namespace and define
+      protection.
+    data_spec: Dataspec of the model.
+    input_features: Input features of the model.
+  """
+
+  feature_vars = []
+  feature_index = []
+  feature_sets_1 = []
+  feature_sets_2 = []
+
+  for feature_idx in input_features:
+    col_spec = data_spec.columns[feature_idx]
+    variable = _feature_name_to_variable(col_spec.name)
+
+    if col_spec.type == data_spec_pb2.ColumnType.NUMERICAL:
+      feature_vars.append(f"  serving_api::NumericalFeatureId {variable};")
+      feature_index.append(
+          f"  ASSIGN_OR_RETURN(m.{variable},"
+          f' m.features->GetNumericalFeatureId("{_cc_string(col_spec.name)}"));'
+      )
+      feature_sets_1.append(
+          f"  examples->SetNumerical(/*example_idx=*/0, {variable}, 1.f,"
+          " *features);"
+      )
+      feature_sets_2.append(
+          f"  examples->SetNumerical(/*example_idx=*/1, {variable}, 2.f,"
+          " *features);"
+      )
+    elif col_spec.type == data_spec_pb2.ColumnType.CATEGORICAL:
+      feature_vars.append(f"  serving_api::CategoricalFeatureId {variable};")
+      feature_index.append(
+          f"  ASSIGN_OR_RETURN(m.{variable},"
+          f' m.features->GetCategoricalFeatureId("{_cc_string(col_spec.name)}"));'
+      )
+      feature_sets_1.append(
+          f'  examples->SetCategorical(/*example_idx=*/0, {variable}, "A",'
+          " *features);"
+      )
+      feature_sets_2.append(
+          f'  examples->SetCategorical(/*example_idx=*/1, {variable}, "B",'
+          " *features);"
+      )
+    elif col_spec.type == data_spec_pb2.ColumnType.BOOLEAN:
+      feature_vars.append(f"  serving_api::BooleanFeatureId {variable};")
+      feature_index.append(
+          f"  ASSIGN_OR_RETURN(m.{variable},"
+          f' m.features->GetBooleanFeatureId("{_cc_string(col_spec.name)}"));'
+      )
+      feature_sets_1.append(
+          f"  examples->SetBoolean(/*example_idx=*/0, {variable}, true,"
+          " *features);"
+      )
+      feature_sets_2.append(
+          f"  examples->SetBoolean(/*example_idx=*/1, {variable}, false,"
+          " *features);"
+      )
+    elif col_spec.type == data_spec_pb2.ColumnType.CATEGORICAL_SET:
+      feature_vars.append(f"  serving_api::CategoricalSetFeatureId {variable};")
+      feature_index.append(
+          f"  ASSIGN_OR_RETURN(m.{variable},"
+          f' m.features->GetCategoricalFeatureId("{_cc_string(col_spec.name)}"));'
+      )
+      feature_sets_1.append(
+          f"  examples->SetCategoricalSet(/*example_idx=*/0, {variable},"
+          ' std::vector<std::string>{"hello", "world"}, *features);'
+      )
+      feature_sets_2.append(
+          f"  examples->SetCategoricalSet(/*example_idx=*/1, {variable},"
+          ' std::vector<std::string> {"blue", "dog"}, *features);'
+      )
+    else:
+      raise ValueError(
+          "The automatic c++ exporter does not support yet feature"
+          f" {col_spec.name!r} with semantic"
+          f" {data_spec_pb2.ColumnType.Name(col_spec.type)}. Use the serving"
+          " API manually instead."
+      )
+
+  str_feature_vars = "\n".join(feature_vars)
+  str_feature_index = "\n".join(feature_index)
+  str_feature_sets_1 = "\n".join(feature_sets_1)
+  str_feature_sets_2 = "\n".join(feature_sets_2)
+
+  return f"""\
+// Automatically generated code running an Yggdrasil Decision Forests model in
+// C++. This code was generated with "model.to_cpp()".
+//
+// Date of generation: {datetime.datetime.now()}
+// YDF Version: {version.version}
+//
+// How to use this code:
+//
+// 1. Copy this code in a new .h file.
+// 2. If you use Bazel/Blaze, use the following dependencies:
+//      //third_party/absl/status:statusor
+//      //third_party/absl/strings
+//      //external/ydf_cc/yggdrasil_decision_forests/api:serving
+// 3. In your existing code, include the .h file and do:
+//   // Load the model (to do only once).
+//   namespace ydf = yggdrasil_decision_forests;
+//   const auto model = ydf::exported_model_123::Load(<path to model>);
+//   // Run the model
+//   predictions = model.Predict();
+// 4. By default, the "Predict" function takes no inputs and creates fake
+//   examples. In practice, you want to add your input data as arguments to
+//   "Predict" and call "examples->Set..." functions accordingly.
+// 4. (Bonus)
+//   Allocate one "examples" and "predictions" per thread and reuse them to
+//   speed-up the inference.
+//
+#ifndef YGGDRASIL_DECISION_FORESTS_GENERATED_MODEL_{key}
+#define YGGDRASIL_DECISION_FORESTS_GENERATED_MODEL_{key}
+
+#include <memory>
+#include <vector>
+
+#include "third_party/absl/status/statusor.h"
+#include "third_party/absl/strings/string_view.h"
+#include "external/ydf_cc/yggdrasil_decision_forests/api/serving.h"
+
+namespace yggdrasil_decision_forests {{
+namespace exported_model_{key} {{
+
+struct ServingModel {{
+  std::vector<float> Predict() const;
+
+  // Compiled model.
+  std::unique_ptr<serving_api::FastEngine> engine;
+
+  // Index of the input features of the model.
+  //
+  // Non-owning pointer. The data is owned by the engine.
+  const serving_api::FeaturesDefinition* features;
+
+  // Number of output predictions for each example.
+  // Equal to 1 for regression, ranking and binary classification with compact
+  // format. Equal to the number of classes for classification.
+  int NumPredictionDimension() const {{
+    return engine->NumPredictionDimension();
+  }}
+
+  // Indexes of the input features.
+{str_feature_vars}
+}};
+
+// TODO: Pass input feature values to "Predict".
+inline std::vector<float> ServingModel::Predict() const {{
+  // Allocate memory for 2 examples. Alternatively, for speed-sensitive code,
+  // an "examples" object can be allocated for each thread and reused. It is
+  // okay to allocate more examples than needed.
+  const int num_examples = 2;
+  auto examples = engine->AllocateExamples(num_examples);
+
+  // Set all the values to be missing. The values may then be overridden by the
+  // "Set*" methods. If all the values are set with "Set*" methods,
+  // "FillMissing" can be skipped.
+  examples->FillMissing(*features);
+
+  // Example #0
+{str_feature_sets_1}
+
+  // Example #1
+{str_feature_sets_2}
+
+  // Run the model on the two examples.
+  //
+  // For speed-sensitive code, reuse the same predictions.
+  std::vector<float> predictions;
+  engine->Predict(*examples, num_examples, &predictions);
+  return predictions;
+}}
+
+inline absl::StatusOr<ServingModel> Load(absl::string_view path) {{
+  ServingModel m;
+
+  // Load the model
+  ASSIGN_OR_RETURN(auto model, serving_api::LoadModel(path));
+
+  // Compile the model into an inference engine.
+  ASSIGN_OR_RETURN(m.engine, model->BuildFastEngine());
+
+  // Index the input features of the model.
+  m.features = &m.engine->features();
+
+  // Index the input features.
+{str_feature_index}
+
+  return m;
+}}
+
+}}  // namespace exported_model_{key}
+}}  // namespace yggdrasil_decision_forests
+
+#endif  // YGGDRASIL_DECISION_FORESTS_GENERATED_MODEL_{key}
+"""
```

## ydf/model/tf_model_test.py

 * *Ordering differences only*

```diff
@@ -1,1181 +1,1181 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Model tests depending on TensorFlow.
-
-TensorFlow cannot be compiled in debug mode, so these tests are separated out to
-improve debuggability of the remaining model tests.
-"""
-
-import math
-import os
-import tempfile
-from typing import Any, Callable, Dict, List, Mapping, Optional, Tuple
-
-from absl import logging
-from absl.testing import absltest
-from absl.testing import parameterized
-import numpy as np
-import numpy.testing as npt
-import pandas as pd
-import tensorflow as tf
-import tensorflow_decision_forests as tfdf
-
-from ydf.dataset import dataspec
-from ydf.learner import generic_learner
-from ydf.learner import specialized_learners
-from ydf.model import export_tf
-from ydf.model import model_lib
-from ydf.utils import test_utils
-
-
-class TfModelTest(parameterized.TestCase):
-
-  def create_ds(
-      self, columns: List[str], label: Optional[str], weights: Optional[str]
-  ) -> pd.DataFrame:
-    df = pd.DataFrame({})
-    if "cat_int_small" in columns:
-      df["cat_int_small"] = [1, 2, 3, 4, 3, 2, 1, 0, -1, -2]
-    if "cat_int_large" in columns:
-      df["cat_int_large"] = [100, 200, 300, 400, 300, 20, 100, 0, -100, -200]
-    if "cat_string" in columns:
-      df["cat_string"] = ["a", "a", "a", "b", "b", "b", "c", "c", "c", "d"]
-    if "num" in columns:
-      df["num"] = [0, 1, 2, 3, 4, 5.5, 6.6, 7.7, 8.8, 9.9]
-
-    if weights is not None:
-      df["weights"] = list(range(1, 11))
-
-    if label == "classification_binary_int":
-      df["label"] = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]
-    elif label == "classification_binary_str":
-      df["label"] = ["a", "b", "a", "b", "a", "b", "a", "b", "a", "b"]
-    elif label == "classification_multiclass_int":
-      df["label"] = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2]
-    elif label == "classification_multiclass_str":
-      df["label"] = ["a", "b", "c", "d", "a", "b", "c", "d", "a", "b"]
-    elif label == "regression":
-      df["label"] = [42.5, 43.3, 73.4, 21.1, 26.4, 9.3, -1, -44, -23.4, 234.3]
-    elif label == "ranking":
-      df["ranking_group"] = ["a", "a", "a", "a", "a", "b", "b", "b", "b", "b"]
-      df["label"] = [0, 0, 0, 1, 1, 1, 3, 3, 4, 4]
-    return df
-
-  def create_dataset_v2(self, columns: List[str]) -> Dict[str, Any]:
-    """Creates a dataset with random values."""
-    data = {
-        # Single-dim features
-        "f1": np.random.random(size=100),
-        "f2": np.random.random(size=100),
-        "i1": np.random.randint(100, size=100),
-        "i2": np.random.randint(100, size=100),
-        "c1": np.random.choice(["x", "y", "z"], size=100),
-        "b1": np.random.randint(2, size=100).astype(np.bool_),
-        "b2": np.random.randint(2, size=100).astype(np.bool_),
-        # Cat-set features
-        "cs1": [[], ["a", "b", "c"], ["b", "c"], ["a"]] * 25,
-        # Multi-dim features
-        "multi_f1": np.random.random(size=(100, 5)),
-        "multi_f2": np.random.random(size=(100, 5)),
-        "multi_i1": np.random.randint(100, size=(100, 5)),
-        "multi_c1": np.random.choice(["x", "y", "z"], size=(100, 5)),
-        "multi_b1": np.random.randint(2, size=(100, 5)).astype(np.bool_),
-        # Labels
-        "label_class_binary": np.random.choice(["l1", "l2"], size=100),
-        "label_class_multi": np.random.choice(["l1", "l2", "l3"], size=100),
-        "label_regress": np.random.random(size=100),
-    }
-    return {k: data[k] for k in columns}
-
-  def create_csv(
-      self, columns: List[str], label: Optional[str], weights: Optional[str]
-  ) -> str:
-    tmp_dir = self.create_tempdir()
-    csv_file = tmp_dir.create_file("file.csv")
-    self.create_ds(columns, label, weights).to_csv(
-        csv_file.full_path, index=False
-    )
-    return csv_file.full_path
-
-  def create_tfdf_model_and_test_df(
-      self,
-      ds_type: str,
-      feature_col: str,
-      label_col: str,
-      weight_col: str,
-      model_path: str,
-  ) -> Tuple[tfdf.keras.GradientBoostedTreesModel, pd.DataFrame]:
-    # Set feature-specific hyperparameters of the model.
-    if feature_col.startswith("cat_"):
-      column_semantic = tfdf.keras.FeatureSemantic.CATEGORICAL
-    elif feature_col.startswith("num"):
-      column_semantic = tfdf.keras.FeatureSemantic.NUMERICAL
-    else:
-      raise ValueError(f"Could not determine semantic of column {feature_col}")
-    needs_min_vocab_frequency = (
-        column_semantic == tfdf.keras.FeatureSemantic.CATEGORICAL
-    )
-
-    # Set task-specific hyperparameters of the model.
-    ranking_group = None
-    if label_col.startswith("classification"):
-      task = tfdf.keras.Task.CLASSIFICATION
-    elif label_col.startswith("regression"):
-      task = tfdf.keras.Task.REGRESSION
-    elif label_col.startswith("ranking"):
-      task = tfdf.keras.Task.RANKING
-      ranking_group = "ranking_group"
-    else:
-      raise ValueError(f"Could not determine task for label {label_col}")
-
-    # Create an empty, small TF-DF model with the given hyperparameters.
-    tfdf_model = tfdf.keras.GradientBoostedTreesModel(
-        task=task,
-        min_examples=1,
-        num_trees=5,
-        ranking_group=ranking_group,
-        features=[
-            tfdf.keras.FeatureUsage(
-                name=feature_col,
-                semantic=column_semantic,
-                min_vocab_frequency=2 if needs_min_vocab_frequency else None,
-            )
-        ],
-    )
-
-    # Create the training dataset and fit the model to it.
-    if ds_type == "pd":
-      train_df = self.create_ds(
-          columns=[feature_col], label=label_col, weights=weight_col
-      )
-      train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(
-          train_df, label="label", task=task, weight="weights"
-      )
-      tfdf_model.fit(train_ds)
-    elif ds_type == "file":
-      train_ds_path = self.create_csv(
-          columns=[feature_col], label=label_col, weights=weight_col
-      )
-      tfdf_model.fit_on_dataset_path(
-          train_ds_path, label_key="label", weight_key="weights"
-      )
-    else:
-      raise ValueError(f"Unknown dataset type {ds_type}")
-    tfdf_model.save(model_path)
-
-    # Create the test dataset. Note that the test dataset is always a Pandas
-    # dataframe, since predicting from file is not supported.
-    test_df = self.create_ds(columns=[feature_col], label=None, weights=None)
-    return tfdf_model, test_df
-
-  @parameterized.product(
-      feature_col=["cat_int_small", "cat_int_large", "cat_string", "num"],
-      label_col=[
-          "classification_binary_int",
-          "classification_binary_str",
-          "classification_multiclass_int",
-          "classification_multiclass_str",
-          "regression",
-          "ranking",
-      ],
-      ds_type=["file", "pd"],
-  )
-  def test_tfdf_ydf_prediction_equality(
-      self, feature_col: str, label_col: str, ds_type: str
-  ):
-    model_dir = self.create_tempdir().full_path
-    # When reading from file, TF-DF casts integer categories to strings, but it
-    # doesn't when converting from Pandas, so the only way to feed matching data
-    # to the model is to just feed it string data, and we omit the int cases.
-    if ds_type == "file" and feature_col.startswith("cat_int"):
-      self.skipTest(
-          "Categorical Integer features don't work in TF-DF when reading from"
-          " file"
-      )
-
-    tfdf_model, test_df = self.create_tfdf_model_and_test_df(
-        ds_type, feature_col, label_col, "weights", model_dir
-    )
-    test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df)
-
-    tfdf_predictions = tfdf_model.predict(test_ds)
-    ydf_model = model_lib.from_tensorflow_decision_forests(model_dir)
-    ydf_predictions = ydf_model.predict(test_df)
-    npt.assert_allclose(
-        tfdf_predictions.flatten(), ydf_predictions.flatten(), atol=0.0001
-    )
-
-  @parameterized.product(
-      feature_col=["cat_string", "num"],
-      label_col=[
-          "classification_binary_int",
-          "classification_binary_str",
-          "classification_multiclass_int",
-          "classification_multiclass_str",
-          "regression",
-          "ranking",
-      ],
-      ds_type=["file", "pd"],
-  )
-  def test_tfdf_convert_back_and_forth(
-      self, feature_col: str, label_col: str, ds_type: str
-  ):
-    model_dir = self.create_tempdir().full_path
-
-    tfdf_model, test_df = self.create_tfdf_model_and_test_df(
-        ds_type, feature_col, label_col, "weights", model_dir
-    )
-    test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df)
-    new_model_dir = self.create_tempdir().full_path
-
-    # Create the YDF model.
-    ydf_model = model_lib.from_tensorflow_decision_forests(model_dir)
-    # Convert the YDF model back to a TF-DF model and load it.
-    ydf_model.to_tensorflow_saved_model(new_model_dir)
-    new_tfdf_model = tf.keras.models.load_model(new_model_dir)
-
-    # Check for prediction equality.
-    tfdf_predictions = tfdf_model.predict(test_ds)
-    new_tfdf_predictions = new_tfdf_model.predict(test_ds)
-    npt.assert_allclose(
-        tfdf_predictions.flatten(), new_tfdf_predictions.flatten(), atol=0.0001
-    )
-
-  @parameterized.product(
-      feature_col=["cat_int_small", "cat_int_large"],
-      label_col=[
-          "classification_binary_int",
-          "classification_binary_str",
-          "classification_multiclass_int",
-          "classification_multiclass_str",
-          "regression",
-          "ranking",
-      ],
-      ds_type=["pd"],
-  )
-  def test_tfdf_convert_back_and_forth_cat_int(
-      self, feature_col: str, label_col: str, ds_type: str
-  ):
-    model_dir = self.create_tempdir().full_path
-
-    tfdf_model, test_df = self.create_tfdf_model_and_test_df(
-        ds_type, feature_col, label_col, "weights", model_dir
-    )
-    test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df)
-    new_model_dir = self.create_tempdir().full_path
-
-    # Create the YDF model.
-    ydf_model = model_lib.from_tensorflow_decision_forests(model_dir)
-    # Convert the YDF model back to a TF-DF model and load it.
-    ydf_model.to_tensorflow_saved_model(new_model_dir)
-    new_tfdf_model = tf.keras.models.load_model(new_model_dir)
-
-    # Prepare the test dataset for the loaded model: Categorical integer
-    # features are now string features.
-    new_test_df = test_df.copy(deep=True)
-    new_test_df[feature_col] = new_test_df.astype(np.str_)
-    new_test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(new_test_df)
-
-    # Check for prediction equality.
-    tfdf_predictions = tfdf_model.predict(test_ds)
-    new_tfdf_predictions = new_tfdf_model.predict(new_test_ds)
-    npt.assert_allclose(
-        tfdf_predictions.flatten(), new_tfdf_predictions.flatten(), atol=0.0001
-    )
-
-  def test_to_tensorflow_saved_model_serialized_input(self):
-    # TODO: b/321204507 - Integrate logic in YDF.
-
-    train_ds_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "adult_train.recordio"
-    )
-    test_ds_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "adult_test.recordio"
-    )
-
-    # Train a model on the tfrecord directly.
-    ydf_model = specialized_learners.GradientBoostedTreesLearner(
-        label="income",
-        num_trees=10,
-    ).train(f"tfrecordv2+tfe:{train_ds_path}")
-
-    test_predictions = ydf_model.predict(f"tfrecordv2+tfe:{test_ds_path}")
-
-    tempdir = self.create_tempdir().full_path
-
-    # Export the model to a TensorFlow Saved Model.
-    #
-    # This model expects inputs in the form of a dictionary of features
-    # values e.g. {"age": [...], "capital_gain": [...], ...}.
-    #
-    # This is referred as the "predict" API.
-    path_wo_signature = os.path.join(tempdir, "mdl")
-    ydf_model.to_tensorflow_saved_model(path_wo_signature)
-
-    # Load the model, and add a serialized tensorflow examples protobuffer
-    # input signature. In other words, the model expects as input a serialized
-    # tensorflow example proto.
-    #
-    # This is often referred the "classify" or "regress" API.
-    path_w_signature = os.path.join(tempdir, "mdl_ws")
-    tf_model_wo_signature = tf.keras.models.load_model(path_wo_signature)
-
-    # The list of input features to read from the tensorflow example proto.
-    # Note that tensorflow example proto dtypes can only be float32, int64 or
-    # string.
-    feature_spec = {
-        # Numerical
-        "age": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),
-        "capital_gain": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),
-        "capital_loss": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),
-        "education_num": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),
-        "fnlwgt": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),
-        "hours_per_week": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),
-        # Categorical
-        "education": tf.io.FixedLenFeature(shape=[], dtype=tf.string),
-        "marital_status": tf.io.FixedLenFeature(shape=[], dtype=tf.string),
-        "native_country": tf.io.FixedLenFeature(
-            shape=[], dtype=tf.string, default_value=""
-        ),
-        "occupation": tf.io.FixedLenFeature(shape=[], dtype=tf.string),
-        "race": tf.io.FixedLenFeature(shape=[], dtype=tf.string),
-        "relationship": tf.io.FixedLenFeature(shape=[], dtype=tf.string),
-        "sex": tf.io.FixedLenFeature(shape=[], dtype=tf.string),
-        "workclass": tf.io.FixedLenFeature(shape=[], dtype=tf.string),
-    }
-
-    # The "classify" requires for all the predictions to return the label
-    # classes. This only make sense for a classification model.
-    label_classes = ydf_model.label_classes()
-
-    # The "make_classify_fn" function combines a tensorflow example proto
-    # parsing stage with the classical model inference.
-    def make_classify_fn(
-        model: tf.keras.Model,
-    ) -> Callable[[tf.Tensor], Mapping[str, tf.Tensor]]:
-      @tf.function(
-          input_signature=[
-              tf.TensorSpec([None], dtype=tf.string, name="inputs")
-          ]
-      )
-      def classify(
-          serialized_tf_examples: tf.Tensor,
-      ) -> Mapping[str, tf.Tensor]:
-        # Parse the serialized tensorflow proto examples into a dictionary of
-        # tensor values.
-        parsed_features = tf.io.parse_example(
-            serialized_tf_examples, feature_spec
-        )
-
-        # Cast all the int64 features into float32 values.
-        #
-        # The SavedModel exported by YDF expects float32 value for all
-        # numerical features (can be overridden with the
-        # `input_model_signature_fn` argument). However, in this dataset, the
-        # numerical features are stored as int64.
-        for feature in parsed_features:
-          if parsed_features[feature].dtype == tf.int64:
-            parsed_features[feature] = tf.cast(
-                parsed_features[feature], tf.float32
-            )
-
-        # Apply the model.
-        outputs = model(parsed_features)
-
-        # Extract the label classes. The "classify" API expects for the label
-        # classes to be returned with every predictions.
-        batch_size = tf.shape(serialized_tf_examples)[0]
-        batched_label_classes = tf.broadcast_to(
-            input=tf.constant(label_classes),
-            shape=(batch_size, len(label_classes)),
-        )
-
-        return {"classes": batched_label_classes, "scores": outputs}
-
-      return classify
-
-    # Save the model to a SavedModel with the "classify" signature.
-    signatures = {
-        "classify": make_classify_fn(tf_model_wo_signature),
-    }
-    tf.saved_model.save(
-        tf_model_wo_signature, path_w_signature, signatures=signatures
-    )
-    tf_model_w_signature = tf.saved_model.load(path_w_signature)
-
-    logging.info("Available signatures: %s", tf_model_w_signature.signatures)
-
-    # Generate predictions with the "classify" signature.
-    classify_model = tf_model_w_signature.signatures["classify"]
-
-    for example_idx, serialized_example in enumerate(
-        tf.data.TFRecordDataset([test_ds_path]).take(10)
-    ):
-      prediction = classify_model(inputs=[serialized_example.numpy()])
-      logging.info("prediction:%s", prediction)
-      npt.assert_almost_equal(
-          prediction["scores"].numpy(), test_predictions[example_idx]
-      )
-      npt.assert_equal(prediction["classes"].numpy(), [[b"<=50K", b">50K"]])
-
-  def test_to_tensorflow_function(self):
-    """A simple function conversion."""
-
-    # Create YDF model
-    columns = ["f1", "f2", "i1", "c1", "b1", "cs1", "label_class_binary"]
-    model = specialized_learners.RandomForestLearner(
-        label="label_class_binary",
-        num_trees=10,
-        features=[("cs1", dataspec.Semantic.CATEGORICAL_SET)],
-        include_all_columns=True,
-    ).train(self.create_dataset_v2(columns))
-
-    # Golden predictions
-    test_ds = self.create_dataset_v2(columns)
-    ydf_predictions = model.predict(test_ds)
-
-    # Convert model to tf function + generate predictions
-    tf_function = model.to_tensorflow_function()
-    tf_test_ds = {
-        "f1": tf.constant(test_ds["f1"]),
-        "f2": tf.constant(test_ds["f2"]),
-        "i1": tf.constant(test_ds["i1"]),
-        "c1": tf.constant(test_ds["c1"]),
-        "b1": tf.constant(test_ds["b1"]),
-        "cs1": tf.ragged.constant(test_ds["cs1"]),
-    }
-    tf_predictions = tf_function(tf_test_ds)
-
-    npt.assert_array_equal(ydf_predictions, tf_predictions)
-
-  @parameterized.product(can_be_saved=[True, False])
-  def test_to_multi_tensorflow_function(self, can_be_saved: bool):
-    """Function export and serialization with multiple YDF models."""
-
-    # Create YDF model
-    columns = ["f1", "f2", "i1", "c1", "label_class_binary"]
-    model_1 = specialized_learners.RandomForestLearner(
-        label="label_class_binary", num_trees=10
-    ).train(self.create_dataset_v2(columns))
-    model_2 = specialized_learners.RandomForestLearner(
-        label="label_class_binary", num_trees=10
-    ).train(self.create_dataset_v2(columns))
-
-    # Golden predictions
-    test_ds = self.create_dataset_v2(columns)
-    ydf_predictions = model_1.predict(test_ds) + model_2.predict(test_ds) * 2
-
-    # Convert to tf function + generate predictions
-    tf_function_m1 = model_1.to_tensorflow_function(can_be_saved=can_be_saved)
-    tf_function_m2 = model_2.to_tensorflow_function(can_be_saved=can_be_saved)
-
-    @tf.function
-    def tf_function(features):
-      return tf_function_m1(features) + tf_function_m2(features) * 2
-
-    tf_test_ds = {k: tf.constant(v) for k, v in test_ds.items()}
-    tf_predictions = tf_function(tf_test_ds)
-
-    # Check predictions
-    npt.assert_array_equal(ydf_predictions, tf_predictions)
-
-    if not can_be_saved:
-      return
-
-    # Serialize / unserialize model
-    with tempfile.TemporaryDirectory() as tmp_dir:
-      tf_model = tf.Module()
-      tf_model.__call__ = tf_function
-      tf_model.tf_function_m1 = tf_function_m1
-      tf_model.tf_function_m2 = tf_function_m2
-      tf.saved_model.save(tf_model, tmp_dir)
-      loaded_tf_model = tf.saved_model.load(tmp_dir)
-
-    # Check predictions gain
-    loaded_tf_predictions = loaded_tf_model(tf_test_ds)
-    npt.assert_array_equal(ydf_predictions, loaded_tf_predictions)
-
-  def test_to_tensorflow_function_with_multidim_input(self):
-    # Train a model
-    columns = [
-        "multi_f1",
-        "multi_i1",
-        "multi_c1",
-        "multi_b1",
-        "label_class_binary",
-    ]
-    train_ds = self.create_dataset_v2(columns)
-    model = specialized_learners.RandomForestLearner(
-        label="label_class_binary"
-    ).train(train_ds)
-    # Generate predictions with YDF
-    ydf_predictions = model.predict(train_ds)
-
-    # Convert ydf model into a tf function
-    tf_function = model.to_tensorflow_function()
-
-    # Validate the tf model predictions
-    tf_test_ds = {k: tf.constant(train_ds[k]) for k in columns[:-1]}
-    tf_predictions = tf_function(tf_test_ds)
-    npt.assert_array_equal(ydf_predictions, tf_predictions)
-
-  def test_to_raw_tensorflow_saved_model(self):
-    """Simple export to SavedModel format."""
-
-    # Create YDF model
-    columns = ["f1", "f2", "i1", "c1", "b1", "cs1", "label_class_binary"]
-    model = specialized_learners.RandomForestLearner(
-        label="label_class_binary",
-        num_trees=10,
-        features=[("cs1", dataspec.Semantic.CATEGORICAL_SET)],
-        include_all_columns=True,
-    ).train(self.create_dataset_v2(columns))
-
-    # Golden predictions
-    test_ds = self.create_dataset_v2(columns[:-1])
-    ydf_predictions = model.predict(test_ds)
-
-    # Save model
-    tmp_dir = self.create_tempdir().full_path
-    model.to_tensorflow_saved_model(
-        tmp_dir,
-        mode="tf",
-        servo_api=False,
-        feed_example_proto=False,
-        feature_dtypes={"f1": tf.float32},
-    )
-
-    # Load model
-    tf_model = tf.saved_model.load(tmp_dir)
-
-    # Test predictions
-    tf_test_ds = {
-        # While f1 was feed as a float64, it was saved as a float32.
-        "f1": tf.constant(test_ds["f1"], tf.float32),
-        "f2": tf.constant(test_ds["f2"]),
-        "i1": tf.constant(test_ds["i1"]),
-        "c1": tf.constant(test_ds["c1"]),
-        "b1": tf.constant(test_ds["b1"]),
-        "cs1": tf.ragged.constant(test_ds["cs1"]),
-    }
-    tf_predictions = tf_model(tf_test_ds)
-    npt.assert_equal(ydf_predictions, tf_predictions)
-
-  @parameterized.parameters(True, False)
-  def test_to_raw_tensorflow_saved_model_with_multidim_input(
-      self, with_filter: bool
-  ):
-    # Create YDF model
-    columns = [
-        "multi_f1",
-        "multi_i1",
-        "multi_c1",
-        "multi_b1",
-        "label_class_binary",
-    ]
-    feature_columns = columns[:-2] if with_filter else columns[:-1]
-    train_ds = self.create_dataset_v2(columns)
-    model = specialized_learners.RandomForestLearner(
-        label="label_class_binary",
-        num_trees=10,
-        features=feature_columns if with_filter else None,
-    ).train(train_ds)
-
-    # Golden predictions
-    ydf_predictions = model.predict(train_ds)
-
-    # Save model
-    tmp_dir = self.create_tempdir().full_path
-    model.to_tensorflow_saved_model(
-        tmp_dir,
-        mode="tf",
-        servo_api=False,
-        feed_example_proto=False,
-    )
-
-    # Load model
-    tf_model = tf.saved_model.load(tmp_dir)
-
-    # Test predictions
-    tf_test_ds = {k: tf.constant(train_ds[k]) for k in feature_columns}
-    tf_predictions = tf_model(tf_test_ds)
-    npt.assert_equal(ydf_predictions, tf_predictions)
-
-  def test_to_tensorflow_saved_model_classify_api(self):
-    """Export to SavedModel format with regress API."""
-    columns = ["f1", "f2", "label_class_multi"]
-    model = specialized_learners.RandomForestLearner(
-        label="label_class_multi",
-        num_trees=10,
-        task=generic_learner.Task.CLASSIFICATION,
-    ).train(self.create_dataset_v2(columns))
-    test_ds = self.create_dataset_v2(columns[:-1])
-    ydf_predictions = model.predict(test_ds)
-    tmp_dir = self.create_tempdir().full_path
-    model.to_tensorflow_saved_model(
-        path=tmp_dir,
-        mode="tf",
-        feed_example_proto=False,
-        servo_api=True,
-    )
-    tf_model = tf.saved_model.load(tmp_dir)
-    tf_model_predict = tf_model.signatures["serving_default"]
-    tf_prediction = tf_model_predict(**test_ds)
-    self.assertEqual(tf_prediction["scores"].shape, (100, 3))
-    self.assertEqual(tf_prediction["classes"].shape, (100, 3))
-    npt.assert_equal(ydf_predictions, tf_prediction["scores"])
-
-  def test_to_tensorflow_saved_model_regress_api(self):
-    """Export to SavedModel format with regress API."""
-    columns = ["f1", "i1", "label_regress"]
-    model = specialized_learners.RandomForestLearner(
-        label="label_regress",
-        num_trees=10,
-        task=generic_learner.Task.REGRESSION,
-    ).train(self.create_dataset_v2(columns))
-    test_ds = self.create_dataset_v2(columns[:-1])
-    ydf_predictions = model.predict(test_ds)
-    tmp_dir = self.create_tempdir().full_path
-    model.to_tensorflow_saved_model(
-        path=tmp_dir,
-        mode="tf",
-        feed_example_proto=False,
-        servo_api=True,
-    )
-    tf_model = tf.saved_model.load(tmp_dir)
-    tf_model_predict = tf_model.signatures["serving_default"]
-    tf_prediction = tf_model_predict(**test_ds)
-    self.assertEqual(tf_prediction["outputs"].shape, (100,))
-    npt.assert_equal(ydf_predictions, tf_prediction["outputs"])
-
-  def test_to_tensorflow_saved_model_with_example_proto(self):
-    """Export to SavedModel format with serialized example inputs."""
-
-    # Create YDF model
-    columns = ["f1", "i1", "i2", "c1", "b1", "b2", "cs1", "label_class_binary"]
-    model = specialized_learners.RandomForestLearner(
-        label="label_class_binary",
-        num_trees=10,
-        features=[("cs1", dataspec.Semantic.CATEGORICAL_SET)],
-        include_all_columns=True,
-    ).train(self.create_dataset_v2(columns))
-
-    test_ds = self.create_dataset_v2(columns[:-1])
-
-    # Save model
-    tmp_dir = self.create_tempdir().full_path
-
-    def pre_processing(features):
-      features = features.copy()
-      features["f1"] = features["f1"] * 2
-      return features
-
-    def post_processing(output):
-      return output * 3
-
-    # Model predictions with all transformations
-    ydf_no_post_process_predictions = model.predict(pre_processing(test_ds))
-    # Add extra dimension: ydf squeeze its predictions, while the servo api
-    # expects non-squeezed predictions.
-    ydf_no_post_process_predictions = np.stack(
-        [
-            1.0 - ydf_no_post_process_predictions,
-            ydf_no_post_process_predictions,
-        ],
-        axis=1,
-    )
-    ydf_predictions = post_processing(ydf_no_post_process_predictions)
-
-    model.to_tensorflow_saved_model(
-        tmp_dir,
-        mode="tf",
-        feature_dtypes={"i2": tf.float32, "b2": tf.float32},
-        pre_processing=pre_processing,
-        post_processing=post_processing,
-        servo_api=True,
-        feed_example_proto=True,
-    )
-
-    # Load model
-    tf_model = tf.saved_model.load(tmp_dir)
-
-    # Raw predictions
-    tf_test_ds = {
-        "f1": tf.constant(test_ds["f1"]),
-        "i1": tf.constant(test_ds["i1"]),
-        "i2": tf.constant(test_ds["i2"], dtype=tf.float32),
-        "c1": tf.constant(test_ds["c1"]),
-        "b1": tf.constant(test_ds["b1"]),
-        "b2": tf.constant(test_ds["b2"], dtype=tf.float32),
-        "cs1": tf.ragged.constant(test_ds["cs1"]),
-    }
-    raw_tf_predictions = tf_model(tf_test_ds)
-    npt.assert_array_equal(ydf_predictions, raw_tf_predictions)
-
-    # Stored pre and post processing.
-    for feature in columns[:-2]:
-      npt.assert_array_equal(
-          tf_model.pre_processing(tf_test_ds)[feature],
-          pre_processing(tf_test_ds)[feature],
-      )
-
-    npt.assert_array_equal(
-        tf_model.post_processing(tf.constant([[1.0, 2.0]], tf.float32)),
-        post_processing(tf.constant([[1.0, 2.0]], tf.float32)),
-    )
-
-    # Servo API predictions
-    tf_model_predict = tf_model.signatures["serving_default"]
-    tf_test_ds = []
-    for example_idx in range(100):
-      tf_test_ds.append(
-          tf.train.Example(
-              features=tf.train.Features(
-                  feature={
-                      "f1": tf.train.Feature(
-                          float_list=tf.train.FloatList(
-                              value=[test_ds["f1"][example_idx]]
-                          )
-                      ),
-                      "i1": tf.train.Feature(
-                          int64_list=tf.train.Int64List(
-                              value=[test_ds["i1"][example_idx]]
-                          )
-                      ),
-                      "i2": tf.train.Feature(
-                          float_list=tf.train.FloatList(
-                              value=[test_ds["i2"][example_idx]]
-                          )
-                      ),
-                      "c1": tf.train.Feature(
-                          bytes_list=tf.train.BytesList(
-                              value=[bytes(test_ds["c1"][example_idx], "utf-8")]
-                          )
-                      ),
-                      "b1": tf.train.Feature(
-                          int64_list=tf.train.Int64List(
-                              value=[test_ds["b1"][example_idx]]
-                          )
-                      ),
-                      "b2": tf.train.Feature(
-                          float_list=tf.train.FloatList(
-                              value=[test_ds["b2"][example_idx]]
-                          )
-                      ),
-                      "cs1": tf.train.Feature(
-                          bytes_list=tf.train.BytesList(
-                              value=[
-                                  bytes(x, "utf-8")
-                                  for x in test_ds["cs1"][example_idx]
-                              ]
-                          )
-                      ),
-                  }
-              )
-          ).SerializeToString()
-      )
-    tf_predictions = tf_model_predict(inputs=tf_test_ds)
-    npt.assert_equal(ydf_predictions, tf_predictions["scores"])
-
-  @parameterized.parameters(True, False)
-  def test_to_tensorflow_saved_model_with_example_proto_multidim(
-      self, with_filter: bool
-  ):
-    """Export to SavedModel format with serialized example inputs."""
-
-    # Create YDF model
-    columns = [
-        "multi_f1",
-        "multi_i1",
-        "multi_c1",
-        "multi_b1",
-        "label_class_binary",
-    ]
-    feature_columns = columns[:-2] if with_filter else columns[:-1]
-    model = specialized_learners.RandomForestLearner(
-        label="label_class_binary",
-        num_trees=10,
-        features=feature_columns if with_filter else None,
-    ).train(self.create_dataset_v2(columns))
-
-    # Golden predictions
-    test_ds = self.create_dataset_v2(feature_columns)
-
-    # Save model
-    tmp_dir = self.create_tempdir().full_path
-
-    # Golden predictions
-    ydf_predictions = model.predict(test_ds)
-    # Add extra dimension: ydf squeeze its predictions, while the servo api
-    # expects non-squeezed predictions.
-    ydf_predictions = np.stack(
-        [
-            1.0 - ydf_predictions,
-            ydf_predictions,
-        ],
-        axis=1,
-    )
-
-    model.to_tensorflow_saved_model(
-        tmp_dir, mode="tf", servo_api=True, feed_example_proto=True
-    )
-
-    # Load model
-    tf_model = tf.saved_model.load(tmp_dir)
-
-    # Raw predictions
-    tf_test_ds = {k: tf.constant(test_ds[k]) for k in feature_columns}
-    raw_tf_predictions = tf_model(tf_test_ds)
-    npt.assert_array_equal(ydf_predictions, raw_tf_predictions)
-
-    # Servo API predictions
-    tf_model_predict = tf_model.signatures["serving_default"]
-    tf_test_ds = []
-    for example_idx in range(100):
-      proto_feature = {
-          "multi_f1": tf.train.Feature(
-              float_list=tf.train.FloatList(
-                  value=test_ds["multi_f1"][example_idx][:]
-              )
-          ),
-          "multi_i1": tf.train.Feature(
-              int64_list=tf.train.Int64List(
-                  value=test_ds["multi_i1"][example_idx][:]
-              )
-          ),
-          "multi_c1": tf.train.Feature(
-              bytes_list=tf.train.BytesList(
-                  value=[
-                      bytes(x, "utf-8")
-                      for x in test_ds["multi_c1"][example_idx]
-                  ]
-              )
-          ),
-      }
-      if not with_filter:
-        proto_feature["multi_b1"] = tf.train.Feature(
-            int64_list=tf.train.Int64List(
-                value=test_ds["multi_b1"][example_idx][:]
-            )
-        )
-      tf_test_ds.append(
-          tf.train.Example(
-              features=tf.train.Features(feature=proto_feature)
-          ).SerializeToString()
-      )
-    tf_predictions = tf_model_predict(inputs=tf_test_ds)
-    npt.assert_equal(ydf_predictions, tf_predictions["scores"])
-
-  def test_to_tensorflow_saved_model_with_resource_postprocessing(self):
-    """Test having the post processing be resource dependent."""
-
-    columns = ["f1", "f2", "label_class_binary"]
-    m1_ds = self.create_dataset_v2(columns)
-    m1_ydf = specialized_learners.RandomForestLearner(
-        label="label_class_binary",
-        num_trees=10,
-        task=generic_learner.Task.CLASSIFICATION,
-    ).train(m1_ds)
-    m1_tf = m1_ydf.to_tensorflow_function()
-
-    class PreProcessing(tf.Module):
-
-      def __call__(self, features):
-        features = features.copy()
-        features["m1"] = m1_tf(features)
-        return features
-
-    pre_processing = PreProcessing()
-    pre_processing.m1_tf = m1_tf
-
-    m2_ds = {**m1_ds, "m1": m1_ydf.predict(m1_ds)}
-    m2 = specialized_learners.RandomForestLearner(
-        label="label_class_binary",
-        num_trees=10,
-        task=generic_learner.Task.CLASSIFICATION,
-    ).train(m2_ds)
-
-    tmp_dir = self.create_tempdir().full_path
-    m2.to_tensorflow_saved_model(
-        path=tmp_dir, mode="tf", pre_processing=pre_processing
-    )
-
-    _ = tf.saved_model.load(tmp_dir)
-
-  def test_to_tensorflow_saved_model_adult_classify_api_serialized_examples(
-      self,
-  ):
-    """Export to SavedModel format of a model trained from file."""
-
-    train_ds_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "adult_train.recordio"
-    )
-    test_ds_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "adult_test.recordio"
-    )
-
-    model = specialized_learners.RandomForestLearner(
-        label="income",
-        num_trees=10,
-    ).train(f"tfrecordv2+tfe:{train_ds_path}")
-
-    ydf_predictions = model.predict(f"tfrecordv2+tfe:{test_ds_path}")
-
-    tempdir = self.create_tempdir().full_path
-
-    # TODO: Implement automatic dtype.
-    model.to_tensorflow_saved_model(
-        tempdir,
-        mode="tf",
-        servo_api=True,
-        feed_example_proto=True,
-    )
-
-    tf_model = tf.saved_model.load(tempdir)
-    tf_predict = tf_model.signatures["serving_default"]
-
-    for example_idx, serialized_example in enumerate(
-        tf.data.TFRecordDataset([test_ds_path]).take(10)
-    ):
-      tf_prediction = tf_predict(inputs=[serialized_example])
-      npt.assert_array_equal(
-          tf_prediction["scores"][:, 1], ydf_predictions[example_idx]
-      )
-      npt.assert_array_equal(tf_prediction["classes"], [[b"<=50K", b">50K"]])
-
-  def test_to_tensorflow_saved_model_wrong_dtype(self):
-    columns = ["f1", "label_class_binary"]
-    model = specialized_learners.RandomForestLearner(
-        label="label_class_binary", num_trees=10
-    ).train(self.create_dataset_v2(columns))
-    tmp_dir = self.create_tempdir().full_path
-    with self.assertRaisesRegex(
-        ValueError,
-        "expected to have type \\[tf.float16, tf.float32, tf.float64\\] or"
-        " \\[tf.int8, tf.int16, tf.int32, tf.int64, tf.uint8, tf.uint16,"
-        " tf.uint32, tf.uint64\\]",
-    ):
-      model.to_tensorflow_saved_model(
-          tmp_dir,
-          mode="tf",
-          feed_example_proto=False,
-          feature_dtypes={"f1": tf.string},
-      )
-
-  def test_tensorflow_raw_input_signature_default(self):
-    columns = ["f1", "i1", "c1", "b1", "label_class_binary"]
-    model = specialized_learners.RandomForestLearner(
-        label="label_class_binary", num_trees=10
-    ).train(self.create_dataset_v2(columns))
-    self.assertEqual(
-        export_tf.tensorflow_raw_input_signature(model, {}),
-        {
-            "f1": tf.TensorSpec(shape=(None,), dtype=tf.float64, name="f1"),
-            "i1": tf.TensorSpec(shape=(None,), dtype=tf.int64, name="i1"),
-            "c1": tf.TensorSpec(shape=(None,), dtype=tf.string, name="c1"),
-            "b1": tf.TensorSpec(shape=(None,), dtype=tf.bool, name="b1"),
-        },
-    )
-
-  def test_tensorflow_raw_input_signature_multidim(self):
-    columns = [
-        "multi_f1",
-        "multi_i1",
-        "multi_c1",
-        "multi_b1",
-        "label_class_binary",
-    ]
-    model = specialized_learners.RandomForestLearner(
-        label="label_class_binary", num_trees=10
-    ).train(self.create_dataset_v2(columns))
-    self.assertEqual(
-        export_tf.tensorflow_raw_input_signature(model, {}),
-        {
-            "multi_f1": tf.TensorSpec(
-                shape=(None, 5), dtype=tf.float64, name="multi_f1"
-            ),
-            "multi_i1": tf.TensorSpec(
-                shape=(None, 5), dtype=tf.int64, name="multi_i1"
-            ),
-            "multi_c1": tf.TensorSpec(
-                shape=(None, 5), dtype=tf.string, name="multi_c1"
-            ),
-            "multi_b1": tf.TensorSpec(
-                shape=(None, 5), dtype=tf.bool, name="multi_b1"
-            ),
-        },
-    )
-
-  def test_tensorflow_raw_input_signature_override(self):
-    columns = ["f1", "i1", "c1", "b1", "label_class_binary"]
-    model = specialized_learners.RandomForestLearner(
-        label="label_class_binary", num_trees=10
-    ).train(self.create_dataset_v2(columns))
-    self.assertEqual(
-        export_tf.tensorflow_raw_input_signature(
-            model,
-            {"f1": tf.int32, "i1": tf.int64, "c1": tf.string, "b1": tf.float16},
-        ),
-        {
-            "f1": tf.TensorSpec(shape=(None,), dtype=tf.int32, name="f1"),
-            "i1": tf.TensorSpec(shape=(None,), dtype=tf.int64, name="i1"),
-            "c1": tf.TensorSpec(shape=(None,), dtype=tf.string, name="c1"),
-            "b1": tf.TensorSpec(shape=(None,), dtype=tf.float16, name="b1"),
-        },
-    )
-
-  def test_tensorflow_raw_input_multidim_signature_override(self):
-    columns = [
-        "multi_f1",
-        "multi_i1",
-        "multi_c1",
-        "multi_b1",
-        "label_class_binary",
-    ]
-    model = specialized_learners.RandomForestLearner(
-        label="label_class_binary", num_trees=10
-    ).train(self.create_dataset_v2(columns))
-    self.assertEqual(
-        export_tf.tensorflow_raw_input_signature(
-            model,
-            {
-                "multi_f1": tf.int32,
-                "multi_i1": tf.int64,
-                "multi_c1": tf.string,
-                "multi_b1": tf.float16,
-            },
-        ),
-        {
-            "multi_f1": tf.TensorSpec(
-                shape=(None, 5), dtype=tf.int32, name="multi_f1"
-            ),
-            "multi_i1": tf.TensorSpec(
-                shape=(None, 5), dtype=tf.int64, name="multi_i1"
-            ),
-            "multi_c1": tf.TensorSpec(
-                shape=(None, 5), dtype=tf.string, name="multi_c1"
-            ),
-            "multi_b1": tf.TensorSpec(
-                shape=(None, 5), dtype=tf.float16, name="multi_b1"
-            ),
-        },
-    )
-
-  def test_tensorflow_feature_spec_default(self):
-    columns = [
-        "f1",
-        "i1",
-        "c1",
-        "b1",
-        "multi_f1",
-        "multi_i1",
-        "multi_c1",
-        "multi_b1",
-        "label_class_binary",
-    ]
-    model = specialized_learners.RandomForestLearner(
-        label="label_class_binary", num_trees=10
-    ).train(self.create_dataset_v2(columns))
-    self.assertEqual(
-        export_tf.tensorflow_feature_spec(model, {}),
-        {
-            "f1": tf.io.FixedLenFeature(
-                shape=[], dtype=tf.float32, default_value=math.nan
-            ),
-            "i1": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),
-            "c1": tf.io.FixedLenFeature(
-                shape=[], dtype=tf.string, default_value=""
-            ),
-            "b1": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),
-            "multi_f1": tf.io.FixedLenFeature(
-                shape=[5], dtype=tf.float32, default_value=[math.nan] * 5
-            ),
-            "multi_i1": tf.io.FixedLenFeature(shape=[5], dtype=tf.int64),
-            "multi_c1": tf.io.FixedLenFeature(
-                shape=[5],
-                dtype=tf.string,
-                default_value=[""] * 5,
-            ),
-            "multi_b1": tf.io.FixedLenFeature(shape=[5], dtype=tf.int64),
-        },
-    )
-
-  def test_usage_example_to_tensorflow_function(self):
-    """Usage example of the "to_tensorflow_function" method."""
-
-    # Train a model.
-    model = specialized_learners.RandomForestLearner(label="l").train({
-        "f1": np.random.random(size=100),
-        "f2": np.random.random(size=100),
-        "l": np.random.randint(2, size=100),
-    })
-
-    # Convert model to a TF module.
-    tf_model = model.to_tensorflow_function()
-
-    # Make predictions with the TF module.
-    tf_predictions = tf_model({
-        "f1": tf.constant([0, 0.5, 1]),
-        "f2": tf.constant([1, 0, 0.5]),
-    })
-
-  def test_usage_example_to_tensorflow_saved_model(self):
-    """Usage example of the "to_tensorflow_saved_model" method."""
-    ydf = specialized_learners
-
-    # Part 1
-
-    # Train a model.
-    model = ydf.RandomForestLearner(label="l").train({
-        "f1": np.random.random(size=100),
-        "f2": np.random.random(size=100).astype(dtype=np.float32),
-        "l": np.random.randint(2, size=100),
-    })
-
-    # Export the model to the TensorFlow SavedModel format.
-    model.to_tensorflow_saved_model(path="/tmp/my_model", mode="tf")
-
-    # Load the saved model.
-    tf_model = tf.saved_model.load("/tmp/my_model")
-
-    # Make predictions
-    tf_predictions = tf_model({
-        "f1": tf.constant(np.random.random(size=10)),
-        "f2": tf.constant(np.random.random(size=10), dtype=tf.float32),
-    })
-
-    # Part 3
-    model.to_tensorflow_saved_model(
-        path="/tmp/my_model",
-        mode="tf",
-        # "f1" is fed as an tf.int64 instead of tf.float64
-        feature_dtypes={"f1": tf.int64},
-    )
-
-    # Part 4
-    def pre_processing(features):
-      features = features.copy()
-      features["f1"] = features["f1"] * 2
-      return features
-
-    model.to_tensorflow_saved_model(
-        path="/tmp/my_model",
-        mode="tf",
-        pre_processing=pre_processing,
-    )
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Model tests depending on TensorFlow.
+
+TensorFlow cannot be compiled in debug mode, so these tests are separated out to
+improve debuggability of the remaining model tests.
+"""
+
+import math
+import os
+import tempfile
+from typing import Any, Callable, Dict, List, Mapping, Optional, Tuple
+
+from absl import logging
+from absl.testing import absltest
+from absl.testing import parameterized
+import numpy as np
+import numpy.testing as npt
+import pandas as pd
+import tensorflow as tf
+import tensorflow_decision_forests as tfdf
+
+from ydf.dataset import dataspec
+from ydf.learner import generic_learner
+from ydf.learner import specialized_learners
+from ydf.model import export_tf
+from ydf.model import model_lib
+from ydf.utils import test_utils
+
+
+class TfModelTest(parameterized.TestCase):
+
+  def create_ds(
+      self, columns: List[str], label: Optional[str], weights: Optional[str]
+  ) -> pd.DataFrame:
+    df = pd.DataFrame({})
+    if "cat_int_small" in columns:
+      df["cat_int_small"] = [1, 2, 3, 4, 3, 2, 1, 0, -1, -2]
+    if "cat_int_large" in columns:
+      df["cat_int_large"] = [100, 200, 300, 400, 300, 20, 100, 0, -100, -200]
+    if "cat_string" in columns:
+      df["cat_string"] = ["a", "a", "a", "b", "b", "b", "c", "c", "c", "d"]
+    if "num" in columns:
+      df["num"] = [0, 1, 2, 3, 4, 5.5, 6.6, 7.7, 8.8, 9.9]
+
+    if weights is not None:
+      df["weights"] = list(range(1, 11))
+
+    if label == "classification_binary_int":
+      df["label"] = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]
+    elif label == "classification_binary_str":
+      df["label"] = ["a", "b", "a", "b", "a", "b", "a", "b", "a", "b"]
+    elif label == "classification_multiclass_int":
+      df["label"] = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2]
+    elif label == "classification_multiclass_str":
+      df["label"] = ["a", "b", "c", "d", "a", "b", "c", "d", "a", "b"]
+    elif label == "regression":
+      df["label"] = [42.5, 43.3, 73.4, 21.1, 26.4, 9.3, -1, -44, -23.4, 234.3]
+    elif label == "ranking":
+      df["ranking_group"] = ["a", "a", "a", "a", "a", "b", "b", "b", "b", "b"]
+      df["label"] = [0, 0, 0, 1, 1, 1, 3, 3, 4, 4]
+    return df
+
+  def create_dataset_v2(self, columns: List[str]) -> Dict[str, Any]:
+    """Creates a dataset with random values."""
+    data = {
+        # Single-dim features
+        "f1": np.random.random(size=100),
+        "f2": np.random.random(size=100),
+        "i1": np.random.randint(100, size=100),
+        "i2": np.random.randint(100, size=100),
+        "c1": np.random.choice(["x", "y", "z"], size=100),
+        "b1": np.random.randint(2, size=100).astype(np.bool_),
+        "b2": np.random.randint(2, size=100).astype(np.bool_),
+        # Cat-set features
+        "cs1": [[], ["a", "b", "c"], ["b", "c"], ["a"]] * 25,
+        # Multi-dim features
+        "multi_f1": np.random.random(size=(100, 5)),
+        "multi_f2": np.random.random(size=(100, 5)),
+        "multi_i1": np.random.randint(100, size=(100, 5)),
+        "multi_c1": np.random.choice(["x", "y", "z"], size=(100, 5)),
+        "multi_b1": np.random.randint(2, size=(100, 5)).astype(np.bool_),
+        # Labels
+        "label_class_binary": np.random.choice(["l1", "l2"], size=100),
+        "label_class_multi": np.random.choice(["l1", "l2", "l3"], size=100),
+        "label_regress": np.random.random(size=100),
+    }
+    return {k: data[k] for k in columns}
+
+  def create_csv(
+      self, columns: List[str], label: Optional[str], weights: Optional[str]
+  ) -> str:
+    tmp_dir = self.create_tempdir()
+    csv_file = tmp_dir.create_file("file.csv")
+    self.create_ds(columns, label, weights).to_csv(
+        csv_file.full_path, index=False
+    )
+    return csv_file.full_path
+
+  def create_tfdf_model_and_test_df(
+      self,
+      ds_type: str,
+      feature_col: str,
+      label_col: str,
+      weight_col: str,
+      model_path: str,
+  ) -> Tuple[tfdf.keras.GradientBoostedTreesModel, pd.DataFrame]:
+    # Set feature-specific hyperparameters of the model.
+    if feature_col.startswith("cat_"):
+      column_semantic = tfdf.keras.FeatureSemantic.CATEGORICAL
+    elif feature_col.startswith("num"):
+      column_semantic = tfdf.keras.FeatureSemantic.NUMERICAL
+    else:
+      raise ValueError(f"Could not determine semantic of column {feature_col}")
+    needs_min_vocab_frequency = (
+        column_semantic == tfdf.keras.FeatureSemantic.CATEGORICAL
+    )
+
+    # Set task-specific hyperparameters of the model.
+    ranking_group = None
+    if label_col.startswith("classification"):
+      task = tfdf.keras.Task.CLASSIFICATION
+    elif label_col.startswith("regression"):
+      task = tfdf.keras.Task.REGRESSION
+    elif label_col.startswith("ranking"):
+      task = tfdf.keras.Task.RANKING
+      ranking_group = "ranking_group"
+    else:
+      raise ValueError(f"Could not determine task for label {label_col}")
+
+    # Create an empty, small TF-DF model with the given hyperparameters.
+    tfdf_model = tfdf.keras.GradientBoostedTreesModel(
+        task=task,
+        min_examples=1,
+        num_trees=5,
+        ranking_group=ranking_group,
+        features=[
+            tfdf.keras.FeatureUsage(
+                name=feature_col,
+                semantic=column_semantic,
+                min_vocab_frequency=2 if needs_min_vocab_frequency else None,
+            )
+        ],
+    )
+
+    # Create the training dataset and fit the model to it.
+    if ds_type == "pd":
+      train_df = self.create_ds(
+          columns=[feature_col], label=label_col, weights=weight_col
+      )
+      train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(
+          train_df, label="label", task=task, weight="weights"
+      )
+      tfdf_model.fit(train_ds)
+    elif ds_type == "file":
+      train_ds_path = self.create_csv(
+          columns=[feature_col], label=label_col, weights=weight_col
+      )
+      tfdf_model.fit_on_dataset_path(
+          train_ds_path, label_key="label", weight_key="weights"
+      )
+    else:
+      raise ValueError(f"Unknown dataset type {ds_type}")
+    tfdf_model.save(model_path)
+
+    # Create the test dataset. Note that the test dataset is always a Pandas
+    # dataframe, since predicting from file is not supported.
+    test_df = self.create_ds(columns=[feature_col], label=None, weights=None)
+    return tfdf_model, test_df
+
+  @parameterized.product(
+      feature_col=["cat_int_small", "cat_int_large", "cat_string", "num"],
+      label_col=[
+          "classification_binary_int",
+          "classification_binary_str",
+          "classification_multiclass_int",
+          "classification_multiclass_str",
+          "regression",
+          "ranking",
+      ],
+      ds_type=["file", "pd"],
+  )
+  def test_tfdf_ydf_prediction_equality(
+      self, feature_col: str, label_col: str, ds_type: str
+  ):
+    model_dir = self.create_tempdir().full_path
+    # When reading from file, TF-DF casts integer categories to strings, but it
+    # doesn't when converting from Pandas, so the only way to feed matching data
+    # to the model is to just feed it string data, and we omit the int cases.
+    if ds_type == "file" and feature_col.startswith("cat_int"):
+      self.skipTest(
+          "Categorical Integer features don't work in TF-DF when reading from"
+          " file"
+      )
+
+    tfdf_model, test_df = self.create_tfdf_model_and_test_df(
+        ds_type, feature_col, label_col, "weights", model_dir
+    )
+    test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df)
+
+    tfdf_predictions = tfdf_model.predict(test_ds)
+    ydf_model = model_lib.from_tensorflow_decision_forests(model_dir)
+    ydf_predictions = ydf_model.predict(test_df)
+    npt.assert_allclose(
+        tfdf_predictions.flatten(), ydf_predictions.flatten(), atol=0.0001
+    )
+
+  @parameterized.product(
+      feature_col=["cat_string", "num"],
+      label_col=[
+          "classification_binary_int",
+          "classification_binary_str",
+          "classification_multiclass_int",
+          "classification_multiclass_str",
+          "regression",
+          "ranking",
+      ],
+      ds_type=["file", "pd"],
+  )
+  def test_tfdf_convert_back_and_forth(
+      self, feature_col: str, label_col: str, ds_type: str
+  ):
+    model_dir = self.create_tempdir().full_path
+
+    tfdf_model, test_df = self.create_tfdf_model_and_test_df(
+        ds_type, feature_col, label_col, "weights", model_dir
+    )
+    test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df)
+    new_model_dir = self.create_tempdir().full_path
+
+    # Create the YDF model.
+    ydf_model = model_lib.from_tensorflow_decision_forests(model_dir)
+    # Convert the YDF model back to a TF-DF model and load it.
+    ydf_model.to_tensorflow_saved_model(new_model_dir)
+    new_tfdf_model = tf.keras.models.load_model(new_model_dir)
+
+    # Check for prediction equality.
+    tfdf_predictions = tfdf_model.predict(test_ds)
+    new_tfdf_predictions = new_tfdf_model.predict(test_ds)
+    npt.assert_allclose(
+        tfdf_predictions.flatten(), new_tfdf_predictions.flatten(), atol=0.0001
+    )
+
+  @parameterized.product(
+      feature_col=["cat_int_small", "cat_int_large"],
+      label_col=[
+          "classification_binary_int",
+          "classification_binary_str",
+          "classification_multiclass_int",
+          "classification_multiclass_str",
+          "regression",
+          "ranking",
+      ],
+      ds_type=["pd"],
+  )
+  def test_tfdf_convert_back_and_forth_cat_int(
+      self, feature_col: str, label_col: str, ds_type: str
+  ):
+    model_dir = self.create_tempdir().full_path
+
+    tfdf_model, test_df = self.create_tfdf_model_and_test_df(
+        ds_type, feature_col, label_col, "weights", model_dir
+    )
+    test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df)
+    new_model_dir = self.create_tempdir().full_path
+
+    # Create the YDF model.
+    ydf_model = model_lib.from_tensorflow_decision_forests(model_dir)
+    # Convert the YDF model back to a TF-DF model and load it.
+    ydf_model.to_tensorflow_saved_model(new_model_dir)
+    new_tfdf_model = tf.keras.models.load_model(new_model_dir)
+
+    # Prepare the test dataset for the loaded model: Categorical integer
+    # features are now string features.
+    new_test_df = test_df.copy(deep=True)
+    new_test_df[feature_col] = new_test_df.astype(np.str_)
+    new_test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(new_test_df)
+
+    # Check for prediction equality.
+    tfdf_predictions = tfdf_model.predict(test_ds)
+    new_tfdf_predictions = new_tfdf_model.predict(new_test_ds)
+    npt.assert_allclose(
+        tfdf_predictions.flatten(), new_tfdf_predictions.flatten(), atol=0.0001
+    )
+
+  def test_to_tensorflow_saved_model_serialized_input(self):
+    # TODO: b/321204507 - Integrate logic in YDF.
+
+    train_ds_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "adult_train.recordio"
+    )
+    test_ds_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "adult_test.recordio"
+    )
+
+    # Train a model on the tfrecord directly.
+    ydf_model = specialized_learners.GradientBoostedTreesLearner(
+        label="income",
+        num_trees=10,
+    ).train(f"tfrecordv2+tfe:{train_ds_path}")
+
+    test_predictions = ydf_model.predict(f"tfrecordv2+tfe:{test_ds_path}")
+
+    tempdir = self.create_tempdir().full_path
+
+    # Export the model to a TensorFlow Saved Model.
+    #
+    # This model expects inputs in the form of a dictionary of features
+    # values e.g. {"age": [...], "capital_gain": [...], ...}.
+    #
+    # This is referred as the "predict" API.
+    path_wo_signature = os.path.join(tempdir, "mdl")
+    ydf_model.to_tensorflow_saved_model(path_wo_signature)
+
+    # Load the model, and add a serialized tensorflow examples protobuffer
+    # input signature. In other words, the model expects as input a serialized
+    # tensorflow example proto.
+    #
+    # This is often referred the "classify" or "regress" API.
+    path_w_signature = os.path.join(tempdir, "mdl_ws")
+    tf_model_wo_signature = tf.keras.models.load_model(path_wo_signature)
+
+    # The list of input features to read from the tensorflow example proto.
+    # Note that tensorflow example proto dtypes can only be float32, int64 or
+    # string.
+    feature_spec = {
+        # Numerical
+        "age": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),
+        "capital_gain": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),
+        "capital_loss": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),
+        "education_num": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),
+        "fnlwgt": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),
+        "hours_per_week": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),
+        # Categorical
+        "education": tf.io.FixedLenFeature(shape=[], dtype=tf.string),
+        "marital_status": tf.io.FixedLenFeature(shape=[], dtype=tf.string),
+        "native_country": tf.io.FixedLenFeature(
+            shape=[], dtype=tf.string, default_value=""
+        ),
+        "occupation": tf.io.FixedLenFeature(shape=[], dtype=tf.string),
+        "race": tf.io.FixedLenFeature(shape=[], dtype=tf.string),
+        "relationship": tf.io.FixedLenFeature(shape=[], dtype=tf.string),
+        "sex": tf.io.FixedLenFeature(shape=[], dtype=tf.string),
+        "workclass": tf.io.FixedLenFeature(shape=[], dtype=tf.string),
+    }
+
+    # The "classify" requires for all the predictions to return the label
+    # classes. This only make sense for a classification model.
+    label_classes = ydf_model.label_classes()
+
+    # The "make_classify_fn" function combines a tensorflow example proto
+    # parsing stage with the classical model inference.
+    def make_classify_fn(
+        model: tf.keras.Model,
+    ) -> Callable[[tf.Tensor], Mapping[str, tf.Tensor]]:
+      @tf.function(
+          input_signature=[
+              tf.TensorSpec([None], dtype=tf.string, name="inputs")
+          ]
+      )
+      def classify(
+          serialized_tf_examples: tf.Tensor,
+      ) -> Mapping[str, tf.Tensor]:
+        # Parse the serialized tensorflow proto examples into a dictionary of
+        # tensor values.
+        parsed_features = tf.io.parse_example(
+            serialized_tf_examples, feature_spec
+        )
+
+        # Cast all the int64 features into float32 values.
+        #
+        # The SavedModel exported by YDF expects float32 value for all
+        # numerical features (can be overridden with the
+        # `input_model_signature_fn` argument). However, in this dataset, the
+        # numerical features are stored as int64.
+        for feature in parsed_features:
+          if parsed_features[feature].dtype == tf.int64:
+            parsed_features[feature] = tf.cast(
+                parsed_features[feature], tf.float32
+            )
+
+        # Apply the model.
+        outputs = model(parsed_features)
+
+        # Extract the label classes. The "classify" API expects for the label
+        # classes to be returned with every predictions.
+        batch_size = tf.shape(serialized_tf_examples)[0]
+        batched_label_classes = tf.broadcast_to(
+            input=tf.constant(label_classes),
+            shape=(batch_size, len(label_classes)),
+        )
+
+        return {"classes": batched_label_classes, "scores": outputs}
+
+      return classify
+
+    # Save the model to a SavedModel with the "classify" signature.
+    signatures = {
+        "classify": make_classify_fn(tf_model_wo_signature),
+    }
+    tf.saved_model.save(
+        tf_model_wo_signature, path_w_signature, signatures=signatures
+    )
+    tf_model_w_signature = tf.saved_model.load(path_w_signature)
+
+    logging.info("Available signatures: %s", tf_model_w_signature.signatures)
+
+    # Generate predictions with the "classify" signature.
+    classify_model = tf_model_w_signature.signatures["classify"]
+
+    for example_idx, serialized_example in enumerate(
+        tf.data.TFRecordDataset([test_ds_path]).take(10)
+    ):
+      prediction = classify_model(inputs=[serialized_example.numpy()])
+      logging.info("prediction:%s", prediction)
+      npt.assert_almost_equal(
+          prediction["scores"].numpy(), test_predictions[example_idx]
+      )
+      npt.assert_equal(prediction["classes"].numpy(), [[b"<=50K", b">50K"]])
+
+  def test_to_tensorflow_function(self):
+    """A simple function conversion."""
+
+    # Create YDF model
+    columns = ["f1", "f2", "i1", "c1", "b1", "cs1", "label_class_binary"]
+    model = specialized_learners.RandomForestLearner(
+        label="label_class_binary",
+        num_trees=10,
+        features=[("cs1", dataspec.Semantic.CATEGORICAL_SET)],
+        include_all_columns=True,
+    ).train(self.create_dataset_v2(columns))
+
+    # Golden predictions
+    test_ds = self.create_dataset_v2(columns)
+    ydf_predictions = model.predict(test_ds)
+
+    # Convert model to tf function + generate predictions
+    tf_function = model.to_tensorflow_function()
+    tf_test_ds = {
+        "f1": tf.constant(test_ds["f1"]),
+        "f2": tf.constant(test_ds["f2"]),
+        "i1": tf.constant(test_ds["i1"]),
+        "c1": tf.constant(test_ds["c1"]),
+        "b1": tf.constant(test_ds["b1"]),
+        "cs1": tf.ragged.constant(test_ds["cs1"]),
+    }
+    tf_predictions = tf_function(tf_test_ds)
+
+    npt.assert_array_equal(ydf_predictions, tf_predictions)
+
+  @parameterized.product(can_be_saved=[True, False])
+  def test_to_multi_tensorflow_function(self, can_be_saved: bool):
+    """Function export and serialization with multiple YDF models."""
+
+    # Create YDF model
+    columns = ["f1", "f2", "i1", "c1", "label_class_binary"]
+    model_1 = specialized_learners.RandomForestLearner(
+        label="label_class_binary", num_trees=10
+    ).train(self.create_dataset_v2(columns))
+    model_2 = specialized_learners.RandomForestLearner(
+        label="label_class_binary", num_trees=10
+    ).train(self.create_dataset_v2(columns))
+
+    # Golden predictions
+    test_ds = self.create_dataset_v2(columns)
+    ydf_predictions = model_1.predict(test_ds) + model_2.predict(test_ds) * 2
+
+    # Convert to tf function + generate predictions
+    tf_function_m1 = model_1.to_tensorflow_function(can_be_saved=can_be_saved)
+    tf_function_m2 = model_2.to_tensorflow_function(can_be_saved=can_be_saved)
+
+    @tf.function
+    def tf_function(features):
+      return tf_function_m1(features) + tf_function_m2(features) * 2
+
+    tf_test_ds = {k: tf.constant(v) for k, v in test_ds.items()}
+    tf_predictions = tf_function(tf_test_ds)
+
+    # Check predictions
+    npt.assert_array_equal(ydf_predictions, tf_predictions)
+
+    if not can_be_saved:
+      return
+
+    # Serialize / unserialize model
+    with tempfile.TemporaryDirectory() as tmp_dir:
+      tf_model = tf.Module()
+      tf_model.__call__ = tf_function
+      tf_model.tf_function_m1 = tf_function_m1
+      tf_model.tf_function_m2 = tf_function_m2
+      tf.saved_model.save(tf_model, tmp_dir)
+      loaded_tf_model = tf.saved_model.load(tmp_dir)
+
+    # Check predictions gain
+    loaded_tf_predictions = loaded_tf_model(tf_test_ds)
+    npt.assert_array_equal(ydf_predictions, loaded_tf_predictions)
+
+  def test_to_tensorflow_function_with_multidim_input(self):
+    # Train a model
+    columns = [
+        "multi_f1",
+        "multi_i1",
+        "multi_c1",
+        "multi_b1",
+        "label_class_binary",
+    ]
+    train_ds = self.create_dataset_v2(columns)
+    model = specialized_learners.RandomForestLearner(
+        label="label_class_binary"
+    ).train(train_ds)
+    # Generate predictions with YDF
+    ydf_predictions = model.predict(train_ds)
+
+    # Convert ydf model into a tf function
+    tf_function = model.to_tensorflow_function()
+
+    # Validate the tf model predictions
+    tf_test_ds = {k: tf.constant(train_ds[k]) for k in columns[:-1]}
+    tf_predictions = tf_function(tf_test_ds)
+    npt.assert_array_equal(ydf_predictions, tf_predictions)
+
+  def test_to_raw_tensorflow_saved_model(self):
+    """Simple export to SavedModel format."""
+
+    # Create YDF model
+    columns = ["f1", "f2", "i1", "c1", "b1", "cs1", "label_class_binary"]
+    model = specialized_learners.RandomForestLearner(
+        label="label_class_binary",
+        num_trees=10,
+        features=[("cs1", dataspec.Semantic.CATEGORICAL_SET)],
+        include_all_columns=True,
+    ).train(self.create_dataset_v2(columns))
+
+    # Golden predictions
+    test_ds = self.create_dataset_v2(columns[:-1])
+    ydf_predictions = model.predict(test_ds)
+
+    # Save model
+    tmp_dir = self.create_tempdir().full_path
+    model.to_tensorflow_saved_model(
+        tmp_dir,
+        mode="tf",
+        servo_api=False,
+        feed_example_proto=False,
+        feature_dtypes={"f1": tf.float32},
+    )
+
+    # Load model
+    tf_model = tf.saved_model.load(tmp_dir)
+
+    # Test predictions
+    tf_test_ds = {
+        # While f1 was feed as a float64, it was saved as a float32.
+        "f1": tf.constant(test_ds["f1"], tf.float32),
+        "f2": tf.constant(test_ds["f2"]),
+        "i1": tf.constant(test_ds["i1"]),
+        "c1": tf.constant(test_ds["c1"]),
+        "b1": tf.constant(test_ds["b1"]),
+        "cs1": tf.ragged.constant(test_ds["cs1"]),
+    }
+    tf_predictions = tf_model(tf_test_ds)
+    npt.assert_equal(ydf_predictions, tf_predictions)
+
+  @parameterized.parameters(True, False)
+  def test_to_raw_tensorflow_saved_model_with_multidim_input(
+      self, with_filter: bool
+  ):
+    # Create YDF model
+    columns = [
+        "multi_f1",
+        "multi_i1",
+        "multi_c1",
+        "multi_b1",
+        "label_class_binary",
+    ]
+    feature_columns = columns[:-2] if with_filter else columns[:-1]
+    train_ds = self.create_dataset_v2(columns)
+    model = specialized_learners.RandomForestLearner(
+        label="label_class_binary",
+        num_trees=10,
+        features=feature_columns if with_filter else None,
+    ).train(train_ds)
+
+    # Golden predictions
+    ydf_predictions = model.predict(train_ds)
+
+    # Save model
+    tmp_dir = self.create_tempdir().full_path
+    model.to_tensorflow_saved_model(
+        tmp_dir,
+        mode="tf",
+        servo_api=False,
+        feed_example_proto=False,
+    )
+
+    # Load model
+    tf_model = tf.saved_model.load(tmp_dir)
+
+    # Test predictions
+    tf_test_ds = {k: tf.constant(train_ds[k]) for k in feature_columns}
+    tf_predictions = tf_model(tf_test_ds)
+    npt.assert_equal(ydf_predictions, tf_predictions)
+
+  def test_to_tensorflow_saved_model_classify_api(self):
+    """Export to SavedModel format with regress API."""
+    columns = ["f1", "f2", "label_class_multi"]
+    model = specialized_learners.RandomForestLearner(
+        label="label_class_multi",
+        num_trees=10,
+        task=generic_learner.Task.CLASSIFICATION,
+    ).train(self.create_dataset_v2(columns))
+    test_ds = self.create_dataset_v2(columns[:-1])
+    ydf_predictions = model.predict(test_ds)
+    tmp_dir = self.create_tempdir().full_path
+    model.to_tensorflow_saved_model(
+        path=tmp_dir,
+        mode="tf",
+        feed_example_proto=False,
+        servo_api=True,
+    )
+    tf_model = tf.saved_model.load(tmp_dir)
+    tf_model_predict = tf_model.signatures["serving_default"]
+    tf_prediction = tf_model_predict(**test_ds)
+    self.assertEqual(tf_prediction["scores"].shape, (100, 3))
+    self.assertEqual(tf_prediction["classes"].shape, (100, 3))
+    npt.assert_equal(ydf_predictions, tf_prediction["scores"])
+
+  def test_to_tensorflow_saved_model_regress_api(self):
+    """Export to SavedModel format with regress API."""
+    columns = ["f1", "i1", "label_regress"]
+    model = specialized_learners.RandomForestLearner(
+        label="label_regress",
+        num_trees=10,
+        task=generic_learner.Task.REGRESSION,
+    ).train(self.create_dataset_v2(columns))
+    test_ds = self.create_dataset_v2(columns[:-1])
+    ydf_predictions = model.predict(test_ds)
+    tmp_dir = self.create_tempdir().full_path
+    model.to_tensorflow_saved_model(
+        path=tmp_dir,
+        mode="tf",
+        feed_example_proto=False,
+        servo_api=True,
+    )
+    tf_model = tf.saved_model.load(tmp_dir)
+    tf_model_predict = tf_model.signatures["serving_default"]
+    tf_prediction = tf_model_predict(**test_ds)
+    self.assertEqual(tf_prediction["outputs"].shape, (100,))
+    npt.assert_equal(ydf_predictions, tf_prediction["outputs"])
+
+  def test_to_tensorflow_saved_model_with_example_proto(self):
+    """Export to SavedModel format with serialized example inputs."""
+
+    # Create YDF model
+    columns = ["f1", "i1", "i2", "c1", "b1", "b2", "cs1", "label_class_binary"]
+    model = specialized_learners.RandomForestLearner(
+        label="label_class_binary",
+        num_trees=10,
+        features=[("cs1", dataspec.Semantic.CATEGORICAL_SET)],
+        include_all_columns=True,
+    ).train(self.create_dataset_v2(columns))
+
+    test_ds = self.create_dataset_v2(columns[:-1])
+
+    # Save model
+    tmp_dir = self.create_tempdir().full_path
+
+    def pre_processing(features):
+      features = features.copy()
+      features["f1"] = features["f1"] * 2
+      return features
+
+    def post_processing(output):
+      return output * 3
+
+    # Model predictions with all transformations
+    ydf_no_post_process_predictions = model.predict(pre_processing(test_ds))
+    # Add extra dimension: ydf squeeze its predictions, while the servo api
+    # expects non-squeezed predictions.
+    ydf_no_post_process_predictions = np.stack(
+        [
+            1.0 - ydf_no_post_process_predictions,
+            ydf_no_post_process_predictions,
+        ],
+        axis=1,
+    )
+    ydf_predictions = post_processing(ydf_no_post_process_predictions)
+
+    model.to_tensorflow_saved_model(
+        tmp_dir,
+        mode="tf",
+        feature_dtypes={"i2": tf.float32, "b2": tf.float32},
+        pre_processing=pre_processing,
+        post_processing=post_processing,
+        servo_api=True,
+        feed_example_proto=True,
+    )
+
+    # Load model
+    tf_model = tf.saved_model.load(tmp_dir)
+
+    # Raw predictions
+    tf_test_ds = {
+        "f1": tf.constant(test_ds["f1"]),
+        "i1": tf.constant(test_ds["i1"]),
+        "i2": tf.constant(test_ds["i2"], dtype=tf.float32),
+        "c1": tf.constant(test_ds["c1"]),
+        "b1": tf.constant(test_ds["b1"]),
+        "b2": tf.constant(test_ds["b2"], dtype=tf.float32),
+        "cs1": tf.ragged.constant(test_ds["cs1"]),
+    }
+    raw_tf_predictions = tf_model(tf_test_ds)
+    npt.assert_array_equal(ydf_predictions, raw_tf_predictions)
+
+    # Stored pre and post processing.
+    for feature in columns[:-2]:
+      npt.assert_array_equal(
+          tf_model.pre_processing(tf_test_ds)[feature],
+          pre_processing(tf_test_ds)[feature],
+      )
+
+    npt.assert_array_equal(
+        tf_model.post_processing(tf.constant([[1.0, 2.0]], tf.float32)),
+        post_processing(tf.constant([[1.0, 2.0]], tf.float32)),
+    )
+
+    # Servo API predictions
+    tf_model_predict = tf_model.signatures["serving_default"]
+    tf_test_ds = []
+    for example_idx in range(100):
+      tf_test_ds.append(
+          tf.train.Example(
+              features=tf.train.Features(
+                  feature={
+                      "f1": tf.train.Feature(
+                          float_list=tf.train.FloatList(
+                              value=[test_ds["f1"][example_idx]]
+                          )
+                      ),
+                      "i1": tf.train.Feature(
+                          int64_list=tf.train.Int64List(
+                              value=[test_ds["i1"][example_idx]]
+                          )
+                      ),
+                      "i2": tf.train.Feature(
+                          float_list=tf.train.FloatList(
+                              value=[test_ds["i2"][example_idx]]
+                          )
+                      ),
+                      "c1": tf.train.Feature(
+                          bytes_list=tf.train.BytesList(
+                              value=[bytes(test_ds["c1"][example_idx], "utf-8")]
+                          )
+                      ),
+                      "b1": tf.train.Feature(
+                          int64_list=tf.train.Int64List(
+                              value=[test_ds["b1"][example_idx]]
+                          )
+                      ),
+                      "b2": tf.train.Feature(
+                          float_list=tf.train.FloatList(
+                              value=[test_ds["b2"][example_idx]]
+                          )
+                      ),
+                      "cs1": tf.train.Feature(
+                          bytes_list=tf.train.BytesList(
+                              value=[
+                                  bytes(x, "utf-8")
+                                  for x in test_ds["cs1"][example_idx]
+                              ]
+                          )
+                      ),
+                  }
+              )
+          ).SerializeToString()
+      )
+    tf_predictions = tf_model_predict(inputs=tf_test_ds)
+    npt.assert_equal(ydf_predictions, tf_predictions["scores"])
+
+  @parameterized.parameters(True, False)
+  def test_to_tensorflow_saved_model_with_example_proto_multidim(
+      self, with_filter: bool
+  ):
+    """Export to SavedModel format with serialized example inputs."""
+
+    # Create YDF model
+    columns = [
+        "multi_f1",
+        "multi_i1",
+        "multi_c1",
+        "multi_b1",
+        "label_class_binary",
+    ]
+    feature_columns = columns[:-2] if with_filter else columns[:-1]
+    model = specialized_learners.RandomForestLearner(
+        label="label_class_binary",
+        num_trees=10,
+        features=feature_columns if with_filter else None,
+    ).train(self.create_dataset_v2(columns))
+
+    # Golden predictions
+    test_ds = self.create_dataset_v2(feature_columns)
+
+    # Save model
+    tmp_dir = self.create_tempdir().full_path
+
+    # Golden predictions
+    ydf_predictions = model.predict(test_ds)
+    # Add extra dimension: ydf squeeze its predictions, while the servo api
+    # expects non-squeezed predictions.
+    ydf_predictions = np.stack(
+        [
+            1.0 - ydf_predictions,
+            ydf_predictions,
+        ],
+        axis=1,
+    )
+
+    model.to_tensorflow_saved_model(
+        tmp_dir, mode="tf", servo_api=True, feed_example_proto=True
+    )
+
+    # Load model
+    tf_model = tf.saved_model.load(tmp_dir)
+
+    # Raw predictions
+    tf_test_ds = {k: tf.constant(test_ds[k]) for k in feature_columns}
+    raw_tf_predictions = tf_model(tf_test_ds)
+    npt.assert_array_equal(ydf_predictions, raw_tf_predictions)
+
+    # Servo API predictions
+    tf_model_predict = tf_model.signatures["serving_default"]
+    tf_test_ds = []
+    for example_idx in range(100):
+      proto_feature = {
+          "multi_f1": tf.train.Feature(
+              float_list=tf.train.FloatList(
+                  value=test_ds["multi_f1"][example_idx][:]
+              )
+          ),
+          "multi_i1": tf.train.Feature(
+              int64_list=tf.train.Int64List(
+                  value=test_ds["multi_i1"][example_idx][:]
+              )
+          ),
+          "multi_c1": tf.train.Feature(
+              bytes_list=tf.train.BytesList(
+                  value=[
+                      bytes(x, "utf-8")
+                      for x in test_ds["multi_c1"][example_idx]
+                  ]
+              )
+          ),
+      }
+      if not with_filter:
+        proto_feature["multi_b1"] = tf.train.Feature(
+            int64_list=tf.train.Int64List(
+                value=test_ds["multi_b1"][example_idx][:]
+            )
+        )
+      tf_test_ds.append(
+          tf.train.Example(
+              features=tf.train.Features(feature=proto_feature)
+          ).SerializeToString()
+      )
+    tf_predictions = tf_model_predict(inputs=tf_test_ds)
+    npt.assert_equal(ydf_predictions, tf_predictions["scores"])
+
+  def test_to_tensorflow_saved_model_with_resource_postprocessing(self):
+    """Test having the post processing be resource dependent."""
+
+    columns = ["f1", "f2", "label_class_binary"]
+    m1_ds = self.create_dataset_v2(columns)
+    m1_ydf = specialized_learners.RandomForestLearner(
+        label="label_class_binary",
+        num_trees=10,
+        task=generic_learner.Task.CLASSIFICATION,
+    ).train(m1_ds)
+    m1_tf = m1_ydf.to_tensorflow_function()
+
+    class PreProcessing(tf.Module):
+
+      def __call__(self, features):
+        features = features.copy()
+        features["m1"] = m1_tf(features)
+        return features
+
+    pre_processing = PreProcessing()
+    pre_processing.m1_tf = m1_tf
+
+    m2_ds = {**m1_ds, "m1": m1_ydf.predict(m1_ds)}
+    m2 = specialized_learners.RandomForestLearner(
+        label="label_class_binary",
+        num_trees=10,
+        task=generic_learner.Task.CLASSIFICATION,
+    ).train(m2_ds)
+
+    tmp_dir = self.create_tempdir().full_path
+    m2.to_tensorflow_saved_model(
+        path=tmp_dir, mode="tf", pre_processing=pre_processing
+    )
+
+    _ = tf.saved_model.load(tmp_dir)
+
+  def test_to_tensorflow_saved_model_adult_classify_api_serialized_examples(
+      self,
+  ):
+    """Export to SavedModel format of a model trained from file."""
+
+    train_ds_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "adult_train.recordio"
+    )
+    test_ds_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "adult_test.recordio"
+    )
+
+    model = specialized_learners.RandomForestLearner(
+        label="income",
+        num_trees=10,
+    ).train(f"tfrecordv2+tfe:{train_ds_path}")
+
+    ydf_predictions = model.predict(f"tfrecordv2+tfe:{test_ds_path}")
+
+    tempdir = self.create_tempdir().full_path
+
+    # TODO: Implement automatic dtype.
+    model.to_tensorflow_saved_model(
+        tempdir,
+        mode="tf",
+        servo_api=True,
+        feed_example_proto=True,
+    )
+
+    tf_model = tf.saved_model.load(tempdir)
+    tf_predict = tf_model.signatures["serving_default"]
+
+    for example_idx, serialized_example in enumerate(
+        tf.data.TFRecordDataset([test_ds_path]).take(10)
+    ):
+      tf_prediction = tf_predict(inputs=[serialized_example])
+      npt.assert_array_equal(
+          tf_prediction["scores"][:, 1], ydf_predictions[example_idx]
+      )
+      npt.assert_array_equal(tf_prediction["classes"], [[b"<=50K", b">50K"]])
+
+  def test_to_tensorflow_saved_model_wrong_dtype(self):
+    columns = ["f1", "label_class_binary"]
+    model = specialized_learners.RandomForestLearner(
+        label="label_class_binary", num_trees=10
+    ).train(self.create_dataset_v2(columns))
+    tmp_dir = self.create_tempdir().full_path
+    with self.assertRaisesRegex(
+        ValueError,
+        "expected to have type \\[tf.float16, tf.float32, tf.float64\\] or"
+        " \\[tf.int8, tf.int16, tf.int32, tf.int64, tf.uint8, tf.uint16,"
+        " tf.uint32, tf.uint64\\]",
+    ):
+      model.to_tensorflow_saved_model(
+          tmp_dir,
+          mode="tf",
+          feed_example_proto=False,
+          feature_dtypes={"f1": tf.string},
+      )
+
+  def test_tensorflow_raw_input_signature_default(self):
+    columns = ["f1", "i1", "c1", "b1", "label_class_binary"]
+    model = specialized_learners.RandomForestLearner(
+        label="label_class_binary", num_trees=10
+    ).train(self.create_dataset_v2(columns))
+    self.assertEqual(
+        export_tf.tensorflow_raw_input_signature(model, {}),
+        {
+            "f1": tf.TensorSpec(shape=(None,), dtype=tf.float64, name="f1"),
+            "i1": tf.TensorSpec(shape=(None,), dtype=tf.int64, name="i1"),
+            "c1": tf.TensorSpec(shape=(None,), dtype=tf.string, name="c1"),
+            "b1": tf.TensorSpec(shape=(None,), dtype=tf.bool, name="b1"),
+        },
+    )
+
+  def test_tensorflow_raw_input_signature_multidim(self):
+    columns = [
+        "multi_f1",
+        "multi_i1",
+        "multi_c1",
+        "multi_b1",
+        "label_class_binary",
+    ]
+    model = specialized_learners.RandomForestLearner(
+        label="label_class_binary", num_trees=10
+    ).train(self.create_dataset_v2(columns))
+    self.assertEqual(
+        export_tf.tensorflow_raw_input_signature(model, {}),
+        {
+            "multi_f1": tf.TensorSpec(
+                shape=(None, 5), dtype=tf.float64, name="multi_f1"
+            ),
+            "multi_i1": tf.TensorSpec(
+                shape=(None, 5), dtype=tf.int64, name="multi_i1"
+            ),
+            "multi_c1": tf.TensorSpec(
+                shape=(None, 5), dtype=tf.string, name="multi_c1"
+            ),
+            "multi_b1": tf.TensorSpec(
+                shape=(None, 5), dtype=tf.bool, name="multi_b1"
+            ),
+        },
+    )
+
+  def test_tensorflow_raw_input_signature_override(self):
+    columns = ["f1", "i1", "c1", "b1", "label_class_binary"]
+    model = specialized_learners.RandomForestLearner(
+        label="label_class_binary", num_trees=10
+    ).train(self.create_dataset_v2(columns))
+    self.assertEqual(
+        export_tf.tensorflow_raw_input_signature(
+            model,
+            {"f1": tf.int32, "i1": tf.int64, "c1": tf.string, "b1": tf.float16},
+        ),
+        {
+            "f1": tf.TensorSpec(shape=(None,), dtype=tf.int32, name="f1"),
+            "i1": tf.TensorSpec(shape=(None,), dtype=tf.int64, name="i1"),
+            "c1": tf.TensorSpec(shape=(None,), dtype=tf.string, name="c1"),
+            "b1": tf.TensorSpec(shape=(None,), dtype=tf.float16, name="b1"),
+        },
+    )
+
+  def test_tensorflow_raw_input_multidim_signature_override(self):
+    columns = [
+        "multi_f1",
+        "multi_i1",
+        "multi_c1",
+        "multi_b1",
+        "label_class_binary",
+    ]
+    model = specialized_learners.RandomForestLearner(
+        label="label_class_binary", num_trees=10
+    ).train(self.create_dataset_v2(columns))
+    self.assertEqual(
+        export_tf.tensorflow_raw_input_signature(
+            model,
+            {
+                "multi_f1": tf.int32,
+                "multi_i1": tf.int64,
+                "multi_c1": tf.string,
+                "multi_b1": tf.float16,
+            },
+        ),
+        {
+            "multi_f1": tf.TensorSpec(
+                shape=(None, 5), dtype=tf.int32, name="multi_f1"
+            ),
+            "multi_i1": tf.TensorSpec(
+                shape=(None, 5), dtype=tf.int64, name="multi_i1"
+            ),
+            "multi_c1": tf.TensorSpec(
+                shape=(None, 5), dtype=tf.string, name="multi_c1"
+            ),
+            "multi_b1": tf.TensorSpec(
+                shape=(None, 5), dtype=tf.float16, name="multi_b1"
+            ),
+        },
+    )
+
+  def test_tensorflow_feature_spec_default(self):
+    columns = [
+        "f1",
+        "i1",
+        "c1",
+        "b1",
+        "multi_f1",
+        "multi_i1",
+        "multi_c1",
+        "multi_b1",
+        "label_class_binary",
+    ]
+    model = specialized_learners.RandomForestLearner(
+        label="label_class_binary", num_trees=10
+    ).train(self.create_dataset_v2(columns))
+    self.assertEqual(
+        export_tf.tensorflow_feature_spec(model, {}),
+        {
+            "f1": tf.io.FixedLenFeature(
+                shape=[], dtype=tf.float32, default_value=math.nan
+            ),
+            "i1": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),
+            "c1": tf.io.FixedLenFeature(
+                shape=[], dtype=tf.string, default_value=""
+            ),
+            "b1": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),
+            "multi_f1": tf.io.FixedLenFeature(
+                shape=[5], dtype=tf.float32, default_value=[math.nan] * 5
+            ),
+            "multi_i1": tf.io.FixedLenFeature(shape=[5], dtype=tf.int64),
+            "multi_c1": tf.io.FixedLenFeature(
+                shape=[5],
+                dtype=tf.string,
+                default_value=[""] * 5,
+            ),
+            "multi_b1": tf.io.FixedLenFeature(shape=[5], dtype=tf.int64),
+        },
+    )
+
+  def test_usage_example_to_tensorflow_function(self):
+    """Usage example of the "to_tensorflow_function" method."""
+
+    # Train a model.
+    model = specialized_learners.RandomForestLearner(label="l").train({
+        "f1": np.random.random(size=100),
+        "f2": np.random.random(size=100),
+        "l": np.random.randint(2, size=100),
+    })
+
+    # Convert model to a TF module.
+    tf_model = model.to_tensorflow_function()
+
+    # Make predictions with the TF module.
+    tf_predictions = tf_model({
+        "f1": tf.constant([0, 0.5, 1]),
+        "f2": tf.constant([1, 0, 0.5]),
+    })
+
+  def test_usage_example_to_tensorflow_saved_model(self):
+    """Usage example of the "to_tensorflow_saved_model" method."""
+    ydf = specialized_learners
+
+    # Part 1
+
+    # Train a model.
+    model = ydf.RandomForestLearner(label="l").train({
+        "f1": np.random.random(size=100),
+        "f2": np.random.random(size=100).astype(dtype=np.float32),
+        "l": np.random.randint(2, size=100),
+    })
+
+    # Export the model to the TensorFlow SavedModel format.
+    model.to_tensorflow_saved_model(path="/tmp/my_model", mode="tf")
+
+    # Load the saved model.
+    tf_model = tf.saved_model.load("/tmp/my_model")
+
+    # Make predictions
+    tf_predictions = tf_model({
+        "f1": tf.constant(np.random.random(size=10)),
+        "f2": tf.constant(np.random.random(size=10), dtype=tf.float32),
+    })
+
+    # Part 3
+    model.to_tensorflow_saved_model(
+        path="/tmp/my_model",
+        mode="tf",
+        # "f1" is fed as an tf.int64 instead of tf.float64
+        feature_dtypes={"f1": tf.int64},
+    )
+
+    # Part 4
+    def pre_processing(features):
+      features = features.copy()
+      features["f1"] = features["f1"] * 2
+      return features
+
+    model.to_tensorflow_saved_model(
+        path="/tmp/my_model",
+        mode="tf",
+        pre_processing=pre_processing,
+    )
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/model/decision_forest_model/__init__.py

 * *Ordering differences only*

```diff
@@ -1,14 +1,14 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
```

## ydf/model/decision_forest_model/decision_forest_model.py

 * *Ordering differences only*

```diff
@@ -1,241 +1,241 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Definitions for generic decision forest models."""
-
-import sys
-from typing import Iterator, Optional, Sequence
-
-import numpy as np
-
-from ydf.cc import ydf
-from ydf.dataset import dataset
-from ydf.model import generic_model
-from ydf.model.tree import plot as plot_lib
-from ydf.model.tree import tree as tree_lib
-
-
-class DecisionForestModel(generic_model.GenericModel):
-  """A generic decision forest model for prediction and inspection."""
-
-  _model: ydf.DecisionForestCCModel
-
-  def num_trees(self):
-    """Returns the number of trees in the decision forest."""
-    return self._model.num_trees()
-
-  def get_tree(self, tree_idx: int) -> tree_lib.Tree:
-    """Gets a single tree of the model.
-
-    Args:
-      tree_idx: Index of the tree. Should be in [0, num_trees()).
-
-    Returns:
-      The tree.
-    """
-    nodes = self._model.GetTree(tree_idx)
-    return tree_lib.proto_nodes_to_tree(nodes, self.data_spec())
-
-  def get_all_trees(self) -> Sequence[tree_lib.Tree]:
-    """Returns all the trees in the model."""
-
-    return list(self.iter_trees())
-
-  def iter_trees(self) -> Iterator[tree_lib.Tree]:
-    """Returns an iterator over all the trees in the model."""
-
-    return (self.get_tree(tree_idx) for tree_idx in range(self.num_trees()))
-
-  def print_tree(self, tree_idx: int = 0, file=sys.stdout) -> None:
-    """Prints a tree in the terminal.
-
-    Usage example:
-
-    ```python
-    # Create a dataset
-    train_ds = pd.DataFrame({
-        "c1": [1.0, 1.1, 2.0, 3.5, 4.2] + list(range(10)),
-        "label": ["a", "b", "b", "a", "a"] * 3,
-    })
-    # Train a CART model
-    model = ydf.CartLearner(label="label").train(train_ds)
-    # Make sure the model is a CART
-    assert isinstance(model, ydf.CARTModel)
-    # Print the tree
-    model.print_tree()
-    ```
-
-    Args:
-      tree_idx: Index of the tree. Should be in [0, self.num_trees()).
-      file: Where to print the tree. By default, prints on the terminal standard
-        output.
-    """
-
-    file.write(self.get_tree(tree_idx).pretty(self.data_spec()))
-
-  def plot_tree(
-      self,
-      tree_idx: int = 0,
-      max_depth: Optional[int] = None,
-      options: Optional[plot_lib.PlotOptions] = None,
-      d3js_url: str = "https://d3js.org/d3.v6.min.js",
-  ) -> plot_lib.TreePlot:
-    """Plots an interactive HTML rendering of the tree.
-
-    Usage example:
-
-    ```python
-    # Create a dataset
-    train_ds = pd.DataFrame({
-        "c1": [1.0, 1.1, 2.0, 3.5, 4.2] + list(range(10)),
-        "label": ["a", "b", "b", "a", "a"] * 3,
-    })
-    # Train a CART model
-    model = ydf.CartLearner(label="label").train(train_ds)
-    # Make sure the model is a CART
-    assert isinstance(model, ydf.CARTModel)
-    # Plot the tree in Colab
-    model.plot_tree()
-    ```
-
-    Args:
-      tree_idx: Index of the tree. Should be in [0, self.num_trees()).
-      max_depth: Maximum tree depth of the plot. Set to None for full depth.
-      options: Advanced options for plotting. Set to None for default style.
-      d3js_url: URL to load the d3.js library from.
-
-    Returns:
-      In interactive environments, an interactive plot. The HTML source can also
-      be exported to file.
-    """
-    label_classes = None
-    if self.task() == generic_model.Task.CLASSIFICATION:
-      label_classes = self.label_classes()
-
-    return self.get_tree(tree_idx).plot(
-        self.data_spec(), max_depth, label_classes, options, d3js_url
-    )
-
-  def set_tree(self, tree_idx: int, tree: tree_lib.Tree) -> None:
-    """Overrides a single tree of the model.
-
-    Args:
-      tree_idx: Index of the tree. Should be in [0, num_trees()).
-      tree: New tree.
-    """
-    proto_nodes = tree_lib.tree_to_proto_nodes(tree, self.data_spec())
-    self._model.SetTree(tree_idx, proto_nodes)
-
-  def add_tree(self, tree: tree_lib.Tree) -> None:
-    """Adds a single tree of the model.
-
-    Args:
-      tree: New tree.
-    """
-    proto_nodes = tree_lib.tree_to_proto_nodes(tree, self.data_spec())
-    self._model.AddTree(proto_nodes)
-
-  def remove_tree(self, tree_idx: int) -> None:
-    """Removes a single tree of the model.
-
-    Args:
-      tree_idx: Index of the tree. Should be in [0, num_trees()).
-    """
-    self._model.RemoveTree(tree_idx)
-
-  def predict_leaves(self, data: dataset.InputDataset) -> np.ndarray:
-    """Gets the index of the active leaf in each tree.
-
-    The active leaf is the leave that that receive the example during inference.
-
-    The returned value "leaves[i,j]" is the index of the active leaf for the
-    i-th example and the j-th tree. Leaves are indexed by depth first
-    exploration with the negative child visited before the positive one.
-
-    Args:
-      data: Dataset.
-
-    Returns:
-      Index of the active leaf for each tree in the model.
-    """
-
-    ds = dataset.create_vertical_dataset(
-        data, data_spec=self._model.data_spec()
-    )
-    return self._model.PredictLeaves(ds._dataset)  # pylint: disable=protected-access
-
-  def distance(
-      self,
-      data1: dataset.InputDataset,
-      data2: Optional[dataset.InputDataset] = None,
-  ) -> np.ndarray:
-    """Computes the pairwise distance between examples in "data1" and "data2".
-
-    If "data2" is not provided, computes the pairwise distance between examples
-    in "data1".
-
-    Usage example:
-
-    ```python
-    import pandas as pd
-    import ydf
-
-    # Train model
-    train_ds = pd.read_csv("train.csv")
-    model = ydf.RandomForestLearner(label="label").Train(train_ds)
-
-    test_ds = pd.read_csv("test.csv")
-    distances = model.distance(test_ds, train_ds)
-    # "distances[i,j]" is the distance between the i-th test example and the
-    # j-th train example.
-    ```
-
-    Different models are free to implement different distances with different
-    definitions. For this reasons, unless indicated by the model, distances
-    from different models cannot be compared.
-
-    The distance is not guaranteed to satisfy the triangular inequality
-    property of metric distances.
-
-    Not all models can compute distances. In this case, this function will raise
-    an Exception.
-
-    Args:
-      data1: Dataset. Can be a dictionary of list or numpy array of values,
-        Pandas DataFrame, or a VerticalDataset.
-      data2: Dataset. Can be a dictionary of list or numpy array of values,
-        Pandas DataFrame, or a VerticalDataset.
-
-    Returns:
-      Pairwise distance
-    """
-
-    ds1 = dataset.create_vertical_dataset(
-        data1, data_spec=self._model.data_spec()
-    )
-    if data2 is None:
-      ds2 = ds1
-    else:
-      ds2 = dataset.create_vertical_dataset(
-          data2, data_spec=self._model.data_spec()
-      )
-    return self._model.Distance(ds1._dataset, ds2._dataset)  # pylint: disable=protected-access
-
-  def set_node_format(self, node_format: generic_model.NodeFormat) -> None:
-    """Set the serialization format for the nodes.
-
-    Args:
-      node_format: Node format to use when saving the model.
-    """
-    self._model.set_node_format(node_format.name)
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Definitions for generic decision forest models."""
+
+import sys
+from typing import Iterator, Optional, Sequence
+
+import numpy as np
+
+from ydf.cc import ydf
+from ydf.dataset import dataset
+from ydf.model import generic_model
+from ydf.model.tree import plot as plot_lib
+from ydf.model.tree import tree as tree_lib
+
+
+class DecisionForestModel(generic_model.GenericModel):
+  """A generic decision forest model for prediction and inspection."""
+
+  _model: ydf.DecisionForestCCModel
+
+  def num_trees(self):
+    """Returns the number of trees in the decision forest."""
+    return self._model.num_trees()
+
+  def get_tree(self, tree_idx: int) -> tree_lib.Tree:
+    """Gets a single tree of the model.
+
+    Args:
+      tree_idx: Index of the tree. Should be in [0, num_trees()).
+
+    Returns:
+      The tree.
+    """
+    nodes = self._model.GetTree(tree_idx)
+    return tree_lib.proto_nodes_to_tree(nodes, self.data_spec())
+
+  def get_all_trees(self) -> Sequence[tree_lib.Tree]:
+    """Returns all the trees in the model."""
+
+    return list(self.iter_trees())
+
+  def iter_trees(self) -> Iterator[tree_lib.Tree]:
+    """Returns an iterator over all the trees in the model."""
+
+    return (self.get_tree(tree_idx) for tree_idx in range(self.num_trees()))
+
+  def print_tree(self, tree_idx: int = 0, file=sys.stdout) -> None:
+    """Prints a tree in the terminal.
+
+    Usage example:
+
+    ```python
+    # Create a dataset
+    train_ds = pd.DataFrame({
+        "c1": [1.0, 1.1, 2.0, 3.5, 4.2] + list(range(10)),
+        "label": ["a", "b", "b", "a", "a"] * 3,
+    })
+    # Train a CART model
+    model = ydf.CartLearner(label="label").train(train_ds)
+    # Make sure the model is a CART
+    assert isinstance(model, ydf.CARTModel)
+    # Print the tree
+    model.print_tree()
+    ```
+
+    Args:
+      tree_idx: Index of the tree. Should be in [0, self.num_trees()).
+      file: Where to print the tree. By default, prints on the terminal standard
+        output.
+    """
+
+    file.write(self.get_tree(tree_idx).pretty(self.data_spec()))
+
+  def plot_tree(
+      self,
+      tree_idx: int = 0,
+      max_depth: Optional[int] = None,
+      options: Optional[plot_lib.PlotOptions] = None,
+      d3js_url: str = "https://d3js.org/d3.v6.min.js",
+  ) -> plot_lib.TreePlot:
+    """Plots an interactive HTML rendering of the tree.
+
+    Usage example:
+
+    ```python
+    # Create a dataset
+    train_ds = pd.DataFrame({
+        "c1": [1.0, 1.1, 2.0, 3.5, 4.2] + list(range(10)),
+        "label": ["a", "b", "b", "a", "a"] * 3,
+    })
+    # Train a CART model
+    model = ydf.CartLearner(label="label").train(train_ds)
+    # Make sure the model is a CART
+    assert isinstance(model, ydf.CARTModel)
+    # Plot the tree in Colab
+    model.plot_tree()
+    ```
+
+    Args:
+      tree_idx: Index of the tree. Should be in [0, self.num_trees()).
+      max_depth: Maximum tree depth of the plot. Set to None for full depth.
+      options: Advanced options for plotting. Set to None for default style.
+      d3js_url: URL to load the d3.js library from.
+
+    Returns:
+      In interactive environments, an interactive plot. The HTML source can also
+      be exported to file.
+    """
+    label_classes = None
+    if self.task() == generic_model.Task.CLASSIFICATION:
+      label_classes = self.label_classes()
+
+    return self.get_tree(tree_idx).plot(
+        self.data_spec(), max_depth, label_classes, options, d3js_url
+    )
+
+  def set_tree(self, tree_idx: int, tree: tree_lib.Tree) -> None:
+    """Overrides a single tree of the model.
+
+    Args:
+      tree_idx: Index of the tree. Should be in [0, num_trees()).
+      tree: New tree.
+    """
+    proto_nodes = tree_lib.tree_to_proto_nodes(tree, self.data_spec())
+    self._model.SetTree(tree_idx, proto_nodes)
+
+  def add_tree(self, tree: tree_lib.Tree) -> None:
+    """Adds a single tree of the model.
+
+    Args:
+      tree: New tree.
+    """
+    proto_nodes = tree_lib.tree_to_proto_nodes(tree, self.data_spec())
+    self._model.AddTree(proto_nodes)
+
+  def remove_tree(self, tree_idx: int) -> None:
+    """Removes a single tree of the model.
+
+    Args:
+      tree_idx: Index of the tree. Should be in [0, num_trees()).
+    """
+    self._model.RemoveTree(tree_idx)
+
+  def predict_leaves(self, data: dataset.InputDataset) -> np.ndarray:
+    """Gets the index of the active leaf in each tree.
+
+    The active leaf is the leave that that receive the example during inference.
+
+    The returned value "leaves[i,j]" is the index of the active leaf for the
+    i-th example and the j-th tree. Leaves are indexed by depth first
+    exploration with the negative child visited before the positive one.
+
+    Args:
+      data: Dataset.
+
+    Returns:
+      Index of the active leaf for each tree in the model.
+    """
+
+    ds = dataset.create_vertical_dataset(
+        data, data_spec=self._model.data_spec()
+    )
+    return self._model.PredictLeaves(ds._dataset)  # pylint: disable=protected-access
+
+  def distance(
+      self,
+      data1: dataset.InputDataset,
+      data2: Optional[dataset.InputDataset] = None,
+  ) -> np.ndarray:
+    """Computes the pairwise distance between examples in "data1" and "data2".
+
+    If "data2" is not provided, computes the pairwise distance between examples
+    in "data1".
+
+    Usage example:
+
+    ```python
+    import pandas as pd
+    import ydf
+
+    # Train model
+    train_ds = pd.read_csv("train.csv")
+    model = ydf.RandomForestLearner(label="label").Train(train_ds)
+
+    test_ds = pd.read_csv("test.csv")
+    distances = model.distance(test_ds, train_ds)
+    # "distances[i,j]" is the distance between the i-th test example and the
+    # j-th train example.
+    ```
+
+    Different models are free to implement different distances with different
+    definitions. For this reasons, unless indicated by the model, distances
+    from different models cannot be compared.
+
+    The distance is not guaranteed to satisfy the triangular inequality
+    property of metric distances.
+
+    Not all models can compute distances. In this case, this function will raise
+    an Exception.
+
+    Args:
+      data1: Dataset. Can be a dictionary of list or numpy array of values,
+        Pandas DataFrame, or a VerticalDataset.
+      data2: Dataset. Can be a dictionary of list or numpy array of values,
+        Pandas DataFrame, or a VerticalDataset.
+
+    Returns:
+      Pairwise distance
+    """
+
+    ds1 = dataset.create_vertical_dataset(
+        data1, data_spec=self._model.data_spec()
+    )
+    if data2 is None:
+      ds2 = ds1
+    else:
+      ds2 = dataset.create_vertical_dataset(
+          data2, data_spec=self._model.data_spec()
+      )
+    return self._model.Distance(ds1._dataset, ds2._dataset)  # pylint: disable=protected-access
+
+  def set_node_format(self, node_format: generic_model.NodeFormat) -> None:
+    """Set the serialization format for the nodes.
+
+    Args:
+      node_format: Node format to use when saving the model.
+    """
+    self._model.set_node_format(node_format.name)
```

## ydf/model/decision_forest_model/decision_forest_model_test.py

```diff
@@ -1,125 +1,125 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Tests for the YDF models."""
-
-import os
-
-from absl.testing import absltest
-from absl.testing import parameterized
-import numpy as np
-import pandas as pd
-
-from yggdrasil_decision_forests.model.random_forest import random_forest_pb2
-from ydf.model import generic_model
-from ydf.model import model_lib
-from ydf.model.decision_forest_model import decision_forest_model
-from ydf.utils import test_utils
-
-
-class DecisionForestModelTest(parameterized.TestCase):
-
-  def setUp(self):
-    super().setUp()
-    # Loading models needed in many unittests.
-    model_dir = os.path.join(test_utils.ydf_test_data_path(), "model")
-    # This model is a Random Forest classification model without training logs.
-    self.adult_binary_class_rf: decision_forest_model.DecisionForestModel = (
-        model_lib.load_model(os.path.join(model_dir, "adult_binary_class_rf"))
-    )
-    # This model is a GBDT classification model without training logs.
-    self.adult_binary_class_gbdt: decision_forest_model.DecisionForestModel = (
-        model_lib.load_model(os.path.join(model_dir, "adult_binary_class_gbdt"))
-    )
-    # This model is a GBDT regression model without training logs.
-    self.abalone_regression_gbdt: decision_forest_model.DecisionForestModel = (
-        model_lib.load_model(os.path.join(model_dir, "abalone_regression_gbdt"))
-    )
-    # This model is a RF uplift model
-    self.sim_pte_categorical_uplift_rf: (
-        decision_forest_model.DecisionForestModel
-    ) = model_lib.load_model(
-        os.path.join(model_dir, "sim_pte_categorical_uplift_rf")
-    )
-
-  def test_num_trees(self):
-    self.assertEqual(self.adult_binary_class_rf.num_trees(), 100)
-
-  def test_predict_leaves(self):
-    dataset_path = os.path.join(
-        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
-    )
-    dataset = pd.read_csv(dataset_path)
-
-    leaves = self.adult_binary_class_gbdt.predict_leaves(dataset)
-    self.assertEqual(
-        leaves.shape,
-        (dataset.shape[0], self.adult_binary_class_gbdt.num_trees()),
-    )
-    self.assertTrue(np.all(leaves >= 0))
-
-  @parameterized.parameters(x for x in generic_model.NodeFormat)
-  def test_node_format(self, node_format: generic_model.NodeFormat):
-    """Test that the node format is saved correctly."""
-    self.adult_binary_class_rf.set_node_format(node_format=node_format)
-    model_save_path = self.create_tempdir().full_path
-    self.adult_binary_class_rf.save(
-        model_save_path,
-        advanced_options=generic_model.ModelIOOptions(file_prefix=""),
-    )
-    # Read the proto to see if the format is set correctly
-    # TODO: Consider exposing the proto directly in ydf.
-    random_forest_header = random_forest_pb2.Header()
-    random_forest_header_path = os.path.join(
-        model_save_path, "random_forest_header.pb"
-    )
-    self.assertTrue(os.path.exists(random_forest_header_path))
-    with open(random_forest_header_path, "rb") as f:
-      random_forest_header.ParseFromString(f.read())
-    self.assertEqual(random_forest_header.node_format, node_format.name)
-
-  def test_plot_classification(self):
-    plot = self.adult_binary_class_gbdt.plot_tree()
-    root_as_html = (
-        '{"value": {"type": "REGRESSION", "value": -4.158827948685939e-09,'
-        ' "num_examples": 0.0}, "condition": {"type": "CATEGORICAL_IS_IN",'
-        ' "attribute": "marital_status", "mask": ["Never-married", "Divorced",'
-        ' "Widowed", "Separated", "Married-spouse-absent",'
-        ' "Married-AF-spouse"]}'
-    )
-    self.assertIn(root_as_html, plot.html())
-
-  def test_plot_regression(self):
-    plot = self.abalone_regression_gbdt.plot_tree()
-    root_as_html = (
-        '{"value": {"type": "REGRESSION", "value": -4.225819338898873e-08,'
-        ' "num_examples": 2663.0, "standard_deviation": 3.227639690862905},'
-        ' "condition": {"type": "NUMERICAL_IS_HIGHER_THAN", "attribute":'
-        ' "ShellWeight", "threshold": 0.1537500023841858}'
-    )
-    self.assertIn(root_as_html, plot.html())
-
-  def test_plot_uplift(self):
-    plot = self.sim_pte_categorical_uplift_rf.plot_tree()
-    root_as_html = (
-        '{"value": {"type": "UPLIFT", "treatment_effect":'
-        ' [-0.019851017743349075], "num_examples": 1000.0}, "condition":'
-        ' {"type": "NUMERICAL_IS_HIGHER_THAN", "attribute": "X16", "threshold":'
-        " 1.782151699066162}"
-    )
-    self.assertIn(root_as_html, plot.html())
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Tests for the YDF models."""
+
+import os
+
+from absl.testing import absltest
+from absl.testing import parameterized
+import numpy as np
+import pandas as pd
+
+from ydf.proto.model.random_forest import random_forest_pb2
+from ydf.model import generic_model
+from ydf.model import model_lib
+from ydf.model.decision_forest_model import decision_forest_model
+from ydf.utils import test_utils
+
+
+class DecisionForestModelTest(parameterized.TestCase):
+
+  def setUp(self):
+    super().setUp()
+    # Loading models needed in many unittests.
+    model_dir = os.path.join(test_utils.ydf_test_data_path(), "model")
+    # This model is a Random Forest classification model without training logs.
+    self.adult_binary_class_rf: decision_forest_model.DecisionForestModel = (
+        model_lib.load_model(os.path.join(model_dir, "adult_binary_class_rf"))
+    )
+    # This model is a GBDT classification model without training logs.
+    self.adult_binary_class_gbdt: decision_forest_model.DecisionForestModel = (
+        model_lib.load_model(os.path.join(model_dir, "adult_binary_class_gbdt"))
+    )
+    # This model is a GBDT regression model without training logs.
+    self.abalone_regression_gbdt: decision_forest_model.DecisionForestModel = (
+        model_lib.load_model(os.path.join(model_dir, "abalone_regression_gbdt"))
+    )
+    # This model is a RF uplift model
+    self.sim_pte_categorical_uplift_rf: (
+        decision_forest_model.DecisionForestModel
+    ) = model_lib.load_model(
+        os.path.join(model_dir, "sim_pte_categorical_uplift_rf")
+    )
+
+  def test_num_trees(self):
+    self.assertEqual(self.adult_binary_class_rf.num_trees(), 100)
+
+  def test_predict_leaves(self):
+    dataset_path = os.path.join(
+        test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
+    )
+    dataset = pd.read_csv(dataset_path)
+
+    leaves = self.adult_binary_class_gbdt.predict_leaves(dataset)
+    self.assertEqual(
+        leaves.shape,
+        (dataset.shape[0], self.adult_binary_class_gbdt.num_trees()),
+    )
+    self.assertTrue(np.all(leaves >= 0))
+
+  @parameterized.parameters(x for x in generic_model.NodeFormat)
+  def test_node_format(self, node_format: generic_model.NodeFormat):
+    """Test that the node format is saved correctly."""
+    self.adult_binary_class_rf.set_node_format(node_format=node_format)
+    model_save_path = self.create_tempdir().full_path
+    self.adult_binary_class_rf.save(
+        model_save_path,
+        advanced_options=generic_model.ModelIOOptions(file_prefix=""),
+    )
+    # Read the proto to see if the format is set correctly
+    # TODO: Consider exposing the proto directly in ydf.
+    random_forest_header = random_forest_pb2.Header()
+    random_forest_header_path = os.path.join(
+        model_save_path, "random_forest_header.pb"
+    )
+    self.assertTrue(os.path.exists(random_forest_header_path))
+    with open(random_forest_header_path, "rb") as f:
+      random_forest_header.ParseFromString(f.read())
+    self.assertEqual(random_forest_header.node_format, node_format.name)
+
+  def test_plot_classification(self):
+    plot = self.adult_binary_class_gbdt.plot_tree()
+    root_as_html = (
+        '{"value": {"type": "REGRESSION", "value": -4.158827948685939e-09,'
+        ' "num_examples": 0.0}, "condition": {"type": "CATEGORICAL_IS_IN",'
+        ' "attribute": "marital_status", "mask": ["Never-married", "Divorced",'
+        ' "Widowed", "Separated", "Married-spouse-absent",'
+        ' "Married-AF-spouse"]}'
+    )
+    self.assertIn(root_as_html, plot.html())
+
+  def test_plot_regression(self):
+    plot = self.abalone_regression_gbdt.plot_tree()
+    root_as_html = (
+        '{"value": {"type": "REGRESSION", "value": -4.225819338898873e-08,'
+        ' "num_examples": 2663.0, "standard_deviation": 3.227639690862905},'
+        ' "condition": {"type": "NUMERICAL_IS_HIGHER_THAN", "attribute":'
+        ' "ShellWeight", "threshold": 0.1537500023841858}'
+    )
+    self.assertIn(root_as_html, plot.html())
+
+  def test_plot_uplift(self):
+    plot = self.sim_pte_categorical_uplift_rf.plot_tree()
+    root_as_html = (
+        '{"value": {"type": "UPLIFT", "treatment_effect":'
+        ' [-0.019851017743349075], "num_examples": 1000.0}, "condition":'
+        ' {"type": "NUMERICAL_IS_HIGHER_THAN", "attribute": "X16", "threshold":'
+        " 1.782151699066162}"
+    )
+    self.assertIn(root_as_html, plot.html())
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/model/gradient_boosted_trees_model/__init__.py

 * *Ordering differences only*

```diff
@@ -1,14 +1,14 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
```

## ydf/model/gradient_boosted_trees_model/gradient_boosted_trees_model.py

```diff
@@ -1,101 +1,101 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Definitions for Gradient Boosted Trees models."""
-
-import math
-from typing import Optional
-
-import numpy.typing as npt
-
-from yggdrasil_decision_forests.metric import metric_pb2
-from ydf.cc import ydf
-from ydf.metric import metric
-from ydf.model.decision_forest_model import decision_forest_model
-
-
-class GradientBoostedTreesModel(decision_forest_model.DecisionForestModel):
-  """A Gradient Boosted Trees model for prediction and inspection."""
-
-  _model: ydf.GradientBoostedTreesCCModel
-
-  def validation_loss(self) -> Optional[float]:
-    """Returns loss on the validation dataset if available."""
-    loss = self._model.validation_loss()
-    return loss if not math.isnan(loss) else None
-
-  def initial_predictions(self) -> npt.NDArray[float]:
-    """Returns the model's initial predictions (i.e. the model bias)."""
-    return self._model.initial_predictions()
-
-  def validation_evaluation(self) -> Optional[metric.Evaluation]:
-    """Returns the validation evaluation of the model, if available.
-
-    Gradient Boosted Trees use a validation dataset for early stopping.
-
-    Returns None if no validation evaluation been computed or it has been
-    removed from the model.
-
-    Usage example:
-
-    ```python
-    import pandas as pd
-    import ydf
-
-    # Train model
-    train_ds = pd.read_csv("train.csv")
-    model = ydf.GradientBoostedTreesLearner(label="label").train(train_ds)
-
-    validation_evaluation = model.validation_evaluation()
-    # In an interactive Python environment, print a rich evaluation report.
-    validation_evaluation
-    ```
-    """
-    validation_evaluation_proto = self._model.validation_evaluation()
-    # There is no canonical way of checking if a proto is empty. This workaround
-    # just checks if the evaluation proto is valid.
-    if not validation_evaluation_proto.HasField("task"):
-      return None
-    return metric.Evaluation(self._model.validation_evaluation())
-
-  def self_evaluation(self) -> Optional[metric.Evaluation]:
-    """Returns the model's self-evaluation.
-
-    For Gradient Boosted Trees models, the self-evaluation is the evaluation on
-    the validation dataset. Note that the validation dataset is extracted
-    automatically if not explicitly given. If the validation dataset is
-    deactivated, no self-evaluation is computed.
-
-    Different models use different methods for self-evaluation. Notably, Random
-    Forests use the last Out-Of-Bag evaluation. Therefore, self-evaluations are
-    not comparable between different model types.
-
-    Returns None if no self-evaluation has been computed.
-
-    Usage example:
-
-    ```python
-    import pandas as pd
-    import ydf
-
-    # Train model
-    train_ds = pd.read_csv("train.csv")
-    model = ydf.GradientBoostedTreesLearner(label="label").train(train_ds)
-
-    self_evaluation = model.self_evaluation()
-    # In an interactive Python environment, print a rich evaluation report.
-    self_evaluation
-    ```
-    """
-    return self.validation_evaluation()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Definitions for Gradient Boosted Trees models."""
+
+import math
+from typing import Optional
+
+import numpy.typing as npt
+
+from ydf.proto.metric import metric_pb2
+from ydf.cc import ydf
+from ydf.metric import metric
+from ydf.model.decision_forest_model import decision_forest_model
+
+
+class GradientBoostedTreesModel(decision_forest_model.DecisionForestModel):
+  """A Gradient Boosted Trees model for prediction and inspection."""
+
+  _model: ydf.GradientBoostedTreesCCModel
+
+  def validation_loss(self) -> Optional[float]:
+    """Returns loss on the validation dataset if available."""
+    loss = self._model.validation_loss()
+    return loss if not math.isnan(loss) else None
+
+  def initial_predictions(self) -> npt.NDArray[float]:
+    """Returns the model's initial predictions (i.e. the model bias)."""
+    return self._model.initial_predictions()
+
+  def validation_evaluation(self) -> Optional[metric.Evaluation]:
+    """Returns the validation evaluation of the model, if available.
+
+    Gradient Boosted Trees use a validation dataset for early stopping.
+
+    Returns None if no validation evaluation been computed or it has been
+    removed from the model.
+
+    Usage example:
+
+    ```python
+    import pandas as pd
+    import ydf
+
+    # Train model
+    train_ds = pd.read_csv("train.csv")
+    model = ydf.GradientBoostedTreesLearner(label="label").train(train_ds)
+
+    validation_evaluation = model.validation_evaluation()
+    # In an interactive Python environment, print a rich evaluation report.
+    validation_evaluation
+    ```
+    """
+    validation_evaluation_proto = self._model.validation_evaluation()
+    # There is no canonical way of checking if a proto is empty. This workaround
+    # just checks if the evaluation proto is valid.
+    if not validation_evaluation_proto.HasField("task"):
+      return None
+    return metric.Evaluation(self._model.validation_evaluation())
+
+  def self_evaluation(self) -> Optional[metric.Evaluation]:
+    """Returns the model's self-evaluation.
+
+    For Gradient Boosted Trees models, the self-evaluation is the evaluation on
+    the validation dataset. Note that the validation dataset is extracted
+    automatically if not explicitly given. If the validation dataset is
+    deactivated, no self-evaluation is computed.
+
+    Different models use different methods for self-evaluation. Notably, Random
+    Forests use the last Out-Of-Bag evaluation. Therefore, self-evaluations are
+    not comparable between different model types.
+
+    Returns None if no self-evaluation has been computed.
+
+    Usage example:
+
+    ```python
+    import pandas as pd
+    import ydf
+
+    # Train model
+    train_ds = pd.read_csv("train.csv")
+    model = ydf.GradientBoostedTreesLearner(label="label").train(train_ds)
+
+    self_evaluation = model.self_evaluation()
+    # In an interactive Python environment, print a rich evaluation report.
+    self_evaluation
+    ```
+    """
+    return self.validation_evaluation()
```

## ydf/model/gradient_boosted_trees_model/gradient_boosted_trees_model_test.py

 * *Ordering differences only*

```diff
@@ -1,512 +1,512 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Tests for the Gradient Boosted trees models."""
-
-import os
-from typing import Dict, Tuple
-
-from absl import logging
-from absl.testing import absltest
-import numpy as np
-import numpy.testing as npt
-import pandas as pd
-
-from ydf.dataset import dataspec
-from ydf.learner import specialized_learners
-from ydf.model import generic_model
-from ydf.model import model_lib
-from ydf.model.gradient_boosted_trees_model import gradient_boosted_trees_model
-from ydf.model.tree import condition as condition_lib
-from ydf.model.tree import node as node_lib
-from ydf.model.tree import tree as tree_lib
-from ydf.model.tree import value as value_lib
-from ydf.utils import test_utils
-
-RegressionValue = value_lib.RegressionValue
-Leaf = node_lib.Leaf
-NonLeaf = node_lib.NonLeaf
-NumericalHigherThanCondition = condition_lib.NumericalHigherThanCondition
-Tree = tree_lib.Tree
-
-
-class GradientBoostedTreesTest(absltest.TestCase):
-
-  def setUp(self):
-    super().setUp()
-    # This model is a classification model for pure serving.
-    adult_binary_class_gbdt_path = os.path.join(
-        test_utils.ydf_test_data_path(), "model", "adult_binary_class_gbdt"
-    )
-    self.adult_binary_class_gbdt = model_lib.load_model(
-        adult_binary_class_gbdt_path
-    )
-    # This model is a classification model with full training logs.
-    gbt_adult_base_with_na_path = os.path.join(
-        test_utils.ydf_test_data_path(), "golden", "gbt_adult_base_with_na"
-    )
-    self.gbt_adult_base_with_na = model_lib.load_model(
-        gbt_adult_base_with_na_path
-    )
-
-  def test_input_feature_names(self):
-    self.assertEqual(
-        self.adult_binary_class_gbdt.input_feature_names(),
-        [
-            "age",
-            "workclass",
-            "fnlwgt",
-            "education",
-            "education_num",
-            "marital_status",
-            "occupation",
-            "relationship",
-            "race",
-            "sex",
-            "capital_gain",
-            "capital_loss",
-            "hours_per_week",
-            "native_country",
-        ],
-    )
-
-  def test_input_features(self):
-    InputFeature = generic_model.InputFeature
-    NUMERICAL = dataspec.Semantic.NUMERICAL
-    CATEGORICAL = dataspec.Semantic.CATEGORICAL
-    self.assertEqual(
-        self.adult_binary_class_gbdt.input_features(),
-        [
-            InputFeature("age", NUMERICAL, 0),
-            InputFeature("workclass", CATEGORICAL, 1),
-            InputFeature("fnlwgt", NUMERICAL, 2),
-            InputFeature("education", CATEGORICAL, 3),
-            InputFeature("education_num", CATEGORICAL, 4),
-            InputFeature("marital_status", CATEGORICAL, 5),
-            InputFeature("occupation", CATEGORICAL, 6),
-            InputFeature("relationship", CATEGORICAL, 7),
-            InputFeature("race", CATEGORICAL, 8),
-            InputFeature("sex", CATEGORICAL, 9),
-            InputFeature("capital_gain", NUMERICAL, 10),
-            InputFeature("capital_loss", NUMERICAL, 11),
-            InputFeature("hours_per_week", NUMERICAL, 12),
-            InputFeature("native_country", CATEGORICAL, 13),
-        ],
-    )
-
-  def test_task(self):
-    self.assertEqual(
-        self.adult_binary_class_gbdt.task(), generic_model.Task.CLASSIFICATION
-    )
-
-  def test_label_classes(self):
-    self.assertEqual(
-        self.adult_binary_class_gbdt.label_classes(), ["<=50K", ">50K"]
-    )
-
-  def test_label(self):
-    self.assertEqual(self.adult_binary_class_gbdt.label(), "income")
-
-  def test_validation_loss(self):
-    validation_loss = self.adult_binary_class_gbdt.validation_loss()
-    self.assertAlmostEqual(validation_loss, 0.573842942, places=6)
-
-  def test_validation_loss_if_no_validation_dataset(self):
-    dataset = {"x": np.array([0, 0, 1, 1]), "y": np.array([0, 0, 0, 1])}
-    model = specialized_learners.GradientBoostedTreesLearner(
-        label="y", validation_ratio=0.0, num_trees=2
-    ).train(dataset)
-    validation_loss = model.validation_loss()
-    self.assertIsNone(validation_loss)
-
-  def test_initial_predictions(self):
-    initial_predictions = self.adult_binary_class_gbdt.initial_predictions()
-    np.testing.assert_allclose(initial_predictions, [-1.1630996])
-
-  def test_validation_evaluation_empty(self):
-    dataset = {
-        "x1": np.array([0, 0, 0, 1, 1, 1]),
-        "y": np.array([0, 0, 0, 0, 1, 1]),
-    }
-    model = specialized_learners.GradientBoostedTreesLearner(
-        label="y",
-        num_trees=1,
-        max_depth=4,
-        min_examples=1,
-        validation_ratio=0.0,
-    ).train(dataset)
-    self.assertIsInstance(
-        model, gradient_boosted_trees_model.GradientBoostedTreesModel
-    )
-    validation_evaluation = model.validation_evaluation()
-    self.assertIsNone(validation_evaluation)
-
-  def test_validation_evaluation_no_training_logs(self):
-    validation_evaluation = self.adult_binary_class_gbdt.validation_evaluation()
-    self.assertIsNone(validation_evaluation.accuracy)
-    self.assertAlmostEqual(validation_evaluation.loss, 0.57384294)
-
-  def test_validation_evaluation_with_content(self):
-    validation_evaluation = self.gbt_adult_base_with_na.validation_evaluation()
-    self.assertAlmostEqual(validation_evaluation.accuracy, 0.8498403)
-
-  def test_variable_importances(self):
-    model_path = os.path.join(
-        test_utils.ydf_test_data_path(),
-        "model",
-        "synthetic_ranking_gbdt_numerical",
-    )
-    model = model_lib.load_model(model_path)
-    variable_importances = model.variable_importances()
-    self.assertEqual(
-        variable_importances,
-        {
-            "NUM_NODES": [
-                (355.0, "num_2"),
-                (326.0, "num_0"),
-                (248.0, "num_1"),
-                (193.0, "num_3"),
-            ],
-            "INV_MEAN_MIN_DEPTH": [
-                (0.54955206094026765, "num_0"),
-                (0.43300866801748344, "num_2"),
-                (0.21987296105251422, "num_1"),
-                (0.20886402442940008, "num_3"),
-            ],
-            "SUM_SCORE": [
-                (331.52462868355724, "num_0"),
-                (297.70595154801595, "num_2"),
-                (103.86176226850876, "num_1"),
-                (52.43193327602421, "num_3"),
-            ],
-            "NUM_AS_ROOT": [
-                (35.0, "num_0"),
-                (12.0, "num_2"),
-                (1.0, "num_3"),
-            ],
-        },
-    )
-
-  def test_predict_distance(self):
-    dataset = pd.read_csv(
-        os.path.join(
-            test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
-        ),
-        nrows=500,
-    )
-
-    distances = self.adult_binary_class_gbdt.distance(dataset)
-    logging.info("distances:\n%s", distances)
-    self.assertEqual(distances.shape, (dataset.shape[0], dataset.shape[0]))
-
-    # Find in "dataset2", the example most similar to "dataset1[0]".
-    most_similar_example_idx = np.argmin(distances[0, :])
-    logging.info("most_similar_example_idx: %s", most_similar_example_idx)
-    logging.info("Seed example:\n%s", dataset.iloc[0])
-    logging.info(
-        "Most similar example:\n%s", dataset.iloc[most_similar_example_idx]
-    )
-
-    # High likelihood that the labels are the same (true in this example).
-    self.assertEqual(
-        dataset.iloc[most_similar_example_idx]["income"],
-        dataset.iloc[0]["income"],
-    )
-
-  def test_model_inspector_get_valid_tree(self):
-    self.assertEqual(self.adult_binary_class_gbdt.num_trees(), 68)
-    self.assertLen(
-        self.adult_binary_class_gbdt.get_all_trees(),
-        self.adult_binary_class_gbdt.num_trees(),
-    )
-
-    tree = self.adult_binary_class_gbdt.get_tree(1)
-    self.assertFalse(tree.root.is_leaf)
-    # Validated with: external/ydf_cc/yggdrasil_decision_forests/cli:show_model
-    self.assertEqual(
-        tree.root.condition,
-        condition_lib.CategoricalIsInCondition(
-            missing=False,
-            score=3275.003662109375,
-            attribute=5,
-            mask=[2, 3, 4, 5, 6, 7],
-        ),
-    )
-    self.assertEqual(
-        tree.root.value,
-        value_lib.RegressionValue(
-            value=-0.0006140652694739401, num_examples=0.0
-        ),
-    )
-
-  def test_model_inspector_get_wrong_tree(self):
-    with self.assertRaisesRegex(ValueError, "Invalid tree index"):
-      _ = self.adult_binary_class_gbdt.get_tree(-1)
-    with self.assertRaisesRegex(ValueError, "Invalid tree index"):
-      _ = self.adult_binary_class_gbdt.get_tree(
-          self.adult_binary_class_gbdt.num_trees()
-      )
-
-  def test_model_inspector_print_tree(self):
-    tree = self.adult_binary_class_gbdt.get_tree(1)
-    test_utils.golden_check_string(
-        self,
-        tree.pretty(self.adult_binary_class_gbdt.data_spec()),
-        os.path.join(test_utils.pydf_test_data_path(), "adult_gbt_tree_0.txt"),
-    )
-
-
-class EditModelTest(absltest.TestCase):
-
-  def create_model_and_dataset(
-      self,
-  ) -> Tuple[
-      gradient_boosted_trees_model.GradientBoostedTreesModel,
-      Dict[str, np.ndarray],
-  ]:
-    dataset = {
-        "x1": np.array([0, 0, 0, 1, 1, 1]),
-        "x2": np.array([1, 1, 0, 0, 1, 1]),
-        "y": np.array([0, 0, 0, 0, 1, 1]),
-    }
-    model = specialized_learners.GradientBoostedTreesLearner(
-        label="y",
-        num_trees=1,
-        max_depth=4,
-        apply_link_function=False,
-        min_examples=1,
-    ).train(dataset)
-    assert isinstance(
-        model, gradient_boosted_trees_model.GradientBoostedTreesModel
-    )
-    return model, dataset
-
-  def test_create_model_and_dataset(self):
-    model, dataset = self.create_model_and_dataset()
-    tree = model.get_tree(0)
-    self.assertEqual(model.num_trees(), 1)
-    self.assertEqual(
-        tree.pretty(model.data_spec()),
-        """\
-'x1' >= 0.5 [score=0.11111 missing=True]
-    (pos) 'x2' >= 0.5 [score=0.22222 missing=True]
-            (pos) value=0.3
-            (neg) value=-0.15 sd=4.069e-05
-    (neg) value=-0.15 sd=4.069e-05
-""",
-    )
-    bias = -0.693147
-    npt.assert_almost_equal(model.initial_predictions(), [bias], decimal=4)
-    npt.assert_almost_equal(
-        model.predict(dataset),
-        [
-            bias - 0.15,
-            bias - 0.15,
-            bias - 0.15,
-            bias - 0.15,
-            bias + 0.3,
-            bias + 0.3,
-        ],
-        decimal=4,
-    )
-
-  def test_set_tree(self):
-    model, dataset = self.create_model_and_dataset()
-    tree = model.get_tree(0)
-    assert isinstance(tree.root, node_lib.NonLeaf)
-    assert isinstance(tree.root.pos_child.pos_child, node_lib.Leaf)
-    assert isinstance(
-        tree.root.pos_child.pos_child.value, value_lib.RegressionValue
-    )
-    tree.root.pos_child.pos_child.value.value = 0.1
-
-    expected_tree_repr = """\
-'x1' >= 0.5 [score=0.11111 missing=True]
-    (pos) 'x2' >= 0.5 [score=0.22222 missing=True]
-            (pos) value=0.1
-            (neg) value=-0.15 sd=4.069e-05
-    (neg) value=-0.15 sd=4.069e-05
-"""
-
-    self.assertEqual(tree.pretty(model.data_spec()), expected_tree_repr)
-    model.set_tree(0, tree)
-    self.assertEqual(model.num_trees(), 1)
-    self.assertEqual(
-        model.get_tree(0).pretty(model.data_spec()), expected_tree_repr
-    )
-    bias = -0.693147
-    npt.assert_almost_equal(
-        model.predict(dataset),
-        [
-            bias - 0.15,
-            bias - 0.15,
-            bias - 0.15,
-            bias - 0.15,
-            bias + 0.1,
-            bias + 0.1,
-        ],
-        decimal=4,
-    )
-
-  def test_add_tree(self):
-    model, dataset = self.create_model_and_dataset()
-
-    tree = Tree(
-        root=NonLeaf(
-            condition=NumericalHigherThanCondition(
-                missing=False, score=3.0, attribute=1, threshold=0.6
-            ),
-            pos_child=Leaf(value=RegressionValue(num_examples=1.0, value=2.0)),
-            neg_child=Leaf(value=RegressionValue(num_examples=1.0, value=-2.0)),
-        )
-    )
-    expected_tree_repr = """\
-'x1' >= 0.6 [score=3 missing=False]
-    (pos) value=2
-    (neg) value=-2
-"""
-
-    self.assertEqual(tree.pretty(model.data_spec()), expected_tree_repr)
-    model.add_tree(tree)
-    self.assertEqual(model.num_trees(), 2)
-    self.assertEqual(
-        model.get_tree(1).pretty(model.data_spec()), expected_tree_repr
-    )
-    bias = -0.693147
-    npt.assert_almost_equal(
-        model.predict(dataset),
-        [
-            bias - 0.15 - 2.0,
-            bias - 0.15 - 2.0,
-            bias - 0.15 - 2.0,
-            bias - 0.15 + 2.0,
-            bias + 0.3 + 2.0,
-            bias + 0.3 + 2.0,
-        ],
-        decimal=4,
-    )
-
-  def test_remove_tree(self):
-    model, dataset = self.create_model_and_dataset()
-    model.remove_tree(0)
-    self.assertEqual(model.num_trees(), 0)
-    bias = -0.693147
-    npt.assert_almost_equal(
-        model.predict(dataset),
-        [bias] * 6,
-        decimal=4,
-    )
-
-  def test_invalid_inference(self):
-    dataset = {
-        "x1": np.array([0, 0, 2, 2, 1, 1]),
-        "y": np.array([0, 0, 1, 1, 2, 2]),
-    }
-    model = specialized_learners.GradientBoostedTreesLearner(
-        label="y",
-        num_trees=1,
-        max_depth=4,
-        apply_link_function=False,
-        min_examples=1,
-    ).train(dataset)
-    assert isinstance(
-        model, gradient_boosted_trees_model.GradientBoostedTreesModel
-    )
-    self.assertEqual(model.num_trees(), 3)
-    self.assertEqual(
-        model.get_tree(0).pretty(model.data_spec()),
-        """\
-'x1' >= 0.5 [score=0.22222 missing=True]
-    (pos) value=-0.15 sd=4.069e-05
-    (neg) value=0.3
-""",
-    )
-    self.assertEqual(
-        model.get_tree(1).pretty(model.data_spec()),
-        """\
-'x1' >= 1.5 [score=0.22222 missing=False]
-    (pos) value=0.3
-    (neg) value=-0.15 sd=4.069e-05
-""",
-    )
-    self.assertEqual(
-        model.get_tree(2).pretty(model.data_spec()),
-        """\
-'x1' >= 0.5 [score=0.055556 missing=True]
-    (pos) 'x1' >= 1.5 [score=0.25 missing=False]
-            (pos) value=-0.15 sd=4.069e-05
-            (neg) value=0.3
-    (neg) value=-0.15 sd=4.069e-05
-""",
-    )
-
-    tree = Tree(
-        root=NonLeaf(
-            condition=NumericalHigherThanCondition(
-                missing=False, score=3.0, attribute=1, threshold=0.6
-            ),
-            pos_child=Leaf(value=RegressionValue(num_examples=1.0, value=2.0)),
-            neg_child=Leaf(value=RegressionValue(num_examples=1.0, value=-2.0)),
-        )
-    )
-    expected_tree_repr = """\
-'x1' >= 0.6 [score=3 missing=False]
-    (pos) value=2
-    (neg) value=-2
-"""
-    self.assertEqual(tree.pretty(model.data_spec()), expected_tree_repr)
-    model.add_tree(tree)
-    self.assertEqual(model.num_trees(), 4)
-
-    with self.assertRaisesRegex(
-        ValueError, "Invalid number of trees in the gradient boosted tree"
-    ):
-      _ = model.predict(dataset)
-
-  def test_add_tree_to_empty_forest(self):
-    dataset = {
-        "x1": np.array([0, 1]),
-        "x2": np.array([0, 1]),
-        "x3": np.array([0, 1]),
-        "y": np.array([0, 1]),
-    }
-    model = specialized_learners.GradientBoostedTreesLearner(
-        label="y", num_trees=0
-    ).train(dataset)
-    assert isinstance(
-        model, gradient_boosted_trees_model.GradientBoostedTreesModel
-    )
-    self.assertEqual(model.num_trees(), 0)
-    self.assertSequenceEqual(model.input_feature_names(), ["x1", "x2", "x3"])
-
-    tree = Tree(
-        root=NonLeaf(
-            condition=NumericalHigherThanCondition(
-                missing=False, score=3.0, attribute=2, threshold=0.6
-            ),
-            pos_child=Leaf(value=RegressionValue(num_examples=1.0, value=2.0)),
-            neg_child=Leaf(value=RegressionValue(num_examples=1.0, value=-2.0)),
-        )
-    )
-    model.add_tree(tree)
-    self.assertEqual(model.num_trees(), 1)
-    self.assertSequenceEqual(model.input_feature_names(), ["x1", "x2", "x3"])
-
-    model.remove_tree(0)
-    self.assertEqual(model.num_trees(), 0)
-    self.assertSequenceEqual(model.input_feature_names(), ["x1", "x2", "x3"])
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Tests for the Gradient Boosted trees models."""
+
+import os
+from typing import Dict, Tuple
+
+from absl import logging
+from absl.testing import absltest
+import numpy as np
+import numpy.testing as npt
+import pandas as pd
+
+from ydf.dataset import dataspec
+from ydf.learner import specialized_learners
+from ydf.model import generic_model
+from ydf.model import model_lib
+from ydf.model.gradient_boosted_trees_model import gradient_boosted_trees_model
+from ydf.model.tree import condition as condition_lib
+from ydf.model.tree import node as node_lib
+from ydf.model.tree import tree as tree_lib
+from ydf.model.tree import value as value_lib
+from ydf.utils import test_utils
+
+RegressionValue = value_lib.RegressionValue
+Leaf = node_lib.Leaf
+NonLeaf = node_lib.NonLeaf
+NumericalHigherThanCondition = condition_lib.NumericalHigherThanCondition
+Tree = tree_lib.Tree
+
+
+class GradientBoostedTreesTest(absltest.TestCase):
+
+  def setUp(self):
+    super().setUp()
+    # This model is a classification model for pure serving.
+    adult_binary_class_gbdt_path = os.path.join(
+        test_utils.ydf_test_data_path(), "model", "adult_binary_class_gbdt"
+    )
+    self.adult_binary_class_gbdt = model_lib.load_model(
+        adult_binary_class_gbdt_path
+    )
+    # This model is a classification model with full training logs.
+    gbt_adult_base_with_na_path = os.path.join(
+        test_utils.ydf_test_data_path(), "golden", "gbt_adult_base_with_na"
+    )
+    self.gbt_adult_base_with_na = model_lib.load_model(
+        gbt_adult_base_with_na_path
+    )
+
+  def test_input_feature_names(self):
+    self.assertEqual(
+        self.adult_binary_class_gbdt.input_feature_names(),
+        [
+            "age",
+            "workclass",
+            "fnlwgt",
+            "education",
+            "education_num",
+            "marital_status",
+            "occupation",
+            "relationship",
+            "race",
+            "sex",
+            "capital_gain",
+            "capital_loss",
+            "hours_per_week",
+            "native_country",
+        ],
+    )
+
+  def test_input_features(self):
+    InputFeature = generic_model.InputFeature
+    NUMERICAL = dataspec.Semantic.NUMERICAL
+    CATEGORICAL = dataspec.Semantic.CATEGORICAL
+    self.assertEqual(
+        self.adult_binary_class_gbdt.input_features(),
+        [
+            InputFeature("age", NUMERICAL, 0),
+            InputFeature("workclass", CATEGORICAL, 1),
+            InputFeature("fnlwgt", NUMERICAL, 2),
+            InputFeature("education", CATEGORICAL, 3),
+            InputFeature("education_num", CATEGORICAL, 4),
+            InputFeature("marital_status", CATEGORICAL, 5),
+            InputFeature("occupation", CATEGORICAL, 6),
+            InputFeature("relationship", CATEGORICAL, 7),
+            InputFeature("race", CATEGORICAL, 8),
+            InputFeature("sex", CATEGORICAL, 9),
+            InputFeature("capital_gain", NUMERICAL, 10),
+            InputFeature("capital_loss", NUMERICAL, 11),
+            InputFeature("hours_per_week", NUMERICAL, 12),
+            InputFeature("native_country", CATEGORICAL, 13),
+        ],
+    )
+
+  def test_task(self):
+    self.assertEqual(
+        self.adult_binary_class_gbdt.task(), generic_model.Task.CLASSIFICATION
+    )
+
+  def test_label_classes(self):
+    self.assertEqual(
+        self.adult_binary_class_gbdt.label_classes(), ["<=50K", ">50K"]
+    )
+
+  def test_label(self):
+    self.assertEqual(self.adult_binary_class_gbdt.label(), "income")
+
+  def test_validation_loss(self):
+    validation_loss = self.adult_binary_class_gbdt.validation_loss()
+    self.assertAlmostEqual(validation_loss, 0.573842942, places=6)
+
+  def test_validation_loss_if_no_validation_dataset(self):
+    dataset = {"x": np.array([0, 0, 1, 1]), "y": np.array([0, 0, 0, 1])}
+    model = specialized_learners.GradientBoostedTreesLearner(
+        label="y", validation_ratio=0.0, num_trees=2
+    ).train(dataset)
+    validation_loss = model.validation_loss()
+    self.assertIsNone(validation_loss)
+
+  def test_initial_predictions(self):
+    initial_predictions = self.adult_binary_class_gbdt.initial_predictions()
+    np.testing.assert_allclose(initial_predictions, [-1.1630996])
+
+  def test_validation_evaluation_empty(self):
+    dataset = {
+        "x1": np.array([0, 0, 0, 1, 1, 1]),
+        "y": np.array([0, 0, 0, 0, 1, 1]),
+    }
+    model = specialized_learners.GradientBoostedTreesLearner(
+        label="y",
+        num_trees=1,
+        max_depth=4,
+        min_examples=1,
+        validation_ratio=0.0,
+    ).train(dataset)
+    self.assertIsInstance(
+        model, gradient_boosted_trees_model.GradientBoostedTreesModel
+    )
+    validation_evaluation = model.validation_evaluation()
+    self.assertIsNone(validation_evaluation)
+
+  def test_validation_evaluation_no_training_logs(self):
+    validation_evaluation = self.adult_binary_class_gbdt.validation_evaluation()
+    self.assertIsNone(validation_evaluation.accuracy)
+    self.assertAlmostEqual(validation_evaluation.loss, 0.57384294)
+
+  def test_validation_evaluation_with_content(self):
+    validation_evaluation = self.gbt_adult_base_with_na.validation_evaluation()
+    self.assertAlmostEqual(validation_evaluation.accuracy, 0.8498403)
+
+  def test_variable_importances(self):
+    model_path = os.path.join(
+        test_utils.ydf_test_data_path(),
+        "model",
+        "synthetic_ranking_gbdt_numerical",
+    )
+    model = model_lib.load_model(model_path)
+    variable_importances = model.variable_importances()
+    self.assertEqual(
+        variable_importances,
+        {
+            "NUM_NODES": [
+                (355.0, "num_2"),
+                (326.0, "num_0"),
+                (248.0, "num_1"),
+                (193.0, "num_3"),
+            ],
+            "INV_MEAN_MIN_DEPTH": [
+                (0.54955206094026765, "num_0"),
+                (0.43300866801748344, "num_2"),
+                (0.21987296105251422, "num_1"),
+                (0.20886402442940008, "num_3"),
+            ],
+            "SUM_SCORE": [
+                (331.52462868355724, "num_0"),
+                (297.70595154801595, "num_2"),
+                (103.86176226850876, "num_1"),
+                (52.43193327602421, "num_3"),
+            ],
+            "NUM_AS_ROOT": [
+                (35.0, "num_0"),
+                (12.0, "num_2"),
+                (1.0, "num_3"),
+            ],
+        },
+    )
+
+  def test_predict_distance(self):
+    dataset = pd.read_csv(
+        os.path.join(
+            test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
+        ),
+        nrows=500,
+    )
+
+    distances = self.adult_binary_class_gbdt.distance(dataset)
+    logging.info("distances:\n%s", distances)
+    self.assertEqual(distances.shape, (dataset.shape[0], dataset.shape[0]))
+
+    # Find in "dataset2", the example most similar to "dataset1[0]".
+    most_similar_example_idx = np.argmin(distances[0, :])
+    logging.info("most_similar_example_idx: %s", most_similar_example_idx)
+    logging.info("Seed example:\n%s", dataset.iloc[0])
+    logging.info(
+        "Most similar example:\n%s", dataset.iloc[most_similar_example_idx]
+    )
+
+    # High likelihood that the labels are the same (true in this example).
+    self.assertEqual(
+        dataset.iloc[most_similar_example_idx]["income"],
+        dataset.iloc[0]["income"],
+    )
+
+  def test_model_inspector_get_valid_tree(self):
+    self.assertEqual(self.adult_binary_class_gbdt.num_trees(), 68)
+    self.assertLen(
+        self.adult_binary_class_gbdt.get_all_trees(),
+        self.adult_binary_class_gbdt.num_trees(),
+    )
+
+    tree = self.adult_binary_class_gbdt.get_tree(1)
+    self.assertFalse(tree.root.is_leaf)
+    # Validated with: external/ydf_cc/yggdrasil_decision_forests/cli:show_model
+    self.assertEqual(
+        tree.root.condition,
+        condition_lib.CategoricalIsInCondition(
+            missing=False,
+            score=3275.003662109375,
+            attribute=5,
+            mask=[2, 3, 4, 5, 6, 7],
+        ),
+    )
+    self.assertEqual(
+        tree.root.value,
+        value_lib.RegressionValue(
+            value=-0.0006140652694739401, num_examples=0.0
+        ),
+    )
+
+  def test_model_inspector_get_wrong_tree(self):
+    with self.assertRaisesRegex(ValueError, "Invalid tree index"):
+      _ = self.adult_binary_class_gbdt.get_tree(-1)
+    with self.assertRaisesRegex(ValueError, "Invalid tree index"):
+      _ = self.adult_binary_class_gbdt.get_tree(
+          self.adult_binary_class_gbdt.num_trees()
+      )
+
+  def test_model_inspector_print_tree(self):
+    tree = self.adult_binary_class_gbdt.get_tree(1)
+    test_utils.golden_check_string(
+        self,
+        tree.pretty(self.adult_binary_class_gbdt.data_spec()),
+        os.path.join(test_utils.pydf_test_data_path(), "adult_gbt_tree_0.txt"),
+    )
+
+
+class EditModelTest(absltest.TestCase):
+
+  def create_model_and_dataset(
+      self,
+  ) -> Tuple[
+      gradient_boosted_trees_model.GradientBoostedTreesModel,
+      Dict[str, np.ndarray],
+  ]:
+    dataset = {
+        "x1": np.array([0, 0, 0, 1, 1, 1]),
+        "x2": np.array([1, 1, 0, 0, 1, 1]),
+        "y": np.array([0, 0, 0, 0, 1, 1]),
+    }
+    model = specialized_learners.GradientBoostedTreesLearner(
+        label="y",
+        num_trees=1,
+        max_depth=4,
+        apply_link_function=False,
+        min_examples=1,
+    ).train(dataset)
+    assert isinstance(
+        model, gradient_boosted_trees_model.GradientBoostedTreesModel
+    )
+    return model, dataset
+
+  def test_create_model_and_dataset(self):
+    model, dataset = self.create_model_and_dataset()
+    tree = model.get_tree(0)
+    self.assertEqual(model.num_trees(), 1)
+    self.assertEqual(
+        tree.pretty(model.data_spec()),
+        """\
+'x1' >= 0.5 [score=0.11111 missing=True]
+    (pos) 'x2' >= 0.5 [score=0.22222 missing=True]
+            (pos) value=0.3
+            (neg) value=-0.15 sd=4.069e-05
+    (neg) value=-0.15 sd=4.069e-05
+""",
+    )
+    bias = -0.693147
+    npt.assert_almost_equal(model.initial_predictions(), [bias], decimal=4)
+    npt.assert_almost_equal(
+        model.predict(dataset),
+        [
+            bias - 0.15,
+            bias - 0.15,
+            bias - 0.15,
+            bias - 0.15,
+            bias + 0.3,
+            bias + 0.3,
+        ],
+        decimal=4,
+    )
+
+  def test_set_tree(self):
+    model, dataset = self.create_model_and_dataset()
+    tree = model.get_tree(0)
+    assert isinstance(tree.root, node_lib.NonLeaf)
+    assert isinstance(tree.root.pos_child.pos_child, node_lib.Leaf)
+    assert isinstance(
+        tree.root.pos_child.pos_child.value, value_lib.RegressionValue
+    )
+    tree.root.pos_child.pos_child.value.value = 0.1
+
+    expected_tree_repr = """\
+'x1' >= 0.5 [score=0.11111 missing=True]
+    (pos) 'x2' >= 0.5 [score=0.22222 missing=True]
+            (pos) value=0.1
+            (neg) value=-0.15 sd=4.069e-05
+    (neg) value=-0.15 sd=4.069e-05
+"""
+
+    self.assertEqual(tree.pretty(model.data_spec()), expected_tree_repr)
+    model.set_tree(0, tree)
+    self.assertEqual(model.num_trees(), 1)
+    self.assertEqual(
+        model.get_tree(0).pretty(model.data_spec()), expected_tree_repr
+    )
+    bias = -0.693147
+    npt.assert_almost_equal(
+        model.predict(dataset),
+        [
+            bias - 0.15,
+            bias - 0.15,
+            bias - 0.15,
+            bias - 0.15,
+            bias + 0.1,
+            bias + 0.1,
+        ],
+        decimal=4,
+    )
+
+  def test_add_tree(self):
+    model, dataset = self.create_model_and_dataset()
+
+    tree = Tree(
+        root=NonLeaf(
+            condition=NumericalHigherThanCondition(
+                missing=False, score=3.0, attribute=1, threshold=0.6
+            ),
+            pos_child=Leaf(value=RegressionValue(num_examples=1.0, value=2.0)),
+            neg_child=Leaf(value=RegressionValue(num_examples=1.0, value=-2.0)),
+        )
+    )
+    expected_tree_repr = """\
+'x1' >= 0.6 [score=3 missing=False]
+    (pos) value=2
+    (neg) value=-2
+"""
+
+    self.assertEqual(tree.pretty(model.data_spec()), expected_tree_repr)
+    model.add_tree(tree)
+    self.assertEqual(model.num_trees(), 2)
+    self.assertEqual(
+        model.get_tree(1).pretty(model.data_spec()), expected_tree_repr
+    )
+    bias = -0.693147
+    npt.assert_almost_equal(
+        model.predict(dataset),
+        [
+            bias - 0.15 - 2.0,
+            bias - 0.15 - 2.0,
+            bias - 0.15 - 2.0,
+            bias - 0.15 + 2.0,
+            bias + 0.3 + 2.0,
+            bias + 0.3 + 2.0,
+        ],
+        decimal=4,
+    )
+
+  def test_remove_tree(self):
+    model, dataset = self.create_model_and_dataset()
+    model.remove_tree(0)
+    self.assertEqual(model.num_trees(), 0)
+    bias = -0.693147
+    npt.assert_almost_equal(
+        model.predict(dataset),
+        [bias] * 6,
+        decimal=4,
+    )
+
+  def test_invalid_inference(self):
+    dataset = {
+        "x1": np.array([0, 0, 2, 2, 1, 1]),
+        "y": np.array([0, 0, 1, 1, 2, 2]),
+    }
+    model = specialized_learners.GradientBoostedTreesLearner(
+        label="y",
+        num_trees=1,
+        max_depth=4,
+        apply_link_function=False,
+        min_examples=1,
+    ).train(dataset)
+    assert isinstance(
+        model, gradient_boosted_trees_model.GradientBoostedTreesModel
+    )
+    self.assertEqual(model.num_trees(), 3)
+    self.assertEqual(
+        model.get_tree(0).pretty(model.data_spec()),
+        """\
+'x1' >= 0.5 [score=0.22222 missing=True]
+    (pos) value=-0.15 sd=4.069e-05
+    (neg) value=0.3
+""",
+    )
+    self.assertEqual(
+        model.get_tree(1).pretty(model.data_spec()),
+        """\
+'x1' >= 1.5 [score=0.22222 missing=False]
+    (pos) value=0.3
+    (neg) value=-0.15 sd=4.069e-05
+""",
+    )
+    self.assertEqual(
+        model.get_tree(2).pretty(model.data_spec()),
+        """\
+'x1' >= 0.5 [score=0.055556 missing=True]
+    (pos) 'x1' >= 1.5 [score=0.25 missing=False]
+            (pos) value=-0.15 sd=4.069e-05
+            (neg) value=0.3
+    (neg) value=-0.15 sd=4.069e-05
+""",
+    )
+
+    tree = Tree(
+        root=NonLeaf(
+            condition=NumericalHigherThanCondition(
+                missing=False, score=3.0, attribute=1, threshold=0.6
+            ),
+            pos_child=Leaf(value=RegressionValue(num_examples=1.0, value=2.0)),
+            neg_child=Leaf(value=RegressionValue(num_examples=1.0, value=-2.0)),
+        )
+    )
+    expected_tree_repr = """\
+'x1' >= 0.6 [score=3 missing=False]
+    (pos) value=2
+    (neg) value=-2
+"""
+    self.assertEqual(tree.pretty(model.data_spec()), expected_tree_repr)
+    model.add_tree(tree)
+    self.assertEqual(model.num_trees(), 4)
+
+    with self.assertRaisesRegex(
+        ValueError, "Invalid number of trees in the gradient boosted tree"
+    ):
+      _ = model.predict(dataset)
+
+  def test_add_tree_to_empty_forest(self):
+    dataset = {
+        "x1": np.array([0, 1]),
+        "x2": np.array([0, 1]),
+        "x3": np.array([0, 1]),
+        "y": np.array([0, 1]),
+    }
+    model = specialized_learners.GradientBoostedTreesLearner(
+        label="y", num_trees=0
+    ).train(dataset)
+    assert isinstance(
+        model, gradient_boosted_trees_model.GradientBoostedTreesModel
+    )
+    self.assertEqual(model.num_trees(), 0)
+    self.assertSequenceEqual(model.input_feature_names(), ["x1", "x2", "x3"])
+
+    tree = Tree(
+        root=NonLeaf(
+            condition=NumericalHigherThanCondition(
+                missing=False, score=3.0, attribute=2, threshold=0.6
+            ),
+            pos_child=Leaf(value=RegressionValue(num_examples=1.0, value=2.0)),
+            neg_child=Leaf(value=RegressionValue(num_examples=1.0, value=-2.0)),
+        )
+    )
+    model.add_tree(tree)
+    self.assertEqual(model.num_trees(), 1)
+    self.assertSequenceEqual(model.input_feature_names(), ["x1", "x2", "x3"])
+
+    model.remove_tree(0)
+    self.assertEqual(model.num_trees(), 0)
+    self.assertSequenceEqual(model.input_feature_names(), ["x1", "x2", "x3"])
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/model/random_forest_model/__init__.py

 * *Ordering differences only*

```diff
@@ -1,14 +1,14 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
```

## ydf/model/random_forest_model/random_forest_model.py

```diff
@@ -1,142 +1,142 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Definitions for Random Forest models."""
-
-import dataclasses
-from typing import Optional, Sequence
-from yggdrasil_decision_forests.metric import metric_pb2
-from yggdrasil_decision_forests.model.random_forest import random_forest_pb2
-from ydf.cc import ydf
-from ydf.metric import metric
-from ydf.model.decision_forest_model import decision_forest_model
-
-
-@dataclasses.dataclass(frozen=True)
-class OutOfBagEvaluation:
-  """A collection of out-of-bag metrics.
-
-  Attributes:
-    number_of_trees: Number of trees when the evaluation was created.
-    evaluation: Rich evaluation object containing the OOB evaluation metrics.
-  """
-
-  number_of_trees: int
-  evaluation: metric.Evaluation
-
-
-class RandomForestModel(decision_forest_model.DecisionForestModel):
-  """A Random Forest model for prediction and inspection."""
-
-  _model: ydf.RandomForestCCModel
-
-  def out_of_bag_evaluations(self) -> Sequence[OutOfBagEvaluation]:
-    """Returns the Out-Of-Bag evaluations of the model, if available.
-
-    Each tree in a random forest is only trained on a fraction of the training
-    examples. Out-of-bag (OOB) evaluations evaluate each training example on the
-    trees that have not seen it in training. This creates a self-evaluation
-    method that does not require a training dataset. See
-    https://developers.google.com/machine-learning/decision-forests/out-of-bag
-    for details.
-
-    Computing OOB metrics slows down training and requires hyperparameter
-    `compute_oob_performances` to be set. The learner then computes the OOB
-    evaluation at regular intervals during the training. The returned list of
-    evaluations is sorted by the number of trees and its last element is the OOB
-    evaluation of the full model.
-
-    If no OOB evaluations have been computed, an empty list is returned.
-
-    Usage example:
-
-    ```python
-    import pandas as pd
-    import ydf
-
-    # Train model
-    train_ds = pd.read_csv("train.csv")
-    learner = ydf.RandomForestLearner(label="label",
-                                      compute_oob_performances=True)
-    model = learner.train(train_ds)
-
-    oob_evaluations = model.out_of_bag_evaluations()
-    # In an interactive Python environment, print a rich evaluation report.
-    oob_evaluations[-1].evaluation
-    ```
-    """
-    raw_evaluations: Sequence[random_forest_pb2.OutOfBagTrainingEvaluations] = (
-        self._model.out_of_bag_evaluations()
-    )
-    return [
-        OutOfBagEvaluation(
-            number_of_trees=evaluation_proto.number_of_trees,
-            evaluation=metric.Evaluation(evaluation_proto.evaluation),
-        )
-        for evaluation_proto in raw_evaluations
-    ]
-
-  def winner_takes_all(self) -> bool:
-    """Returns if the model uses a winner-takes-all strategy for classification.
-
-    This parameter determines how to aggregate individual tree votes during
-    inference in a classification random forest. It is defined by the
-    `winner_take_all` Random Forest learner hyper-parameter,
-
-    If true, each tree votes for a single class, which is the traditional random
-    forest inference method. If false, each tree outputs a probability
-    distribution across all classes.
-
-    If the model is not a classification model, the return value of this
-    function is arbitrary and does not influence model inference.
-    """
-    return self._model.winner_takes_all()
-
-  def self_evaluation(self) -> Optional[metric.Evaluation]:
-    """Returns the model's self-evaluation.
-
-    For Random Forest models, the self-evaluation is out-of-bag evaluation on
-    the full model. Note that the Random Forest models do not use a validation
-    dataset. If out-of-bag evaluation is not enabled, no self-evaluation is
-    computed.
-
-    Different models use different methods for self-evaluation. Notably,
-    Gradient Boosted Trees use the evaluation on the validation dataset.
-    Therefore, self-evaluations are not comparable between different model
-    types.
-
-    Returns None if no self-evaluation has been computed.
-
-    Usage example:
-
-    ```python
-    import pandas as pd
-    import ydf
-
-    # Train model
-    train_ds = pd.read_csv("train.csv")
-    learner = ydf.RandomForestLearner(label="label",
-                                    compute_oob_performances=True)
-    model = learner.train(train_ds)
-
-    self_evaluation = model.self_evaluation()
-    # In an interactive Python environment, print a rich evaluation report.
-    self_evaluation
-    ```
-    """
-    oob_evaluation = self.out_of_bag_evaluations()
-    if oob_evaluation:
-      return oob_evaluation[-1].evaluation
-    # Return an empty evaluation object.
-    return None
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Definitions for Random Forest models."""
+
+import dataclasses
+from typing import Optional, Sequence
+from ydf.proto.metric import metric_pb2
+from ydf.proto.model.random_forest import random_forest_pb2
+from ydf.cc import ydf
+from ydf.metric import metric
+from ydf.model.decision_forest_model import decision_forest_model
+
+
+@dataclasses.dataclass(frozen=True)
+class OutOfBagEvaluation:
+  """A collection of out-of-bag metrics.
+
+  Attributes:
+    number_of_trees: Number of trees when the evaluation was created.
+    evaluation: Rich evaluation object containing the OOB evaluation metrics.
+  """
+
+  number_of_trees: int
+  evaluation: metric.Evaluation
+
+
+class RandomForestModel(decision_forest_model.DecisionForestModel):
+  """A Random Forest model for prediction and inspection."""
+
+  _model: ydf.RandomForestCCModel
+
+  def out_of_bag_evaluations(self) -> Sequence[OutOfBagEvaluation]:
+    """Returns the Out-Of-Bag evaluations of the model, if available.
+
+    Each tree in a random forest is only trained on a fraction of the training
+    examples. Out-of-bag (OOB) evaluations evaluate each training example on the
+    trees that have not seen it in training. This creates a self-evaluation
+    method that does not require a training dataset. See
+    https://developers.google.com/machine-learning/decision-forests/out-of-bag
+    for details.
+
+    Computing OOB metrics slows down training and requires hyperparameter
+    `compute_oob_performances` to be set. The learner then computes the OOB
+    evaluation at regular intervals during the training. The returned list of
+    evaluations is sorted by the number of trees and its last element is the OOB
+    evaluation of the full model.
+
+    If no OOB evaluations have been computed, an empty list is returned.
+
+    Usage example:
+
+    ```python
+    import pandas as pd
+    import ydf
+
+    # Train model
+    train_ds = pd.read_csv("train.csv")
+    learner = ydf.RandomForestLearner(label="label",
+                                      compute_oob_performances=True)
+    model = learner.train(train_ds)
+
+    oob_evaluations = model.out_of_bag_evaluations()
+    # In an interactive Python environment, print a rich evaluation report.
+    oob_evaluations[-1].evaluation
+    ```
+    """
+    raw_evaluations: Sequence[random_forest_pb2.OutOfBagTrainingEvaluations] = (
+        self._model.out_of_bag_evaluations()
+    )
+    return [
+        OutOfBagEvaluation(
+            number_of_trees=evaluation_proto.number_of_trees,
+            evaluation=metric.Evaluation(evaluation_proto.evaluation),
+        )
+        for evaluation_proto in raw_evaluations
+    ]
+
+  def winner_takes_all(self) -> bool:
+    """Returns if the model uses a winner-takes-all strategy for classification.
+
+    This parameter determines how to aggregate individual tree votes during
+    inference in a classification random forest. It is defined by the
+    `winner_take_all` Random Forest learner hyper-parameter,
+
+    If true, each tree votes for a single class, which is the traditional random
+    forest inference method. If false, each tree outputs a probability
+    distribution across all classes.
+
+    If the model is not a classification model, the return value of this
+    function is arbitrary and does not influence model inference.
+    """
+    return self._model.winner_takes_all()
+
+  def self_evaluation(self) -> Optional[metric.Evaluation]:
+    """Returns the model's self-evaluation.
+
+    For Random Forest models, the self-evaluation is out-of-bag evaluation on
+    the full model. Note that the Random Forest models do not use a validation
+    dataset. If out-of-bag evaluation is not enabled, no self-evaluation is
+    computed.
+
+    Different models use different methods for self-evaluation. Notably,
+    Gradient Boosted Trees use the evaluation on the validation dataset.
+    Therefore, self-evaluations are not comparable between different model
+    types.
+
+    Returns None if no self-evaluation has been computed.
+
+    Usage example:
+
+    ```python
+    import pandas as pd
+    import ydf
+
+    # Train model
+    train_ds = pd.read_csv("train.csv")
+    learner = ydf.RandomForestLearner(label="label",
+                                    compute_oob_performances=True)
+    model = learner.train(train_ds)
+
+    self_evaluation = model.self_evaluation()
+    # In an interactive Python environment, print a rich evaluation report.
+    self_evaluation
+    ```
+    """
+    oob_evaluation = self.out_of_bag_evaluations()
+    if oob_evaluation:
+      return oob_evaluation[-1].evaluation
+    # Return an empty evaluation object.
+    return None
```

## ydf/model/random_forest_model/random_forest_model_test.py

 * *Ordering differences only*

```diff
@@ -1,106 +1,106 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Tests for the random forest models."""
-
-import logging
-import os
-
-from absl.testing import absltest
-import numpy as np
-import pandas as pd
-
-from ydf.model import model_lib
-from ydf.utils import test_utils
-
-
-class RandomForestModelTest(absltest.TestCase):
-
-  def setUp(self):
-    super().setUp()
-    model_path = os.path.join(
-        test_utils.ydf_test_data_path(), "model", "adult_binary_class_rf"
-    )
-    self.adult_binary_class_rf = model_lib.load_model(model_path)
-
-  def test_out_of_bag_evaluations(self):
-    oob_evaluations = self.adult_binary_class_rf.out_of_bag_evaluations()
-
-    self.assertLen(oob_evaluations, 2)
-    self.assertEqual(oob_evaluations[0].number_of_trees, 1)
-    self.assertAlmostEqual(oob_evaluations[0].evaluation.loss, 1.80617348178)
-    self.assertEqual(oob_evaluations[1].number_of_trees, 100)
-    self.assertAlmostEqual(oob_evaluations[1].evaluation.loss, 0.31474323732)
-
-  def test_empty_out_of_bag_evaluations(self):
-    # Uplift models do not have OOB evaluations.
-    model_path = os.path.join(
-        test_utils.ydf_test_data_path(),
-        "model",
-        "sim_pte_categorical_uplift_rf",
-    )
-    model = model_lib.load_model(model_path)
-
-    oob_evaluations = model.out_of_bag_evaluations()
-
-    self.assertEmpty(oob_evaluations)
-
-  def test_predict_distance(self):
-    dataset1 = pd.read_csv(
-        os.path.join(
-            test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
-        ),
-        nrows=500,
-    )
-    dataset2 = pd.read_csv(
-        os.path.join(
-            test_utils.ydf_test_data_path(), "dataset", "adult_train.csv"
-        ),
-        nrows=800,
-    )
-
-    distances = self.adult_binary_class_rf.distance(dataset1, dataset2)
-    logging.info("distances:\n%s", distances)
-    self.assertEqual(distances.shape, (dataset1.shape[0], dataset2.shape[0]))
-
-    # Find in "dataset2", the example most similar to "dataset1[0]".
-    most_similar_example_idx = np.argmin(distances[0, :])
-    logging.info("most_similar_example_idx: %s", most_similar_example_idx)
-    logging.info("Seed example:\n%s", dataset1.iloc[0])
-    logging.info(
-        "Most similar example:\n%s", dataset2.iloc[most_similar_example_idx]
-    )
-
-    # High likelihood that the labels are the same (true in this example).
-    self.assertEqual(
-        dataset2.iloc[most_similar_example_idx]["income"],
-        dataset1.iloc[0]["income"],
-    )
-
-  def test_winner_takes_all_false(self):
-    self.assertFalse(self.adult_binary_class_rf.winner_takes_all())
-
-  def test_winner_takes_all_true(self):
-    model_path = os.path.join(
-        test_utils.ydf_test_data_path(),
-        "golden",
-        "rf_adult_base",
-    )
-    model = model_lib.load_model(model_path)
-
-    self.assertTrue(model.winner_takes_all())
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Tests for the random forest models."""
+
+import logging
+import os
+
+from absl.testing import absltest
+import numpy as np
+import pandas as pd
+
+from ydf.model import model_lib
+from ydf.utils import test_utils
+
+
+class RandomForestModelTest(absltest.TestCase):
+
+  def setUp(self):
+    super().setUp()
+    model_path = os.path.join(
+        test_utils.ydf_test_data_path(), "model", "adult_binary_class_rf"
+    )
+    self.adult_binary_class_rf = model_lib.load_model(model_path)
+
+  def test_out_of_bag_evaluations(self):
+    oob_evaluations = self.adult_binary_class_rf.out_of_bag_evaluations()
+
+    self.assertLen(oob_evaluations, 2)
+    self.assertEqual(oob_evaluations[0].number_of_trees, 1)
+    self.assertAlmostEqual(oob_evaluations[0].evaluation.loss, 1.80617348178)
+    self.assertEqual(oob_evaluations[1].number_of_trees, 100)
+    self.assertAlmostEqual(oob_evaluations[1].evaluation.loss, 0.31474323732)
+
+  def test_empty_out_of_bag_evaluations(self):
+    # Uplift models do not have OOB evaluations.
+    model_path = os.path.join(
+        test_utils.ydf_test_data_path(),
+        "model",
+        "sim_pte_categorical_uplift_rf",
+    )
+    model = model_lib.load_model(model_path)
+
+    oob_evaluations = model.out_of_bag_evaluations()
+
+    self.assertEmpty(oob_evaluations)
+
+  def test_predict_distance(self):
+    dataset1 = pd.read_csv(
+        os.path.join(
+            test_utils.ydf_test_data_path(), "dataset", "adult_test.csv"
+        ),
+        nrows=500,
+    )
+    dataset2 = pd.read_csv(
+        os.path.join(
+            test_utils.ydf_test_data_path(), "dataset", "adult_train.csv"
+        ),
+        nrows=800,
+    )
+
+    distances = self.adult_binary_class_rf.distance(dataset1, dataset2)
+    logging.info("distances:\n%s", distances)
+    self.assertEqual(distances.shape, (dataset1.shape[0], dataset2.shape[0]))
+
+    # Find in "dataset2", the example most similar to "dataset1[0]".
+    most_similar_example_idx = np.argmin(distances[0, :])
+    logging.info("most_similar_example_idx: %s", most_similar_example_idx)
+    logging.info("Seed example:\n%s", dataset1.iloc[0])
+    logging.info(
+        "Most similar example:\n%s", dataset2.iloc[most_similar_example_idx]
+    )
+
+    # High likelihood that the labels are the same (true in this example).
+    self.assertEqual(
+        dataset2.iloc[most_similar_example_idx]["income"],
+        dataset1.iloc[0]["income"],
+    )
+
+  def test_winner_takes_all_false(self):
+    self.assertFalse(self.adult_binary_class_rf.winner_takes_all())
+
+  def test_winner_takes_all_true(self):
+    model_path = os.path.join(
+        test_utils.ydf_test_data_path(),
+        "golden",
+        "rf_adult_base",
+    )
+    model = model_lib.load_model(model_path)
+
+    self.assertTrue(model.winner_takes_all())
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/model/tree/__init__.py

 * *Ordering differences only*

```diff
@@ -1,46 +1,46 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""User facing API to inspect and edit trees."""
-
-# pylint: disable=g-importing-member,g-import-not-at-top,g-bad-import-order,reimported
-
-# Conditions
-from ydf.model.tree.condition import AbstractCondition
-from ydf.model.tree.condition import CategoricalIsInCondition
-from ydf.model.tree.condition import CategoricalSetContainsCondition
-from ydf.model.tree.condition import DiscretizedNumericalHigherThanCondition
-from ydf.model.tree.condition import IsMissingInCondition
-from ydf.model.tree.condition import IsTrueCondition
-from ydf.model.tree.condition import NumericalHigherThanCondition
-from ydf.model.tree.condition import NumericalSparseObliqueCondition
-
-# Node
-from ydf.model.tree.node import AbstractNode
-from ydf.model.tree.node import Leaf
-from ydf.model.tree.node import NonLeaf
-
-# Tree
-from ydf.model.tree.tree import Tree
-
-# Value
-from ydf.model.tree.value import AbstractValue
-from ydf.model.tree.value import RegressionValue
-from ydf.model.tree.value import ProbabilityValue
-from ydf.model.tree.value import UpliftValue
-
-# Plotting
-from ydf.model.tree.plot import PlotOptions
-
-# pylint: enable=g-importing-member,g-import-not-at-top,g-bad-import-order,reimported
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""User facing API to inspect and edit trees."""
+
+# pylint: disable=g-importing-member,g-import-not-at-top,g-bad-import-order,reimported
+
+# Conditions
+from ydf.model.tree.condition import AbstractCondition
+from ydf.model.tree.condition import CategoricalIsInCondition
+from ydf.model.tree.condition import CategoricalSetContainsCondition
+from ydf.model.tree.condition import DiscretizedNumericalHigherThanCondition
+from ydf.model.tree.condition import IsMissingInCondition
+from ydf.model.tree.condition import IsTrueCondition
+from ydf.model.tree.condition import NumericalHigherThanCondition
+from ydf.model.tree.condition import NumericalSparseObliqueCondition
+
+# Node
+from ydf.model.tree.node import AbstractNode
+from ydf.model.tree.node import Leaf
+from ydf.model.tree.node import NonLeaf
+
+# Tree
+from ydf.model.tree.tree import Tree
+
+# Value
+from ydf.model.tree.value import AbstractValue
+from ydf.model.tree.value import RegressionValue
+from ydf.model.tree.value import ProbabilityValue
+from ydf.model.tree.value import UpliftValue
+
+# Plotting
+from ydf.model.tree.plot import PlotOptions
+
+# pylint: enable=g-importing-member,g-import-not-at-top,g-bad-import-order,reimported
```

## ydf/model/tree/condition.py

```diff
@@ -1,573 +1,573 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Conditions / splits for non-leaf nodes."""
-
-import abc
-import dataclasses
-import functools
-from typing import Any, Dict, Sequence, Union
-from yggdrasil_decision_forests.dataset import data_spec_pb2
-from yggdrasil_decision_forests.model.decision_tree import decision_tree_pb2
-from ydf.dataset import dataspec as dataspec_lib
-
-ColumnType = data_spec_pb2.ColumnType
-
-
-# TODO: b/310218604 - Use kw_only with default value score = 0.
-@dataclasses.dataclass
-class AbstractCondition(metaclass=abc.ABCMeta):
-  """Generic condition.
-
-  Attrs:
-    missing: Result of the evaluation of the condition if the input feature is
-      missing.
-    score: Score of a condition. The semantic depends on the learning algorithm.
-  """
-
-  missing: bool
-  score: float
-
-  @abc.abstractmethod
-  def pretty(self, dataspec: data_spec_pb2.DataSpecification) -> str:
-    raise NotImplementedError
-
-  def _tag(self) -> str:
-    return f"score={self.score:.5g} missing={self.missing}"
-
-
-@dataclasses.dataclass
-class IsMissingInCondition(AbstractCondition):
-  """Condition of the form "attribute is missing".
-
-  Attrs:
-    attribute: Attribute (or one of the attributes) tested by the condition.
-  """
-
-  attribute: int
-
-  def pretty(self, dataspec: data_spec_pb2.DataSpecification) -> str:
-    attribute_name = dataspec.columns[self.attribute].name
-    return f"{attribute_name!r} is missing [{self._tag()}]"
-
-
-@dataclasses.dataclass
-class IsTrueCondition(AbstractCondition):
-  """Condition of the form "attribute is true".
-
-  Attrs:
-    attribute: Attribute tested by the condition.
-  """
-
-  attribute: int
-
-  def pretty(self, dataspec: data_spec_pb2.DataSpecification) -> str:
-    attribute_name = dataspec.columns[self.attribute].name
-    return f"{attribute_name!r} is True [{self._tag()}]"
-
-
-@dataclasses.dataclass
-class NumericalHigherThanCondition(AbstractCondition):
-  """Condition of the form "attribute >= threshold".
-
-  Attrs:
-    attribute: Attribute tested by the condition.
-    threshold: Threshold.
-  """
-
-  attribute: int
-  threshold: float
-
-  def pretty(self, dataspec: data_spec_pb2.DataSpecification) -> str:
-    attribute_name = dataspec.columns[self.attribute].name
-    return f"{attribute_name!r} >= {self.threshold:g} [{self._tag()}]"
-
-
-@dataclasses.dataclass
-class DiscretizedNumericalHigherThanCondition(AbstractCondition):
-  """Condition of the form "attribute >= bounds[threshold]".
-
-  Attrs:
-    attribute: Attribute tested by the condition.
-    threshold_idx: Index of threshold in dataspec.
-  """
-
-  attribute: int
-  threshold_idx: int
-
-  def pretty(self, dataspec: data_spec_pb2.DataSpecification) -> str:
-    column_spec = dataspec.columns[self.attribute]
-    threshold = column_spec.discretized_numerical.boundaries[
-        self.threshold_idx - 1
-    ]
-    return (
-        f"{column_spec.name!r} >="
-        f" {threshold:g} [threshold_idx={self.threshold_idx} {self._tag()}]"
-    )
-
-
-@dataclasses.dataclass
-class CategoricalIsInCondition(AbstractCondition):
-  """Condition of the form "attribute in mask".
-
-  Attrs:
-    attribute: Attribute tested by the condition.
-    mask: Sorted mask values.
-  """
-
-  attribute: int
-  mask: Sequence[int]
-
-  def pretty(self, dataspec: data_spec_pb2.DataSpecification) -> str:
-    column_spec = dataspec.columns[self.attribute]
-    if column_spec.categorical.is_already_integerized:
-      mask_repr = list(self.mask)
-    else:
-      vocab = dataspec_lib.categorical_column_dictionary_to_list(column_spec)
-      mask_repr = [vocab[item] for item in self.mask]
-    return f"{column_spec.name!r} in {mask_repr} [{self._tag()}]"
-
-
-@dataclasses.dataclass
-class CategoricalSetContainsCondition(AbstractCondition):
-  """Condition of the form "attribute intersect mask != empty".
-
-  Attrs:
-    attribute: Attribute tested by the condition.
-    mask: Sorted mask values.
-  """
-
-  attribute: int
-  mask: Sequence[int]
-
-  def pretty(self, dataspec: data_spec_pb2.DataSpecification) -> str:
-    column_spec = dataspec.columns[self.attribute]
-    if column_spec.categorical.is_already_integerized:
-      mask_repr = list(self.mask)
-    else:
-      vocab = dataspec_lib.categorical_column_dictionary_to_list(column_spec)
-      mask_repr = [vocab[item] for item in self.mask]
-    return f"{column_spec.name!r} intersect {mask_repr} [{self._tag()}]"
-
-
-@dataclasses.dataclass
-class NumericalSparseObliqueCondition(AbstractCondition):
-  """Condition of the form "attributes * weights >= threshold".
-
-  Attrs:
-    attributes: Attribute tested by the condition.
-    weights: Weights for each of the attributes.
-    threshold: Threshold value of the condition.
-  """
-
-  attributes: Sequence[int]
-  weights: Sequence[float]
-  threshold: float
-
-  def pretty(self, dataspec: data_spec_pb2.DataSpecification) -> str:
-    text = " + ".join(
-        f"{dataspec.columns[attribute].name!r} x {weight:g}"
-        for attribute, weight in zip(self.attributes, self.weights)
-    )
-    if not text:
-      text = "*nothing*"
-    return f"{text} >= {self.threshold:g} [{self._tag()}]"
-
-
-def to_condition(
-    proto_condition: decision_tree_pb2.NodeCondition,
-    dataspec: data_spec_pb2.DataSpecification,
-) -> AbstractCondition:
-  """Extracts the "condition" part of a proto node."""
-
-  base_kwargs = {
-      "missing": proto_condition.na_value,
-      "score": proto_condition.split_score,
-  }
-  condition_type = proto_condition.condition
-  attribute_type = dataspec.columns[proto_condition.attribute].type
-
-  if condition_type.HasField("na_condition"):
-    return IsMissingInCondition(
-        attribute=proto_condition.attribute, **base_kwargs
-    )
-
-  elif condition_type.HasField("true_value_condition"):
-    return IsTrueCondition(attribute=proto_condition.attribute, **base_kwargs)
-
-  elif condition_type.HasField("higher_condition"):
-    return NumericalHigherThanCondition(
-        attribute=proto_condition.attribute,
-        threshold=condition_type.higher_condition.threshold,
-        **base_kwargs,
-    )
-
-  elif condition_type.HasField("contains_bitmap_condition"):
-    items = bitmap_to_items(
-        dataspec.columns[proto_condition.attribute],
-        condition_type.contains_bitmap_condition.elements_bitmap,
-    )
-    if attribute_type == ColumnType.CATEGORICAL:
-      return CategoricalIsInCondition(
-          attribute=proto_condition.attribute,
-          mask=items,
-          **base_kwargs,
-      )
-    elif attribute_type == ColumnType.CATEGORICAL_SET:
-      return CategoricalSetContainsCondition(
-          attribute=proto_condition.attribute,
-          mask=items,
-          **base_kwargs,
-      )
-    else:
-      raise ValueError("Invalid attribute type")
-
-  elif condition_type.HasField("contains_condition"):
-    if attribute_type == ColumnType.CATEGORICAL:
-      return CategoricalIsInCondition(
-          attribute=proto_condition.attribute,
-          mask=condition_type.contains_condition.elements,
-          **base_kwargs,
-      )
-    elif attribute_type == ColumnType.CATEGORICAL_SET:
-      return CategoricalSetContainsCondition(
-          attribute=proto_condition.attribute,
-          mask=condition_type.contains_condition.elements,
-          **base_kwargs,
-      )
-    else:
-      raise ValueError("Invalid attribute type")
-
-  elif condition_type.HasField("discretized_higher_condition"):
-    return DiscretizedNumericalHigherThanCondition(
-        attribute=proto_condition.attribute,
-        threshold_idx=condition_type.discretized_higher_condition.threshold,
-        **base_kwargs,
-    )
-
-  elif condition_type.HasField("oblique_condition"):
-    return NumericalSparseObliqueCondition(
-        attributes=condition_type.oblique_condition.attributes,
-        weights=condition_type.oblique_condition.weights,
-        threshold=condition_type.oblique_condition.threshold,
-        **base_kwargs,
-    )
-  else:
-    raise ValueError(f"Non supported condition type: {proto_condition}")
-
-
-@functools.singledispatch
-def to_json(
-    condition: AbstractCondition, dataspec: data_spec_pb2.DataSpecification
-) -> Dict[str, Any]:
-  """Creates a JSON-compatible object of the condition.
-
-  Note: While public, this logic is not part of the API. This is why this
-  methode's code is not an abstract method in AbstractValue.
-
-  Args:
-    condition: Input condition.
-    dataspec: Dataspec of the model.
-
-  Returns:
-    JSON condition.
-  """
-  raise NotImplementedError("Unsupported value type")
-
-
-@to_json.register
-def _to_json_is_missing(
-    condition: IsMissingInCondition, dataspec: data_spec_pb2.DataSpecification
-) -> Dict[str, Any]:
-  attribute_name = dataspec.columns[condition.attribute].name
-  return {"type": "IS_MISSING", "attribute": attribute_name}
-
-
-@to_json.register
-def _to_json_is_true(
-    condition: IsTrueCondition,
-    dataspec: data_spec_pb2.DataSpecification,
-) -> Dict[str, Any]:
-  attribute_name = dataspec.columns[condition.attribute].name
-  return {"type": "IS_TRUE", "attribute": attribute_name}
-
-
-@to_json.register
-def _to_json_higher_than(
-    condition: NumericalHigherThanCondition,
-    dataspec: data_spec_pb2.DataSpecification,
-) -> Dict[str, Any]:
-  attribute_name = dataspec.columns[condition.attribute].name
-  return {
-      "type": "NUMERICAL_IS_HIGHER_THAN",
-      "attribute": attribute_name,
-      "threshold": condition.threshold,
-  }
-
-
-@to_json.register
-def _to_json_discretized_higher_than(
-    condition: DiscretizedNumericalHigherThanCondition,
-    dataspec: data_spec_pb2.DataSpecification,
-) -> Dict[str, Any]:
-  attribute_name = dataspec.columns[condition.attribute].name
-  return {
-      "type": "DISCRETIZED_NUMERICAL_IS_HIGHER_THAN",
-      "attribute": attribute_name,
-      "threshold_idx": condition.threshold_idx,
-  }
-
-
-@to_json.register
-def _to_json_categorical(
-    condition: CategoricalIsInCondition,
-    dataspec: data_spec_pb2.DataSpecification,
-) -> Dict[str, Any]:
-  """Returns a JSON-compatible dict for Categorical conditions."""
-  attribute_name = dataspec.columns[condition.attribute].name
-  column_spec = dataspec.columns[condition.attribute]
-  if column_spec.categorical.is_already_integerized:
-    mask_repr = list(condition.mask)
-  else:
-    vocab = dataspec_lib.categorical_column_dictionary_to_list(column_spec)
-    mask_repr = [vocab[item] for item in condition.mask]
-  return {
-      "type": "CATEGORICAL_IS_IN",
-      "attribute": attribute_name,
-      "mask": mask_repr,
-  }
-
-
-@to_json.register
-def _to_json_categorical_set(
-    condition: CategoricalSetContainsCondition,
-    dataspec: data_spec_pb2.DataSpecification,
-) -> Dict[str, Any]:
-  """Returns a JSON-compatible dict for CategoricalSet conditions."""
-  attribute_name = dataspec.columns[condition.attribute].name
-  column_spec = dataspec.columns[condition.attribute]
-  if column_spec.categorical.is_already_integerized:
-    mask_repr = list(condition.mask)
-  else:
-    vocab = dataspec_lib.categorical_column_dictionary_to_list(column_spec)
-    mask_repr = [vocab[item] for item in condition.mask]
-  return {
-      "type": "CATEGORICAL_SET_CONTAINS",
-      "attribute": attribute_name,
-      "mask": mask_repr,
-  }
-
-
-@to_json.register
-def _to_json_numerical_sparse_oblique(
-    condition: NumericalSparseObliqueCondition,
-    dataspec: data_spec_pb2.DataSpecification,
-) -> Dict[str, Any]:
-  return {
-      "type": "NUMERICAL_SPARSE_OBLIQUE",
-      "attributes": [dataspec.columns[f].name for f in condition.attributes],
-      "weights": condition.weights,
-      "threshold": condition.threshold,
-  }
-
-
-@functools.singledispatch
-def to_proto_condition(
-    condition: AbstractCondition,
-    dataspec: data_spec_pb2.DataSpecification,
-) -> decision_tree_pb2.NodeCondition:
-  """Sets the "condition" part in a proto node.
-
-  Note: While public, this logic is not part of the API. This is why this
-  methode's code is not an abstract method in AbstractValue.
-
-  Args:
-    condition: Input condition.
-    dataspec: Dataspec of the model.
-
-  Returns:
-    Proto condition.
-  """
-  raise NotImplementedError("Unsupported value type")
-
-
-@to_proto_condition.register
-def _to_proto_condition_is_missing(
-    condition: IsMissingInCondition,
-    dataspec: data_spec_pb2.DataSpecification,
-) -> decision_tree_pb2.NodeCondition:
-  return decision_tree_pb2.NodeCondition(
-      na_value=condition.missing,
-      split_score=condition.score,
-      attribute=condition.attribute,
-      condition=decision_tree_pb2.Condition(
-          na_condition=decision_tree_pb2.Condition.NA()
-      ),
-  )
-
-
-@to_proto_condition.register
-def _to_proto_condition_is_true(
-    condition: IsTrueCondition,
-    dataspec: data_spec_pb2.DataSpecification,
-) -> decision_tree_pb2.NodeCondition:
-  return decision_tree_pb2.NodeCondition(
-      na_value=condition.missing,
-      split_score=condition.score,
-      attribute=condition.attribute,
-      condition=decision_tree_pb2.Condition(
-          true_value_condition=decision_tree_pb2.Condition.TrueValue()
-      ),
-  )
-
-
-@to_proto_condition.register
-def _to_proto_condition_is_higher(
-    condition: NumericalHigherThanCondition,
-    dataspec: data_spec_pb2.DataSpecification,
-) -> decision_tree_pb2.NodeCondition:
-  return decision_tree_pb2.NodeCondition(
-      na_value=condition.missing,
-      split_score=condition.score,
-      attribute=condition.attribute,
-      condition=decision_tree_pb2.Condition(
-          higher_condition=decision_tree_pb2.Condition.Higher(
-              threshold=condition.threshold
-          ),
-      ),
-  )
-
-
-@to_proto_condition.register
-def _to_proto_condition_discretized_is_higher(
-    condition: DiscretizedNumericalHigherThanCondition,
-    dataspec: data_spec_pb2.DataSpecification,
-) -> decision_tree_pb2.NodeCondition:
-  return decision_tree_pb2.NodeCondition(
-      na_value=condition.missing,
-      split_score=condition.score,
-      attribute=condition.attribute,
-      condition=decision_tree_pb2.Condition(
-          discretized_higher_condition=decision_tree_pb2.Condition.DiscretizedHigher(
-              threshold=condition.threshold_idx
-          ),
-      ),
-  )
-
-
-@to_proto_condition.register(CategoricalIsInCondition)
-@to_proto_condition.register(CategoricalSetContainsCondition)
-def _to_proto_condition_is_in(
-    condition: Union[CategoricalIsInCondition, CategoricalSetContainsCondition],
-    dataspec: data_spec_pb2.DataSpecification,
-) -> decision_tree_pb2.NodeCondition:
-  """Converts a "is in" condition for a categorical or categorical-set feature.
-
-  This function selects the most compact approach (bitmap or list of items) to
-  encode the condition mask.
-
-  Args:
-    condition: "is in" input condition.
-    dataspec: Dataspec of the model.
-
-  Returns:
-    A proto condition.
-  """
-
-  proto_condition = decision_tree_pb2.NodeCondition(
-      na_value=condition.missing,
-      split_score=condition.score,
-      attribute=condition.attribute,
-  )
-  feature_column = dataspec.columns[proto_condition.attribute]
-  # Select the most efficient way to represent the mask.
-  #
-  # A list of indices takes 32bits per active item. A bitmap takes 1 bit per
-  # item (active or not).
-  if (
-      len(condition.mask) * 32 * 8
-      > feature_column.categorical.number_of_unique_values
-  ):
-    # A bitmap is more efficient.
-    proto_condition.condition.contains_bitmap_condition.elements_bitmap = (
-        items_to_bitmap(feature_column, condition.mask)
-    )
-  else:
-    # A list of indices is more efficient.
-    proto_condition.condition.contains_condition.elements[:] = condition.mask
-  return proto_condition
-
-
-@to_proto_condition.register
-def _to_proto_condition_oblique(
-    condition: NumericalSparseObliqueCondition,
-    dataspec: data_spec_pb2.DataSpecification,
-) -> decision_tree_pb2.NodeCondition:
-  return decision_tree_pb2.NodeCondition(
-      na_value=condition.missing,
-      split_score=condition.score,
-      attribute=condition.attributes[0] if condition.attributes else -1,
-      condition=decision_tree_pb2.Condition(
-          oblique_condition=decision_tree_pb2.Condition.Oblique(
-              attributes=condition.attributes,
-              weights=condition.weights,
-              threshold=condition.threshold,
-          ),
-      ),
-  )
-
-
-def _bitmap_has_item(bitmap: bytes, value: int) -> bool:
-  """Checks if the "value"-th bit is set."""
-
-  byte_idx = value // 8
-  sub_bit_idx = value & 7
-  return (bitmap[byte_idx] & (1 << sub_bit_idx)) != 0
-
-
-def bitmap_to_items(
-    column_spec: data_spec_pb2.Column, bitmap: bytes
-) -> Sequence[int]:
-  """Returns the list of true bits in a bitmap."""
-
-  return [
-      value_idx
-      for value_idx in range(column_spec.categorical.number_of_unique_values)
-      if _bitmap_has_item(bitmap, value_idx)
-  ]
-
-
-def items_to_bitmap(
-    column_spec: data_spec_pb2.Column, items: Sequence[int]
-) -> bytes:
-  """Returns a bitmap with the "items"-th bits set to true.
-
-  Setting multiple times the same bits is allowed.
-
-  Args:
-    column_spec: Column spec of a categorical column.
-    items: Bit indexes.
-  """
-
-  # Note: num_bytes a rounded-up integer division between
-  # p=number_of_unique_values and q=8 i.e. (p+q-1)/q.
-  num_bytes = (column_spec.categorical.number_of_unique_values + 7) // 8
-  # Allocate a zero-bitmap.
-  bitmap = bytearray(num_bytes)
-
-  for item in items:
-    if item < 0 or item >= column_spec.categorical.number_of_unique_values:
-      raise ValueError(f"Invalid item {item}")
-    bitmap[item // 8] |= 1 << item % 8
-  return bytes(bitmap)
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Conditions / splits for non-leaf nodes."""
+
+import abc
+import dataclasses
+import functools
+from typing import Any, Dict, Sequence, Union
+from ydf.proto.dataset import data_spec_pb2
+from ydf.proto.model.decision_tree import decision_tree_pb2
+from ydf.dataset import dataspec as dataspec_lib
+
+ColumnType = data_spec_pb2.ColumnType
+
+
+# TODO: b/310218604 - Use kw_only with default value score = 0.
+@dataclasses.dataclass
+class AbstractCondition(metaclass=abc.ABCMeta):
+  """Generic condition.
+
+  Attrs:
+    missing: Result of the evaluation of the condition if the input feature is
+      missing.
+    score: Score of a condition. The semantic depends on the learning algorithm.
+  """
+
+  missing: bool
+  score: float
+
+  @abc.abstractmethod
+  def pretty(self, dataspec: data_spec_pb2.DataSpecification) -> str:
+    raise NotImplementedError
+
+  def _tag(self) -> str:
+    return f"score={self.score:.5g} missing={self.missing}"
+
+
+@dataclasses.dataclass
+class IsMissingInCondition(AbstractCondition):
+  """Condition of the form "attribute is missing".
+
+  Attrs:
+    attribute: Attribute (or one of the attributes) tested by the condition.
+  """
+
+  attribute: int
+
+  def pretty(self, dataspec: data_spec_pb2.DataSpecification) -> str:
+    attribute_name = dataspec.columns[self.attribute].name
+    return f"{attribute_name!r} is missing [{self._tag()}]"
+
+
+@dataclasses.dataclass
+class IsTrueCondition(AbstractCondition):
+  """Condition of the form "attribute is true".
+
+  Attrs:
+    attribute: Attribute tested by the condition.
+  """
+
+  attribute: int
+
+  def pretty(self, dataspec: data_spec_pb2.DataSpecification) -> str:
+    attribute_name = dataspec.columns[self.attribute].name
+    return f"{attribute_name!r} is True [{self._tag()}]"
+
+
+@dataclasses.dataclass
+class NumericalHigherThanCondition(AbstractCondition):
+  """Condition of the form "attribute >= threshold".
+
+  Attrs:
+    attribute: Attribute tested by the condition.
+    threshold: Threshold.
+  """
+
+  attribute: int
+  threshold: float
+
+  def pretty(self, dataspec: data_spec_pb2.DataSpecification) -> str:
+    attribute_name = dataspec.columns[self.attribute].name
+    return f"{attribute_name!r} >= {self.threshold:g} [{self._tag()}]"
+
+
+@dataclasses.dataclass
+class DiscretizedNumericalHigherThanCondition(AbstractCondition):
+  """Condition of the form "attribute >= bounds[threshold]".
+
+  Attrs:
+    attribute: Attribute tested by the condition.
+    threshold_idx: Index of threshold in dataspec.
+  """
+
+  attribute: int
+  threshold_idx: int
+
+  def pretty(self, dataspec: data_spec_pb2.DataSpecification) -> str:
+    column_spec = dataspec.columns[self.attribute]
+    threshold = column_spec.discretized_numerical.boundaries[
+        self.threshold_idx - 1
+    ]
+    return (
+        f"{column_spec.name!r} >="
+        f" {threshold:g} [threshold_idx={self.threshold_idx} {self._tag()}]"
+    )
+
+
+@dataclasses.dataclass
+class CategoricalIsInCondition(AbstractCondition):
+  """Condition of the form "attribute in mask".
+
+  Attrs:
+    attribute: Attribute tested by the condition.
+    mask: Sorted mask values.
+  """
+
+  attribute: int
+  mask: Sequence[int]
+
+  def pretty(self, dataspec: data_spec_pb2.DataSpecification) -> str:
+    column_spec = dataspec.columns[self.attribute]
+    if column_spec.categorical.is_already_integerized:
+      mask_repr = list(self.mask)
+    else:
+      vocab = dataspec_lib.categorical_column_dictionary_to_list(column_spec)
+      mask_repr = [vocab[item] for item in self.mask]
+    return f"{column_spec.name!r} in {mask_repr} [{self._tag()}]"
+
+
+@dataclasses.dataclass
+class CategoricalSetContainsCondition(AbstractCondition):
+  """Condition of the form "attribute intersect mask != empty".
+
+  Attrs:
+    attribute: Attribute tested by the condition.
+    mask: Sorted mask values.
+  """
+
+  attribute: int
+  mask: Sequence[int]
+
+  def pretty(self, dataspec: data_spec_pb2.DataSpecification) -> str:
+    column_spec = dataspec.columns[self.attribute]
+    if column_spec.categorical.is_already_integerized:
+      mask_repr = list(self.mask)
+    else:
+      vocab = dataspec_lib.categorical_column_dictionary_to_list(column_spec)
+      mask_repr = [vocab[item] for item in self.mask]
+    return f"{column_spec.name!r} intersect {mask_repr} [{self._tag()}]"
+
+
+@dataclasses.dataclass
+class NumericalSparseObliqueCondition(AbstractCondition):
+  """Condition of the form "attributes * weights >= threshold".
+
+  Attrs:
+    attributes: Attribute tested by the condition.
+    weights: Weights for each of the attributes.
+    threshold: Threshold value of the condition.
+  """
+
+  attributes: Sequence[int]
+  weights: Sequence[float]
+  threshold: float
+
+  def pretty(self, dataspec: data_spec_pb2.DataSpecification) -> str:
+    text = " + ".join(
+        f"{dataspec.columns[attribute].name!r} x {weight:g}"
+        for attribute, weight in zip(self.attributes, self.weights)
+    )
+    if not text:
+      text = "*nothing*"
+    return f"{text} >= {self.threshold:g} [{self._tag()}]"
+
+
+def to_condition(
+    proto_condition: decision_tree_pb2.NodeCondition,
+    dataspec: data_spec_pb2.DataSpecification,
+) -> AbstractCondition:
+  """Extracts the "condition" part of a proto node."""
+
+  base_kwargs = {
+      "missing": proto_condition.na_value,
+      "score": proto_condition.split_score,
+  }
+  condition_type = proto_condition.condition
+  attribute_type = dataspec.columns[proto_condition.attribute].type
+
+  if condition_type.HasField("na_condition"):
+    return IsMissingInCondition(
+        attribute=proto_condition.attribute, **base_kwargs
+    )
+
+  elif condition_type.HasField("true_value_condition"):
+    return IsTrueCondition(attribute=proto_condition.attribute, **base_kwargs)
+
+  elif condition_type.HasField("higher_condition"):
+    return NumericalHigherThanCondition(
+        attribute=proto_condition.attribute,
+        threshold=condition_type.higher_condition.threshold,
+        **base_kwargs,
+    )
+
+  elif condition_type.HasField("contains_bitmap_condition"):
+    items = bitmap_to_items(
+        dataspec.columns[proto_condition.attribute],
+        condition_type.contains_bitmap_condition.elements_bitmap,
+    )
+    if attribute_type == ColumnType.CATEGORICAL:
+      return CategoricalIsInCondition(
+          attribute=proto_condition.attribute,
+          mask=items,
+          **base_kwargs,
+      )
+    elif attribute_type == ColumnType.CATEGORICAL_SET:
+      return CategoricalSetContainsCondition(
+          attribute=proto_condition.attribute,
+          mask=items,
+          **base_kwargs,
+      )
+    else:
+      raise ValueError("Invalid attribute type")
+
+  elif condition_type.HasField("contains_condition"):
+    if attribute_type == ColumnType.CATEGORICAL:
+      return CategoricalIsInCondition(
+          attribute=proto_condition.attribute,
+          mask=condition_type.contains_condition.elements,
+          **base_kwargs,
+      )
+    elif attribute_type == ColumnType.CATEGORICAL_SET:
+      return CategoricalSetContainsCondition(
+          attribute=proto_condition.attribute,
+          mask=condition_type.contains_condition.elements,
+          **base_kwargs,
+      )
+    else:
+      raise ValueError("Invalid attribute type")
+
+  elif condition_type.HasField("discretized_higher_condition"):
+    return DiscretizedNumericalHigherThanCondition(
+        attribute=proto_condition.attribute,
+        threshold_idx=condition_type.discretized_higher_condition.threshold,
+        **base_kwargs,
+    )
+
+  elif condition_type.HasField("oblique_condition"):
+    return NumericalSparseObliqueCondition(
+        attributes=condition_type.oblique_condition.attributes,
+        weights=condition_type.oblique_condition.weights,
+        threshold=condition_type.oblique_condition.threshold,
+        **base_kwargs,
+    )
+  else:
+    raise ValueError(f"Non supported condition type: {proto_condition}")
+
+
+@functools.singledispatch
+def to_json(
+    condition: AbstractCondition, dataspec: data_spec_pb2.DataSpecification
+) -> Dict[str, Any]:
+  """Creates a JSON-compatible object of the condition.
+
+  Note: While public, this logic is not part of the API. This is why this
+  methode's code is not an abstract method in AbstractValue.
+
+  Args:
+    condition: Input condition.
+    dataspec: Dataspec of the model.
+
+  Returns:
+    JSON condition.
+  """
+  raise NotImplementedError("Unsupported value type")
+
+
+@to_json.register
+def _to_json_is_missing(
+    condition: IsMissingInCondition, dataspec: data_spec_pb2.DataSpecification
+) -> Dict[str, Any]:
+  attribute_name = dataspec.columns[condition.attribute].name
+  return {"type": "IS_MISSING", "attribute": attribute_name}
+
+
+@to_json.register
+def _to_json_is_true(
+    condition: IsTrueCondition,
+    dataspec: data_spec_pb2.DataSpecification,
+) -> Dict[str, Any]:
+  attribute_name = dataspec.columns[condition.attribute].name
+  return {"type": "IS_TRUE", "attribute": attribute_name}
+
+
+@to_json.register
+def _to_json_higher_than(
+    condition: NumericalHigherThanCondition,
+    dataspec: data_spec_pb2.DataSpecification,
+) -> Dict[str, Any]:
+  attribute_name = dataspec.columns[condition.attribute].name
+  return {
+      "type": "NUMERICAL_IS_HIGHER_THAN",
+      "attribute": attribute_name,
+      "threshold": condition.threshold,
+  }
+
+
+@to_json.register
+def _to_json_discretized_higher_than(
+    condition: DiscretizedNumericalHigherThanCondition,
+    dataspec: data_spec_pb2.DataSpecification,
+) -> Dict[str, Any]:
+  attribute_name = dataspec.columns[condition.attribute].name
+  return {
+      "type": "DISCRETIZED_NUMERICAL_IS_HIGHER_THAN",
+      "attribute": attribute_name,
+      "threshold_idx": condition.threshold_idx,
+  }
+
+
+@to_json.register
+def _to_json_categorical(
+    condition: CategoricalIsInCondition,
+    dataspec: data_spec_pb2.DataSpecification,
+) -> Dict[str, Any]:
+  """Returns a JSON-compatible dict for Categorical conditions."""
+  attribute_name = dataspec.columns[condition.attribute].name
+  column_spec = dataspec.columns[condition.attribute]
+  if column_spec.categorical.is_already_integerized:
+    mask_repr = list(condition.mask)
+  else:
+    vocab = dataspec_lib.categorical_column_dictionary_to_list(column_spec)
+    mask_repr = [vocab[item] for item in condition.mask]
+  return {
+      "type": "CATEGORICAL_IS_IN",
+      "attribute": attribute_name,
+      "mask": mask_repr,
+  }
+
+
+@to_json.register
+def _to_json_categorical_set(
+    condition: CategoricalSetContainsCondition,
+    dataspec: data_spec_pb2.DataSpecification,
+) -> Dict[str, Any]:
+  """Returns a JSON-compatible dict for CategoricalSet conditions."""
+  attribute_name = dataspec.columns[condition.attribute].name
+  column_spec = dataspec.columns[condition.attribute]
+  if column_spec.categorical.is_already_integerized:
+    mask_repr = list(condition.mask)
+  else:
+    vocab = dataspec_lib.categorical_column_dictionary_to_list(column_spec)
+    mask_repr = [vocab[item] for item in condition.mask]
+  return {
+      "type": "CATEGORICAL_SET_CONTAINS",
+      "attribute": attribute_name,
+      "mask": mask_repr,
+  }
+
+
+@to_json.register
+def _to_json_numerical_sparse_oblique(
+    condition: NumericalSparseObliqueCondition,
+    dataspec: data_spec_pb2.DataSpecification,
+) -> Dict[str, Any]:
+  return {
+      "type": "NUMERICAL_SPARSE_OBLIQUE",
+      "attributes": [dataspec.columns[f].name for f in condition.attributes],
+      "weights": condition.weights,
+      "threshold": condition.threshold,
+  }
+
+
+@functools.singledispatch
+def to_proto_condition(
+    condition: AbstractCondition,
+    dataspec: data_spec_pb2.DataSpecification,
+) -> decision_tree_pb2.NodeCondition:
+  """Sets the "condition" part in a proto node.
+
+  Note: While public, this logic is not part of the API. This is why this
+  methode's code is not an abstract method in AbstractValue.
+
+  Args:
+    condition: Input condition.
+    dataspec: Dataspec of the model.
+
+  Returns:
+    Proto condition.
+  """
+  raise NotImplementedError("Unsupported value type")
+
+
+@to_proto_condition.register
+def _to_proto_condition_is_missing(
+    condition: IsMissingInCondition,
+    dataspec: data_spec_pb2.DataSpecification,
+) -> decision_tree_pb2.NodeCondition:
+  return decision_tree_pb2.NodeCondition(
+      na_value=condition.missing,
+      split_score=condition.score,
+      attribute=condition.attribute,
+      condition=decision_tree_pb2.Condition(
+          na_condition=decision_tree_pb2.Condition.NA()
+      ),
+  )
+
+
+@to_proto_condition.register
+def _to_proto_condition_is_true(
+    condition: IsTrueCondition,
+    dataspec: data_spec_pb2.DataSpecification,
+) -> decision_tree_pb2.NodeCondition:
+  return decision_tree_pb2.NodeCondition(
+      na_value=condition.missing,
+      split_score=condition.score,
+      attribute=condition.attribute,
+      condition=decision_tree_pb2.Condition(
+          true_value_condition=decision_tree_pb2.Condition.TrueValue()
+      ),
+  )
+
+
+@to_proto_condition.register
+def _to_proto_condition_is_higher(
+    condition: NumericalHigherThanCondition,
+    dataspec: data_spec_pb2.DataSpecification,
+) -> decision_tree_pb2.NodeCondition:
+  return decision_tree_pb2.NodeCondition(
+      na_value=condition.missing,
+      split_score=condition.score,
+      attribute=condition.attribute,
+      condition=decision_tree_pb2.Condition(
+          higher_condition=decision_tree_pb2.Condition.Higher(
+              threshold=condition.threshold
+          ),
+      ),
+  )
+
+
+@to_proto_condition.register
+def _to_proto_condition_discretized_is_higher(
+    condition: DiscretizedNumericalHigherThanCondition,
+    dataspec: data_spec_pb2.DataSpecification,
+) -> decision_tree_pb2.NodeCondition:
+  return decision_tree_pb2.NodeCondition(
+      na_value=condition.missing,
+      split_score=condition.score,
+      attribute=condition.attribute,
+      condition=decision_tree_pb2.Condition(
+          discretized_higher_condition=decision_tree_pb2.Condition.DiscretizedHigher(
+              threshold=condition.threshold_idx
+          ),
+      ),
+  )
+
+
+@to_proto_condition.register(CategoricalIsInCondition)
+@to_proto_condition.register(CategoricalSetContainsCondition)
+def _to_proto_condition_is_in(
+    condition: Union[CategoricalIsInCondition, CategoricalSetContainsCondition],
+    dataspec: data_spec_pb2.DataSpecification,
+) -> decision_tree_pb2.NodeCondition:
+  """Converts a "is in" condition for a categorical or categorical-set feature.
+
+  This function selects the most compact approach (bitmap or list of items) to
+  encode the condition mask.
+
+  Args:
+    condition: "is in" input condition.
+    dataspec: Dataspec of the model.
+
+  Returns:
+    A proto condition.
+  """
+
+  proto_condition = decision_tree_pb2.NodeCondition(
+      na_value=condition.missing,
+      split_score=condition.score,
+      attribute=condition.attribute,
+  )
+  feature_column = dataspec.columns[proto_condition.attribute]
+  # Select the most efficient way to represent the mask.
+  #
+  # A list of indices takes 32bits per active item. A bitmap takes 1 bit per
+  # item (active or not).
+  if (
+      len(condition.mask) * 32 * 8
+      > feature_column.categorical.number_of_unique_values
+  ):
+    # A bitmap is more efficient.
+    proto_condition.condition.contains_bitmap_condition.elements_bitmap = (
+        items_to_bitmap(feature_column, condition.mask)
+    )
+  else:
+    # A list of indices is more efficient.
+    proto_condition.condition.contains_condition.elements[:] = condition.mask
+  return proto_condition
+
+
+@to_proto_condition.register
+def _to_proto_condition_oblique(
+    condition: NumericalSparseObliqueCondition,
+    dataspec: data_spec_pb2.DataSpecification,
+) -> decision_tree_pb2.NodeCondition:
+  return decision_tree_pb2.NodeCondition(
+      na_value=condition.missing,
+      split_score=condition.score,
+      attribute=condition.attributes[0] if condition.attributes else -1,
+      condition=decision_tree_pb2.Condition(
+          oblique_condition=decision_tree_pb2.Condition.Oblique(
+              attributes=condition.attributes,
+              weights=condition.weights,
+              threshold=condition.threshold,
+          ),
+      ),
+  )
+
+
+def _bitmap_has_item(bitmap: bytes, value: int) -> bool:
+  """Checks if the "value"-th bit is set."""
+
+  byte_idx = value // 8
+  sub_bit_idx = value & 7
+  return (bitmap[byte_idx] & (1 << sub_bit_idx)) != 0
+
+
+def bitmap_to_items(
+    column_spec: data_spec_pb2.Column, bitmap: bytes
+) -> Sequence[int]:
+  """Returns the list of true bits in a bitmap."""
+
+  return [
+      value_idx
+      for value_idx in range(column_spec.categorical.number_of_unique_values)
+      if _bitmap_has_item(bitmap, value_idx)
+  ]
+
+
+def items_to_bitmap(
+    column_spec: data_spec_pb2.Column, items: Sequence[int]
+) -> bytes:
+  """Returns a bitmap with the "items"-th bits set to true.
+
+  Setting multiple times the same bits is allowed.
+
+  Args:
+    column_spec: Column spec of a categorical column.
+    items: Bit indexes.
+  """
+
+  # Note: num_bytes a rounded-up integer division between
+  # p=number_of_unique_values and q=8 i.e. (p+q-1)/q.
+  num_bytes = (column_spec.categorical.number_of_unique_values + 7) // 8
+  # Allocate a zero-bitmap.
+  bitmap = bytearray(num_bytes)
+
+  for item in items:
+    if item < 0 or item >= column_spec.categorical.number_of_unique_values:
+      raise ValueError(f"Invalid item {item}")
+    bitmap[item // 8] |= 1 << item % 8
+  return bytes(bitmap)
```

## ydf/model/tree/condition_test.py

```diff
@@ -1,525 +1,525 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from typing import Sequence
-from absl.testing import absltest
-from absl.testing import parameterized
-from yggdrasil_decision_forests.dataset import data_spec_pb2
-from yggdrasil_decision_forests.model.decision_tree import decision_tree_pb2
-from ydf.model.tree import condition as condition_lib
-from ydf.utils import test_utils
-
-
-class ConditionTest(parameterized.TestCase):
-
-  def setUp(self):
-    self.data_spec_columns = {
-        "f_nothing_1": data_spec_pb2.Column(name="f_nothing_1"),
-        "f_nothing_2": data_spec_pb2.Column(name="f_nothing_2"),
-        "f_numerical_1": data_spec_pb2.Column(
-            name="f_numerical_1",
-            type=data_spec_pb2.ColumnType.NUMERICAL,
-            numerical=data_spec_pb2.NumericalSpec(),
-        ),
-        "f_numerical_2": data_spec_pb2.Column(
-            name="f_numerical_2",
-            type=data_spec_pb2.ColumnType.NUMERICAL,
-            numerical=data_spec_pb2.NumericalSpec(),
-        ),
-        "f_discretized": data_spec_pb2.Column(
-            name="f_discretized",
-            discretized_numerical=data_spec_pb2.DiscretizedNumericalSpec(
-                boundaries=[10, 20, 30, 40]
-            ),
-        ),
-        "f_categorical_large_vocab": data_spec_pb2.Column(
-            name="f_categorical_large_vocab",
-            type=data_spec_pb2.ColumnType.CATEGORICAL,
-            categorical=data_spec_pb2.CategoricalSpec(
-                number_of_unique_values=1000,
-                is_already_integerized=True,
-            ),
-        ),
-        "f_categorical_small_vocab": data_spec_pb2.Column(
-            name="f_categorical_small_vocab",
-            type=data_spec_pb2.ColumnType.CATEGORICAL,
-            categorical=data_spec_pb2.CategoricalSpec(
-                number_of_unique_values=4,
-                items={
-                    "OOD": data_spec_pb2.CategoricalSpec.VocabValue(index=0),
-                    "A": data_spec_pb2.CategoricalSpec.VocabValue(index=1),
-                    "B": data_spec_pb2.CategoricalSpec.VocabValue(index=2),
-                    "D": data_spec_pb2.CategoricalSpec.VocabValue(index=3),
-                },
-            ),
-        ),
-        "f_categorical_set_large_vocab": data_spec_pb2.Column(
-            name="f_categorical_set_large_vocab",
-            type=data_spec_pb2.ColumnType.CATEGORICAL_SET,
-            categorical=data_spec_pb2.CategoricalSpec(
-                number_of_unique_values=1000,
-                is_already_integerized=True,
-            ),
-        ),
-        "f_categorical_set_small_vocab": data_spec_pb2.Column(
-            name="f_categorical_set_small_vocab",
-            type=data_spec_pb2.ColumnType.CATEGORICAL_SET,
-            categorical=data_spec_pb2.CategoricalSpec(
-                number_of_unique_values=4,
-                is_already_integerized=True,
-            ),
-        ),
-    }
-    self.full_data_spec = data_spec_pb2.DataSpecification(
-        columns=self.data_spec_columns.values()
-    )
-    super().setUp()
-
-  @parameterized.named_parameters(
-      (
-          "two random bytes",
-          b"\x2D\x03",
-          10,
-          [0, 2, 3, 5, 8, 9],
-      ),  # b1100101101 => 0x032D
-      ("empty", b"", 0, []),
-      ("full of 0s", b"\x00\x00", 16, []),
-      ("full of 1s", b"\xFF\xFF", 16, list(range(16))),
-  )
-  def test_bitmap_to_items(
-      self,
-      bitmap: bytes,
-      number_of_unique_values: int,
-      expected_items: Sequence[int],
-  ):
-    column_spec = data_spec_pb2.Column(
-        categorical=data_spec_pb2.CategoricalSpec(
-            number_of_unique_values=number_of_unique_values
-        )
-    )
-
-    self.assertEqual(
-        condition_lib.bitmap_to_items(column_spec, bitmap),
-        expected_items,
-    )
-
-  @parameterized.named_parameters(
-      (
-          "two random bytes",
-          [0, 2, 3, 5, 8, 9],
-          10,
-          b"\x2D\x03",
-      ),  # b1100101101 => 0x032D
-      ("empty", [], 0, b""),
-      ("repeated bits", [3, 4, 3], 10, b"\x18\x00"),  # b00011000 => 0x0018
-      ("set high bit in 1 byte", [7], 8, b"\x80"),
-      ("set low bit in 1 byte", [0], 8, b"\x01"),
-      ("set high bit in 4 bytes", [31], 32, b"\x00\x00\x00\x80"),
-      ("set low bit in 4 bytes", [0], 32, b"\x01\x00\x00\x00"),
-      ("full of 0s", [], 32, b"\x00\x00\x00\x00"),
-      ("full of 1s", range(32), 32, b"\xFF\xFF\xFF\xFF"),
-  )
-  def test_items_to_bitmap_with_valid_input(
-      self,
-      items: Sequence[int],
-      number_of_unique_values: int,
-      expected_bitmap: bytes,
-  ):
-    column_spec = data_spec_pb2.Column(
-        categorical=data_spec_pb2.CategoricalSpec(
-            number_of_unique_values=number_of_unique_values
-        )
-    )
-
-    self.assertEqual(
-        condition_lib.items_to_bitmap(column_spec, items),
-        expected_bitmap,
-    )
-
-  def test_items_to_bitmap_invalid_item_is_negative(self):
-    column_spec = data_spec_pb2.Column(
-        categorical=data_spec_pb2.CategoricalSpec(number_of_unique_values=10)
-    )
-
-    with self.assertRaisesRegex(ValueError, "-1"):
-      condition_lib.items_to_bitmap(column_spec, [-1])
-
-  def test_items_to_bitmap_invalid_item_is_too_large(self):
-    column_spec = data_spec_pb2.Column(
-        categorical=data_spec_pb2.CategoricalSpec(number_of_unique_values=10)
-    )
-
-    with self.assertRaisesRegex(ValueError, "10"):
-      condition_lib.items_to_bitmap(column_spec, [10])
-
-  def _assert_conditions_equivalent(
-      self,
-      condition: condition_lib.AbstractCondition,
-      proto_condition: decision_tree_pb2.NodeCondition,
-      dataspec: data_spec_pb2.DataSpecification,
-  ) -> None:
-    """Assets that a condition and a proto condition are equivalent."""
-
-    # Condition to proto condition.
-    test_utils.assertProto2Equal(
-        self,
-        condition_lib.to_proto_condition(condition, dataspec),
-        proto_condition,
-    )
-
-    # Proto condition to condition.
-    self.assertEqual(
-        condition_lib.to_condition(proto_condition, dataspec), condition
-    )
-
-  @parameterized.parameters(0, 1)
-  def test_condition_is_missing_with_valid_input(self, attribute_idx):
-    condition = condition_lib.IsMissingInCondition(
-        attribute=attribute_idx, missing=False, score=2
-    )
-    dataspec = data_spec_pb2.DataSpecification(
-        columns=[data_spec_pb2.Column(name="f_nothing_2")]
-    )
-    if attribute_idx > 0:
-      dataspec = self.full_data_spec
-    proto_condition = decision_tree_pb2.NodeCondition(
-        na_value=False,
-        split_score=2,
-        attribute=attribute_idx,
-        condition=decision_tree_pb2.Condition(
-            na_condition=decision_tree_pb2.Condition.NA(),
-        ),
-    )
-
-    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
-
-    self.assertEqual(
-        condition.pretty(dataspec),
-        "'f_nothing_2' is missing [score=2 missing=False]",
-    )
-
-  @parameterized.parameters(0, 1)
-  def test_condition_is_true_valid_input(self, attribute_idx):
-    condition = condition_lib.IsTrueCondition(
-        attribute=attribute_idx, missing=False, score=2
-    )
-    dataspec = data_spec_pb2.DataSpecification(
-        columns=[data_spec_pb2.Column(name="f_nothing_2")]
-    )
-    if attribute_idx > 0:
-      dataspec = self.full_data_spec
-    proto_condition = decision_tree_pb2.NodeCondition(
-        na_value=False,
-        split_score=2,
-        attribute=attribute_idx,
-        condition=decision_tree_pb2.Condition(
-            true_value_condition=decision_tree_pb2.Condition.TrueValue(),
-        ),
-    )
-
-    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
-
-    self.assertEqual(
-        condition.pretty(dataspec),
-        "'f_nothing_2' is True [score=2 missing=False]",
-    )
-
-  @parameterized.parameters(0, 1)
-  def test_condition_is_higher_valid_input(self, attribute_idx):
-    condition = condition_lib.NumericalHigherThanCondition(
-        attribute=attribute_idx, missing=False, score=2, threshold=3
-    )
-    dataspec = data_spec_pb2.DataSpecification(
-        columns=[data_spec_pb2.Column(name="f_nothing_2")]
-    )
-    if attribute_idx > 0:
-      dataspec = self.full_data_spec
-    proto_condition = decision_tree_pb2.NodeCondition(
-        na_value=False,
-        split_score=2,
-        attribute=attribute_idx,
-        condition=decision_tree_pb2.Condition(
-            higher_condition=decision_tree_pb2.Condition.Higher(threshold=3),
-        ),
-    )
-
-    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
-
-    self.assertEqual(
-        condition.pretty(dataspec),
-        "'f_nothing_2' >= 3 [score=2 missing=False]",
-    )
-
-  @parameterized.parameters(0, 4)
-  def test_condition_discretized_is_higher_valid_input(self, attribute_idx):
-    condition = condition_lib.DiscretizedNumericalHigherThanCondition(
-        attribute=attribute_idx, missing=False, score=2, threshold_idx=2
-    )
-    dataspec = data_spec_pb2.DataSpecification(
-        columns=[
-            data_spec_pb2.Column(
-                name="f_discretized",
-                discretized_numerical=data_spec_pb2.DiscretizedNumericalSpec(
-                    boundaries=[10, 20, 30, 40]
-                ),
-            )
-        ]
-    )
-    if attribute_idx > 0:
-      dataspec = self.full_data_spec
-    proto_condition = decision_tree_pb2.NodeCondition(
-        na_value=False,
-        split_score=2,
-        attribute=attribute_idx,
-        condition=decision_tree_pb2.Condition(
-            discretized_higher_condition=decision_tree_pb2.Condition.DiscretizedHigher(
-                threshold=2
-            ),
-        ),
-    )
-
-    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
-
-    self.assertEqual(
-        condition.pretty(dataspec),
-        "'f_discretized' >= 20 [threshold_idx=2 score=2 missing=False]",
-    )
-
-  @parameterized.parameters(0, 5)
-  def test_condition_is_in_categorical_valid_input(self, attribute_idx):
-    condition = condition_lib.CategoricalIsInCondition(
-        attribute=attribute_idx,
-        missing=False,
-        score=2,
-        mask=[1, 3],
-    )
-    dataspec = data_spec_pb2.DataSpecification(
-        columns=[
-            data_spec_pb2.Column(
-                name="f_categorical_large_vocab",
-                type=data_spec_pb2.ColumnType.CATEGORICAL,
-                categorical=data_spec_pb2.CategoricalSpec(
-                    number_of_unique_values=1000,
-                    is_already_integerized=True,
-                ),
-            )
-        ]
-    )
-    if attribute_idx > 0:
-      dataspec = self.full_data_spec
-    proto_condition = decision_tree_pb2.NodeCondition(
-        na_value=False,
-        split_score=2,
-        attribute=attribute_idx,
-        condition=decision_tree_pb2.Condition(
-            contains_condition=decision_tree_pb2.Condition.ContainsVector(
-                elements=[1, 3]
-            ),
-        ),
-    )
-
-    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
-
-    self.assertEqual(
-        condition.pretty(dataspec),
-        "'f_categorical_large_vocab' in [1, 3] [score=2 missing=False]",
-    )
-
-  @parameterized.parameters(0, 6)
-  def test_condition_is_in_categorical_bitmap_valid_input(self, attribute_idx):
-    condition = condition_lib.CategoricalIsInCondition(
-        attribute=attribute_idx,
-        missing=False,
-        score=2,
-        mask=[1, 3],
-    )
-    dataspec = data_spec_pb2.DataSpecification(
-        columns=[
-            data_spec_pb2.Column(
-                name="f_categorical_small_vocab",
-                type=data_spec_pb2.ColumnType.CATEGORICAL,
-                categorical=data_spec_pb2.CategoricalSpec(
-                    number_of_unique_values=4,
-                    items={
-                        "OOD": data_spec_pb2.CategoricalSpec.VocabValue(
-                            index=0
-                        ),
-                        "A": data_spec_pb2.CategoricalSpec.VocabValue(index=1),
-                        "B": data_spec_pb2.CategoricalSpec.VocabValue(index=2),
-                        "D": data_spec_pb2.CategoricalSpec.VocabValue(index=3),
-                    },
-                ),
-            )
-        ]
-    )
-    if attribute_idx > 0:
-      dataspec = self.full_data_spec
-    proto_condition = decision_tree_pb2.NodeCondition(
-        na_value=False,
-        split_score=2,
-        attribute=attribute_idx,
-        condition=decision_tree_pb2.Condition(
-            contains_bitmap_condition=decision_tree_pb2.Condition.ContainsBitmap(
-                elements_bitmap=b"\x0A"
-            ),
-        ),
-    )
-
-    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
-
-    self.assertEqual(
-        condition.pretty(dataspec),
-        "'f_categorical_small_vocab' in ['A', 'D'] [score=2 missing=False]",
-    )
-
-  @parameterized.parameters(0, 7)
-  def test_condition_is_in_categorical_set_valid_input(self, attribute_idx):
-    condition = condition_lib.CategoricalSetContainsCondition(
-        attribute=attribute_idx,
-        missing=False,
-        score=2,
-        mask=[1, 3],
-    )
-    dataspec = data_spec_pb2.DataSpecification(
-        columns=[
-            data_spec_pb2.Column(
-                name="f_categorical_set_large_vocab",
-                type=data_spec_pb2.ColumnType.CATEGORICAL_SET,
-                categorical=data_spec_pb2.CategoricalSpec(
-                    number_of_unique_values=1000,
-                    is_already_integerized=True,
-                ),
-            )
-        ]
-    )
-    if attribute_idx > 0:
-      dataspec = self.full_data_spec
-    proto_condition = decision_tree_pb2.NodeCondition(
-        na_value=False,
-        split_score=2,
-        attribute=attribute_idx,
-        condition=decision_tree_pb2.Condition(
-            contains_condition=decision_tree_pb2.Condition.ContainsVector(
-                elements=[1, 3]
-            ),
-        ),
-    )
-
-    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
-
-    self.assertEqual(
-        condition.pretty(dataspec),
-        "'f_categorical_set_large_vocab' intersect [1, 3] [score=2"
-        " missing=False]",
-    )
-
-  @parameterized.parameters(0, 8)
-  def test_condition_is_in_categorical_set_bitmap_valid_input(
-      self, attribute_idx
-  ):
-    condition = condition_lib.CategoricalSetContainsCondition(
-        attribute=attribute_idx,
-        missing=False,
-        score=2,
-        mask=[1, 3],
-    )
-    dataspec = data_spec_pb2.DataSpecification(
-        columns=[
-            data_spec_pb2.Column(
-                name="f_categorical_set_small_vocab",
-                type=data_spec_pb2.ColumnType.CATEGORICAL_SET,
-                categorical=data_spec_pb2.CategoricalSpec(
-                    number_of_unique_values=4
-                ),
-            )
-        ]
-    )
-    if attribute_idx > 0:
-      dataspec = self.full_data_spec
-    proto_condition = decision_tree_pb2.NodeCondition(
-        na_value=False,
-        split_score=2,
-        attribute=attribute_idx,
-        condition=decision_tree_pb2.Condition(
-            contains_bitmap_condition=decision_tree_pb2.Condition.ContainsBitmap(
-                elements_bitmap=b"\x0A"
-            ),
-        ),
-    )
-
-    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
-
-  def test_condition_sparse_oblique_valid_input(self):
-    condition = condition_lib.NumericalSparseObliqueCondition(
-        attributes=[0, 1],
-        missing=False,
-        score=2,
-        weights=[1, 2],
-        threshold=3,
-    )
-    dataspec = data_spec_pb2.DataSpecification(
-        columns=[data_spec_pb2.Column(name="f"), data_spec_pb2.Column(name="g")]
-    )
-    proto_condition = decision_tree_pb2.NodeCondition(
-        na_value=False,
-        split_score=2,
-        attribute=0,
-        condition=decision_tree_pb2.Condition(
-            oblique_condition=decision_tree_pb2.Condition.Oblique(
-                attributes=[0, 1],
-                weights=[1, 2],
-                threshold=3,
-            ),
-        ),
-    )
-
-    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
-
-    self.assertEqual(
-        condition.pretty(dataspec),
-        "'f' x 1 + 'g' x 2 >= 3 [score=2 missing=False]",
-    )
-
-  def test_condition_sparse_oblique_empty_valid_input(self):
-    condition = condition_lib.NumericalSparseObliqueCondition(
-        attributes=[],
-        missing=False,
-        score=2,
-        weights=[],
-        threshold=3,
-    )
-    dataspec = data_spec_pb2.DataSpecification(columns=[data_spec_pb2.Column()])
-    proto_condition = decision_tree_pb2.NodeCondition(
-        na_value=False,
-        split_score=2,
-        attribute=-1,
-        condition=decision_tree_pb2.Condition(
-            oblique_condition=decision_tree_pb2.Condition.Oblique(
-                attributes=[],
-                weights=[],
-                threshold=3,
-            ),
-        ),
-    )
-
-    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
-
-    self.assertEqual(
-        condition.pretty(dataspec),
-        "*nothing* >= 3 [score=2 missing=False]",
-    )
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from typing import Sequence
+from absl.testing import absltest
+from absl.testing import parameterized
+from ydf.proto.dataset import data_spec_pb2
+from ydf.proto.model.decision_tree import decision_tree_pb2
+from ydf.model.tree import condition as condition_lib
+from ydf.utils import test_utils
+
+
+class ConditionTest(parameterized.TestCase):
+
+  def setUp(self):
+    self.data_spec_columns = {
+        "f_nothing_1": data_spec_pb2.Column(name="f_nothing_1"),
+        "f_nothing_2": data_spec_pb2.Column(name="f_nothing_2"),
+        "f_numerical_1": data_spec_pb2.Column(
+            name="f_numerical_1",
+            type=data_spec_pb2.ColumnType.NUMERICAL,
+            numerical=data_spec_pb2.NumericalSpec(),
+        ),
+        "f_numerical_2": data_spec_pb2.Column(
+            name="f_numerical_2",
+            type=data_spec_pb2.ColumnType.NUMERICAL,
+            numerical=data_spec_pb2.NumericalSpec(),
+        ),
+        "f_discretized": data_spec_pb2.Column(
+            name="f_discretized",
+            discretized_numerical=data_spec_pb2.DiscretizedNumericalSpec(
+                boundaries=[10, 20, 30, 40]
+            ),
+        ),
+        "f_categorical_large_vocab": data_spec_pb2.Column(
+            name="f_categorical_large_vocab",
+            type=data_spec_pb2.ColumnType.CATEGORICAL,
+            categorical=data_spec_pb2.CategoricalSpec(
+                number_of_unique_values=1000,
+                is_already_integerized=True,
+            ),
+        ),
+        "f_categorical_small_vocab": data_spec_pb2.Column(
+            name="f_categorical_small_vocab",
+            type=data_spec_pb2.ColumnType.CATEGORICAL,
+            categorical=data_spec_pb2.CategoricalSpec(
+                number_of_unique_values=4,
+                items={
+                    "OOD": data_spec_pb2.CategoricalSpec.VocabValue(index=0),
+                    "A": data_spec_pb2.CategoricalSpec.VocabValue(index=1),
+                    "B": data_spec_pb2.CategoricalSpec.VocabValue(index=2),
+                    "D": data_spec_pb2.CategoricalSpec.VocabValue(index=3),
+                },
+            ),
+        ),
+        "f_categorical_set_large_vocab": data_spec_pb2.Column(
+            name="f_categorical_set_large_vocab",
+            type=data_spec_pb2.ColumnType.CATEGORICAL_SET,
+            categorical=data_spec_pb2.CategoricalSpec(
+                number_of_unique_values=1000,
+                is_already_integerized=True,
+            ),
+        ),
+        "f_categorical_set_small_vocab": data_spec_pb2.Column(
+            name="f_categorical_set_small_vocab",
+            type=data_spec_pb2.ColumnType.CATEGORICAL_SET,
+            categorical=data_spec_pb2.CategoricalSpec(
+                number_of_unique_values=4,
+                is_already_integerized=True,
+            ),
+        ),
+    }
+    self.full_data_spec = data_spec_pb2.DataSpecification(
+        columns=self.data_spec_columns.values()
+    )
+    super().setUp()
+
+  @parameterized.named_parameters(
+      (
+          "two random bytes",
+          b"\x2D\x03",
+          10,
+          [0, 2, 3, 5, 8, 9],
+      ),  # b1100101101 => 0x032D
+      ("empty", b"", 0, []),
+      ("full of 0s", b"\x00\x00", 16, []),
+      ("full of 1s", b"\xFF\xFF", 16, list(range(16))),
+  )
+  def test_bitmap_to_items(
+      self,
+      bitmap: bytes,
+      number_of_unique_values: int,
+      expected_items: Sequence[int],
+  ):
+    column_spec = data_spec_pb2.Column(
+        categorical=data_spec_pb2.CategoricalSpec(
+            number_of_unique_values=number_of_unique_values
+        )
+    )
+
+    self.assertEqual(
+        condition_lib.bitmap_to_items(column_spec, bitmap),
+        expected_items,
+    )
+
+  @parameterized.named_parameters(
+      (
+          "two random bytes",
+          [0, 2, 3, 5, 8, 9],
+          10,
+          b"\x2D\x03",
+      ),  # b1100101101 => 0x032D
+      ("empty", [], 0, b""),
+      ("repeated bits", [3, 4, 3], 10, b"\x18\x00"),  # b00011000 => 0x0018
+      ("set high bit in 1 byte", [7], 8, b"\x80"),
+      ("set low bit in 1 byte", [0], 8, b"\x01"),
+      ("set high bit in 4 bytes", [31], 32, b"\x00\x00\x00\x80"),
+      ("set low bit in 4 bytes", [0], 32, b"\x01\x00\x00\x00"),
+      ("full of 0s", [], 32, b"\x00\x00\x00\x00"),
+      ("full of 1s", range(32), 32, b"\xFF\xFF\xFF\xFF"),
+  )
+  def test_items_to_bitmap_with_valid_input(
+      self,
+      items: Sequence[int],
+      number_of_unique_values: int,
+      expected_bitmap: bytes,
+  ):
+    column_spec = data_spec_pb2.Column(
+        categorical=data_spec_pb2.CategoricalSpec(
+            number_of_unique_values=number_of_unique_values
+        )
+    )
+
+    self.assertEqual(
+        condition_lib.items_to_bitmap(column_spec, items),
+        expected_bitmap,
+    )
+
+  def test_items_to_bitmap_invalid_item_is_negative(self):
+    column_spec = data_spec_pb2.Column(
+        categorical=data_spec_pb2.CategoricalSpec(number_of_unique_values=10)
+    )
+
+    with self.assertRaisesRegex(ValueError, "-1"):
+      condition_lib.items_to_bitmap(column_spec, [-1])
+
+  def test_items_to_bitmap_invalid_item_is_too_large(self):
+    column_spec = data_spec_pb2.Column(
+        categorical=data_spec_pb2.CategoricalSpec(number_of_unique_values=10)
+    )
+
+    with self.assertRaisesRegex(ValueError, "10"):
+      condition_lib.items_to_bitmap(column_spec, [10])
+
+  def _assert_conditions_equivalent(
+      self,
+      condition: condition_lib.AbstractCondition,
+      proto_condition: decision_tree_pb2.NodeCondition,
+      dataspec: data_spec_pb2.DataSpecification,
+  ) -> None:
+    """Assets that a condition and a proto condition are equivalent."""
+
+    # Condition to proto condition.
+    test_utils.assertProto2Equal(
+        self,
+        condition_lib.to_proto_condition(condition, dataspec),
+        proto_condition,
+    )
+
+    # Proto condition to condition.
+    self.assertEqual(
+        condition_lib.to_condition(proto_condition, dataspec), condition
+    )
+
+  @parameterized.parameters(0, 1)
+  def test_condition_is_missing_with_valid_input(self, attribute_idx):
+    condition = condition_lib.IsMissingInCondition(
+        attribute=attribute_idx, missing=False, score=2
+    )
+    dataspec = data_spec_pb2.DataSpecification(
+        columns=[data_spec_pb2.Column(name="f_nothing_2")]
+    )
+    if attribute_idx > 0:
+      dataspec = self.full_data_spec
+    proto_condition = decision_tree_pb2.NodeCondition(
+        na_value=False,
+        split_score=2,
+        attribute=attribute_idx,
+        condition=decision_tree_pb2.Condition(
+            na_condition=decision_tree_pb2.Condition.NA(),
+        ),
+    )
+
+    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
+
+    self.assertEqual(
+        condition.pretty(dataspec),
+        "'f_nothing_2' is missing [score=2 missing=False]",
+    )
+
+  @parameterized.parameters(0, 1)
+  def test_condition_is_true_valid_input(self, attribute_idx):
+    condition = condition_lib.IsTrueCondition(
+        attribute=attribute_idx, missing=False, score=2
+    )
+    dataspec = data_spec_pb2.DataSpecification(
+        columns=[data_spec_pb2.Column(name="f_nothing_2")]
+    )
+    if attribute_idx > 0:
+      dataspec = self.full_data_spec
+    proto_condition = decision_tree_pb2.NodeCondition(
+        na_value=False,
+        split_score=2,
+        attribute=attribute_idx,
+        condition=decision_tree_pb2.Condition(
+            true_value_condition=decision_tree_pb2.Condition.TrueValue(),
+        ),
+    )
+
+    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
+
+    self.assertEqual(
+        condition.pretty(dataspec),
+        "'f_nothing_2' is True [score=2 missing=False]",
+    )
+
+  @parameterized.parameters(0, 1)
+  def test_condition_is_higher_valid_input(self, attribute_idx):
+    condition = condition_lib.NumericalHigherThanCondition(
+        attribute=attribute_idx, missing=False, score=2, threshold=3
+    )
+    dataspec = data_spec_pb2.DataSpecification(
+        columns=[data_spec_pb2.Column(name="f_nothing_2")]
+    )
+    if attribute_idx > 0:
+      dataspec = self.full_data_spec
+    proto_condition = decision_tree_pb2.NodeCondition(
+        na_value=False,
+        split_score=2,
+        attribute=attribute_idx,
+        condition=decision_tree_pb2.Condition(
+            higher_condition=decision_tree_pb2.Condition.Higher(threshold=3),
+        ),
+    )
+
+    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
+
+    self.assertEqual(
+        condition.pretty(dataspec),
+        "'f_nothing_2' >= 3 [score=2 missing=False]",
+    )
+
+  @parameterized.parameters(0, 4)
+  def test_condition_discretized_is_higher_valid_input(self, attribute_idx):
+    condition = condition_lib.DiscretizedNumericalHigherThanCondition(
+        attribute=attribute_idx, missing=False, score=2, threshold_idx=2
+    )
+    dataspec = data_spec_pb2.DataSpecification(
+        columns=[
+            data_spec_pb2.Column(
+                name="f_discretized",
+                discretized_numerical=data_spec_pb2.DiscretizedNumericalSpec(
+                    boundaries=[10, 20, 30, 40]
+                ),
+            )
+        ]
+    )
+    if attribute_idx > 0:
+      dataspec = self.full_data_spec
+    proto_condition = decision_tree_pb2.NodeCondition(
+        na_value=False,
+        split_score=2,
+        attribute=attribute_idx,
+        condition=decision_tree_pb2.Condition(
+            discretized_higher_condition=decision_tree_pb2.Condition.DiscretizedHigher(
+                threshold=2
+            ),
+        ),
+    )
+
+    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
+
+    self.assertEqual(
+        condition.pretty(dataspec),
+        "'f_discretized' >= 20 [threshold_idx=2 score=2 missing=False]",
+    )
+
+  @parameterized.parameters(0, 5)
+  def test_condition_is_in_categorical_valid_input(self, attribute_idx):
+    condition = condition_lib.CategoricalIsInCondition(
+        attribute=attribute_idx,
+        missing=False,
+        score=2,
+        mask=[1, 3],
+    )
+    dataspec = data_spec_pb2.DataSpecification(
+        columns=[
+            data_spec_pb2.Column(
+                name="f_categorical_large_vocab",
+                type=data_spec_pb2.ColumnType.CATEGORICAL,
+                categorical=data_spec_pb2.CategoricalSpec(
+                    number_of_unique_values=1000,
+                    is_already_integerized=True,
+                ),
+            )
+        ]
+    )
+    if attribute_idx > 0:
+      dataspec = self.full_data_spec
+    proto_condition = decision_tree_pb2.NodeCondition(
+        na_value=False,
+        split_score=2,
+        attribute=attribute_idx,
+        condition=decision_tree_pb2.Condition(
+            contains_condition=decision_tree_pb2.Condition.ContainsVector(
+                elements=[1, 3]
+            ),
+        ),
+    )
+
+    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
+
+    self.assertEqual(
+        condition.pretty(dataspec),
+        "'f_categorical_large_vocab' in [1, 3] [score=2 missing=False]",
+    )
+
+  @parameterized.parameters(0, 6)
+  def test_condition_is_in_categorical_bitmap_valid_input(self, attribute_idx):
+    condition = condition_lib.CategoricalIsInCondition(
+        attribute=attribute_idx,
+        missing=False,
+        score=2,
+        mask=[1, 3],
+    )
+    dataspec = data_spec_pb2.DataSpecification(
+        columns=[
+            data_spec_pb2.Column(
+                name="f_categorical_small_vocab",
+                type=data_spec_pb2.ColumnType.CATEGORICAL,
+                categorical=data_spec_pb2.CategoricalSpec(
+                    number_of_unique_values=4,
+                    items={
+                        "OOD": data_spec_pb2.CategoricalSpec.VocabValue(
+                            index=0
+                        ),
+                        "A": data_spec_pb2.CategoricalSpec.VocabValue(index=1),
+                        "B": data_spec_pb2.CategoricalSpec.VocabValue(index=2),
+                        "D": data_spec_pb2.CategoricalSpec.VocabValue(index=3),
+                    },
+                ),
+            )
+        ]
+    )
+    if attribute_idx > 0:
+      dataspec = self.full_data_spec
+    proto_condition = decision_tree_pb2.NodeCondition(
+        na_value=False,
+        split_score=2,
+        attribute=attribute_idx,
+        condition=decision_tree_pb2.Condition(
+            contains_bitmap_condition=decision_tree_pb2.Condition.ContainsBitmap(
+                elements_bitmap=b"\x0A"
+            ),
+        ),
+    )
+
+    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
+
+    self.assertEqual(
+        condition.pretty(dataspec),
+        "'f_categorical_small_vocab' in ['A', 'D'] [score=2 missing=False]",
+    )
+
+  @parameterized.parameters(0, 7)
+  def test_condition_is_in_categorical_set_valid_input(self, attribute_idx):
+    condition = condition_lib.CategoricalSetContainsCondition(
+        attribute=attribute_idx,
+        missing=False,
+        score=2,
+        mask=[1, 3],
+    )
+    dataspec = data_spec_pb2.DataSpecification(
+        columns=[
+            data_spec_pb2.Column(
+                name="f_categorical_set_large_vocab",
+                type=data_spec_pb2.ColumnType.CATEGORICAL_SET,
+                categorical=data_spec_pb2.CategoricalSpec(
+                    number_of_unique_values=1000,
+                    is_already_integerized=True,
+                ),
+            )
+        ]
+    )
+    if attribute_idx > 0:
+      dataspec = self.full_data_spec
+    proto_condition = decision_tree_pb2.NodeCondition(
+        na_value=False,
+        split_score=2,
+        attribute=attribute_idx,
+        condition=decision_tree_pb2.Condition(
+            contains_condition=decision_tree_pb2.Condition.ContainsVector(
+                elements=[1, 3]
+            ),
+        ),
+    )
+
+    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
+
+    self.assertEqual(
+        condition.pretty(dataspec),
+        "'f_categorical_set_large_vocab' intersect [1, 3] [score=2"
+        " missing=False]",
+    )
+
+  @parameterized.parameters(0, 8)
+  def test_condition_is_in_categorical_set_bitmap_valid_input(
+      self, attribute_idx
+  ):
+    condition = condition_lib.CategoricalSetContainsCondition(
+        attribute=attribute_idx,
+        missing=False,
+        score=2,
+        mask=[1, 3],
+    )
+    dataspec = data_spec_pb2.DataSpecification(
+        columns=[
+            data_spec_pb2.Column(
+                name="f_categorical_set_small_vocab",
+                type=data_spec_pb2.ColumnType.CATEGORICAL_SET,
+                categorical=data_spec_pb2.CategoricalSpec(
+                    number_of_unique_values=4
+                ),
+            )
+        ]
+    )
+    if attribute_idx > 0:
+      dataspec = self.full_data_spec
+    proto_condition = decision_tree_pb2.NodeCondition(
+        na_value=False,
+        split_score=2,
+        attribute=attribute_idx,
+        condition=decision_tree_pb2.Condition(
+            contains_bitmap_condition=decision_tree_pb2.Condition.ContainsBitmap(
+                elements_bitmap=b"\x0A"
+            ),
+        ),
+    )
+
+    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
+
+  def test_condition_sparse_oblique_valid_input(self):
+    condition = condition_lib.NumericalSparseObliqueCondition(
+        attributes=[0, 1],
+        missing=False,
+        score=2,
+        weights=[1, 2],
+        threshold=3,
+    )
+    dataspec = data_spec_pb2.DataSpecification(
+        columns=[data_spec_pb2.Column(name="f"), data_spec_pb2.Column(name="g")]
+    )
+    proto_condition = decision_tree_pb2.NodeCondition(
+        na_value=False,
+        split_score=2,
+        attribute=0,
+        condition=decision_tree_pb2.Condition(
+            oblique_condition=decision_tree_pb2.Condition.Oblique(
+                attributes=[0, 1],
+                weights=[1, 2],
+                threshold=3,
+            ),
+        ),
+    )
+
+    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
+
+    self.assertEqual(
+        condition.pretty(dataspec),
+        "'f' x 1 + 'g' x 2 >= 3 [score=2 missing=False]",
+    )
+
+  def test_condition_sparse_oblique_empty_valid_input(self):
+    condition = condition_lib.NumericalSparseObliqueCondition(
+        attributes=[],
+        missing=False,
+        score=2,
+        weights=[],
+        threshold=3,
+    )
+    dataspec = data_spec_pb2.DataSpecification(columns=[data_spec_pb2.Column()])
+    proto_condition = decision_tree_pb2.NodeCondition(
+        na_value=False,
+        split_score=2,
+        attribute=-1,
+        condition=decision_tree_pb2.Condition(
+            oblique_condition=decision_tree_pb2.Condition.Oblique(
+                attributes=[],
+                weights=[],
+                threshold=3,
+            ),
+        ),
+    )
+
+    self._assert_conditions_equivalent(condition, proto_condition, dataspec)
+
+    self.assertEqual(
+        condition.pretty(dataspec),
+        "*nothing* >= 3 [score=2 missing=False]",
+    )
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/model/tree/node.py

```diff
@@ -1,204 +1,204 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Node in a tree."""
-
-import abc
-import dataclasses
-import functools
-from typing import Any, Dict, Optional
-from yggdrasil_decision_forests.dataset import data_spec_pb2
-from ydf.model.tree import condition as condition_lib
-from ydf.model.tree import value as value_lib
-
-# Number of spaces printed on the left side of nodes with pretty print.
-_PRETTY_MARGIN = 4
-# Length / number of characters (e.g. "-") in an edge with pretty print.
-_PRETTY_EDGE_LENGTH = 4
-
-
-class AbstractNode(metaclass=abc.ABCMeta):
-
-  @property
-  @abc.abstractmethod
-  def is_leaf(self) -> bool:
-    """Tells if a node is a leaf."""
-    raise NotImplementedError
-
-  @abc.abstractmethod
-  def pretty(
-      self,
-      dataspec: data_spec_pb2.DataSpecification,
-      prefix: str,
-      is_pos: Optional[bool],
-      depth: int,
-      max_depth: Optional[int],
-  ) -> str:
-    raise NotImplementedError
-
-
-@dataclasses.dataclass
-class Leaf(AbstractNode):
-  value: value_lib.AbstractValue
-
-  @property
-  def is_leaf(self) -> bool:
-    return True
-
-  def pretty(
-      self,
-      dataspec: data_spec_pb2.DataSpecification,
-      prefix: str,
-      is_pos: Optional[bool],
-      depth: int,
-      max_depth: Optional[int],
-  ) -> str:
-    return prefix + _pretty_local_prefix(is_pos) + self.value.pretty() + "\n"
-
-
-@dataclasses.dataclass
-class NonLeaf(AbstractNode):
-  value: Optional[value_lib.AbstractValue] = None
-  condition: Optional[condition_lib.AbstractCondition] = None
-  pos_child: Optional[AbstractNode] = None
-  neg_child: Optional[AbstractNode] = None
-
-  @property
-  def is_leaf(self) -> bool:
-    return False
-
-  def pretty(
-      self,
-      dataspec: data_spec_pb2.DataSpecification,
-      prefix: str,
-      is_pos: Optional[bool],
-      depth: int,
-      max_depth: Optional[int],
-  ) -> str:
-
-    # Prefix for the children of this node.
-    children_prefix = prefix
-    if is_pos is None:
-      pass
-    elif is_pos:
-      children_prefix += " " * _PRETTY_MARGIN + "" + " " * _PRETTY_EDGE_LENGTH
-    elif not is_pos:
-      children_prefix += " " * (_PRETTY_MARGIN + 1 + _PRETTY_EDGE_LENGTH)
-
-    # Node's condition.
-    condition_prefix = prefix + _pretty_local_prefix(is_pos)
-    if self.condition is not None:
-      condition_prefix += self.condition.pretty(dataspec)
-    else:
-      condition_prefix += "No condition"
-    condition_prefix += "\n"
-
-    # Children of the node.
-    if max_depth is not None and depth >= max_depth:
-      return condition_prefix + children_prefix + "...\n"
-    else:
-      children_text = condition_prefix
-      if self.pos_child is not None:
-        children_text += self.pos_child.pretty(
-            dataspec, children_prefix, True, depth + 1, max_depth
-        )
-      else:
-        children_text += "No positive child\n"
-      if self.neg_child is not None:
-        children_text += self.neg_child.pretty(
-            dataspec, children_prefix, False, depth + 1, max_depth
-        )
-      else:
-        children_text += "No negative child\n"
-      return children_text
-
-
-@functools.singledispatch
-def to_json(
-    node: AbstractNode,
-    depth: int,
-    max_depth: Optional[int],
-    dataspec: data_spec_pb2.DataSpecification,
-) -> Dict[str, Any]:
-  """Creates a JSON-compatible object of the node.
-
-  Note: While public, this logic is not part of the API. This is why this
-  methode's code is not an abstract method in AbstractValue.
-
-  Args:
-    node: Input node.
-    depth: Depth of the current node
-    max_depth: Maximum depth of the tree in the json
-    dataspec: Dataspec associated with this tree.
-
-  Returns:
-    JSON node.
-  """
-  raise NotImplementedError("Unsupported node type")
-
-
-@to_json.register
-def _to_json_leaf(
-    node: Leaf,
-    depth: int,
-    max_depth: Optional[int],
-    dataspec: data_spec_pb2.DataSpecification,
-) -> Dict[str, Any]:
-  return {"value": value_lib.to_json(node.value)}
-
-
-@to_json.register
-def _to_json_non_leaf(
-    node: NonLeaf,
-    depth: int,
-    max_depth: Optional[int],
-    dataspec: data_spec_pb2.DataSpecification,
-) -> Dict[str, Any]:
-  dst = {}
-  if node.value is not None:
-    dst["value"] = value_lib.to_json(node.value)
-
-  if node.condition is not None:
-    dst["condition"] = condition_lib.to_json(node.condition, dataspec)
-  if (
-      (max_depth is None or (max_depth is not None and depth < max_depth))
-      and node.pos_child is not None
-      and node.neg_child is not None
-  ):
-    dst["children"] = [
-        to_json(node.pos_child, depth + 1, max_depth, dataspec),
-        to_json(node.neg_child, depth + 1, max_depth, dataspec),
-    ]
-  return dst
-
-
-def _pretty_local_prefix(is_pos: Optional[bool]) -> str:
-  """Prefix added in front of a node with pretty print.
-
-  Args:
-    is_pos: True/False if the node is a positive/negative child. None if the
-      node is a root.
-
-  Returns:
-    The node prefix.
-  """
-
-  if is_pos is None:
-    # Root node. No prefix.
-    return ""
-  elif is_pos:
-    # Positive nodes are assumed to be printed before negative ones.
-    return " " * _PRETTY_MARGIN + "(pos) "
-  else:
-    return " " * _PRETTY_MARGIN + "(neg) "
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Node in a tree."""
+
+import abc
+import dataclasses
+import functools
+from typing import Any, Dict, Optional
+from ydf.proto.dataset import data_spec_pb2
+from ydf.model.tree import condition as condition_lib
+from ydf.model.tree import value as value_lib
+
+# Number of spaces printed on the left side of nodes with pretty print.
+_PRETTY_MARGIN = 4
+# Length / number of characters (e.g. "-") in an edge with pretty print.
+_PRETTY_EDGE_LENGTH = 4
+
+
+class AbstractNode(metaclass=abc.ABCMeta):
+
+  @property
+  @abc.abstractmethod
+  def is_leaf(self) -> bool:
+    """Tells if a node is a leaf."""
+    raise NotImplementedError
+
+  @abc.abstractmethod
+  def pretty(
+      self,
+      dataspec: data_spec_pb2.DataSpecification,
+      prefix: str,
+      is_pos: Optional[bool],
+      depth: int,
+      max_depth: Optional[int],
+  ) -> str:
+    raise NotImplementedError
+
+
+@dataclasses.dataclass
+class Leaf(AbstractNode):
+  value: value_lib.AbstractValue
+
+  @property
+  def is_leaf(self) -> bool:
+    return True
+
+  def pretty(
+      self,
+      dataspec: data_spec_pb2.DataSpecification,
+      prefix: str,
+      is_pos: Optional[bool],
+      depth: int,
+      max_depth: Optional[int],
+  ) -> str:
+    return prefix + _pretty_local_prefix(is_pos) + self.value.pretty() + "\n"
+
+
+@dataclasses.dataclass
+class NonLeaf(AbstractNode):
+  value: Optional[value_lib.AbstractValue] = None
+  condition: Optional[condition_lib.AbstractCondition] = None
+  pos_child: Optional[AbstractNode] = None
+  neg_child: Optional[AbstractNode] = None
+
+  @property
+  def is_leaf(self) -> bool:
+    return False
+
+  def pretty(
+      self,
+      dataspec: data_spec_pb2.DataSpecification,
+      prefix: str,
+      is_pos: Optional[bool],
+      depth: int,
+      max_depth: Optional[int],
+  ) -> str:
+
+    # Prefix for the children of this node.
+    children_prefix = prefix
+    if is_pos is None:
+      pass
+    elif is_pos:
+      children_prefix += " " * _PRETTY_MARGIN + "" + " " * _PRETTY_EDGE_LENGTH
+    elif not is_pos:
+      children_prefix += " " * (_PRETTY_MARGIN + 1 + _PRETTY_EDGE_LENGTH)
+
+    # Node's condition.
+    condition_prefix = prefix + _pretty_local_prefix(is_pos)
+    if self.condition is not None:
+      condition_prefix += self.condition.pretty(dataspec)
+    else:
+      condition_prefix += "No condition"
+    condition_prefix += "\n"
+
+    # Children of the node.
+    if max_depth is not None and depth >= max_depth:
+      return condition_prefix + children_prefix + "...\n"
+    else:
+      children_text = condition_prefix
+      if self.pos_child is not None:
+        children_text += self.pos_child.pretty(
+            dataspec, children_prefix, True, depth + 1, max_depth
+        )
+      else:
+        children_text += "No positive child\n"
+      if self.neg_child is not None:
+        children_text += self.neg_child.pretty(
+            dataspec, children_prefix, False, depth + 1, max_depth
+        )
+      else:
+        children_text += "No negative child\n"
+      return children_text
+
+
+@functools.singledispatch
+def to_json(
+    node: AbstractNode,
+    depth: int,
+    max_depth: Optional[int],
+    dataspec: data_spec_pb2.DataSpecification,
+) -> Dict[str, Any]:
+  """Creates a JSON-compatible object of the node.
+
+  Note: While public, this logic is not part of the API. This is why this
+  methode's code is not an abstract method in AbstractValue.
+
+  Args:
+    node: Input node.
+    depth: Depth of the current node
+    max_depth: Maximum depth of the tree in the json
+    dataspec: Dataspec associated with this tree.
+
+  Returns:
+    JSON node.
+  """
+  raise NotImplementedError("Unsupported node type")
+
+
+@to_json.register
+def _to_json_leaf(
+    node: Leaf,
+    depth: int,
+    max_depth: Optional[int],
+    dataspec: data_spec_pb2.DataSpecification,
+) -> Dict[str, Any]:
+  return {"value": value_lib.to_json(node.value)}
+
+
+@to_json.register
+def _to_json_non_leaf(
+    node: NonLeaf,
+    depth: int,
+    max_depth: Optional[int],
+    dataspec: data_spec_pb2.DataSpecification,
+) -> Dict[str, Any]:
+  dst = {}
+  if node.value is not None:
+    dst["value"] = value_lib.to_json(node.value)
+
+  if node.condition is not None:
+    dst["condition"] = condition_lib.to_json(node.condition, dataspec)
+  if (
+      (max_depth is None or (max_depth is not None and depth < max_depth))
+      and node.pos_child is not None
+      and node.neg_child is not None
+  ):
+    dst["children"] = [
+        to_json(node.pos_child, depth + 1, max_depth, dataspec),
+        to_json(node.neg_child, depth + 1, max_depth, dataspec),
+    ]
+  return dst
+
+
+def _pretty_local_prefix(is_pos: Optional[bool]) -> str:
+  """Prefix added in front of a node with pretty print.
+
+  Args:
+    is_pos: True/False if the node is a positive/negative child. None if the
+      node is a root.
+
+  Returns:
+    The node prefix.
+  """
+
+  if is_pos is None:
+    # Root node. No prefix.
+    return ""
+  elif is_pos:
+    # Positive nodes are assumed to be printed before negative ones.
+    return " " * _PRETTY_MARGIN + "(pos) "
+  else:
+    return " " * _PRETTY_MARGIN + "(neg) "
```

## ydf/model/tree/node_test.py

 * *Ordering differences only*

```diff
@@ -1,40 +1,40 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from absl.testing import absltest
-from ydf.model.tree import condition as condition_lib
-from ydf.model.tree import node as node_lib
-from ydf.model.tree import value as value_lib
-
-RegressionValue = value_lib.RegressionValue
-Leaf = node_lib.Leaf
-NonLeaf = node_lib.NonLeaf
-IsMissingInCondition = condition_lib.IsMissingInCondition
-
-
-class NodeTest(absltest.TestCase):
-
-  def test_valid_leaf(self):
-    Leaf(value=RegressionValue(value=5, num_examples=1))
-
-  def test_valid_non_leaf(self):
-    NonLeaf(
-        condition=IsMissingInCondition(missing=True, score=3, attribute=0),
-        pos_child=Leaf(value=RegressionValue(value=1, num_examples=1)),
-        neg_child=Leaf(value=RegressionValue(value=2, num_examples=1)),
-    )
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from absl.testing import absltest
+from ydf.model.tree import condition as condition_lib
+from ydf.model.tree import node as node_lib
+from ydf.model.tree import value as value_lib
+
+RegressionValue = value_lib.RegressionValue
+Leaf = node_lib.Leaf
+NonLeaf = node_lib.NonLeaf
+IsMissingInCondition = condition_lib.IsMissingInCondition
+
+
+class NodeTest(absltest.TestCase):
+
+  def test_valid_leaf(self):
+    Leaf(value=RegressionValue(value=5, num_examples=1))
+
+  def test_valid_non_leaf(self):
+    NonLeaf(
+        condition=IsMissingInCondition(missing=True, score=3, attribute=0),
+        pos_child=Leaf(value=RegressionValue(value=1, num_examples=1)),
+        neg_child=Leaf(value=RegressionValue(value=2, num_examples=1)),
+    )
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/model/tree/plot.py

 * *Ordering differences only*

```diff
@@ -1,123 +1,123 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Utilities for plotting a tree."""
-
-import dataclasses
-import json
-import string
-from typing import Any, Dict, Optional, Sequence
-import uuid
-
-
-@dataclasses.dataclass(frozen=True)
-class PlotOptions:
-  """Options for plotting trees.
-
-  All the values are expressed in pixel.
-  """
-
-  # Margin around the entire plot.
-  margin: Optional[int] = 10
-
-  # Size of a tree node.
-  node_x_size: Optional[int] = 160
-  node_y_size: Optional[int] = 12 * 2 + 4
-
-  # Space between tree nodes.
-  node_x_offset: Optional[int] = 160 + 20
-  node_y_offset: Optional[int] = 12 * 2 + 4 + 5
-
-  # Text size in px.
-  font_size: Optional[int] = 10
-
-  # Rounding effect of the edges.
-  # This value is the distance (in pixel) of the Bezier control anchor from
-  # the source point.
-  edge_rounding: Optional[int] = 20
-
-  # Padding inside nodes.
-  node_padding: Optional[int] = 2
-
-  # Show a bb box around the plot. For debugging only.
-  show_plot_bounding_box: Optional[bool] = False
-
-
-class TreePlot:
-  """Class for plotting a tree in IPython / Colab and as string."""
-
-  def __init__(
-      self,
-      tree_json: Dict[str, Any],
-      label_classes: Optional[Sequence[str]],
-      options: PlotOptions,
-      d3js_url: str,
-  ):
-    """Initializes the instance based on the tree and its options.
-
-    Args:
-      tree_json: A JSON-serializable dictionary of the tree.
-      label_classes: For classification problems, the names of the label
-        classes, None otherwise.
-      options: Options for the visual presentation of the plot.
-      d3js_url: The URL to load d3.js from.
-    """
-    self._tree_json = tree_json
-    self._d3js_url = d3js_url
-    self._options = dataclasses.asdict(options)
-    if label_classes is not None:
-      self._options["labels"] = label_classes
-
-  def __str__(self) -> str:
-    """Returns an explanation how to display the plot."""
-    return (
-        "A plot of a tree. Use a notebook cell to display the plot."
-        " Alternatively, export the plot with"
-        ' `plot.to_file("plot.html")` or print the html source with'
-        " `print(plot.html())`."
-    )
-
-  def html(self) -> str:
-    """Returns HTML plot of the tree."""
-    return self._repr_html_()
-
-  def _repr_html_(self) -> str:
-    """Returns HTML plot of the tree."""
-    # Plotting library.
-    import pkgutil
-
-    plotter_js = pkgutil.get_data(__name__, "plotter.js").decode()
-
-    container_id = "tree_plot_" + uuid.uuid4().hex
-
-    html_content = string.Template("""
-  <script src='${d3js_url}'></script>
-  <div id="${container_id}"></div>
-  <script>
-  ${plotter_js}
-  display_tree(${options}, ${json_tree_content}, "#${container_id}")
-  </script>
-  """).substitute(
-        d3js_url=self._d3js_url,
-        options=json.dumps(self._options),
-        plotter_js=plotter_js,
-        container_id=container_id,
-        json_tree_content=json.dumps(self._tree_json),
-    )
-    return html_content
-
-  def to_file(self, path: str) -> None:
-    """Exports the plot to a html file."""
-    with open(path, "w") as f:
-      f.write(self.html())
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Utilities for plotting a tree."""
+
+import dataclasses
+import json
+import string
+from typing import Any, Dict, Optional, Sequence
+import uuid
+
+
+@dataclasses.dataclass(frozen=True)
+class PlotOptions:
+  """Options for plotting trees.
+
+  All the values are expressed in pixel.
+  """
+
+  # Margin around the entire plot.
+  margin: Optional[int] = 10
+
+  # Size of a tree node.
+  node_x_size: Optional[int] = 160
+  node_y_size: Optional[int] = 12 * 2 + 4
+
+  # Space between tree nodes.
+  node_x_offset: Optional[int] = 160 + 20
+  node_y_offset: Optional[int] = 12 * 2 + 4 + 5
+
+  # Text size in px.
+  font_size: Optional[int] = 10
+
+  # Rounding effect of the edges.
+  # This value is the distance (in pixel) of the Bezier control anchor from
+  # the source point.
+  edge_rounding: Optional[int] = 20
+
+  # Padding inside nodes.
+  node_padding: Optional[int] = 2
+
+  # Show a bb box around the plot. For debugging only.
+  show_plot_bounding_box: Optional[bool] = False
+
+
+class TreePlot:
+  """Class for plotting a tree in IPython / Colab and as string."""
+
+  def __init__(
+      self,
+      tree_json: Dict[str, Any],
+      label_classes: Optional[Sequence[str]],
+      options: PlotOptions,
+      d3js_url: str,
+  ):
+    """Initializes the instance based on the tree and its options.
+
+    Args:
+      tree_json: A JSON-serializable dictionary of the tree.
+      label_classes: For classification problems, the names of the label
+        classes, None otherwise.
+      options: Options for the visual presentation of the plot.
+      d3js_url: The URL to load d3.js from.
+    """
+    self._tree_json = tree_json
+    self._d3js_url = d3js_url
+    self._options = dataclasses.asdict(options)
+    if label_classes is not None:
+      self._options["labels"] = label_classes
+
+  def __str__(self) -> str:
+    """Returns an explanation how to display the plot."""
+    return (
+        "A plot of a tree. Use a notebook cell to display the plot."
+        " Alternatively, export the plot with"
+        ' `plot.to_file("plot.html")` or print the html source with'
+        " `print(plot.html())`."
+    )
+
+  def html(self) -> str:
+    """Returns HTML plot of the tree."""
+    return self._repr_html_()
+
+  def _repr_html_(self) -> str:
+    """Returns HTML plot of the tree."""
+    # Plotting library.
+    import pkgutil
+
+    plotter_js = pkgutil.get_data(__name__, "plotter.js").decode()
+
+    container_id = "tree_plot_" + uuid.uuid4().hex
+
+    html_content = string.Template("""
+  <script src='${d3js_url}'></script>
+  <div id="${container_id}"></div>
+  <script>
+  ${plotter_js}
+  display_tree(${options}, ${json_tree_content}, "#${container_id}")
+  </script>
+  """).substitute(
+        d3js_url=self._d3js_url,
+        options=json.dumps(self._options),
+        plotter_js=plotter_js,
+        container_id=container_id,
+        json_tree_content=json.dumps(self._tree_json),
+    )
+    return html_content
+
+  def to_file(self, path: str) -> None:
+    """Exports the plot to a html file."""
+    with open(path, "w") as f:
+      f.write(self.html())
```

## ydf/model/tree/plot_test.py

 * *Ordering differences only*

```diff
@@ -1,83 +1,83 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-import os
-
-from absl.testing import absltest
-
-from ydf.model.tree import plot as plot_lib
-from ydf.utils import test_utils
-
-
-def plotter_js_path() -> str:
-  return os.path.join(test_utils.data_root_path(), "ydf/model/tree/plotter.js")
-
-
-class PlotTest(absltest.TestCase):
-
-  def test_plotter_js_included(self):
-    tree_plot = plot_lib.TreePlot(
-        {}, None, plot_lib.PlotOptions(), "https://google.com"
-    )
-    plotter_js_data = open(plotter_js_path(), "r").read()
-    self.assertIn(plotter_js_data, tree_plot.html())
-
-  def test_url(self):
-    tree_plot = plot_lib.TreePlot(
-        {}, None, plot_lib.PlotOptions(), "https://google.com/bla/d3js.js"
-    )
-    self.assertIn(
-        "https://google.com/bla/d3js.js",
-        tree_plot.html(),
-    )
-
-  def test_options(self):
-    options = plot_lib.PlotOptions(margin=12345, show_plot_bounding_box=True)
-    tree_plot = plot_lib.TreePlot({}, None, options, "https://google.com")
-    self.assertIn(
-        '"margin": 12345',
-        tree_plot.html(),
-    )
-    self.assertIn(
-        '"show_plot_bounding_box": true',
-        tree_plot.html(),
-    )
-
-  def test_label_classes(self):
-    tree_plot = plot_lib.TreePlot(
-        {},
-        ["class_1", "class_2", "class_3"],
-        plot_lib.PlotOptions(),
-        "https://google.com",
-    )
-    self.assertIn(
-        '"labels": ["class_1", "class_2", "class_3"]',
-        tree_plot.html(),
-    )
-
-  def test_tree(self):
-    tree_plot = plot_lib.TreePlot(
-        {"value": 654321},
-        None,
-        plot_lib.PlotOptions(),
-        "https://google.com",
-    )
-    self.assertIn(
-        '{"value": 654321}',
-        tree_plot.html(),
-    )
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import os
+
+from absl.testing import absltest
+
+from ydf.model.tree import plot as plot_lib
+from ydf.utils import test_utils
+
+
+def plotter_js_path() -> str:
+  return os.path.join(test_utils.data_root_path(), "ydf/model/tree/plotter.js")
+
+
+class PlotTest(absltest.TestCase):
+
+  def test_plotter_js_included(self):
+    tree_plot = plot_lib.TreePlot(
+        {}, None, plot_lib.PlotOptions(), "https://google.com"
+    )
+    plotter_js_data = open(plotter_js_path(), "r").read()
+    self.assertIn(plotter_js_data, tree_plot.html())
+
+  def test_url(self):
+    tree_plot = plot_lib.TreePlot(
+        {}, None, plot_lib.PlotOptions(), "https://google.com/bla/d3js.js"
+    )
+    self.assertIn(
+        "https://google.com/bla/d3js.js",
+        tree_plot.html(),
+    )
+
+  def test_options(self):
+    options = plot_lib.PlotOptions(margin=12345, show_plot_bounding_box=True)
+    tree_plot = plot_lib.TreePlot({}, None, options, "https://google.com")
+    self.assertIn(
+        '"margin": 12345',
+        tree_plot.html(),
+    )
+    self.assertIn(
+        '"show_plot_bounding_box": true',
+        tree_plot.html(),
+    )
+
+  def test_label_classes(self):
+    tree_plot = plot_lib.TreePlot(
+        {},
+        ["class_1", "class_2", "class_3"],
+        plot_lib.PlotOptions(),
+        "https://google.com",
+    )
+    self.assertIn(
+        '"labels": ["class_1", "class_2", "class_3"]',
+        tree_plot.html(),
+    )
+
+  def test_tree(self):
+    tree_plot = plot_lib.TreePlot(
+        {"value": 654321},
+        None,
+        plot_lib.PlotOptions(),
+        "https://google.com",
+    )
+    self.assertIn(
+        '{"value": 654321}',
+        tree_plot.html(),
+    )
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/model/tree/tree.py

```diff
@@ -1,170 +1,170 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""A decision tree."""
-
-import dataclasses
-from typing import Any, Dict, Iterator, List, Optional, Sequence
-from yggdrasil_decision_forests.dataset import data_spec_pb2
-from yggdrasil_decision_forests.model.decision_tree import decision_tree_pb2
-from ydf.model.tree import condition as condition_lib
-from ydf.model.tree import node as node_lib
-from ydf.model.tree import plot as plot_lib
-from ydf.model.tree import value as value_lib
-
-
-@dataclasses.dataclass
-class Tree:
-  root: node_lib.AbstractNode
-
-  def pretty(
-      self,
-      dataspec: data_spec_pb2.DataSpecification,
-      max_depth: Optional[int] = 6,
-  ) -> str:
-    """Returns a printable representation of the decision tree.
-
-    Usage example:
-
-    ```python
-    model = ydf.load_model("my_model")
-    tree = model.get_tree(0)
-    print(tree.pretty(model.data_spec()))
-    ```
-
-    Args:
-      dataspec: Dataspec of the tree.
-      max_depth: Maximum printed depth.
-    """
-
-    if self.root:
-      return self.root.pretty(
-          dataspec=dataspec,
-          prefix="",
-          is_pos=None,
-          depth=1,
-          max_depth=max_depth,
-      )
-    else:
-      return "No root\n"
-
-  def _to_json(
-      self, dataspec: data_spec_pb2.DataSpecification, max_depth: Optional[int]
-  ) -> Dict[str, Any]:
-    return node_lib.to_json(self.root, 0, max_depth, dataspec)
-
-  def plot(
-      self,
-      dataspec: data_spec_pb2.DataSpecification,
-      max_depth: Optional[int],
-      label_classes: Optional[Sequence[str]],
-      options: Optional[plot_lib.PlotOptions] = None,
-      d3js_url: str = "https://d3js.org/d3.v6.min.js",
-  ) -> plot_lib.TreePlot:
-    """Plots a decision tree.
-
-    Args:
-      dataspec: Dataspec of the tree.
-      max_depth: Maximum tree depth of the plot. Set to None for full depth.
-      label_classes: For classification, label classes of the dataset.
-      options: Advanced options for plotting. Set to None for default style.
-      d3js_url: URL to load the d3.js library from.
-
-    Returns:
-      The html content displaying the tree.
-    """
-
-    if options is None:
-      options = plot_lib.PlotOptions()
-    # Converts the tree into its json representation.
-    json_tree = self._to_json(dataspec, max_depth)
-
-    return plot_lib.TreePlot(json_tree, label_classes, options, d3js_url)
-
-
-def _recusive_build_tree(
-    node_iterator: Iterator[decision_tree_pb2.Node],
-    dataspec: data_spec_pb2.DataSpecification,
-) -> node_lib.AbstractNode:
-  """Creates recursively a node from a node iterator.
-
-  The nodes should be produced as a depth-first, negative-first, transversal
-  order.
-
-  Args:
-    node_iterator: Node iterator.
-    dataspec: Model dataspec.
-
-  Returns:
-    The root node.
-  """
-
-  proto_node = next(node_iterator)
-  if proto_node.HasField("condition"):
-    # If the non-leaf contains a value
-    if proto_node.WhichOneof("output") is not None:
-      value = value_lib.to_value(proto_node)
-    else:
-      value = None
-
-    return node_lib.NonLeaf(
-        value=value,
-        condition=condition_lib.to_condition(proto_node.condition, dataspec),
-        neg_child=_recusive_build_tree(node_iterator, dataspec),
-        pos_child=_recusive_build_tree(node_iterator, dataspec),
-    )
-  else:
-    return node_lib.Leaf(value=value_lib.to_value(proto_node))
-
-
-def proto_nodes_to_tree(
-    nodes: Sequence[decision_tree_pb2.Node],
-    dataspec: data_spec_pb2.DataSpecification,
-) -> Tree:
-  """Creates a tree from an depth-first, negative-first list of nodes."""
-  return Tree(root=_recusive_build_tree(iter(nodes), dataspec))
-
-
-def _list_nodes(
-    node: node_lib.AbstractNode,
-    dataspec: data_spec_pb2.DataSpecification,
-    nodes: List[decision_tree_pb2.Node],
-) -> None:
-  """Unrolls the nodes of a tree."""
-
-  if node.is_leaf:
-    assert isinstance(node, node_lib.Leaf)
-    proto_node = decision_tree_pb2.Node()
-    value_lib.set_proto_node(node.value, proto_node)
-    nodes.append(proto_node)
-  else:
-    assert isinstance(node, node_lib.NonLeaf)
-    proto_node = decision_tree_pb2.Node(
-        condition=condition_lib.to_proto_condition(node.condition, dataspec)
-    )
-    if node.value is not None:
-      value_lib.set_proto_node(node.value, proto_node)
-    nodes.append(proto_node)
-    _list_nodes(node.neg_child, dataspec, nodes)
-    _list_nodes(node.pos_child, dataspec, nodes)
-
-
-def tree_to_proto_nodes(
-    tree: Tree,
-    dataspec: data_spec_pb2.DataSpecification,
-) -> Sequence[decision_tree_pb2.Node]:
-  """Creates a depth-first list of proto nodes from a tree."""
-  nodes = []
-  _list_nodes(tree.root, dataspec, nodes)
-  return nodes
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""A decision tree."""
+
+import dataclasses
+from typing import Any, Dict, Iterator, List, Optional, Sequence
+from ydf.proto.dataset import data_spec_pb2
+from ydf.proto.model.decision_tree import decision_tree_pb2
+from ydf.model.tree import condition as condition_lib
+from ydf.model.tree import node as node_lib
+from ydf.model.tree import plot as plot_lib
+from ydf.model.tree import value as value_lib
+
+
+@dataclasses.dataclass
+class Tree:
+  root: node_lib.AbstractNode
+
+  def pretty(
+      self,
+      dataspec: data_spec_pb2.DataSpecification,
+      max_depth: Optional[int] = 6,
+  ) -> str:
+    """Returns a printable representation of the decision tree.
+
+    Usage example:
+
+    ```python
+    model = ydf.load_model("my_model")
+    tree = model.get_tree(0)
+    print(tree.pretty(model.data_spec()))
+    ```
+
+    Args:
+      dataspec: Dataspec of the tree.
+      max_depth: Maximum printed depth.
+    """
+
+    if self.root:
+      return self.root.pretty(
+          dataspec=dataspec,
+          prefix="",
+          is_pos=None,
+          depth=1,
+          max_depth=max_depth,
+      )
+    else:
+      return "No root\n"
+
+  def _to_json(
+      self, dataspec: data_spec_pb2.DataSpecification, max_depth: Optional[int]
+  ) -> Dict[str, Any]:
+    return node_lib.to_json(self.root, 0, max_depth, dataspec)
+
+  def plot(
+      self,
+      dataspec: data_spec_pb2.DataSpecification,
+      max_depth: Optional[int],
+      label_classes: Optional[Sequence[str]],
+      options: Optional[plot_lib.PlotOptions] = None,
+      d3js_url: str = "https://d3js.org/d3.v6.min.js",
+  ) -> plot_lib.TreePlot:
+    """Plots a decision tree.
+
+    Args:
+      dataspec: Dataspec of the tree.
+      max_depth: Maximum tree depth of the plot. Set to None for full depth.
+      label_classes: For classification, label classes of the dataset.
+      options: Advanced options for plotting. Set to None for default style.
+      d3js_url: URL to load the d3.js library from.
+
+    Returns:
+      The html content displaying the tree.
+    """
+
+    if options is None:
+      options = plot_lib.PlotOptions()
+    # Converts the tree into its json representation.
+    json_tree = self._to_json(dataspec, max_depth)
+
+    return plot_lib.TreePlot(json_tree, label_classes, options, d3js_url)
+
+
+def _recusive_build_tree(
+    node_iterator: Iterator[decision_tree_pb2.Node],
+    dataspec: data_spec_pb2.DataSpecification,
+) -> node_lib.AbstractNode:
+  """Creates recursively a node from a node iterator.
+
+  The nodes should be produced as a depth-first, negative-first, transversal
+  order.
+
+  Args:
+    node_iterator: Node iterator.
+    dataspec: Model dataspec.
+
+  Returns:
+    The root node.
+  """
+
+  proto_node = next(node_iterator)
+  if proto_node.HasField("condition"):
+    # If the non-leaf contains a value
+    if proto_node.WhichOneof("output") is not None:
+      value = value_lib.to_value(proto_node)
+    else:
+      value = None
+
+    return node_lib.NonLeaf(
+        value=value,
+        condition=condition_lib.to_condition(proto_node.condition, dataspec),
+        neg_child=_recusive_build_tree(node_iterator, dataspec),
+        pos_child=_recusive_build_tree(node_iterator, dataspec),
+    )
+  else:
+    return node_lib.Leaf(value=value_lib.to_value(proto_node))
+
+
+def proto_nodes_to_tree(
+    nodes: Sequence[decision_tree_pb2.Node],
+    dataspec: data_spec_pb2.DataSpecification,
+) -> Tree:
+  """Creates a tree from an depth-first, negative-first list of nodes."""
+  return Tree(root=_recusive_build_tree(iter(nodes), dataspec))
+
+
+def _list_nodes(
+    node: node_lib.AbstractNode,
+    dataspec: data_spec_pb2.DataSpecification,
+    nodes: List[decision_tree_pb2.Node],
+) -> None:
+  """Unrolls the nodes of a tree."""
+
+  if node.is_leaf:
+    assert isinstance(node, node_lib.Leaf)
+    proto_node = decision_tree_pb2.Node()
+    value_lib.set_proto_node(node.value, proto_node)
+    nodes.append(proto_node)
+  else:
+    assert isinstance(node, node_lib.NonLeaf)
+    proto_node = decision_tree_pb2.Node(
+        condition=condition_lib.to_proto_condition(node.condition, dataspec)
+    )
+    if node.value is not None:
+      value_lib.set_proto_node(node.value, proto_node)
+    nodes.append(proto_node)
+    _list_nodes(node.neg_child, dataspec, nodes)
+    _list_nodes(node.pos_child, dataspec, nodes)
+
+
+def tree_to_proto_nodes(
+    tree: Tree,
+    dataspec: data_spec_pb2.DataSpecification,
+) -> Sequence[decision_tree_pb2.Node]:
+  """Creates a depth-first list of proto nodes from a tree."""
+  nodes = []
+  _list_nodes(tree.root, dataspec, nodes)
+  return nodes
```

## ydf/model/tree/tree_test.py

```diff
@@ -1,156 +1,156 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-import os
-import re
-from absl.testing import absltest
-from yggdrasil_decision_forests.dataset import data_spec_pb2
-from yggdrasil_decision_forests.model.decision_tree import decision_tree_pb2
-from ydf.model.tree import condition as condition_lib
-from ydf.model.tree import node as node_lib
-from ydf.model.tree import tree as tree_lib
-from ydf.model.tree import value as value_lib
-from ydf.utils import test_utils
-
-RegressionValue = value_lib.RegressionValue
-Leaf = node_lib.Leaf
-NonLeaf = node_lib.NonLeaf
-IsMissingInCondition = condition_lib.IsMissingInCondition
-NumericalHigherThanCondition = condition_lib.NumericalHigherThanCondition
-Tree = tree_lib.Tree
-ProtoNode = decision_tree_pb2.Node
-ProtoNodeCondition = decision_tree_pb2.NodeCondition
-ProtoCondition = decision_tree_pb2.Condition
-ProtoNodeRegressorOutput = decision_tree_pb2.NodeRegressorOutput
-
-
-class TreeTest(absltest.TestCase):
-
-  def setUp(self):
-    super().setUp()
-
-    self.dataspec = data_spec_pb2.DataSpecification(
-        columns=[
-            data_spec_pb2.Column(
-                name="f1", type=data_spec_pb2.ColumnType.NUMERICAL
-            )
-        ]
-    )
-    self.nodes = [
-        ProtoNode(
-            condition=ProtoNodeCondition(
-                attribute=0,
-                condition=ProtoCondition(
-                    higher_condition=ProtoCondition.Higher(threshold=2)
-                ),
-                split_score=3.0,
-                na_value=False,
-            ),
-        ),
-        ProtoNode(
-            condition=ProtoNodeCondition(
-                attribute=0,
-                condition=ProtoCondition(
-                    higher_condition=ProtoCondition.Higher(threshold=4)
-                ),
-                split_score=5.0,
-                na_value=False,
-            ),
-            # Non leaf with a value
-            regressor=ProtoNodeRegressorOutput(top_value=9.0),
-        ),
-        ProtoNode(regressor=ProtoNodeRegressorOutput(top_value=6.0)),
-        ProtoNode(regressor=ProtoNodeRegressorOutput(top_value=7.0)),
-        ProtoNode(regressor=ProtoNodeRegressorOutput(top_value=8.0)),
-    ]
-    self.tree = Tree(
-        root=NonLeaf(
-            condition=NumericalHigherThanCondition(
-                missing=False, score=3.0, attribute=0, threshold=2.0
-            ),
-            pos_child=Leaf(value=RegressionValue(num_examples=0.0, value=8.0)),
-            neg_child=NonLeaf(
-                value=RegressionValue(num_examples=0.0, value=9.0),
-                condition=NumericalHigherThanCondition(
-                    missing=False, score=5.0, attribute=0, threshold=4.0
-                ),
-                pos_child=Leaf(
-                    value=RegressionValue(num_examples=0.0, value=7.0)
-                ),
-                neg_child=Leaf(
-                    value=RegressionValue(num_examples=0.0, value=6.0)
-                ),
-            ),
-        )
-    )
-
-  def test_valid_input(self):
-    Tree(
-        root=NonLeaf(
-            condition=IsMissingInCondition(missing=True, score=3, attribute=0),
-            pos_child=Leaf(value=RegressionValue(value=1, num_examples=1)),
-            neg_child=Leaf(value=RegressionValue(value=2, num_examples=1)),
-        )
-    )
-
-  def test_proto_nodes_to_tree_with_valid_input(self):
-    tree = tree_lib.proto_nodes_to_tree(self.nodes, self.dataspec)
-    self.assertEqual(tree, self.tree)
-    test_utils.golden_check_string(
-        self,
-        tree.pretty(self.dataspec),
-        os.path.join(test_utils.pydf_test_data_path(), "toy_tree.txt"),
-    )
-
-  def test_tree_to_proto_nodes_with_valid_input(self):
-    nodes = tree_lib.tree_to_proto_nodes(self.tree, self.dataspec)
-    self.assertEqual(nodes, self.nodes)
-
-  def test_plot_toy_tree(self):
-    tree_plot = self.tree.plot(
-        self.dataspec, max_depth=None, label_classes=None
-    )
-    raw_output = tree_plot.html()
-    pattern = r"tree_plot_[a-f|\d]{32}"
-    reproducible_html = re.sub(pattern, "tree_plot_element", raw_output)
-    reproducible_html = reproducible_html.replace(
-        """/*
- * Copyright 2022 Google LLC.
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     https://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-""",
-        "",
-    )
-    test_utils.golden_check_string(
-        self,
-        reproducible_html,
-        os.path.join(
-            test_utils.pydf_test_data_path(), "golden_toy_tree_plot.txt"
-        ),
-    )
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import os
+import re
+from absl.testing import absltest
+from ydf.proto.dataset import data_spec_pb2
+from ydf.proto.model.decision_tree import decision_tree_pb2
+from ydf.model.tree import condition as condition_lib
+from ydf.model.tree import node as node_lib
+from ydf.model.tree import tree as tree_lib
+from ydf.model.tree import value as value_lib
+from ydf.utils import test_utils
+
+RegressionValue = value_lib.RegressionValue
+Leaf = node_lib.Leaf
+NonLeaf = node_lib.NonLeaf
+IsMissingInCondition = condition_lib.IsMissingInCondition
+NumericalHigherThanCondition = condition_lib.NumericalHigherThanCondition
+Tree = tree_lib.Tree
+ProtoNode = decision_tree_pb2.Node
+ProtoNodeCondition = decision_tree_pb2.NodeCondition
+ProtoCondition = decision_tree_pb2.Condition
+ProtoNodeRegressorOutput = decision_tree_pb2.NodeRegressorOutput
+
+
+class TreeTest(absltest.TestCase):
+
+  def setUp(self):
+    super().setUp()
+
+    self.dataspec = data_spec_pb2.DataSpecification(
+        columns=[
+            data_spec_pb2.Column(
+                name="f1", type=data_spec_pb2.ColumnType.NUMERICAL
+            )
+        ]
+    )
+    self.nodes = [
+        ProtoNode(
+            condition=ProtoNodeCondition(
+                attribute=0,
+                condition=ProtoCondition(
+                    higher_condition=ProtoCondition.Higher(threshold=2)
+                ),
+                split_score=3.0,
+                na_value=False,
+            ),
+        ),
+        ProtoNode(
+            condition=ProtoNodeCondition(
+                attribute=0,
+                condition=ProtoCondition(
+                    higher_condition=ProtoCondition.Higher(threshold=4)
+                ),
+                split_score=5.0,
+                na_value=False,
+            ),
+            # Non leaf with a value
+            regressor=ProtoNodeRegressorOutput(top_value=9.0),
+        ),
+        ProtoNode(regressor=ProtoNodeRegressorOutput(top_value=6.0)),
+        ProtoNode(regressor=ProtoNodeRegressorOutput(top_value=7.0)),
+        ProtoNode(regressor=ProtoNodeRegressorOutput(top_value=8.0)),
+    ]
+    self.tree = Tree(
+        root=NonLeaf(
+            condition=NumericalHigherThanCondition(
+                missing=False, score=3.0, attribute=0, threshold=2.0
+            ),
+            pos_child=Leaf(value=RegressionValue(num_examples=0.0, value=8.0)),
+            neg_child=NonLeaf(
+                value=RegressionValue(num_examples=0.0, value=9.0),
+                condition=NumericalHigherThanCondition(
+                    missing=False, score=5.0, attribute=0, threshold=4.0
+                ),
+                pos_child=Leaf(
+                    value=RegressionValue(num_examples=0.0, value=7.0)
+                ),
+                neg_child=Leaf(
+                    value=RegressionValue(num_examples=0.0, value=6.0)
+                ),
+            ),
+        )
+    )
+
+  def test_valid_input(self):
+    Tree(
+        root=NonLeaf(
+            condition=IsMissingInCondition(missing=True, score=3, attribute=0),
+            pos_child=Leaf(value=RegressionValue(value=1, num_examples=1)),
+            neg_child=Leaf(value=RegressionValue(value=2, num_examples=1)),
+        )
+    )
+
+  def test_proto_nodes_to_tree_with_valid_input(self):
+    tree = tree_lib.proto_nodes_to_tree(self.nodes, self.dataspec)
+    self.assertEqual(tree, self.tree)
+    test_utils.golden_check_string(
+        self,
+        tree.pretty(self.dataspec),
+        os.path.join(test_utils.pydf_test_data_path(), "toy_tree.txt"),
+    )
+
+  def test_tree_to_proto_nodes_with_valid_input(self):
+    nodes = tree_lib.tree_to_proto_nodes(self.tree, self.dataspec)
+    self.assertEqual(nodes, self.nodes)
+
+  def test_plot_toy_tree(self):
+    tree_plot = self.tree.plot(
+        self.dataspec, max_depth=None, label_classes=None
+    )
+    raw_output = tree_plot.html()
+    pattern = r"tree_plot_[a-f|\d]{32}"
+    reproducible_html = re.sub(pattern, "tree_plot_element", raw_output)
+    reproducible_html = reproducible_html.replace(
+        """/*
+ * Copyright 2022 Google LLC.
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     https://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+""",
+        "",
+    )
+    test_utils.golden_check_string(
+        self,
+        reproducible_html,
+        os.path.join(
+            test_utils.pydf_test_data_path(), "golden_toy_tree_plot.txt"
+        ),
+    )
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/model/tree/value.py

```diff
@@ -1,229 +1,229 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""The value / prediction of a leaf."""
-
-import abc
-import dataclasses
-import functools
-import math
-from typing import Any, Dict, Optional, Sequence
-import numpy as np
-from yggdrasil_decision_forests.model.decision_tree import decision_tree_pb2
-
-
-# TODO: 310218604 - Use kw_only with default value num_examples = 1.
-@dataclasses.dataclass
-class AbstractValue(metaclass=abc.ABCMeta):
-  """A generic value/prediction/output.
-
-  Attrs:
-    num_examples: Number of example in the node.
-  """
-
-  num_examples: float
-
-  @abc.abstractmethod
-  def pretty(self) -> str:
-    raise NotImplementedError
-
-  def __str__(self):
-    return self.pretty()
-
-
-@dataclasses.dataclass
-class RegressionValue(AbstractValue):
-  """The regression value of a regressive tree.
-
-  Can also be used in gradient-boosted-trees for classification and ranking.
-
-  Attrs:
-    value: Value of the tree. The semantic depends on the tree: For Regression
-      Random Forest and Regression GBDT, this value is a regressive value in the
-      same unit as the label. For classification and ranking GBDTs, this value
-      is a logit.
-    standard_deviation: Optional standard deviation attached to the value.
-  """
-
-  value: float
-  standard_deviation: Optional[float] = None
-
-  def pretty(self) -> str:
-    text = f"value={self.value:.5g}"
-    if self.standard_deviation is not None:
-      text += f" sd={self.standard_deviation:.5g}"
-    return text
-
-
-@dataclasses.dataclass
-class ProbabilityValue(AbstractValue):
-  """A probability distribution value.
-
-  Used for random Forest / CART classification trees.
-
-  Attrs:
-    probability: An array of probabilities of the label classes i.e. the i-th
-      value is the probability of the "label_value_idx_to_value(..., i)" class.
-      Note that the first value is reserved for the Out-of-vocabulary
-  """
-
-  probability: Sequence[float]
-
-  def pretty(self) -> str:
-    return f"value={self.probability}"
-
-
-@dataclasses.dataclass
-class UpliftValue(AbstractValue):
-  """The uplift value of a classification or regression uplift tree.
-
-  Attrs:
-    treatment_effect: An array of the effects on the treatment groups. The i-th
-      element of this array is the effect of the "i+1"th treatment compared to
-      the control group.
-  """
-
-  treatment_effect: Sequence[float]
-
-  def pretty(self) -> str:
-    return f"value={self.treatment_effect}"
-
-
-def to_value(proto_node: decision_tree_pb2.Node) -> AbstractValue:
-  """Extracts the "value" part of a proto node."""
-
-  if proto_node.HasField("classifier"):
-    dist = proto_node.classifier.distribution
-    # Note: The first value (out-of-dictionary) is removed.
-    probabilities = np.array(dist.counts[1:]) / dist.sum
-    return ProbabilityValue(
-        probability=probabilities.tolist(), num_examples=dist.sum
-    )
-
-  if proto_node.HasField("regressor"):
-    dist = proto_node.regressor.distribution
-    standard_deviation = None
-    if dist.HasField("sum_squares") and dist.count > 0:
-      variance = dist.sum_squares / dist.count - dist.sum**2 / dist.count**2
-      if variance >= 0:
-        standard_deviation = math.sqrt(variance)
-    return RegressionValue(
-        value=proto_node.regressor.top_value,
-        num_examples=dist.count,
-        standard_deviation=standard_deviation,
-    )
-
-  if proto_node.HasField("uplift"):
-    return UpliftValue(
-        treatment_effect=proto_node.uplift.treatment_effect[:],
-        num_examples=proto_node.uplift.sum_weights,
-    )
-
-  raise ValueError("Unsupported value")
-
-
-@functools.singledispatch
-def to_json(value: AbstractValue) -> Dict[str, Any]:
-  """Creates a JSON-compatible object of the value.
-
-  Note: While public, this logic is not part of the API. This is why this
-  methode's code is not an abstract method in AbstractValue.
-
-  Args:
-    value: Input value.
-
-  Returns:
-    JSON value.
-  """
-  raise NotImplementedError("Unsupported value type")
-
-
-@to_json.register
-def _to_json_regression(value: RegressionValue) -> Dict[str, Any]:
-  value_as_json = {
-      "type": "REGRESSION",
-      "value": value.value,
-      "num_examples": value.num_examples,
-  }
-  if value.standard_deviation is not None:
-    value_as_json["standard_deviation"] = value.standard_deviation
-  return value_as_json
-
-
-@to_json.register
-def _to_json_probability(value: ProbabilityValue) -> Dict[str, Any]:
-  return {
-      "type": "PROBABILITY",
-      "distribution": value.probability,
-      "num_examples": value.num_examples,
-  }
-
-
-@to_json.register
-def _to_json_uplift(value: UpliftValue) -> Dict[str, Any]:
-  return {
-      "type": "UPLIFT",
-      "treatment_effect": value.treatment_effect,
-      "num_examples": value.num_examples,
-  }
-
-
-@functools.singledispatch
-def set_proto_node(value: AbstractValue, proto_node: decision_tree_pb2.Node):
-  """Sets the "value" part in a proto node.
-
-  Note: While public, this logic is not part of the API. This is why this
-  methode's code is not an abstract method in AbstractValue.
-
-  Args:
-    value: Input value.
-    proto_node: Proto node to populate with the input value.
-  """
-  del value
-  del proto_node
-  raise NotImplementedError("Unsupported value type")
-
-
-@set_proto_node.register
-def _set_proto_node_from_probability(
-    value: ProbabilityValue, proto_node: decision_tree_pb2.Node
-):
-  dist = proto_node.classifier.distribution
-  dist.sum = value.num_examples
-  # Add an extra 0 for the out-of-vocabulary item.
-  dist.counts[:] = np.array([0.0, *value.probability]) * dist.sum
-  proto_node.classifier.top_value = np.argmax(dist.counts)
-
-
-@set_proto_node.register
-def _set_proto_node_from_regression(
-    value: RegressionValue, proto_node: decision_tree_pb2.Node
-):
-  proto_node.regressor.top_value = value.value
-  if value.standard_deviation is not None:
-    dist = proto_node.regressor.distribution
-    dist.count = value.num_examples
-    dist.sum = value.value * value.num_examples
-    dist.sum_squares = (
-        value.standard_deviation**2 * value.num_examples
-        + dist.sum**2 / value.num_examples
-    )
-
-
-@set_proto_node.register
-def _set_proto_node_from_uplift(
-    value: UpliftValue, proto_node: decision_tree_pb2.Node
-):
-  proto_node.uplift.treatment_effect[:] = value.treatment_effect
-  proto_node.uplift.sum_weights = value.num_examples
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""The value / prediction of a leaf."""
+
+import abc
+import dataclasses
+import functools
+import math
+from typing import Any, Dict, Optional, Sequence
+import numpy as np
+from ydf.proto.model.decision_tree import decision_tree_pb2
+
+
+# TODO: 310218604 - Use kw_only with default value num_examples = 1.
+@dataclasses.dataclass
+class AbstractValue(metaclass=abc.ABCMeta):
+  """A generic value/prediction/output.
+
+  Attrs:
+    num_examples: Number of example in the node.
+  """
+
+  num_examples: float
+
+  @abc.abstractmethod
+  def pretty(self) -> str:
+    raise NotImplementedError
+
+  def __str__(self):
+    return self.pretty()
+
+
+@dataclasses.dataclass
+class RegressionValue(AbstractValue):
+  """The regression value of a regressive tree.
+
+  Can also be used in gradient-boosted-trees for classification and ranking.
+
+  Attrs:
+    value: Value of the tree. The semantic depends on the tree: For Regression
+      Random Forest and Regression GBDT, this value is a regressive value in the
+      same unit as the label. For classification and ranking GBDTs, this value
+      is a logit.
+    standard_deviation: Optional standard deviation attached to the value.
+  """
+
+  value: float
+  standard_deviation: Optional[float] = None
+
+  def pretty(self) -> str:
+    text = f"value={self.value:.5g}"
+    if self.standard_deviation is not None:
+      text += f" sd={self.standard_deviation:.5g}"
+    return text
+
+
+@dataclasses.dataclass
+class ProbabilityValue(AbstractValue):
+  """A probability distribution value.
+
+  Used for random Forest / CART classification trees.
+
+  Attrs:
+    probability: An array of probabilities of the label classes i.e. the i-th
+      value is the probability of the "label_value_idx_to_value(..., i)" class.
+      Note that the first value is reserved for the Out-of-vocabulary
+  """
+
+  probability: Sequence[float]
+
+  def pretty(self) -> str:
+    return f"value={self.probability}"
+
+
+@dataclasses.dataclass
+class UpliftValue(AbstractValue):
+  """The uplift value of a classification or regression uplift tree.
+
+  Attrs:
+    treatment_effect: An array of the effects on the treatment groups. The i-th
+      element of this array is the effect of the "i+1"th treatment compared to
+      the control group.
+  """
+
+  treatment_effect: Sequence[float]
+
+  def pretty(self) -> str:
+    return f"value={self.treatment_effect}"
+
+
+def to_value(proto_node: decision_tree_pb2.Node) -> AbstractValue:
+  """Extracts the "value" part of a proto node."""
+
+  if proto_node.HasField("classifier"):
+    dist = proto_node.classifier.distribution
+    # Note: The first value (out-of-dictionary) is removed.
+    probabilities = np.array(dist.counts[1:]) / dist.sum
+    return ProbabilityValue(
+        probability=probabilities.tolist(), num_examples=dist.sum
+    )
+
+  if proto_node.HasField("regressor"):
+    dist = proto_node.regressor.distribution
+    standard_deviation = None
+    if dist.HasField("sum_squares") and dist.count > 0:
+      variance = dist.sum_squares / dist.count - dist.sum**2 / dist.count**2
+      if variance >= 0:
+        standard_deviation = math.sqrt(variance)
+    return RegressionValue(
+        value=proto_node.regressor.top_value,
+        num_examples=dist.count,
+        standard_deviation=standard_deviation,
+    )
+
+  if proto_node.HasField("uplift"):
+    return UpliftValue(
+        treatment_effect=proto_node.uplift.treatment_effect[:],
+        num_examples=proto_node.uplift.sum_weights,
+    )
+
+  raise ValueError("Unsupported value")
+
+
+@functools.singledispatch
+def to_json(value: AbstractValue) -> Dict[str, Any]:
+  """Creates a JSON-compatible object of the value.
+
+  Note: While public, this logic is not part of the API. This is why this
+  methode's code is not an abstract method in AbstractValue.
+
+  Args:
+    value: Input value.
+
+  Returns:
+    JSON value.
+  """
+  raise NotImplementedError("Unsupported value type")
+
+
+@to_json.register
+def _to_json_regression(value: RegressionValue) -> Dict[str, Any]:
+  value_as_json = {
+      "type": "REGRESSION",
+      "value": value.value,
+      "num_examples": value.num_examples,
+  }
+  if value.standard_deviation is not None:
+    value_as_json["standard_deviation"] = value.standard_deviation
+  return value_as_json
+
+
+@to_json.register
+def _to_json_probability(value: ProbabilityValue) -> Dict[str, Any]:
+  return {
+      "type": "PROBABILITY",
+      "distribution": value.probability,
+      "num_examples": value.num_examples,
+  }
+
+
+@to_json.register
+def _to_json_uplift(value: UpliftValue) -> Dict[str, Any]:
+  return {
+      "type": "UPLIFT",
+      "treatment_effect": value.treatment_effect,
+      "num_examples": value.num_examples,
+  }
+
+
+@functools.singledispatch
+def set_proto_node(value: AbstractValue, proto_node: decision_tree_pb2.Node):
+  """Sets the "value" part in a proto node.
+
+  Note: While public, this logic is not part of the API. This is why this
+  methode's code is not an abstract method in AbstractValue.
+
+  Args:
+    value: Input value.
+    proto_node: Proto node to populate with the input value.
+  """
+  del value
+  del proto_node
+  raise NotImplementedError("Unsupported value type")
+
+
+@set_proto_node.register
+def _set_proto_node_from_probability(
+    value: ProbabilityValue, proto_node: decision_tree_pb2.Node
+):
+  dist = proto_node.classifier.distribution
+  dist.sum = value.num_examples
+  # Add an extra 0 for the out-of-vocabulary item.
+  dist.counts[:] = np.array([0.0, *value.probability]) * dist.sum
+  proto_node.classifier.top_value = np.argmax(dist.counts)
+
+
+@set_proto_node.register
+def _set_proto_node_from_regression(
+    value: RegressionValue, proto_node: decision_tree_pb2.Node
+):
+  proto_node.regressor.top_value = value.value
+  if value.standard_deviation is not None:
+    dist = proto_node.regressor.distribution
+    dist.count = value.num_examples
+    dist.sum = value.value * value.num_examples
+    dist.sum_squares = (
+        value.standard_deviation**2 * value.num_examples
+        + dist.sum**2 / value.num_examples
+    )
+
+
+@set_proto_node.register
+def _set_proto_node_from_uplift(
+    value: UpliftValue, proto_node: decision_tree_pb2.Node
+):
+  proto_node.uplift.treatment_effect[:] = value.treatment_effect
+  proto_node.uplift.sum_weights = value.num_examples
```

## ydf/model/tree/value_test.py

```diff
@@ -1,150 +1,150 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from absl.testing import absltest
-from yggdrasil_decision_forests.model.decision_tree import decision_tree_pb2
-from ydf.model.tree import value as value_lib
-from ydf.utils import test_utils
-from yggdrasil_decision_forests.utils import distribution_pb2
-
-
-class ValueTest(absltest.TestCase):
-
-  def test_to_value_classifier_given_valid_input(self):
-    proto_node = decision_tree_pb2.Node(
-        classifier=decision_tree_pb2.NodeClassifierOutput(
-            distribution=distribution_pb2.IntegerDistributionDouble(
-                counts=[0.0, 8.0, 2.0], sum=10.0
-            )
-        )
-    )
-    self.assertEqual(
-        value_lib.to_value(proto_node),
-        value_lib.ProbabilityValue(probability=[0.8, 0.2], num_examples=10),
-    )
-
-  def test_to_value_regressor_given_valid_input(self):
-    proto_node = decision_tree_pb2.Node(
-        regressor=decision_tree_pb2.NodeRegressorOutput(
-            top_value=1,
-            distribution=distribution_pb2.NormalDistributionDouble(
-                sum=10, sum_squares=20, count=10
-            ),
-        )
-    )
-    self.assertEqual(
-        value_lib.to_value(proto_node),
-        value_lib.RegressionValue(
-            value=1.0, num_examples=10, standard_deviation=1.0
-        ),
-    )
-
-  def test_to_value_uplift_given_valid_input(self):
-    proto_node = decision_tree_pb2.Node(
-        uplift=decision_tree_pb2.NodeUpliftOutput(
-            treatment_effect=[0.0, 8.0, 2.0], sum_weights=10
-        )
-    )
-    self.assertEqual(
-        value_lib.to_value(proto_node),
-        value_lib.UpliftValue(
-            treatment_effect=[0.0, 8.0, 2.0], num_examples=10.0
-        ),
-    )
-
-  def test_classifier_proto_node_is_set_given_valid_input(self):
-    proto_node = decision_tree_pb2.Node()
-    value_lib.set_proto_node(
-        value_lib.ProbabilityValue(probability=[0.8, 0.2], num_examples=10),
-        proto_node,
-    )
-    test_utils.assertProto2Equal(
-        self,
-        proto_node,
-        decision_tree_pb2.Node(
-            classifier=decision_tree_pb2.NodeClassifierOutput(
-                top_value=1,
-                distribution=distribution_pb2.IntegerDistributionDouble(
-                    counts=[0.0, 8.0, 2.0], sum=10.0
-                ),
-            )
-        ),
-    )
-
-  def test_regressive_proto_node_is_set_given_valid_input(self):
-    proto_node = decision_tree_pb2.Node()
-    value_lib.set_proto_node(
-        value_lib.RegressionValue(
-            value=1.0, num_examples=10, standard_deviation=1.0
-        ),
-        proto_node,
-    )
-    test_utils.assertProto2Equal(
-        self,
-        proto_node,
-        decision_tree_pb2.Node(
-            regressor=decision_tree_pb2.NodeRegressorOutput(
-                top_value=1,
-                distribution=distribution_pb2.NormalDistributionDouble(
-                    sum=10, sum_squares=20, count=10
-                ),
-            )
-        ),
-    )
-
-  def test_uplift_proto_node_is_set_given_valid_input(self):
-    proto_node = decision_tree_pb2.Node()
-    value_lib.set_proto_node(
-        value_lib.UpliftValue(
-            treatment_effect=[0.0, 8.0, 2.0], num_examples=10.0
-        ),
-        proto_node,
-    )
-    test_utils.assertProto2Equal(
-        self,
-        proto_node,
-        decision_tree_pb2.Node(
-            uplift=decision_tree_pb2.NodeUpliftOutput(
-                treatment_effect=[0.0, 8.0, 2.0], sum_weights=10
-            )
-        ),
-    )
-
-  def test_pretty_classification(self):
-    self.assertEqual(
-        value_lib.ProbabilityValue(
-            probability=[0.8, 0.2], num_examples=10
-        ).pretty(),
-        "value=[0.8, 0.2]",
-    )
-
-  def test_pretty_regression(self):
-    self.assertEqual(
-        value_lib.RegressionValue(
-            value=1.0, num_examples=10, standard_deviation=1.0
-        ).pretty(),
-        "value=1 sd=1",
-    )
-
-  def test_pretty_uplift(self):
-    self.assertEqual(
-        value_lib.UpliftValue(
-            treatment_effect=[0.0, 8.0, 2.0], num_examples=10.0
-        ).pretty(),
-        "value=[0.0, 8.0, 2.0]",
-    )
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from absl.testing import absltest
+from ydf.proto.model.decision_tree import decision_tree_pb2
+from ydf.model.tree import value as value_lib
+from ydf.utils import test_utils
+from ydf.proto.utils import distribution_pb2
+
+
+class ValueTest(absltest.TestCase):
+
+  def test_to_value_classifier_given_valid_input(self):
+    proto_node = decision_tree_pb2.Node(
+        classifier=decision_tree_pb2.NodeClassifierOutput(
+            distribution=distribution_pb2.IntegerDistributionDouble(
+                counts=[0.0, 8.0, 2.0], sum=10.0
+            )
+        )
+    )
+    self.assertEqual(
+        value_lib.to_value(proto_node),
+        value_lib.ProbabilityValue(probability=[0.8, 0.2], num_examples=10),
+    )
+
+  def test_to_value_regressor_given_valid_input(self):
+    proto_node = decision_tree_pb2.Node(
+        regressor=decision_tree_pb2.NodeRegressorOutput(
+            top_value=1,
+            distribution=distribution_pb2.NormalDistributionDouble(
+                sum=10, sum_squares=20, count=10
+            ),
+        )
+    )
+    self.assertEqual(
+        value_lib.to_value(proto_node),
+        value_lib.RegressionValue(
+            value=1.0, num_examples=10, standard_deviation=1.0
+        ),
+    )
+
+  def test_to_value_uplift_given_valid_input(self):
+    proto_node = decision_tree_pb2.Node(
+        uplift=decision_tree_pb2.NodeUpliftOutput(
+            treatment_effect=[0.0, 8.0, 2.0], sum_weights=10
+        )
+    )
+    self.assertEqual(
+        value_lib.to_value(proto_node),
+        value_lib.UpliftValue(
+            treatment_effect=[0.0, 8.0, 2.0], num_examples=10.0
+        ),
+    )
+
+  def test_classifier_proto_node_is_set_given_valid_input(self):
+    proto_node = decision_tree_pb2.Node()
+    value_lib.set_proto_node(
+        value_lib.ProbabilityValue(probability=[0.8, 0.2], num_examples=10),
+        proto_node,
+    )
+    test_utils.assertProto2Equal(
+        self,
+        proto_node,
+        decision_tree_pb2.Node(
+            classifier=decision_tree_pb2.NodeClassifierOutput(
+                top_value=1,
+                distribution=distribution_pb2.IntegerDistributionDouble(
+                    counts=[0.0, 8.0, 2.0], sum=10.0
+                ),
+            )
+        ),
+    )
+
+  def test_regressive_proto_node_is_set_given_valid_input(self):
+    proto_node = decision_tree_pb2.Node()
+    value_lib.set_proto_node(
+        value_lib.RegressionValue(
+            value=1.0, num_examples=10, standard_deviation=1.0
+        ),
+        proto_node,
+    )
+    test_utils.assertProto2Equal(
+        self,
+        proto_node,
+        decision_tree_pb2.Node(
+            regressor=decision_tree_pb2.NodeRegressorOutput(
+                top_value=1,
+                distribution=distribution_pb2.NormalDistributionDouble(
+                    sum=10, sum_squares=20, count=10
+                ),
+            )
+        ),
+    )
+
+  def test_uplift_proto_node_is_set_given_valid_input(self):
+    proto_node = decision_tree_pb2.Node()
+    value_lib.set_proto_node(
+        value_lib.UpliftValue(
+            treatment_effect=[0.0, 8.0, 2.0], num_examples=10.0
+        ),
+        proto_node,
+    )
+    test_utils.assertProto2Equal(
+        self,
+        proto_node,
+        decision_tree_pb2.Node(
+            uplift=decision_tree_pb2.NodeUpliftOutput(
+                treatment_effect=[0.0, 8.0, 2.0], sum_weights=10
+            )
+        ),
+    )
+
+  def test_pretty_classification(self):
+    self.assertEqual(
+        value_lib.ProbabilityValue(
+            probability=[0.8, 0.2], num_examples=10
+        ).pretty(),
+        "value=[0.8, 0.2]",
+    )
+
+  def test_pretty_regression(self):
+    self.assertEqual(
+        value_lib.RegressionValue(
+            value=1.0, num_examples=10, standard_deviation=1.0
+        ).pretty(),
+        "value=1 sd=1",
+    )
+
+  def test_pretty_uplift(self):
+    self.assertEqual(
+        value_lib.UpliftValue(
+            treatment_effect=[0.0, 8.0, 2.0], num_examples=10.0
+        ).pretty(),
+        "value=[0.0, 8.0, 2.0]",
+    )
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/utils/__init__.py

 * *Ordering differences only*

```diff
@@ -1,14 +1,14 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
```

## ydf/utils/documentation.py

 * *Ordering differences only*

```diff
@@ -1,26 +1,26 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Documentation URLs for Python code.
-
-See also: external/ydf_cc/yggdrasil_decision_forests/utils/documentation.h
-"""
-
-
-URL_DOCUMENTATION = "https://ydf.readthedocs.io/en/latest/"
-
-URL_GLOSSARY = f"{URL_DOCUMENTATION}/glossary"
-URL_CONFUSION_MATRIX = f"{URL_GLOSSARY}#confusion-matrix"
-URL_NUM_EXAMPLES = f"{URL_GLOSSARY}#number-of-examples"
-URL_WEIGHTED_NUM_EXAMPLES = f"{URL_GLOSSARY}#weighted-number-of-examples"
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Documentation URLs for Python code.
+
+See also: external/ydf_cc/yggdrasil_decision_forests/utils/documentation.h
+"""
+
+
+URL_DOCUMENTATION = "https://ydf.readthedocs.io/en/latest/"
+
+URL_GLOSSARY = f"{URL_DOCUMENTATION}/glossary"
+URL_CONFUSION_MATRIX = f"{URL_GLOSSARY}#confusion-matrix"
+URL_NUM_EXAMPLES = f"{URL_GLOSSARY}#number-of-examples"
+URL_WEIGHTED_NUM_EXAMPLES = f"{URL_GLOSSARY}#weighted-number-of-examples"
```

## ydf/utils/html.py

 * *Ordering differences only*

```diff
@@ -1,118 +1,118 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Html utilities."""
-
-from typing import Any, Dict, Tuple, Union
-from xml.dom import minidom
-
-Doc = minidom.Document
-Elem = minidom.Element
-
-
-class HtmlNotebookDisplay:
-  """An object printed as html in a notebook."""
-
-  def __init__(self, html: str):
-    self._html = html
-
-  def _repr_html_(self) -> str:
-    return self._html
-
-  def _repr_(self) -> str:
-    return self._html
-
-
-def create_doc() -> Tuple[Doc, Elem]:
-  """Creates a html document.
-
-  Example:
-
-  ```python
-  doc, root = create_doc()
-  root.appendChild(bold(doc, "Some text"))
-  print(root.toxml())
-  ```
-
-  Returns:
-    A html document and its root node.
-  """
-
-  impl = minidom.getDOMImplementation()
-  assert impl is not None
-  doc = impl.createDocument(None, "div", None)
-  return doc, doc.documentElement
-
-
-def with_style(doc: Doc, item: Union[Elem, str], style: Dict[str, Any]) -> Elem:
-  """Creates html element with given style.
-
-  Example:
-
-  ```python
-  doc, root = create_doc()
-  root.appendChild(with_style(doc, "My text", {"font-weight": "bold"}))
-  print(root.toxml())
-  ```
-
-  Args:
-    doc: Html document.
-    item: Item to display. Can be a string or another Html element.
-    style: style.
-
-  Returns:
-    Html element.
-  """
-
-  if isinstance(item, minidom.Element):
-    raw_item = item
-    item = doc.createElement("span")
-    item.appendChild(raw_item)
-  elif isinstance(item, str):
-    raw_item = item
-    item = doc.createElement("span")
-    item.appendChild(doc.createTextNode(raw_item))
-  else:
-    raise ValueError(f"Non supported element {item}")
-
-  style_key = "style"
-
-  # Get existing style, if any.
-  style_text = (
-      item.getAttribute(style_key) + "; "
-      if item.hasAttribute(style_key)
-      else ""
-  )
-
-  # Add new style
-  style_text += "; ".join([f"{k}:{v}" for k, v in style.items()])
-  item.setAttribute("style", style_text)
-  return item
-
-
-def bold(doc: Doc, item: Union[Elem, str]) -> Elem:
-  return with_style(doc, item, {"font-weight": "bold"})
-
-
-def italic(doc: Doc, item: Union[Elem, str]) -> Elem:
-  return with_style(doc, item, {"font-style": "italic"})
-
-
-def link(doc: Doc, url: str) -> Elem:
-  """Creates a link (i.e., <a>) element."""
-
-  html_a = doc.createElement("a")
-  html_a.setAttribute("href", url)
-  html_a.setAttribute("target", "_blank")
-  return html_a
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Html utilities."""
+
+from typing import Any, Dict, Tuple, Union
+from xml.dom import minidom
+
+Doc = minidom.Document
+Elem = minidom.Element
+
+
+class HtmlNotebookDisplay:
+  """An object printed as html in a notebook."""
+
+  def __init__(self, html: str):
+    self._html = html
+
+  def _repr_html_(self) -> str:
+    return self._html
+
+  def _repr_(self) -> str:
+    return self._html
+
+
+def create_doc() -> Tuple[Doc, Elem]:
+  """Creates a html document.
+
+  Example:
+
+  ```python
+  doc, root = create_doc()
+  root.appendChild(bold(doc, "Some text"))
+  print(root.toxml())
+  ```
+
+  Returns:
+    A html document and its root node.
+  """
+
+  impl = minidom.getDOMImplementation()
+  assert impl is not None
+  doc = impl.createDocument(None, "div", None)
+  return doc, doc.documentElement
+
+
+def with_style(doc: Doc, item: Union[Elem, str], style: Dict[str, Any]) -> Elem:
+  """Creates html element with given style.
+
+  Example:
+
+  ```python
+  doc, root = create_doc()
+  root.appendChild(with_style(doc, "My text", {"font-weight": "bold"}))
+  print(root.toxml())
+  ```
+
+  Args:
+    doc: Html document.
+    item: Item to display. Can be a string or another Html element.
+    style: style.
+
+  Returns:
+    Html element.
+  """
+
+  if isinstance(item, minidom.Element):
+    raw_item = item
+    item = doc.createElement("span")
+    item.appendChild(raw_item)
+  elif isinstance(item, str):
+    raw_item = item
+    item = doc.createElement("span")
+    item.appendChild(doc.createTextNode(raw_item))
+  else:
+    raise ValueError(f"Non supported element {item}")
+
+  style_key = "style"
+
+  # Get existing style, if any.
+  style_text = (
+      item.getAttribute(style_key) + "; "
+      if item.hasAttribute(style_key)
+      else ""
+  )
+
+  # Add new style
+  style_text += "; ".join([f"{k}:{v}" for k, v in style.items()])
+  item.setAttribute("style", style_text)
+  return item
+
+
+def bold(doc: Doc, item: Union[Elem, str]) -> Elem:
+  return with_style(doc, item, {"font-weight": "bold"})
+
+
+def italic(doc: Doc, item: Union[Elem, str]) -> Elem:
+  return with_style(doc, item, {"font-style": "italic"})
+
+
+def link(doc: Doc, url: str) -> Elem:
+  """Creates a link (i.e., <a>) element."""
+
+  html_a = doc.createElement("a")
+  html_a.setAttribute("href", url)
+  html_a.setAttribute("target", "_blank")
+  return html_a
```

## ydf/utils/html_test.py

 * *Ordering differences only*

```diff
@@ -1,55 +1,55 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Test html utilities."""
-
-import textwrap
-
-from absl.testing import absltest
-
-from ydf.utils import html
-
-
-class HtmlTest(absltest.TestCase):
-
-  def test_base(self):
-    doc, root = html.create_doc()
-    root.appendChild(html.bold(doc, "Bold text"))
-    root.appendChild(html.italic(doc, "Italic text"))
-
-    div = doc.createElement("div")
-    div.appendChild(doc.createTextNode("div content"))
-    root.appendChild(html.bold(doc, div))
-
-    link1 = html.link(doc, "url")
-    link1.appendChild(doc.createTextNode("link"))
-    root.appendChild(link1)
-
-    self.assertEqual(
-        root.toprettyxml(indent="  "),
-        textwrap.dedent("""\
-        <div>
-          <span style="font-weight:bold">Bold text</span>
-          <span style="font-style:italic">Italic text</span>
-          <span style="font-weight:bold">
-            <div>div content</div>
-          </span>
-          <a href="url" target="_blank">link</a>
-        </div>
-        """),
-    )
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Test html utilities."""
+
+import textwrap
+
+from absl.testing import absltest
+
+from ydf.utils import html
+
+
+class HtmlTest(absltest.TestCase):
+
+  def test_base(self):
+    doc, root = html.create_doc()
+    root.appendChild(html.bold(doc, "Bold text"))
+    root.appendChild(html.italic(doc, "Italic text"))
+
+    div = doc.createElement("div")
+    div.appendChild(doc.createTextNode("div content"))
+    root.appendChild(html.bold(doc, div))
+
+    link1 = html.link(doc, "url")
+    link1.appendChild(doc.createTextNode("link"))
+    root.appendChild(link1)
+
+    self.assertEqual(
+        root.toprettyxml(indent="  "),
+        textwrap.dedent("""\
+        <div>
+          <span style="font-weight:bold">Bold text</span>
+          <span style="font-style:italic">Italic text</span>
+          <span style="font-weight:bold">
+            <div>div content</div>
+          </span>
+          <a href="url" target="_blank">link</a>
+        </div>
+        """),
+    )
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/utils/log.py

 * *Ordering differences only*

```diff
@@ -1,232 +1,232 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Uniform display and controls the logs displayed on the different surfaces.
-
-For the library developer:
-
-  Python user facing logs are printed using "log.info" and "log.warning".
-  Methods that can produce c++ logs should be wrapped in a CCLog context.
-
-For the user:
-
-  The logs are controlled globally with "ydf.verbose()".
-
-Compatibility
-
-  This code is tested with python, ipython, colab and jupyter notebook.
-"""
-
-import contextlib
-import enum
-import io
-import sys
-from typing import Any, Optional, Set
-
-from ydf.cc import ydf
-
-# Current verbose level. See "verbose" for details.
-_VERBOSE_LEVEL: int = 1
-
-# The strick mode prints more warning messages.
-_STRICT: bool = False
-
-
-@enum.unique
-class WarningMessage(enum.Enum):
-  """All possible warning messages.
-
-  Used to avoid showing warning messages multiple times.
-  """
-
-  CANNOT_SHOW_DETAILS_LOGS = 0
-  CAST_NUMERICAL_TO_FLOAT32 = 1
-
-
-# List of already showed warning message that should not be displayed again.
-_ALREADY_DISPLAYED_WARNING_IDS: Set[WarningMessage] = set()
-
-
-def verbose(level: int = 2) -> int:
-  """Sets the verbose level of YDF.
-
-  The verbose levels are:
-    0: Print no logs.
-    1: Print a few logs in a colab or notebook cell. Print all the logs in the
-        console. This is the default verbose level.
-    2: Prints all the logs on all surfaces.
-
-  Usage example:
-
-  ```python
-  import ydf
-
-  save_verbose = ydf.verbose(0)  # Hide all logs
-  learner = ydf.RandomForestLearner(label="label")
-  model = learner.train(pd.DataFrame({"feature": [0, 1], "label": [0, 1]}))
-  ydf.verbose(save_verbose)  # Restore verbose level
-  ```
-
-  Args:
-    level: New verbose level.
-
-  Returns:
-    The previous verbose level.
-  """
-  global _VERBOSE_LEVEL
-  old = _VERBOSE_LEVEL
-  _VERBOSE_LEVEL = level
-  return old
-
-
-def current_log_level() -> int:
-  """Returns the log level currently set."""
-  return _VERBOSE_LEVEL
-
-
-def info(msg: str, *args: Any) -> None:
-  """Print an info message visible when verbose >=1.
-
-  Usage example:
-    info("Hello %s", "world")
-
-  Args:
-    msg: String message with replacement placeholders e.g. %s.
-    *args: Placeholder replacement values.
-  """
-
-  if _VERBOSE_LEVEL >= 1:
-    print(msg % args, flush=True)
-
-
-def warning(
-    msg: str,
-    *args: Any,
-    message_id: Optional[WarningMessage] = None,
-    is_strict: bool = False
-) -> None:
-  """Print a warning message.
-
-  A warning message is similar to an info message, except that:
-  - There is a "Warning:" prefix.
-  - When displaying multiple warning messages with the same "message_id", only
-    the first one will be displayed.
-
-  Usage example:
-    warning("Hello %s", "world")
-
-  Args:
-    msg: String message with replacement placeholders e.g. %s.
-    *args: Placeholder replacement values.
-    message_id: Id of the warning message. If set, the message is only displayed
-      once.
-    is_strict: If true, the warning message is only disabled if the strict mode
-      is enabled.
-  """
-
-  if is_strict and not _STRICT:
-    return
-
-  if message_id is not None:
-    if message_id in _ALREADY_DISPLAYED_WARNING_IDS:
-      return
-    _ALREADY_DISPLAYED_WARNING_IDS.add(message_id)
-
-  if _VERBOSE_LEVEL >= 1:
-    print("Warning:", msg % args, flush=True, file=sys.stderr)
-
-
-def strict(value: bool = True) -> None:
-  """Sets the strict mode.
-
-  When strict mode is enabled, more warnings are displayed.
-
-
-  Args:
-    value: New value for the strict mode.
-  """
-
-  global _STRICT
-  _STRICT = value
-
-
-def is_direct_output(stream=sys.stdout):
-  """Checks if output stream redirects to the shell/console directly."""
-
-  if stream.isatty():
-    return True
-  if isinstance(stream, io.TextIOWrapper):
-    return is_direct_output(stream.buffer)
-  if isinstance(stream, io.BufferedWriter):
-    return is_direct_output(stream.raw)
-  if isinstance(stream, io.FileIO):
-    return stream.fileno() in [1, 2]
-  return False
-
-
-@contextlib.contextmanager
-def _no_op_context():
-  """Does nothing."""
-  yield
-
-
-@contextlib.contextmanager
-def _hide_cc_logs():
-  """Hide the CC logs in public build."""
-  ydf.SetLoggingLevel(0, False)
-  try:
-    yield
-  finally:
-    ydf.SetLoggingLevel(2, True)
-
-
-def cc_log_context():
-  """Creates a context to display correctly C++ logs to the user."""
-
-  if _VERBOSE_LEVEL == 0:
-    return _hide_cc_logs()
-
-  elif _VERBOSE_LEVEL == 1:
-    # Only show CC logs in the console, but not in colab / notebook cells
-
-    if is_direct_output():
-      return _no_op_context()
-
-    # Hide logs if in notebook. Logs are already hidden in colabs.
-    return _hide_cc_logs()
-
-  else:
-    # Show CC logs everywhere
-
-    # pylint: disable=g-import-not-at-top
-    try:
-      from colabtools.googlelog import CaptureLog  # pytype: disable=import-error
-      # This is a Google Colab
-      return CaptureLog()
-    except ImportError:
-      # Wurlitzer hangs when logs are shown directly.
-      if is_direct_output():
-        return _no_op_context()
-      try:
-        from wurlitzer import sys_pipes  # pytype: disable=import-error
-
-        return sys_pipes()
-      except ImportError:
-        warning(
-            "ydf.verbose(2) but logs cannot be displayed in the cell. Check"
-            " colab logs or install wurlitzer with 'pip install wurlitzer'",
-            message_id=WarningMessage.CANNOT_SHOW_DETAILS_LOGS,
-        )
-      return _no_op_context()
-    # pylint: enable=g-import-not-at-top
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Uniform display and controls the logs displayed on the different surfaces.
+
+For the library developer:
+
+  Python user facing logs are printed using "log.info" and "log.warning".
+  Methods that can produce c++ logs should be wrapped in a CCLog context.
+
+For the user:
+
+  The logs are controlled globally with "ydf.verbose()".
+
+Compatibility
+
+  This code is tested with python, ipython, colab and jupyter notebook.
+"""
+
+import contextlib
+import enum
+import io
+import sys
+from typing import Any, Optional, Set
+
+from ydf.cc import ydf
+
+# Current verbose level. See "verbose" for details.
+_VERBOSE_LEVEL: int = 1
+
+# The strick mode prints more warning messages.
+_STRICT: bool = False
+
+
+@enum.unique
+class WarningMessage(enum.Enum):
+  """All possible warning messages.
+
+  Used to avoid showing warning messages multiple times.
+  """
+
+  CANNOT_SHOW_DETAILS_LOGS = 0
+  CAST_NUMERICAL_TO_FLOAT32 = 1
+
+
+# List of already showed warning message that should not be displayed again.
+_ALREADY_DISPLAYED_WARNING_IDS: Set[WarningMessage] = set()
+
+
+def verbose(level: int = 2) -> int:
+  """Sets the verbose level of YDF.
+
+  The verbose levels are:
+    0: Print no logs.
+    1: Print a few logs in a colab or notebook cell. Print all the logs in the
+        console. This is the default verbose level.
+    2: Prints all the logs on all surfaces.
+
+  Usage example:
+
+  ```python
+  import ydf
+
+  save_verbose = ydf.verbose(0)  # Hide all logs
+  learner = ydf.RandomForestLearner(label="label")
+  model = learner.train(pd.DataFrame({"feature": [0, 1], "label": [0, 1]}))
+  ydf.verbose(save_verbose)  # Restore verbose level
+  ```
+
+  Args:
+    level: New verbose level.
+
+  Returns:
+    The previous verbose level.
+  """
+  global _VERBOSE_LEVEL
+  old = _VERBOSE_LEVEL
+  _VERBOSE_LEVEL = level
+  return old
+
+
+def current_log_level() -> int:
+  """Returns the log level currently set."""
+  return _VERBOSE_LEVEL
+
+
+def info(msg: str, *args: Any) -> None:
+  """Print an info message visible when verbose >=1.
+
+  Usage example:
+    info("Hello %s", "world")
+
+  Args:
+    msg: String message with replacement placeholders e.g. %s.
+    *args: Placeholder replacement values.
+  """
+
+  if _VERBOSE_LEVEL >= 1:
+    print(msg % args, flush=True)
+
+
+def warning(
+    msg: str,
+    *args: Any,
+    message_id: Optional[WarningMessage] = None,
+    is_strict: bool = False
+) -> None:
+  """Print a warning message.
+
+  A warning message is similar to an info message, except that:
+  - There is a "Warning:" prefix.
+  - When displaying multiple warning messages with the same "message_id", only
+    the first one will be displayed.
+
+  Usage example:
+    warning("Hello %s", "world")
+
+  Args:
+    msg: String message with replacement placeholders e.g. %s.
+    *args: Placeholder replacement values.
+    message_id: Id of the warning message. If set, the message is only displayed
+      once.
+    is_strict: If true, the warning message is only disabled if the strict mode
+      is enabled.
+  """
+
+  if is_strict and not _STRICT:
+    return
+
+  if message_id is not None:
+    if message_id in _ALREADY_DISPLAYED_WARNING_IDS:
+      return
+    _ALREADY_DISPLAYED_WARNING_IDS.add(message_id)
+
+  if _VERBOSE_LEVEL >= 1:
+    print("Warning:", msg % args, flush=True, file=sys.stderr)
+
+
+def strict(value: bool = True) -> None:
+  """Sets the strict mode.
+
+  When strict mode is enabled, more warnings are displayed.
+
+
+  Args:
+    value: New value for the strict mode.
+  """
+
+  global _STRICT
+  _STRICT = value
+
+
+def is_direct_output(stream=sys.stdout):
+  """Checks if output stream redirects to the shell/console directly."""
+
+  if stream.isatty():
+    return True
+  if isinstance(stream, io.TextIOWrapper):
+    return is_direct_output(stream.buffer)
+  if isinstance(stream, io.BufferedWriter):
+    return is_direct_output(stream.raw)
+  if isinstance(stream, io.FileIO):
+    return stream.fileno() in [1, 2]
+  return False
+
+
+@contextlib.contextmanager
+def _no_op_context():
+  """Does nothing."""
+  yield
+
+
+@contextlib.contextmanager
+def _hide_cc_logs():
+  """Hide the CC logs in public build."""
+  ydf.SetLoggingLevel(0, False)
+  try:
+    yield
+  finally:
+    ydf.SetLoggingLevel(2, True)
+
+
+def cc_log_context():
+  """Creates a context to display correctly C++ logs to the user."""
+
+  if _VERBOSE_LEVEL == 0:
+    return _hide_cc_logs()
+
+  elif _VERBOSE_LEVEL == 1:
+    # Only show CC logs in the console, but not in colab / notebook cells
+
+    if is_direct_output():
+      return _no_op_context()
+
+    # Hide logs if in notebook. Logs are already hidden in colabs.
+    return _hide_cc_logs()
+
+  else:
+    # Show CC logs everywhere
+
+    # pylint: disable=g-import-not-at-top
+    try:
+      from colabtools.googlelog import CaptureLog  # pytype: disable=import-error
+      # This is a Google Colab
+      return CaptureLog()
+    except ImportError:
+      # Wurlitzer hangs when logs are shown directly.
+      if is_direct_output():
+        return _no_op_context()
+      try:
+        from wurlitzer import sys_pipes  # pytype: disable=import-error
+
+        return sys_pipes()
+      except ImportError:
+        warning(
+            "ydf.verbose(2) but logs cannot be displayed in the cell. Check"
+            " colab logs or install wurlitzer with 'pip install wurlitzer'",
+            message_id=WarningMessage.CANNOT_SHOW_DETAILS_LOGS,
+        )
+      return _no_op_context()
+    # pylint: enable=g-import-not-at-top
```

## ydf/utils/paths.py

 * *Ordering differences only*

```diff
@@ -1,50 +1,50 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Utils for dealing with paths."""
-
-from typing import List
-
-
-def normalize_list_of_paths(paths: List[str]) -> str:
-  """Formats a list of typed paths for consumption by YDFs reader.
-
-  YDF expects a comma-separated list of untyped paths prefixed with the type.
-
-  Args:
-    paths: List of paths
-
-  Returns:
-    A typed string of the paths as comma-separated items.
-  Raises:
-    ValueError: The list is empty, or the paths in the listed are not all typed
-    with the same type.
-  """
-  if not paths:
-    raise ValueError("The list of paths to the dataset may not be empty")
-  split_first_path = paths[0].split(":", maxsplit=1)
-  if len(split_first_path) == 1:
-    raise ValueError(
-        "All typed paths to the dataset be typed with the same type, e.g.,"
-        " ['csv:/path/file1', 'csv:/path/file2']"
-    )
-  else:
-    # The first path is typed, remove types from all paths.
-    prefix = split_first_path[0] + ":"
-    if not all(path.startswith(prefix) for path in paths):
-      raise ValueError(
-          "All typed paths to the dataset should have the same type, e.g.,"
-          " ['csv:/path/file1', 'csv:/path/file2']"
-      )
-    return prefix + ",".join([path[len(prefix) :] for path in paths])
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Utils for dealing with paths."""
+
+from typing import List
+
+
+def normalize_list_of_paths(paths: List[str]) -> str:
+  """Formats a list of typed paths for consumption by YDFs reader.
+
+  YDF expects a comma-separated list of untyped paths prefixed with the type.
+
+  Args:
+    paths: List of paths
+
+  Returns:
+    A typed string of the paths as comma-separated items.
+  Raises:
+    ValueError: The list is empty, or the paths in the listed are not all typed
+    with the same type.
+  """
+  if not paths:
+    raise ValueError("The list of paths to the dataset may not be empty")
+  split_first_path = paths[0].split(":", maxsplit=1)
+  if len(split_first_path) == 1:
+    raise ValueError(
+        "All typed paths to the dataset be typed with the same type, e.g.,"
+        " ['csv:/path/file1', 'csv:/path/file2']"
+    )
+  else:
+    # The first path is typed, remove types from all paths.
+    prefix = split_first_path[0] + ":"
+    if not all(path.startswith(prefix) for path in paths):
+      raise ValueError(
+          "All typed paths to the dataset should have the same type, e.g.,"
+          " ['csv:/path/file1', 'csv:/path/file2']"
+      )
+    return prefix + ",".join([path[len(prefix) :] for path in paths])
```

## ydf/utils/paths_test.py

 * *Ordering differences only*

```diff
@@ -1,51 +1,51 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from absl.testing import absltest
-
-from ydf.utils import paths
-
-
-class PathsTest(absltest.TestCase):
-
-  def test_normalize_list_of_paths_all_paths_typed(self):
-    all_paths_typed = ["csv:/foo/bar", "csv:bar/foo", "csv:asdf"]
-    self.assertEqual(
-        paths.normalize_list_of_paths(all_paths_typed),
-        "csv:/foo/bar,bar/foo,asdf",
-    )
-
-  def test_normalize_list_of_paths_fails_with_different_types(self):
-    all_paths_typed_different_types = ["csv:/foo/bar", "prefix:/bar/foo"]
-    with self.assertRaises(ValueError):
-      _ = paths.normalize_list_of_paths(all_paths_typed_different_types)
-
-  def test_normalize_list_of_paths_fails_with_some_missing_types(self):
-    not_all_paths_typed = ["csv:/foo/bar", "bar.txt"]
-    with self.assertRaises(ValueError):
-      _ = paths.normalize_list_of_paths(not_all_paths_typed)
-
-  def test_normalize_list_of_paths_fails_with_all_missing_types(self):
-    not_all_paths_typed = ["/foo/bar", "bar.txt"]
-    with self.assertRaises(ValueError):
-      _ = paths.normalize_list_of_paths(not_all_paths_typed)
-
-  def test_normalize_list_of_paths_fails_with_empty_list(self):
-    empty_list = []
-    with self.assertRaises(ValueError):
-      paths.normalize_list_of_paths(empty_list)
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from absl.testing import absltest
+
+from ydf.utils import paths
+
+
+class PathsTest(absltest.TestCase):
+
+  def test_normalize_list_of_paths_all_paths_typed(self):
+    all_paths_typed = ["csv:/foo/bar", "csv:bar/foo", "csv:asdf"]
+    self.assertEqual(
+        paths.normalize_list_of_paths(all_paths_typed),
+        "csv:/foo/bar,bar/foo,asdf",
+    )
+
+  def test_normalize_list_of_paths_fails_with_different_types(self):
+    all_paths_typed_different_types = ["csv:/foo/bar", "prefix:/bar/foo"]
+    with self.assertRaises(ValueError):
+      _ = paths.normalize_list_of_paths(all_paths_typed_different_types)
+
+  def test_normalize_list_of_paths_fails_with_some_missing_types(self):
+    not_all_paths_typed = ["csv:/foo/bar", "bar.txt"]
+    with self.assertRaises(ValueError):
+      _ = paths.normalize_list_of_paths(not_all_paths_typed)
+
+  def test_normalize_list_of_paths_fails_with_all_missing_types(self):
+    not_all_paths_typed = ["/foo/bar", "bar.txt"]
+    with self.assertRaises(ValueError):
+      _ = paths.normalize_list_of_paths(not_all_paths_typed)
+
+  def test_normalize_list_of_paths_fails_with_empty_list(self):
+    empty_list = []
+    with self.assertRaises(ValueError):
+      paths.normalize_list_of_paths(empty_list)
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/utils/string_lib.py

 * *Ordering differences only*

```diff
@@ -1,151 +1,151 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""String utilities."""
-
-
-from typing import Any, List, Optional, Sequence
-
-
-def indent(text: str, num_spaces: int = 4) -> str:
-  r"""Indents a possibly multi-line string.
-
-  Example:
-
-  ```python
-    indent("Hello\nWorld\n")
-    >> "    Hello\n    World\n"
-  ```
-
-  Args:
-    text: String to indent.
-    num_spaces: Number of spaces of indentation.
-
-  Returns:
-    Intended string.
-  """
-
-  # TODO: @gbm - Replace with "textwrap.indent".
-  block = " " * num_spaces
-  return block + block.join(text.splitlines(keepends=True))
-
-
-def table(
-    content: Sequence[Sequence[Any]],
-    row_labels: Optional[Sequence[str]] = None,
-    column_labels: Optional[Sequence[str]] = None,
-) -> str:
-  """Returns a string representation of the table.
-
-  Example:
-
-  ```python
-    table(
-      content=[["a", "b"],
-              [5.12345678, 7.0]],
-      column_labels=["X", "Y"],
-      row_labels=["A", "B"],
-      )
-
-  >> +------------+------------+------------+
-  >> |            |          X |          Y |
-  >> +------------+------------+------------+
-  >> |          A |          a |          b |
-  >> +------------+------------+------------+
-  >> |          B | 5.12345678 |          7 |
-  >> +------------+------------+------------+
-  ```
-
-  Floating point without decimals are printed without the final dot. For
-  example 5.0 is printed as 5.
-
-  The table cannot be empty i.e., there should be at least one row and one
-  column.
-
-  The string representation of table elements should not include line breaks.
-
-  Args:
-    content: Content of the table. `content[i][j]` is the cell value at the i-th
-      row and j-th column.
-    row_labels: Row names. If set, `content` should have `len(row_labels)` rows.
-    column_labels: Column names. If set, `content` should have
-      `len(column_labels)` columns.
-  """
-
-  if not content or not content[0]:
-    raise ValueError("Content is empty")
-
-  num_columns = len(content[0])
-  num_rows = len(content)
-
-  if any(num_columns != len(row) for row in content):
-    raise ValueError("All rows should have the same number of values")
-
-  def format_cell(cell: Any) -> str:
-    if isinstance(cell, float):
-      if round(cell) == cell:
-        # Print 7.0 as "7" instead of "7.0".
-        str_cell = str(int(cell))
-      else:
-        str_cell = f"{cell:g}"
-    else:
-      str_cell = str(cell)
-
-    if "\n" in str_cell:
-      raise ValueError(f"Cannot print table with multi-line values: {str_cell}")
-    return str_cell
-
-  # Enforce preconditions (all rows with the same size, rows and column labels
-  # with the expected sizes, no newlines, etc.)
-  # From here on out, there's no more error-checking logic.
-
-  str_content = []
-  if column_labels is not None:
-    if len(column_labels) != num_columns:
-      raise ValueError("`column_labels` inconsistent with `content`")
-    row_label = [""] if row_labels is not None else []
-    str_content.append(row_label + list(column_labels))
-
-  if row_labels is not None:
-    if len(row_labels) != num_rows:
-      raise ValueError("`row_labels` inconsistent with `content`")
-    num_columns += 1
-
-  for row_idx, row in enumerate(content):
-    row_label = [row_labels[row_idx]] if row_labels is not None else []
-    str_content.append(row_label + [format_cell(cell) for cell in row])
-
-  max_length = 0
-  for col_idx in range(num_columns):
-    max_length = max(max_length, max(len(row[col_idx]) for row in str_content))
-
-  # One space before and after each cell content.
-  cell_length = max_length + 2
-
-  vertical_separator = "|"
-  horizontal_separator = "-"
-  dot_separator = "+"
-
-  row_separator = (
-      (dot_separator + horizontal_separator * cell_length) * num_columns
-      + dot_separator
-      + "\n"
-  )
-
-  output = [row_separator]
-  for row in str_content:
-    for col in row:
-      output.append(f"{vertical_separator}{col.rjust(cell_length - 1)} ")
-    output.append(f"{vertical_separator}\n{row_separator}")
-  return "".join(output)
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""String utilities."""
+
+
+from typing import Any, List, Optional, Sequence
+
+
+def indent(text: str, num_spaces: int = 4) -> str:
+  r"""Indents a possibly multi-line string.
+
+  Example:
+
+  ```python
+    indent("Hello\nWorld\n")
+    >> "    Hello\n    World\n"
+  ```
+
+  Args:
+    text: String to indent.
+    num_spaces: Number of spaces of indentation.
+
+  Returns:
+    Intended string.
+  """
+
+  # TODO: @gbm - Replace with "textwrap.indent".
+  block = " " * num_spaces
+  return block + block.join(text.splitlines(keepends=True))
+
+
+def table(
+    content: Sequence[Sequence[Any]],
+    row_labels: Optional[Sequence[str]] = None,
+    column_labels: Optional[Sequence[str]] = None,
+) -> str:
+  """Returns a string representation of the table.
+
+  Example:
+
+  ```python
+    table(
+      content=[["a", "b"],
+              [5.12345678, 7.0]],
+      column_labels=["X", "Y"],
+      row_labels=["A", "B"],
+      )
+
+  >> +------------+------------+------------+
+  >> |            |          X |          Y |
+  >> +------------+------------+------------+
+  >> |          A |          a |          b |
+  >> +------------+------------+------------+
+  >> |          B | 5.12345678 |          7 |
+  >> +------------+------------+------------+
+  ```
+
+  Floating point without decimals are printed without the final dot. For
+  example 5.0 is printed as 5.
+
+  The table cannot be empty i.e., there should be at least one row and one
+  column.
+
+  The string representation of table elements should not include line breaks.
+
+  Args:
+    content: Content of the table. `content[i][j]` is the cell value at the i-th
+      row and j-th column.
+    row_labels: Row names. If set, `content` should have `len(row_labels)` rows.
+    column_labels: Column names. If set, `content` should have
+      `len(column_labels)` columns.
+  """
+
+  if not content or not content[0]:
+    raise ValueError("Content is empty")
+
+  num_columns = len(content[0])
+  num_rows = len(content)
+
+  if any(num_columns != len(row) for row in content):
+    raise ValueError("All rows should have the same number of values")
+
+  def format_cell(cell: Any) -> str:
+    if isinstance(cell, float):
+      if round(cell) == cell:
+        # Print 7.0 as "7" instead of "7.0".
+        str_cell = str(int(cell))
+      else:
+        str_cell = f"{cell:g}"
+    else:
+      str_cell = str(cell)
+
+    if "\n" in str_cell:
+      raise ValueError(f"Cannot print table with multi-line values: {str_cell}")
+    return str_cell
+
+  # Enforce preconditions (all rows with the same size, rows and column labels
+  # with the expected sizes, no newlines, etc.)
+  # From here on out, there's no more error-checking logic.
+
+  str_content = []
+  if column_labels is not None:
+    if len(column_labels) != num_columns:
+      raise ValueError("`column_labels` inconsistent with `content`")
+    row_label = [""] if row_labels is not None else []
+    str_content.append(row_label + list(column_labels))
+
+  if row_labels is not None:
+    if len(row_labels) != num_rows:
+      raise ValueError("`row_labels` inconsistent with `content`")
+    num_columns += 1
+
+  for row_idx, row in enumerate(content):
+    row_label = [row_labels[row_idx]] if row_labels is not None else []
+    str_content.append(row_label + [format_cell(cell) for cell in row])
+
+  max_length = 0
+  for col_idx in range(num_columns):
+    max_length = max(max_length, max(len(row[col_idx]) for row in str_content))
+
+  # One space before and after each cell content.
+  cell_length = max_length + 2
+
+  vertical_separator = "|"
+  horizontal_separator = "-"
+  dot_separator = "+"
+
+  row_separator = (
+      (dot_separator + horizontal_separator * cell_length) * num_columns
+      + dot_separator
+      + "\n"
+  )
+
+  output = [row_separator]
+  for row in str_content:
+    for col in row:
+      output.append(f"{vertical_separator}{col.rjust(cell_length - 1)} ")
+    output.append(f"{vertical_separator}\n{row_separator}")
+  return "".join(output)
```

## ydf/utils/string_lib_test.py

 * *Ordering differences only*

```diff
@@ -1,133 +1,133 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Test string utilities."""
-
-import textwrap
-
-from absl.testing import absltest
-
-from ydf.utils import string_lib
-
-
-class StringTest(absltest.TestCase):
-
-  def test_indent(self):
-    self.assertEqual(string_lib.indent(""), "    ")
-    self.assertEqual(string_lib.indent("hello"), "    hello")
-    self.assertEqual(string_lib.indent("hello\n"), "    hello\n")
-    self.assertEqual(string_lib.indent("hello\nworld"), "    hello\n    world")
-    self.assertEqual(
-        string_lib.indent("hello\nworld\n"), "    hello\n    world\n"
-    )
-
-  def test_table(self):
-    self.assertEqual(
-        string_lib.table(
-            content=[["a", "b"], [5.12345678, 7.0]],
-        ),
-        textwrap.dedent("""\
-        +---------+---------+
-        |       a |       b |
-        +---------+---------+
-        | 5.12346 |       7 |
-        +---------+---------+
-        """),
-    )
-
-  def test_table_with_col(self):
-    self.assertEqual(
-        string_lib.table(
-            content=[["a", "b"], [5.12345678, 7.0]],
-            column_labels=["X", "Y"],
-        ),
-        textwrap.dedent("""\
-        +---------+---------+
-        |       X |       Y |
-        +---------+---------+
-        |       a |       b |
-        +---------+---------+
-        | 5.12346 |       7 |
-        +---------+---------+
-        """),
-    )
-
-  def test_table_with_row(self):
-    self.assertEqual(
-        string_lib.table(
-            content=[["a", "b"], [5.12345678, 7.0]],
-            row_labels=["A", "B"],
-        ),
-        textwrap.dedent("""\
-        +---------+---------+---------+
-        |       A |       a |       b |
-        +---------+---------+---------+
-        |       B | 5.12346 |       7 |
-        +---------+---------+---------+
-        """),
-    )
-
-  def test_table_with_col_and_row(self):
-    self.assertEqual(
-        string_lib.table(
-            content=[["a", "b"], [5.12345678, 7.0]],
-            column_labels=["X", "Y"],
-            row_labels=["A", "B"],
-        ),
-        textwrap.dedent("""\
-        +---------+---------+---------+
-        |         |       X |       Y |
-        +---------+---------+---------+
-        |       A |       a |       b |
-        +---------+---------+---------+
-        |       B | 5.12346 |       7 |
-        +---------+---------+---------+
-        """),
-    )
-
-  def test_table_issue_empty_content(self):
-    with self.assertRaisesRegex(ValueError, "Content is empty"):
-      string_lib.table(content=[])
-
-  def test_table_issue_empty_row(self):
-    with self.assertRaisesRegex(ValueError, "Content is empty"):
-      string_lib.table(content=[[]])
-
-  def test_table_issue_inconsistent_content(self):
-    with self.assertRaisesRegex(
-        ValueError, "All rows should have the same number of values"
-    ):
-      string_lib.table(content=[[1], [2, 3]])
-
-  def test_table_issue_inconsistent_col(self):
-    with self.assertRaisesRegex(
-        ValueError, "`column_labels` inconsistent with `content`"
-    ):
-      string_lib.table(content=[[1], [2]], column_labels=["A", "B"])
-
-  def test_table_issue_inconsistent_row(self):
-    with self.assertRaisesRegex(
-        ValueError, "`row_labels` inconsistent with `content`"
-    ):
-      string_lib.table(content=[[1], [2]], row_labels=["A"])
-
-  def test_table_issue_multiline(self):
-    with self.assertRaisesRegex(
-        ValueError, "Cannot print table with multi-line values"
-    ):
-      string_lib.table(content=[["a\nb"]], row_labels=["A"])
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Test string utilities."""
+
+import textwrap
+
+from absl.testing import absltest
+
+from ydf.utils import string_lib
+
+
+class StringTest(absltest.TestCase):
+
+  def test_indent(self):
+    self.assertEqual(string_lib.indent(""), "    ")
+    self.assertEqual(string_lib.indent("hello"), "    hello")
+    self.assertEqual(string_lib.indent("hello\n"), "    hello\n")
+    self.assertEqual(string_lib.indent("hello\nworld"), "    hello\n    world")
+    self.assertEqual(
+        string_lib.indent("hello\nworld\n"), "    hello\n    world\n"
+    )
+
+  def test_table(self):
+    self.assertEqual(
+        string_lib.table(
+            content=[["a", "b"], [5.12345678, 7.0]],
+        ),
+        textwrap.dedent("""\
+        +---------+---------+
+        |       a |       b |
+        +---------+---------+
+        | 5.12346 |       7 |
+        +---------+---------+
+        """),
+    )
+
+  def test_table_with_col(self):
+    self.assertEqual(
+        string_lib.table(
+            content=[["a", "b"], [5.12345678, 7.0]],
+            column_labels=["X", "Y"],
+        ),
+        textwrap.dedent("""\
+        +---------+---------+
+        |       X |       Y |
+        +---------+---------+
+        |       a |       b |
+        +---------+---------+
+        | 5.12346 |       7 |
+        +---------+---------+
+        """),
+    )
+
+  def test_table_with_row(self):
+    self.assertEqual(
+        string_lib.table(
+            content=[["a", "b"], [5.12345678, 7.0]],
+            row_labels=["A", "B"],
+        ),
+        textwrap.dedent("""\
+        +---------+---------+---------+
+        |       A |       a |       b |
+        +---------+---------+---------+
+        |       B | 5.12346 |       7 |
+        +---------+---------+---------+
+        """),
+    )
+
+  def test_table_with_col_and_row(self):
+    self.assertEqual(
+        string_lib.table(
+            content=[["a", "b"], [5.12345678, 7.0]],
+            column_labels=["X", "Y"],
+            row_labels=["A", "B"],
+        ),
+        textwrap.dedent("""\
+        +---------+---------+---------+
+        |         |       X |       Y |
+        +---------+---------+---------+
+        |       A |       a |       b |
+        +---------+---------+---------+
+        |       B | 5.12346 |       7 |
+        +---------+---------+---------+
+        """),
+    )
+
+  def test_table_issue_empty_content(self):
+    with self.assertRaisesRegex(ValueError, "Content is empty"):
+      string_lib.table(content=[])
+
+  def test_table_issue_empty_row(self):
+    with self.assertRaisesRegex(ValueError, "Content is empty"):
+      string_lib.table(content=[[]])
+
+  def test_table_issue_inconsistent_content(self):
+    with self.assertRaisesRegex(
+        ValueError, "All rows should have the same number of values"
+    ):
+      string_lib.table(content=[[1], [2, 3]])
+
+  def test_table_issue_inconsistent_col(self):
+    with self.assertRaisesRegex(
+        ValueError, "`column_labels` inconsistent with `content`"
+    ):
+      string_lib.table(content=[[1], [2]], column_labels=["A", "B"])
+
+  def test_table_issue_inconsistent_row(self):
+    with self.assertRaisesRegex(
+        ValueError, "`row_labels` inconsistent with `content`"
+    ):
+      string_lib.table(content=[[1], [2]], row_labels=["A"])
+
+  def test_table_issue_multiline(self):
+    with self.assertRaisesRegex(
+        ValueError, "Cannot print table with multi-line values"
+    ):
+      string_lib.table(content=[["a\nb"]], row_labels=["A"])
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## ydf/utils/test_utils.py

 * *Ordering differences only*

```diff
@@ -1,209 +1,209 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-"""Utilities for unit tests."""
-
-import dataclasses
-import logging
-import os
-import pathlib
-from typing import Optional, Sequence
-
-from absl import flags
-from absl.testing import absltest
-import numpy as np
-import pandas as pd
-
-from ydf.dataset import dataset
-from ydf.dataset import dataspec
-
-
-def data_root_path() -> str:
-  """Root directory of the repo."""
-  return ""
-
-def assertProto2Equal(self: absltest.TestCase, a, b):
-  """Checks that protos "a" and "b" are equal."""
-  self.assertEqual(a, b)
-
-def pydf_test_data_path() -> str:
-  return os.path.join(data_root_path(), "test_data")
-
-
-@dataclasses.dataclass(frozen=True)
-class TrainAndTestDataset:
-  """Training / test dataset as path, VerticalDataset and DataFrame."""
-
-  train_path: str
-  test_path: str
-  train_pd: pd.DataFrame
-  test_pd: pd.DataFrame
-  train: dataset.VerticalDataset
-  test: dataset.VerticalDataset
-
-
-def ydf_test_data_path() -> str:
-  return os.path.join(
-      data_root_path(),
-      "external/ydf_cc/yggdrasil_decision_forests/test_data",
-  )
-
-
-def ydf_test_data_pathlib() -> pathlib.Path:
-  return (
-      pathlib.Path(data_root_path())
-      / "external/ydf_cc/yggdrasil_decision_forests/test_data"
-  )
-
-
-def load_datasets(
-    name: str, column_args: Optional[Sequence[dataspec.Column]] = None
-) -> TrainAndTestDataset:
-  """Returns the given dataset loaded as different formats."""
-  train_path = os.path.join(
-      ydf_test_data_path(), "dataset", f"{name}_train.csv"
-  )
-  test_path = os.path.join(ydf_test_data_path(), "dataset", f"{name}_test.csv")
-  train_pd = pd.read_csv(train_path)
-  test_pd = pd.read_csv(test_path)
-  train_vds = dataset.create_vertical_dataset(
-      train_pd, columns=column_args, include_all_columns=True
-  )
-  test_vds = dataset.create_vertical_dataset(
-      test_pd, data_spec=train_vds.data_spec()
-  )
-  return TrainAndTestDataset(
-      train_path, test_path, train_pd, test_pd, train_vds, test_vds
-  )
-
-
-def toy_dataset():
-  df = pd.DataFrame({
-      "col_three_string": ["A", "A", "B", "B", "C"],
-      "col_float": [1, 2.1, 1.3, 5.5, 2.4],
-      "col_two_string": ["bar", "foo", "foo", "foo", "foo"],
-      "weights": [3, 2, 3.1, 28, 3],
-      "binary_int_label": [0, 0, 0, 1, 1],
-  })
-  return df
-
-
-def toy_dataset_uplift():
-  df = pd.DataFrame({
-      "f1": [1, 2, 3, 4] * 10,
-      "treatement": ["A", "A", "B", "B"] * 10,
-      "effect_binary": [0, 1, 1, 1] * 10,
-      "effect_numerical": [0.1, 0.5, 0.6, 0.7] * 10,
-  })
-  return df
-
-
-# Exception raised in python when the c++ raises an invalid argument error
-AbslInvalidArgumentError = (ValueError, RuntimeError)
-
-
-def golden_check_string(
-    test, value: str, golden_path: str, postfix: str = ""
-) -> None:
-  """Ensures that "value" is equal to the content of the file "golden_path".
-
-  Args:
-    test: A test.
-    value: Value to test.
-    golden_path: Path to golden file expressed from the root of the repo.
-    postfix: Optional postfix to the path of the file containing the actual
-      value.
-  """
-
-  golden_data = open(os.path.join(data_root_path(), golden_path)).read()
-
-  if value != golden_data:
-    value_path = os.path.join(
-        absltest.TEST_TMPDIR.value, os.path.basename(golden_path) + postfix
-    )
-    logging.info("os.path.dirname(value_path): %s", os.path.dirname(value_path))
-    os.makedirs(os.path.dirname(value_path), exist_ok=True)
-    logging.info(
-        "Golden test failed. Save the effetive value to %s", value_path
-    )
-    with open(value_path, "w") as f:
-      f.write(value)
-
-  test.assertEqual(value, golden_data)
-
-
-def assert_almost_equal(a, b):
-  msg = test_almost_equal(a, b)
-  if msg is not None:
-    raise AssertionError(msg)
-
-
-def test_almost_equal(a, b) -> Optional[str]:
-  """Checks that two dataclasses are almost equal.
-
-  Unlike "assert_almost_equal_dataclasses" and "self.assertEqual", this method
-  supports dataclasses containing numpy arrays.
-
-  Args:
-    a: First item to compare.
-    b: Second item to compare.
-
-  Returns:
-    None if "a" and "b" are equal. A description of the different otherwise.
-  """
-
-  if type(a) != type(b):
-    return f"Type mismatch: {type(a)} != {type(b)}"
-
-  elif dataclasses.is_dataclass(a):
-    sub_msg = test_almost_equal(dataclasses.asdict(a), dataclasses.asdict(b))
-    if sub_msg is not None:
-      return sub_msg
-
-  elif isinstance(a, np.ndarray):
-    if a.dtype != b.dtype:
-      return f"numpy array type mismatch: {a} != {b}"
-
-    if a.dtype.type in [np.string_, np.bytes_, np.str_]:
-      if not np.equal(a, b).all():
-        return f"numpy array mismatch: {a} != {b}"
-    else:
-      if not np.allclose(a, b):
-        return f"numpy array mismatch: {a} != {b}"
-
-  elif isinstance(a, (bool, str, bytes, int, float, type(None))):
-    if a != b:
-      return f"primitive mismatch: {a} != {b}"
-
-  elif isinstance(a, list):
-    if len(a) != len(b):
-      return f"list len mismatch: {len(a)} != {len(b)}"
-
-    for sa, sb in zip(a, b):
-      sub_msg = test_almost_equal(sa, sb)
-      if sub_msg is not None:
-        return sub_msg
-
-  elif isinstance(a, dict):
-    if set(a) != set(b):
-      return f"dict field mismatch: {set(a)} != {set(b)}"
-    for field in a:
-      sub_msg = test_almost_equal(a[field], b[field])
-      if sub_msg is not None:
-        return sub_msg
-
-  else:
-    raise ValueError(f"Non implemented comparison for type: {type(a)}")
-
-  return None
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Utilities for unit tests."""
+
+import dataclasses
+import logging
+import os
+import pathlib
+from typing import Optional, Sequence
+
+from absl import flags
+from absl.testing import absltest
+import numpy as np
+import pandas as pd
+
+from ydf.dataset import dataset
+from ydf.dataset import dataspec
+
+
+def data_root_path() -> str:
+  """Root directory of the repo."""
+  return ""
+
+def assertProto2Equal(self: absltest.TestCase, a, b):
+  """Checks that protos "a" and "b" are equal."""
+  self.assertEqual(a, b)
+
+def pydf_test_data_path() -> str:
+  return os.path.join(data_root_path(), "test_data")
+
+
+@dataclasses.dataclass(frozen=True)
+class TrainAndTestDataset:
+  """Training / test dataset as path, VerticalDataset and DataFrame."""
+
+  train_path: str
+  test_path: str
+  train_pd: pd.DataFrame
+  test_pd: pd.DataFrame
+  train: dataset.VerticalDataset
+  test: dataset.VerticalDataset
+
+
+def ydf_test_data_path() -> str:
+  return os.path.join(
+      data_root_path(),
+      "external/ydf_cc/yggdrasil_decision_forests/test_data",
+  )
+
+
+def ydf_test_data_pathlib() -> pathlib.Path:
+  return (
+      pathlib.Path(data_root_path())
+      / "external/ydf_cc/yggdrasil_decision_forests/test_data"
+  )
+
+
+def load_datasets(
+    name: str, column_args: Optional[Sequence[dataspec.Column]] = None
+) -> TrainAndTestDataset:
+  """Returns the given dataset loaded as different formats."""
+  train_path = os.path.join(
+      ydf_test_data_path(), "dataset", f"{name}_train.csv"
+  )
+  test_path = os.path.join(ydf_test_data_path(), "dataset", f"{name}_test.csv")
+  train_pd = pd.read_csv(train_path)
+  test_pd = pd.read_csv(test_path)
+  train_vds = dataset.create_vertical_dataset(
+      train_pd, columns=column_args, include_all_columns=True
+  )
+  test_vds = dataset.create_vertical_dataset(
+      test_pd, data_spec=train_vds.data_spec()
+  )
+  return TrainAndTestDataset(
+      train_path, test_path, train_pd, test_pd, train_vds, test_vds
+  )
+
+
+def toy_dataset():
+  df = pd.DataFrame({
+      "col_three_string": ["A", "A", "B", "B", "C"],
+      "col_float": [1, 2.1, 1.3, 5.5, 2.4],
+      "col_two_string": ["bar", "foo", "foo", "foo", "foo"],
+      "weights": [3, 2, 3.1, 28, 3],
+      "binary_int_label": [0, 0, 0, 1, 1],
+  })
+  return df
+
+
+def toy_dataset_uplift():
+  df = pd.DataFrame({
+      "f1": [1, 2, 3, 4] * 10,
+      "treatement": ["A", "A", "B", "B"] * 10,
+      "effect_binary": [0, 1, 1, 1] * 10,
+      "effect_numerical": [0.1, 0.5, 0.6, 0.7] * 10,
+  })
+  return df
+
+
+# Exception raised in python when the c++ raises an invalid argument error
+AbslInvalidArgumentError = (ValueError, RuntimeError)
+
+
+def golden_check_string(
+    test, value: str, golden_path: str, postfix: str = ""
+) -> None:
+  """Ensures that "value" is equal to the content of the file "golden_path".
+
+  Args:
+    test: A test.
+    value: Value to test.
+    golden_path: Path to golden file expressed from the root of the repo.
+    postfix: Optional postfix to the path of the file containing the actual
+      value.
+  """
+
+  golden_data = open(os.path.join(data_root_path(), golden_path)).read()
+
+  if value != golden_data:
+    value_path = os.path.join(
+        absltest.TEST_TMPDIR.value, os.path.basename(golden_path) + postfix
+    )
+    logging.info("os.path.dirname(value_path): %s", os.path.dirname(value_path))
+    os.makedirs(os.path.dirname(value_path), exist_ok=True)
+    logging.info(
+        "Golden test failed. Save the effetive value to %s", value_path
+    )
+    with open(value_path, "w") as f:
+      f.write(value)
+
+  test.assertEqual(value, golden_data)
+
+
+def assert_almost_equal(a, b):
+  msg = test_almost_equal(a, b)
+  if msg is not None:
+    raise AssertionError(msg)
+
+
+def test_almost_equal(a, b) -> Optional[str]:
+  """Checks that two dataclasses are almost equal.
+
+  Unlike "assert_almost_equal_dataclasses" and "self.assertEqual", this method
+  supports dataclasses containing numpy arrays.
+
+  Args:
+    a: First item to compare.
+    b: Second item to compare.
+
+  Returns:
+    None if "a" and "b" are equal. A description of the different otherwise.
+  """
+
+  if type(a) != type(b):
+    return f"Type mismatch: {type(a)} != {type(b)}"
+
+  elif dataclasses.is_dataclass(a):
+    sub_msg = test_almost_equal(dataclasses.asdict(a), dataclasses.asdict(b))
+    if sub_msg is not None:
+      return sub_msg
+
+  elif isinstance(a, np.ndarray):
+    if a.dtype != b.dtype:
+      return f"numpy array type mismatch: {a} != {b}"
+
+    if a.dtype.type in [np.string_, np.bytes_, np.str_]:
+      if not np.equal(a, b).all():
+        return f"numpy array mismatch: {a} != {b}"
+    else:
+      if not np.allclose(a, b):
+        return f"numpy array mismatch: {a} != {b}"
+
+  elif isinstance(a, (bool, str, bytes, int, float, type(None))):
+    if a != b:
+      return f"primitive mismatch: {a} != {b}"
+
+  elif isinstance(a, list):
+    if len(a) != len(b):
+      return f"list len mismatch: {len(a)} != {len(b)}"
+
+    for sa, sb in zip(a, b):
+      sub_msg = test_almost_equal(sa, sb)
+      if sub_msg is not None:
+        return sub_msg
+
+  elif isinstance(a, dict):
+    if set(a) != set(b):
+      return f"dict field mismatch: {set(a)} != {set(b)}"
+    for field in a:
+      sub_msg = test_almost_equal(a[field], b[field])
+      if sub_msg is not None:
+        return sub_msg
+
+  else:
+    raise ValueError(f"Non implemented comparison for type: {type(a)}")
+
+  return None
```

## ydf/utils/test_utils_test.py

 * *Ordering differences only*

```diff
@@ -1,84 +1,84 @@
-# Copyright 2022 Google LLC.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-import dataclasses
-from absl.testing import absltest
-import numpy as np
-from ydf.utils import test_utils
-
-
-@dataclasses.dataclass(frozen=True)
-class A:
-  a: str
-  b: int
-  c: np.ndarray
-
-
-class TestUtilsTest(absltest.TestCase):
-
-  def test_test_almost_equal_true(self):
-    self.assertIsNone(test_utils.test_almost_equal(1, 1))
-    self.assertIsNone(test_utils.test_almost_equal("a", "a"))
-    self.assertIsNone(test_utils.test_almost_equal(True, True))
-    self.assertIsNone(test_utils.test_almost_equal([], []))
-    self.assertIsNone(test_utils.test_almost_equal([1, 2], [1, 2]))
-    self.assertIsNone(
-        test_utils.test_almost_equal(np.array([1, 2, 3]), np.array([1, 2, 3]))
-    )
-    self.assertIsNone(
-        test_utils.test_almost_equal(np.array(["a", "b"]), np.array(["a", "b"]))
-    )
-    self.assertIsNone(test_utils.test_almost_equal({1: {1: 2}}, {1: {1: 2}}))
-    self.assertIsNone(
-        test_utils.test_almost_equal(
-            A("a", 1, np.array([1, 2])), A("a", 1, np.array([1, 2]))
-        )
-    )
-
-  def test_test_almost_equal_false(self):
-    self.assertIsNotNone(test_utils.test_almost_equal(1, 2))
-    self.assertIsNotNone(test_utils.test_almost_equal("a", "b"))
-    self.assertIsNotNone(test_utils.test_almost_equal("a", 1))
-    self.assertIsNotNone(test_utils.test_almost_equal(True, False))
-    self.assertIsNotNone(test_utils.test_almost_equal([], "a"))
-    self.assertIsNotNone(test_utils.test_almost_equal([1, 2], [2, 2]))
-    self.assertIsNotNone(
-        test_utils.test_almost_equal(np.array([1, 2, 3]), np.array([1, 2, 4]))
-    )
-    self.assertIsNotNone(
-        test_utils.test_almost_equal(np.array([1, 2, 3]), np.array(["a", "b"]))
-    )
-    self.assertIsNotNone(
-        test_utils.test_almost_equal(np.array(["a", "b"]), np.array(["a", "c"]))
-    )
-    self.assertIsNotNone(test_utils.test_almost_equal({1: {1: 2}}, {1: {5: 2}}))
-    self.assertIsNotNone(
-        test_utils.test_almost_equal(
-            A("a", 1, np.array([1, 2])), A("a", 1, np.array([1, 3]))
-        )
-    )
-    self.assertIsNotNone(
-        test_utils.test_almost_equal(
-            A("a", 1, np.array([1, 2])), A("b", 1, np.array([1, 2]))
-        )
-    )
-    self.assertIsNotNone(
-        test_utils.test_almost_equal(
-            A("a", 1, np.array([1, 2])), A("b", 2, np.array([1, 2]))
-        )
-    )
-
-
-if __name__ == "__main__":
-  absltest.main()
+# Copyright 2022 Google LLC.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     https://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import dataclasses
+from absl.testing import absltest
+import numpy as np
+from ydf.utils import test_utils
+
+
+@dataclasses.dataclass(frozen=True)
+class A:
+  a: str
+  b: int
+  c: np.ndarray
+
+
+class TestUtilsTest(absltest.TestCase):
+
+  def test_test_almost_equal_true(self):
+    self.assertIsNone(test_utils.test_almost_equal(1, 1))
+    self.assertIsNone(test_utils.test_almost_equal("a", "a"))
+    self.assertIsNone(test_utils.test_almost_equal(True, True))
+    self.assertIsNone(test_utils.test_almost_equal([], []))
+    self.assertIsNone(test_utils.test_almost_equal([1, 2], [1, 2]))
+    self.assertIsNone(
+        test_utils.test_almost_equal(np.array([1, 2, 3]), np.array([1, 2, 3]))
+    )
+    self.assertIsNone(
+        test_utils.test_almost_equal(np.array(["a", "b"]), np.array(["a", "b"]))
+    )
+    self.assertIsNone(test_utils.test_almost_equal({1: {1: 2}}, {1: {1: 2}}))
+    self.assertIsNone(
+        test_utils.test_almost_equal(
+            A("a", 1, np.array([1, 2])), A("a", 1, np.array([1, 2]))
+        )
+    )
+
+  def test_test_almost_equal_false(self):
+    self.assertIsNotNone(test_utils.test_almost_equal(1, 2))
+    self.assertIsNotNone(test_utils.test_almost_equal("a", "b"))
+    self.assertIsNotNone(test_utils.test_almost_equal("a", 1))
+    self.assertIsNotNone(test_utils.test_almost_equal(True, False))
+    self.assertIsNotNone(test_utils.test_almost_equal([], "a"))
+    self.assertIsNotNone(test_utils.test_almost_equal([1, 2], [2, 2]))
+    self.assertIsNotNone(
+        test_utils.test_almost_equal(np.array([1, 2, 3]), np.array([1, 2, 4]))
+    )
+    self.assertIsNotNone(
+        test_utils.test_almost_equal(np.array([1, 2, 3]), np.array(["a", "b"]))
+    )
+    self.assertIsNotNone(
+        test_utils.test_almost_equal(np.array(["a", "b"]), np.array(["a", "c"]))
+    )
+    self.assertIsNotNone(test_utils.test_almost_equal({1: {1: 2}}, {1: {5: 2}}))
+    self.assertIsNotNone(
+        test_utils.test_almost_equal(
+            A("a", 1, np.array([1, 2])), A("a", 1, np.array([1, 3]))
+        )
+    )
+    self.assertIsNotNone(
+        test_utils.test_almost_equal(
+            A("a", 1, np.array([1, 2])), A("b", 1, np.array([1, 2]))
+        )
+    )
+    self.assertIsNotNone(
+        test_utils.test_almost_equal(
+            A("a", 1, np.array([1, 2])), A("b", 2, np.array([1, 2]))
+        )
+    )
+
+
+if __name__ == "__main__":
+  absltest.main()
```

## Comparing `yggdrasil_decision_forests/dataset/data_spec_pb2.py` & `ydf/proto/dataset/data_spec_pb2.py`

 * *Files identical despite different names*

## Comparing `yggdrasil_decision_forests/dataset/example_pb2.py` & `ydf/proto/dataset/example_pb2.py`

 * *Files identical despite different names*

## Comparing `yggdrasil_decision_forests/dataset/weight_pb2.py` & `ydf/proto/dataset/weight_pb2.py`

 * *Files identical despite different names*

## Comparing `yggdrasil_decision_forests/learner/abstract_learner_pb2.py` & `ydf/proto/learner/abstract_learner_pb2.py`

 * *Files 14% similar despite different names*

```diff
@@ -7,20 +7,20 @@
 from google.protobuf import symbol_database as _symbol_database
 from google.protobuf.internal import builder as _builder
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
-from yggdrasil_decision_forests.model import hyperparameter_pb2 as yggdrasil__decision__forests_dot_model_dot_hyperparameter__pb2
-from yggdrasil_decision_forests.dataset import weight_pb2 as yggdrasil__decision__forests_dot_dataset_dot_weight__pb2
-from yggdrasil_decision_forests.model import abstract_model_pb2 as yggdrasil__decision__forests_dot_model_dot_abstract__model__pb2
-from yggdrasil_decision_forests.utils.distribute import distribute_pb2 as yggdrasil__decision__forests_dot_utils_dot_distribute_dot_distribute__pb2
+from ydf.proto.model import hyperparameter_pb2 as yggdrasil__decision__forests_dot_model_dot_hyperparameter__pb2
+from ydf.proto.dataset import weight_pb2 as yggdrasil__decision__forests_dot_dataset_dot_weight__pb2
+from ydf.proto.model import abstract_model_pb2 as yggdrasil__decision__forests_dot_model_dot_abstract__model__pb2
+from ydf.proto.utils.distribute import distribute_pb2 as yggdrasil__decision__forests_dot_utils_dot_distribute_dot_distribute__pb2
 
-from yggdrasil_decision_forests.model.hyperparameter_pb2 import *
+from ydf.proto.model.hyperparameter_pb2 import *
 
 DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n9yggdrasil_decision_forests/learner/abstract_learner.proto\x12&yggdrasil_decision_forests.model.proto\x1a\x35yggdrasil_decision_forests/model/hyperparameter.proto\x1a/yggdrasil_decision_forests/dataset/weight.proto\x1a\x35yggdrasil_decision_forests/model/abstract_model.proto\x1a<yggdrasil_decision_forests/utils/distribute/distribute.proto\"\x8e\x03\n\x10\x44\x65ploymentConfig\x12\x12\n\ncache_path\x18\x01 \x01(\t\x12\x16\n\x0bnum_threads\x18\x02 \x01(\x05:\x01\x36\x12\"\n\x13try_resume_training\x18\x06 \x01(\x08:\x05\x66\x61lse\x12\x37\n)resume_training_snapshot_interval_seconds\x18\x07 \x01(\x03:\x04\x31\x38\x30\x30\x12\x1a\n\x0enum_io_threads\x18\x08 \x01(\x05:\x02\x31\x30\x12\x1d\n\x12max_kept_snapshots\x18\t \x01(\x05:\x01\x33\x12O\n\x05local\x18\x03 \x01(\x0b\x32>.yggdrasil_decision_forests.model.proto.DeploymentConfig.LocalH\x00\x12I\n\ndistribute\x18\x05 \x01(\x0b\x32\x33.yggdrasil_decision_forests.distribute.proto.ConfigH\x00\x1a\x07\n\x05LocalB\x0b\n\texecutionJ\x04\x08\x04\x10\x05\"\xf3\x04\n\x0eTrainingConfig\x12\x0f\n\x07learner\x18\x01 \x01(\t\x12\x10\n\x08\x66\x65\x61tures\x18\x02 \x03(\t\x12\r\n\x05label\x18\x03 \x01(\t\x12\x10\n\x08\x63v_group\x18\x04 \x01(\t\x12J\n\x04task\x18\x05 \x01(\x0e\x32,.yggdrasil_decision_forests.model.proto.Task:\x0e\x43LASSIFICATION\x12U\n\x11weight_definition\x18\x06 \x01(\x0b\x32:.yggdrasil_decision_forests.dataset.proto.WeightDefinition\x12\x1b\n\x0brandom_seed\x18\x07 \x01(\x03:\x06\x31\x32\x33\x34\x35\x36\x12\x15\n\rranking_group\x18\x08 \x01(\t\x12)\n!maximum_training_duration_seconds\x18\t \x01(\x01\x12-\n%maximum_model_size_in_memory_in_bytes\x18\x0b \x01(\x03\x12\x18\n\x10uplift_treatment\x18\x0c \x01(\t\x12\x42\n\x08metadata\x18\r \x01(\x0b\x32\x30.yggdrasil_decision_forests.model.proto.Metadata\x12!\n\x12pure_serving_model\x18\x0e \x01(\x08:\x05\x66\x61lse\x12Z\n\x15monotonic_constraints\x18\x0f \x03(\x0b\x32;.yggdrasil_decision_forests.model.proto.MonotonicConstraint*\t\x08\xe8\x07\x10\x80\x80\x80\x80\x02J\x04\x08\n\x10\x0b\"\xe7\x02\n\x15TrainingConfigLinking\x12\x14\n\x08\x66\x65\x61tures\x18\x01 \x03(\x05\x42\x02\x10\x01\x12\x1e\n\x12numerical_features\x18\t \x03(\x05\x42\x02\x10\x01\x12\r\n\x05label\x18\x02 \x01(\x05\x12\x19\n\x11num_label_classes\x18\x03 \x01(\x05\x12\x10\n\x08\x63v_group\x18\x04 \x01(\x05\x12[\n\x11weight_definition\x18\x07 \x01(\x0b\x32@.yggdrasil_decision_forests.dataset.proto.LinkedWeightDefinition\x12\x19\n\rranking_group\x18\x08 \x01(\x05:\x02-1\x12\x1c\n\x10uplift_treatment\x18\x0c \x01(\x05:\x02-1\x12\x46\n\x0bper_columns\x18\r \x03(\x0b\x32\x31.yggdrasil_decision_forests.model.proto.PerColumn\"\xaa\x01\n PredefinedHyperParameterTemplate\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0f\n\x07version\x18\x02 \x01(\x05\x12\x13\n\x0b\x64\x65scription\x18\x03 \x01(\t\x12R\n\nparameters\x18\x04 \x01(\x0b\x32>.yggdrasil_decision_forests.model.proto.GenericHyperParameters\"\xa2\x02\n\x13LearnerCapabilities\x12,\n\x1dsupport_max_training_duration\x18\x01 \x01(\x08:\x05\x66\x61lse\x12\x1e\n\x0fresume_training\x18\x02 \x01(\x08:\x05\x66\x61lse\x12)\n\x1asupport_validation_dataset\x18\x03 \x01(\x08:\x05\x66\x61lse\x12\x33\n$support_partial_cache_dataset_format\x18\x04 \x01(\x08:\x05\x66\x61lse\x12/\n support_max_model_size_in_memory\x18\x05 \x01(\x08:\x05\x66\x61lse\x12,\n\x1dsupport_monotonic_constraints\x18\x06 \x01(\x08:\x05\x66\x61lse\"\xb9\x01\n\x13MonotonicConstraint\x12\x0f\n\x07\x66\x65\x61ture\x18\x01 \x01(\t\x12\x64\n\tdirection\x18\x02 \x01(\x0e\x32\x45.yggdrasil_decision_forests.model.proto.MonotonicConstraint.Direction:\nINCREASING\"+\n\tDirection\x12\x0e\n\nINCREASING\x10\x00\x12\x0e\n\nDECREASING\x10\x01\"f\n\tPerColumn\x12Y\n\x14monotonic_constraint\x18\x01 \x01(\x0b\x32;.yggdrasil_decision_forests.model.proto.MonotonicConstraintP\x00')
 
 _globals = globals()
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'yggdrasil_decision_forests.learner.abstract_learner_pb2', _globals)
 if _descriptor._USE_C_DESCRIPTORS == False:
```

## Comparing `yggdrasil_decision_forests/learner/hyperparameters_optimizer/hyperparameters_optimizer_pb2.py` & `ydf/proto/learner/hyperparameters_optimizer/hyperparameters_optimizer_pb2.py`

 * *Files 4% similar despite different names*

```diff
@@ -7,21 +7,21 @@
 from google.protobuf import symbol_database as _symbol_database
 from google.protobuf.internal import builder as _builder
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
-from yggdrasil_decision_forests.learner import abstract_learner_pb2 as yggdrasil__decision__forests_dot_learner_dot_abstract__learner__pb2
+from ydf.proto.learner import abstract_learner_pb2 as yggdrasil__decision__forests_dot_learner_dot_abstract__learner__pb2
 try:
   yggdrasil__decision__forests_dot_model_dot_hyperparameter__pb2 = yggdrasil__decision__forests_dot_learner_dot_abstract__learner__pb2.yggdrasil__decision__forests_dot_model_dot_hyperparameter__pb2
 except AttributeError:
   yggdrasil__decision__forests_dot_model_dot_hyperparameter__pb2 = yggdrasil__decision__forests_dot_learner_dot_abstract__learner__pb2.yggdrasil_decision_forests.model.hyperparameter_pb2
-from yggdrasil_decision_forests.metric import metric_pb2 as yggdrasil__decision__forests_dot_metric_dot_metric__pb2
-from yggdrasil_decision_forests.model import hyperparameter_pb2 as yggdrasil__decision__forests_dot_model_dot_hyperparameter__pb2
+from ydf.proto.metric import metric_pb2 as yggdrasil__decision__forests_dot_metric_dot_metric__pb2
+from ydf.proto.model import hyperparameter_pb2 as yggdrasil__decision__forests_dot_model_dot_hyperparameter__pb2
 
 
 DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\\yggdrasil_decision_forests/learner/hyperparameters_optimizer/hyperparameters_optimizer.proto\x12\x43yggdrasil_decision_forests.model.hyperparameters_optimizer_v2.proto\x1a\x39yggdrasil_decision_forests/learner/abstract_learner.proto\x1a.yggdrasil_decision_forests/metric/metric.proto\x1a\x35yggdrasil_decision_forests/model/hyperparameter.proto\"\xce\x05\n-HyperParametersOptimizerLearnerTrainingConfig\x12L\n\x0c\x62\x61se_learner\x18\x01 \x01(\x0b\x32\x36.yggdrasil_decision_forests.model.proto.TrainingConfig\x12\x61\n\toptimizer\x18\x02 \x01(\x0b\x32N.yggdrasil_decision_forests.model.hyperparameters_optimizer_v2.proto.Optimizer\x12\x63\n\nevaluation\x18\x03 \x01(\x0b\x32O.yggdrasil_decision_forests.model.hyperparameters_optimizer_v2.proto.Evaluation\x12Q\n\x0csearch_space\x18\x04 \x01(\x0b\x32;.yggdrasil_decision_forests.model.proto.HyperParameterSpace\x12\x83\x01\n\x17predefined_search_space\x18\x08 \x01(\x0b\x32\x62.yggdrasil_decision_forests.model.hyperparameters_optimizer_v2.proto.PredefinedHyperParameterSpace\x12Y\n\x17\x62\x61se_learner_deployment\x18\x05 \x01(\x0b\x32\x38.yggdrasil_decision_forests.model.proto.DeploymentConfig\x12/\n\x19serialized_dataset_format\x18\x06 \x01(\t:\x0ctfrecord+tfe\x12\"\n\x13retrain_final_model\x18\x07 \x01(\x08:\x05\x66\x61lse\"\x1f\n\x1dPredefinedHyperParameterSpace\"\x8b\x02\n\nEvaluation\x12G\n\x06metric\x18\x01 \x01(\x0b\x32\x37.yggdrasil_decision_forests.metric.proto.MetricAccessor\x12\x17\n\x0fmaximize_metric\x18\x02 \x01(\x08\x12\x7f\n\x15self_model_evaluation\x18\x03 \x01(\x0b\x32^.yggdrasil_decision_forests.model.hyperparameters_optimizer_v2.proto.Evaluation.SelfEvaluationH\x00\x1a\x10\n\x0eSelfEvaluationB\x08\n\x06source\"I\n\tOptimizer\x12\x15\n\roptimizer_key\x18\x01 \x01(\t\x12\x1a\n\x0fparallel_trials\x18\x02 \x01(\x05:\x01\x31*\t\x08\xe8\x07\x10\x80\x80\x80\x80\x02:\xd8\x01\n hyperparameters_optimizer_config\x12\x36.yggdrasil_decision_forests.model.proto.TrainingConfig\x18\xe4\x80\xbd\xcd\x01 \x01(\x0b\x32r.yggdrasil_decision_forests.model.hyperparameters_optimizer_v2.proto.HyperParametersOptimizerLearnerTrainingConfig')
 
 _globals = globals()
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'yggdrasil_decision_forests.learner.hyperparameters_optimizer.hyperparameters_optimizer_pb2', _globals)
```

## Comparing `yggdrasil_decision_forests/learner/hyperparameters_optimizer/optimizers/random_pb2.py` & `ydf/proto/learner/hyperparameters_optimizer/optimizers/random_pb2.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 from google.protobuf import symbol_database as _symbol_database
 from google.protobuf.internal import builder as _builder
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
-from yggdrasil_decision_forests.learner.hyperparameters_optimizer import hyperparameters_optimizer_pb2 as yggdrasil__decision__forests_dot_learner_dot_hyperparameters__optimizer_dot_hyperparameters__optimizer__pb2
+from ydf.proto.learner.hyperparameters_optimizer import hyperparameters_optimizer_pb2 as yggdrasil__decision__forests_dot_learner_dot_hyperparameters__optimizer_dot_hyperparameters__optimizer__pb2
 
 
 DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\nTyggdrasil_decision_forests/learner/hyperparameters_optimizer/optimizers/random.proto\x12\x43yggdrasil_decision_forests.model.hyperparameters_optimizer_v2.proto\x1a\\yggdrasil_decision_forests/learner/hyperparameters_optimizer/hyperparameters_optimizer.proto\"0\n\x15RandomOptimizerConfig\x12\x17\n\nnum_trials\x18\x01 \x01(\x03:\x03\x31\x30\x30:\xbb\x01\n\x06random\x12N.yggdrasil_decision_forests.model.hyperparameters_optimizer_v2.proto.Optimizer\x18\xe8\x07 \x01(\x0b\x32Z.yggdrasil_decision_forests.model.hyperparameters_optimizer_v2.proto.RandomOptimizerConfig')
 
 _globals = globals()
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'yggdrasil_decision_forests.learner.hyperparameters_optimizer.optimizers.random_pb2', _globals)
```

## Comparing `yggdrasil_decision_forests/metric/metric_pb2.py` & `ydf/proto/metric/metric_pb2.py`

 * *Files 4% similar despite different names*

```diff
@@ -7,19 +7,19 @@
 from google.protobuf import symbol_database as _symbol_database
 from google.protobuf.internal import builder as _builder
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
-from yggdrasil_decision_forests.dataset import data_spec_pb2 as yggdrasil__decision__forests_dot_dataset_dot_data__spec__pb2
-from yggdrasil_decision_forests.dataset import weight_pb2 as yggdrasil__decision__forests_dot_dataset_dot_weight__pb2
-from yggdrasil_decision_forests.model import abstract_model_pb2 as yggdrasil__decision__forests_dot_model_dot_abstract__model__pb2
-from yggdrasil_decision_forests.model import prediction_pb2 as yggdrasil__decision__forests_dot_model_dot_prediction__pb2
-from yggdrasil_decision_forests.utils import distribution_pb2 as yggdrasil__decision__forests_dot_utils_dot_distribution__pb2
+from ydf.proto.dataset import data_spec_pb2 as yggdrasil__decision__forests_dot_dataset_dot_data__spec__pb2
+from ydf.proto.dataset import weight_pb2 as yggdrasil__decision__forests_dot_dataset_dot_weight__pb2
+from ydf.proto.model import abstract_model_pb2 as yggdrasil__decision__forests_dot_model_dot_abstract__model__pb2
+from ydf.proto.model import prediction_pb2 as yggdrasil__decision__forests_dot_model_dot_prediction__pb2
+from ydf.proto.utils import distribution_pb2 as yggdrasil__decision__forests_dot_utils_dot_distribution__pb2
 
 
 DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n.yggdrasil_decision_forests/metric/metric.proto\x12\'yggdrasil_decision_forests.metric.proto\x1a\x32yggdrasil_decision_forests/dataset/data_spec.proto\x1a/yggdrasil_decision_forests/dataset/weight.proto\x1a\x35yggdrasil_decision_forests/model/abstract_model.proto\x1a\x31yggdrasil_decision_forests/model/prediction.proto\x1a\x33yggdrasil_decision_forests/utils/distribution.proto\"\x88\x08\n\x11\x45valuationOptions\x12J\n\x04task\x18\x01 \x01(\x0e\x32,.yggdrasil_decision_forests.model.proto.Task:\x0e\x43LASSIFICATION\x12\x63\n\x0e\x63lassification\x18\x02 \x01(\x0b\x32I.yggdrasil_decision_forests.metric.proto.EvaluationOptions.ClassificationH\x00\x12[\n\nregression\x18\x03 \x01(\x0b\x32\x45.yggdrasil_decision_forests.metric.proto.EvaluationOptions.RegressionH\x00\x12U\n\x07ranking\x18\x07 \x01(\x0b\x32\x42.yggdrasil_decision_forests.metric.proto.EvaluationOptions.RankingH\x00\x12S\n\x06uplift\x18\x08 \x01(\x0b\x32\x41.yggdrasil_decision_forests.metric.proto.EvaluationOptions.UpliftH\x00\x12\x1e\n\x13prediction_sampling\x18\x04 \x01(\x02:\x01\x31\x12#\n\x15\x62ootstrapping_samples\x18\x05 \x01(\x03:\x04\x32\x30\x30\x30\x12K\n\x07weights\x18\x06 \x01(\x0b\x32:.yggdrasil_decision_forests.dataset.proto.WeightDefinition\x1a\xef\x01\n\x0e\x43lassification\x12\x18\n\nroc_enable\x18\x01 \x01(\x08:\x04true\x12\x1e\n\x0fmax_roc_samples\x18\x02 \x01(\x03:\x05\x31\x30\x30\x30\x30\x12\x1b\n\x13precision_at_recall\x18\x03 \x03(\x01\x12\x1b\n\x13recall_at_precision\x18\x04 \x03(\x01\x12\x1b\n\x13precision_at_volume\x18\x05 \x03(\x01\x12%\n\x1drecall_at_false_positive_rate\x18\x06 \x03(\x01\x12%\n\x1d\x66\x61lse_positive_rate_at_recall\x18\x07 \x03(\x01\x1a\x33\n\nRegression\x12%\n\x17\x65nable_regression_plots\x18\x01 \x01(\x08:\x04true\x1a\x66\n\x07Ranking\x12\x1a\n\x0fndcg_truncation\x18\x01 \x01(\x05:\x01\x35\x12\x1a\n\x0emrr_truncation\x18\x02 \x01(\x05:\x02\x31\x30\x12#\n\x14\x61llow_only_one_group\x18\x03 \x01(\x08:\x05\x66\x61lse\x1a\x08\n\x06UpliftB\x0e\n\x0ctask_options\"\x95\x0f\n\x11\x45valuationResults\x12\x1c\n\x11\x63ount_predictions\x18\x01 \x01(\x01:\x01\x30\x12&\n\x1b\x63ount_predictions_no_weight\x18\x02 \x01(\x03:\x01\x30\x12O\n\x13sampled_predictions\x18\x03 \x03(\x0b\x32\x32.yggdrasil_decision_forests.model.proto.Prediction\x12$\n\x19\x63ount_sampled_predictions\x18\x04 \x01(\x01:\x01\x30\x12J\n\x04task\x18\x05 \x01(\x0e\x32,.yggdrasil_decision_forests.model.proto.Task:\x0e\x43LASSIFICATION\x12\x63\n\x0e\x63lassification\x18\x06 \x01(\x0b\x32I.yggdrasil_decision_forests.metric.proto.EvaluationResults.ClassificationH\x00\x12[\n\nregression\x18\x07 \x01(\x0b\x32\x45.yggdrasil_decision_forests.metric.proto.EvaluationResults.RegressionH\x00\x12U\n\x07ranking\x18\x0c \x01(\x0b\x32\x42.yggdrasil_decision_forests.metric.proto.EvaluationResults.RankingH\x00\x12S\n\x06uplift\x18\x0e \x01(\x0b\x32\x41.yggdrasil_decision_forests.metric.proto.EvaluationResults.UpliftH\x00\x12\x46\n\x0clabel_column\x18\x08 \x01(\x0b\x32\x30.yggdrasil_decision_forests.dataset.proto.Column\x12$\n\x1ctraining_duration_in_seconds\x18\t \x01(\x02\x12\x12\n\nloss_value\x18\n \x01(\x02\x12\x11\n\tloss_name\x18\x0b \x01(\t\x12\x11\n\tnum_folds\x18\r \x01(\x05\x12\x61\n\x0cuser_metrics\x18\x0f \x03(\x0b\x32K.yggdrasil_decision_forests.metric.proto.EvaluationResults.UserMetricsEntry\x1a\xd7\x01\n\x0e\x43lassification\x12X\n\tconfusion\x18\x01 \x01(\x0b\x32\x45.yggdrasil_decision_forests.utils.proto.IntegersConfusionMatrixDouble\x12:\n\x04rocs\x18\x02 \x03(\x0b\x32,.yggdrasil_decision_forests.metric.proto.Roc\x12\x17\n\x0csum_log_loss\x18\x03 \x01(\x01:\x01\x30\x12\x10\n\x08\x61\x63\x63uracy\x18\x05 \x01(\x01J\x04\x08\x04\x10\x05\x1a\xc8\x01\n\nRegression\x12\x1b\n\x10sum_square_error\x18\x01 \x01(\x01:\x01\x30\x12\x14\n\tsum_label\x18\x02 \x01(\x01:\x01\x30\x12\x1b\n\x10sum_square_label\x18\x03 \x01(\x01:\x01\x30\x12\'\n\x1f\x62ootstrap_rmse_lower_bounds_95p\x18\x04 \x01(\x01\x12\'\n\x1f\x62ootstrap_rmse_upper_bounds_95p\x18\x05 \x01(\x01\x12\x18\n\rsum_abs_error\x18\x06 \x01(\x01:\x01\x30\x1a\xc4\x03\n\x07Ranking\x12\x45\n\x04ndcg\x18\x05 \x01(\x0b\x32\x37.yggdrasil_decision_forests.metric.proto.MetricEstimate\x12\x17\n\x0fndcg_truncation\x18\x02 \x01(\x05\x12\x15\n\nnum_groups\x18\x03 \x01(\x03:\x01\x30\x12!\n\x16min_num_items_in_group\x18\n \x01(\x03:\x01\x30\x12!\n\x16max_num_items_in_group\x18\x0b \x01(\x03:\x01\x30\x12\"\n\x17mean_num_items_in_group\x18\x0c \x01(\x01:\x01\x30\x12\x17\n\x0c\x64\x65\x66\x61ult_ndcg\x18\x04 \x01(\x01:\x01\x30\x12\x44\n\x03mrr\x18\x08 \x01(\x0b\x32\x37.yggdrasil_decision_forests.metric.proto.MetricEstimate\x12\x16\n\x0emrr_truncation\x18\t \x01(\x05\x12O\n\x0eprecision_at_1\x18\r \x01(\x0b\x32\x37.yggdrasil_decision_forests.metric.proto.MetricEstimateJ\x04\x08\x06\x10\x07J\x04\x08\x07\x10\x08J\x04\x08\x01\x10\x02\x1aV\n\x06Uplift\x12\x0c\n\x04\x61uuc\x18\x01 \x01(\x01\x12\x0c\n\x04qini\x18\x02 \x01(\x01\x12\x16\n\x0enum_treatments\x18\x03 \x01(\x05\x12\x18\n\x10\x63\x61te_calibration\x18\x04 \x01(\x01\x1a\x32\n\x10UserMetricsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\x01:\x02\x38\x01\x42\x06\n\x04type\"\xd6\x16\n\x0eMetricAccessor\x12`\n\x0e\x63lassification\x18\x01 \x01(\x0b\x32\x46.yggdrasil_decision_forests.metric.proto.MetricAccessor.ClassificationH\x00\x12X\n\nregression\x18\x02 \x01(\x0b\x32\x42.yggdrasil_decision_forests.metric.proto.MetricAccessor.RegressionH\x00\x12L\n\x04loss\x18\x03 \x01(\x0b\x32<.yggdrasil_decision_forests.metric.proto.MetricAccessor.LossH\x00\x12R\n\x07ranking\x18\x04 \x01(\x0b\x32?.yggdrasil_decision_forests.metric.proto.MetricAccessor.RankingH\x00\x12P\n\x06uplift\x18\x05 \x01(\x0b\x32>.yggdrasil_decision_forests.metric.proto.MetricAccessor.UpliftH\x00\x12Y\n\x0buser_metric\x18\x06 \x01(\x0b\x32\x42.yggdrasil_decision_forests.metric.proto.MetricAccessor.UserMetricH\x00\x1a\xf2\x0c\n\x0e\x43lassification\x12\x63\n\x08\x61\x63\x63uracy\x18\x01 \x01(\x0b\x32O.yggdrasil_decision_forests.metric.proto.MetricAccessor.Classification.AccuracyH\x00\x12\x61\n\x07logloss\x18\x02 \x01(\x0b\x32N.yggdrasil_decision_forests.metric.proto.MetricAccessor.Classification.LogLossH\x00\x12i\n\x0cone_vs_other\x18\x03 \x01(\x0b\x32Q.yggdrasil_decision_forests.metric.proto.MetricAccessor.Classification.OneVsOtherH\x00\x1a\n\n\x08\x41\x63\x63uracy\x1a\t\n\x07LogLoss\x1a\x8d\n\n\nOneVsOther\x12\x16\n\x0epositive_class\x18\x01 \x01(\t\x12\x64\n\x03\x61uc\x18\x02 \x01(\x0b\x32U.yggdrasil_decision_forests.metric.proto.MetricAccessor.Classification.OneVsOther.AucH\x00\x12i\n\x06pr_auc\x18\x03 \x01(\x0b\x32W.yggdrasil_decision_forests.metric.proto.MetricAccessor.Classification.OneVsOther.PrAucH\x00\x12\x62\n\x02\x61p\x18\x04 \x01(\x0b\x32T.yggdrasil_decision_forests.metric.proto.MetricAccessor.Classification.OneVsOther.ApH\x00\x12\x82\x01\n\x13precision_at_recall\x18\x05 \x01(\x0b\x32\x63.yggdrasil_decision_forests.metric.proto.MetricAccessor.Classification.OneVsOther.PrecisionAtRecallH\x00\x12\x82\x01\n\x13recall_at_precision\x18\x06 \x01(\x0b\x32\x63.yggdrasil_decision_forests.metric.proto.MetricAccessor.Classification.OneVsOther.RecallAtPrecisionH\x00\x12\x82\x01\n\x13precision_at_volume\x18\x07 \x01(\x0b\x32\x63.yggdrasil_decision_forests.metric.proto.MetricAccessor.Classification.OneVsOther.PrecisionAtVolumeH\x00\x12\x94\x01\n\x1drecall_at_false_positive_rate\x18\x08 \x01(\x0b\x32k.yggdrasil_decision_forests.metric.proto.MetricAccessor.Classification.OneVsOther.RecallAtFalsePositiveRateH\x00\x12\x94\x01\n\x1d\x66\x61lse_positive_rate_at_recall\x18\t \x01(\x0b\x32k.yggdrasil_decision_forests.metric.proto.MetricAccessor.Classification.OneVsOther.FalsePositiveRateAtRecallH\x00\x1a\x05\n\x03\x41uc\x1a\x07\n\x05PrAuc\x1a\x04\n\x02\x41p\x1a#\n\x11PrecisionAtRecall\x12\x0e\n\x06recall\x18\x01 \x01(\x02\x1a&\n\x11RecallAtPrecision\x12\x11\n\tprecision\x18\x01 \x01(\x02\x1a#\n\x11PrecisionAtVolume\x12\x0e\n\x06volume\x18\x01 \x01(\x02\x1a\x38\n\x19RecallAtFalsePositiveRate\x12\x1b\n\x13\x66\x61lse_positive_rate\x18\x01 \x01(\x02\x1a+\n\x19\x46\x61lsePositiveRateAtRecall\x12\x0e\n\x06recall\x18\x01 \x01(\x02\x42\x06\n\x04TypeB\x06\n\x04Type\x1a\xd3\x01\n\nRegression\x12W\n\x04rmse\x18\x01 \x01(\x0b\x32G.yggdrasil_decision_forests.metric.proto.MetricAccessor.Regression.RmseH\x00\x12U\n\x03mae\x18\x02 \x01(\x0b\x32\x46.yggdrasil_decision_forests.metric.proto.MetricAccessor.Regression.MaeH\x00\x1a\x06\n\x04Rmse\x1a\x05\n\x03MaeB\x06\n\x04Type\x1a\x06\n\x04Loss\x1a\xca\x01\n\x07Ranking\x12T\n\x04ndcg\x18\x01 \x01(\x0b\x32\x44.yggdrasil_decision_forests.metric.proto.MetricAccessor.Ranking.NDCGH\x00\x12R\n\x03mrr\x18\x02 \x01(\x0b\x32\x43.yggdrasil_decision_forests.metric.proto.MetricAccessor.Ranking.MRRH\x00\x1a\x06\n\x04NDCG\x1a\x05\n\x03MRRB\x06\n\x04Type\x1a\xec\x01\n\x06Uplift\x12S\n\x04qini\x18\x01 \x01(\x0b\x32\x43.yggdrasil_decision_forests.metric.proto.MetricAccessor.Uplift.QiniH\x00\x12j\n\x10\x63\x61te_calibration\x18\x02 \x01(\x0b\x32N.yggdrasil_decision_forests.metric.proto.MetricAccessor.Uplift.CateCalibrationH\x00\x1a\x06\n\x04Qini\x1a\x11\n\x0f\x43\x61teCalibrationB\x06\n\x04type\x1a\"\n\nUserMetric\x12\x14\n\x0cmetrics_name\x18\x01 \x01(\tB\x06\n\x04Task\"\x94\x07\n\x03Roc\x12\x41\n\x05\x63urve\x18\x01 \x03(\x0b\x32\x32.yggdrasil_decision_forests.metric.proto.Roc.Point\x12\x19\n\x11\x63ount_predictions\x18\x02 \x01(\x01\x12\x0b\n\x03\x61uc\x18\x03 \x01(\x01\x12\x0e\n\x06pr_auc\x18\x04 \x01(\x01\x12\n\n\x02\x61p\x18\n \x01(\x01\x12T\n\x13precision_at_recall\x18\x05 \x03(\x0b\x32\x37.yggdrasil_decision_forests.metric.proto.Roc.XAtYMetric\x12T\n\x13recall_at_precision\x18\x06 \x03(\x0b\x32\x37.yggdrasil_decision_forests.metric.proto.Roc.XAtYMetric\x12T\n\x13precision_at_volume\x18\x07 \x03(\x0b\x32\x37.yggdrasil_decision_forests.metric.proto.Roc.XAtYMetric\x12^\n\x1drecall_at_false_positive_rate\x18\x08 \x03(\x0b\x32\x37.yggdrasil_decision_forests.metric.proto.Roc.XAtYMetric\x12^\n\x1d\x66\x61lse_positive_rate_at_recall\x18\t \x03(\x0b\x32\x37.yggdrasil_decision_forests.metric.proto.Roc.XAtYMetric\x12P\n\x1a\x62ootstrap_lower_bounds_95p\x18\x0b \x01(\x0b\x32,.yggdrasil_decision_forests.metric.proto.Roc\x12P\n\x1a\x62ootstrap_upper_bounds_95p\x18\x0c \x01(\x0b\x32,.yggdrasil_decision_forests.metric.proto.Roc\x1aJ\n\x05Point\x12\x11\n\tthreshold\x18\x01 \x01(\x02\x12\n\n\x02tp\x18\x02 \x01(\x01\x12\n\n\x02\x66p\x18\x03 \x01(\x01\x12\n\n\x02tn\x18\x04 \x01(\x01\x12\n\n\x02\x66n\x18\x05 \x01(\x01\x1aT\n\nXAtYMetric\x12\x1b\n\x13y_metric_constraint\x18\x01 \x01(\x01\x12\x16\n\x0ex_metric_value\x18\x02 \x01(\x01\x12\x11\n\tthreshold\x18\x03 \x01(\x02\"m\n\x0eMetricEstimate\x12\r\n\x05value\x18\x01 \x01(\x01\x12L\n\x13\x62ootstrap_based_95p\x18\x02 \x01(\x0b\x32/.yggdrasil_decision_forests.metric.proto.Bounds\"&\n\x06\x42ounds\x12\r\n\x05lower\x18\x01 \x01(\x01\x12\r\n\x05upper\x18\x02 \x01(\x01')
 
 _globals = globals()
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'yggdrasil_decision_forests.metric.metric_pb2', _globals)
```

## Comparing `yggdrasil_decision_forests/model/abstract_model_pb2.py` & `ydf/proto/model/abstract_model_pb2.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,16 +7,16 @@
 from google.protobuf import symbol_database as _symbol_database
 from google.protobuf.internal import builder as _builder
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
-from yggdrasil_decision_forests.dataset import weight_pb2 as yggdrasil__decision__forests_dot_dataset_dot_weight__pb2
-from yggdrasil_decision_forests.model import hyperparameter_pb2 as yggdrasil__decision__forests_dot_model_dot_hyperparameter__pb2
+from ydf.proto.dataset import weight_pb2 as yggdrasil__decision__forests_dot_dataset_dot_weight__pb2
+from ydf.proto.model import hyperparameter_pb2 as yggdrasil__decision__forests_dot_model_dot_hyperparameter__pb2
 
 
 DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n5yggdrasil_decision_forests/model/abstract_model.proto\x12&yggdrasil_decision_forests.model.proto\x1a/yggdrasil_decision_forests/dataset/weight.proto\x1a\x35yggdrasil_decision_forests/model/hyperparameter.proto\"\xb4\x06\n\rAbstractModel\x12\x0c\n\x04name\x18\x01 \x01(\t\x12:\n\x04task\x18\x02 \x01(\x0e\x32,.yggdrasil_decision_forests.model.proto.Task\x12\x15\n\rlabel_col_idx\x18\x03 \x01(\x05\x12Q\n\x07weights\x18\x04 \x01(\x0b\x32@.yggdrasil_decision_forests.dataset.proto.LinkedWeightDefinition\x12\x16\n\x0einput_features\x18\x05 \x03(\x05\x12!\n\x15ranking_group_col_idx\x18\x06 \x01(\x05:\x02-1\x12\x83\x01\n precomputed_variable_importances\x18\x07 \x03(\x0b\x32Y.yggdrasil_decision_forests.model.proto.AbstractModel.PrecomputedVariableImportancesEntry\x12\x32\n$classification_outputs_probabilities\x18\x08 \x01(\x08:\x04true\x12$\n\x18uplift_treatment_col_idx\x18\t \x01(\x05:\x02-1\x12\x42\n\x08metadata\x18\n \x01(\x0b\x32\x30.yggdrasil_decision_forests.model.proto.Metadata\x12k\n\x1dhyperparameter_optimizer_logs\x18\x0b \x01(\x0b\x32\x44.yggdrasil_decision_forests.model.proto.HyperparametersOptimizerLogs\x12\x1c\n\ris_pure_model\x18\x0c \x01(\x08:\x05\x66\x61lse\x1a\x84\x01\n#PrecomputedVariableImportancesEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12L\n\x05value\x18\x02 \x01(\x0b\x32=.yggdrasil_decision_forests.model.proto.VariableImportanceSet:\x02\x38\x01\"O\n\x08Metadata\x12\r\n\x05owner\x18\x01 \x01(\t\x12\x14\n\x0c\x63reated_date\x18\x02 \x01(\x03\x12\x0b\n\x03uid\x18\x03 \x01(\x04\x12\x11\n\tframework\x18\x04 \x01(\t\"?\n\x12VariableImportance\x12\x15\n\rattribute_idx\x18\x01 \x01(\x05\x12\x12\n\nimportance\x18\x02 \x01(\x01\"q\n\x15VariableImportanceSet\x12X\n\x14variable_importances\x18\x01 \x03(\x0b\x32:.yggdrasil_decision_forests.model.proto.VariableImportance\"\xe6\x03\n\x1cHyperparametersOptimizerLogs\x12X\n\x05steps\x18\x01 \x03(\x0b\x32I.yggdrasil_decision_forests.model.proto.HyperparametersOptimizerLogs.Step\x12J\n\x05space\x18\x02 \x01(\x0b\x32;.yggdrasil_decision_forests.model.proto.HyperParameterSpace\x12$\n\x1chyperparameter_optimizer_key\x18\x03 \x01(\t\x12\\\n\x14\x62\x65st_hyperparameters\x18\x05 \x01(\x0b\x32>.yggdrasil_decision_forests.model.proto.GenericHyperParameters\x12\x12\n\nbest_score\x18\x04 \x01(\x02\x1a\x87\x01\n\x04Step\x12\x17\n\x0f\x65valuation_time\x18\x01 \x01(\x01\x12W\n\x0fhyperparameters\x18\x02 \x01(\x0b\x32>.yggdrasil_decision_forests.model.proto.GenericHyperParameters\x12\r\n\x05score\x18\x03 \x01(\x02\"k\n\x0fSerializedModel\x12M\n\x0e\x61\x62stract_model\x18\x01 \x01(\x0b\x32\x35.yggdrasil_decision_forests.model.proto.AbstractModel*\t\x08\xe8\x07\x10\x80\x80\x80\x80\x02*t\n\x04Task\x12\r\n\tUNDEFINED\x10\x00\x12\x12\n\x0e\x43LASSIFICATION\x10\x01\x12\x0e\n\nREGRESSION\x10\x02\x12\x0b\n\x07RANKING\x10\x03\x12\x16\n\x12\x43\x41TEGORICAL_UPLIFT\x10\x04\x12\x14\n\x10NUMERICAL_UPLIFT\x10\x05')
 
 _globals = globals()
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'yggdrasil_decision_forests.model.abstract_model_pb2', _globals)
```

## Comparing `yggdrasil_decision_forests/model/hyperparameter_pb2.py` & `ydf/proto/model/hyperparameter_pb2.py`

 * *Files identical despite different names*

## Comparing `yggdrasil_decision_forests/model/prediction_pb2.py` & `ydf/proto/model/prediction_pb2.py`

 * *Files 1% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 from google.protobuf import symbol_database as _symbol_database
 from google.protobuf.internal import builder as _builder
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
-from yggdrasil_decision_forests.utils import distribution_pb2 as yggdrasil__decision__forests_dot_utils_dot_distribution__pb2
+from ydf.proto.utils import distribution_pb2 as yggdrasil__decision__forests_dot_utils_dot_distribution__pb2
 
 
 DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n1yggdrasil_decision_forests/model/prediction.proto\x12&yggdrasil_decision_forests.model.proto\x1a\x33yggdrasil_decision_forests/utils/distribution.proto\"\xc3\x06\n\nPrediction\x12[\n\x0e\x63lassification\x18\x01 \x01(\x0b\x32\x41.yggdrasil_decision_forests.model.proto.Prediction.ClassificationH\x00\x12S\n\nregression\x18\x02 \x01(\x0b\x32=.yggdrasil_decision_forests.model.proto.Prediction.RegressionH\x00\x12M\n\x07ranking\x18\x05 \x01(\x0b\x32:.yggdrasil_decision_forests.model.proto.Prediction.RankingH\x00\x12K\n\x06uplift\x18\x06 \x01(\x0b\x32\x39.yggdrasil_decision_forests.model.proto.Prediction.UpliftH\x00\x12\x11\n\x06weight\x18\x03 \x01(\x02:\x01\x31\x12\x13\n\x0b\x65xample_key\x18\x04 \x01(\t\x1a\x8d\x01\n\x0e\x43lassification\x12\r\n\x05value\x18\x01 \x01(\x05\x12V\n\x0c\x64istribution\x18\x02 \x01(\x0b\x32@.yggdrasil_decision_forests.utils.proto.IntegerDistributionFloat\x12\x14\n\x0cground_truth\x18\x03 \x01(\x05\x1a\x31\n\nRegression\x12\r\n\x05value\x18\x01 \x01(\x02\x12\x14\n\x0cground_truth\x18\x02 \x01(\x02\x1al\n\x07Ranking\x12\x11\n\trelevance\x18\x01 \x01(\x02\x12\x1e\n\x16ground_truth_relevance\x18\x02 \x01(\x02\x12\x1c\n\x10\x64\x65precated_group\x18\x03 \x01(\x05\x42\x02\x18\x01\x12\x10\n\x08group_id\x18\x04 \x01(\x04\x1a\x85\x01\n\x06Uplift\x12\x1c\n\x10treatment_effect\x18\x01 \x03(\x02\x42\x02\x10\x01\x12\x11\n\ttreatment\x18\x02 \x01(\x05\x12\x1d\n\x13outcome_categorical\x18\x03 \x01(\x05H\x00\x12\x1b\n\x11outcome_numerical\x18\x04 \x01(\x02H\x00\x42\x0e\n\x0coutcome_typeB\x06\n\x04type')
 
 _globals = globals()
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'yggdrasil_decision_forests.model.prediction_pb2', _globals)
```

## Comparing `yggdrasil_decision_forests/model/decision_tree/decision_tree_pb2.py` & `ydf/proto/model/decision_tree/decision_tree_pb2.py`

 * *Files 0% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 from google.protobuf import symbol_database as _symbol_database
 from google.protobuf.internal import builder as _builder
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
-from yggdrasil_decision_forests.utils import distribution_pb2 as yggdrasil__decision__forests_dot_utils_dot_distribution__pb2
+from ydf.proto.utils import distribution_pb2 as yggdrasil__decision__forests_dot_utils_dot_distribution__pb2
 
 
 DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\nByggdrasil_decision_forests/model/decision_tree/decision_tree.proto\x12\x34yggdrasil_decision_forests.model.decision_tree.proto\x1a\x33yggdrasil_decision_forests/utils/distribution.proto\"\x82\x01\n\x14NodeClassifierOutput\x12\x11\n\ttop_value\x18\x01 \x01(\x05\x12W\n\x0c\x64istribution\x18\x02 \x01(\x0b\x32\x41.yggdrasil_decision_forests.utils.proto.IntegerDistributionDouble\"\xc2\x01\n\x13NodeRegressorOutput\x12\x11\n\ttop_value\x18\x01 \x01(\x02\x12V\n\x0c\x64istribution\x18\x02 \x01(\x0b\x32@.yggdrasil_decision_forests.utils.proto.NormalDistributionDouble\x12\x15\n\rsum_gradients\x18\x03 \x01(\x01\x12\x14\n\x0csum_hessians\x18\x04 \x01(\x01\x12\x13\n\x0bsum_weights\x18\x05 \x01(\x01\"\xc7\x01\n\x10NodeUpliftOutput\x12\x13\n\x0bsum_weights\x18\x01 \x01(\x01\x12%\n\x19sum_weights_per_treatment\x18\x02 \x03(\x01\x42\x02\x10\x01\x12\x31\n%sum_weights_per_treatment_and_outcome\x18\x03 \x03(\x01\x42\x02\x10\x01\x12\x1c\n\x10treatment_effect\x18\x04 \x03(\x02\x42\x02\x10\x01\x12&\n\x1anum_examples_per_treatment\x18\x05 \x03(\x03\x42\x02\x10\x01\"\x95\x08\n\tCondition\x12Z\n\x0cna_condition\x18\x01 \x01(\x0b\x32\x42.yggdrasil_decision_forests.model.decision_tree.proto.Condition.NAH\x00\x12\x62\n\x10higher_condition\x18\x02 \x01(\x0b\x32\x46.yggdrasil_decision_forests.model.decision_tree.proto.Condition.HigherH\x00\x12i\n\x14true_value_condition\x18\x03 \x01(\x0b\x32I.yggdrasil_decision_forests.model.decision_tree.proto.Condition.TrueValueH\x00\x12l\n\x12\x63ontains_condition\x18\x04 \x01(\x0b\x32N.yggdrasil_decision_forests.model.decision_tree.proto.Condition.ContainsVectorH\x00\x12s\n\x19\x63ontains_bitmap_condition\x18\x05 \x01(\x0b\x32N.yggdrasil_decision_forests.model.decision_tree.proto.Condition.ContainsBitmapH\x00\x12y\n\x1c\x64iscretized_higher_condition\x18\x06 \x01(\x0b\x32Q.yggdrasil_decision_forests.model.decision_tree.proto.Condition.DiscretizedHigherH\x00\x12\x64\n\x11oblique_condition\x18\x07 \x01(\x0b\x32G.yggdrasil_decision_forests.model.decision_tree.proto.Condition.ObliqueH\x00\x1a\x04\n\x02NA\x1a\x0b\n\tTrueValue\x1a\x1b\n\x06Higher\x12\x11\n\tthreshold\x18\x01 \x01(\x02\x1a&\n\x0e\x43ontainsVector\x12\x14\n\x08\x65lements\x18\x01 \x03(\x05\x42\x02\x10\x01\x1a)\n\x0e\x43ontainsBitmap\x12\x17\n\x0f\x65lements_bitmap\x18\x01 \x01(\x0c\x1a&\n\x11\x44iscretizedHigher\x12\x11\n\tthreshold\x18\x01 \x01(\x05\x1a\x66\n\x07Oblique\x12\x16\n\nattributes\x18\x01 \x03(\x05\x42\x02\x10\x01\x12\x13\n\x07weights\x18\x02 \x03(\x02\x42\x02\x10\x01\x12\x11\n\tthreshold\x18\x03 \x01(\x02\x12\x1b\n\x0fna_replacements\x18\x04 \x03(\x02\x42\x02\x10\x01\x42\x06\n\x04type\"\xda\x02\n\rNodeCondition\x12\x10\n\x08na_value\x18\x01 \x01(\x08\x12\x11\n\tattribute\x18\x02 \x01(\x05\x12R\n\tcondition\x18\x03 \x01(\x0b\x32?.yggdrasil_decision_forests.model.decision_tree.proto.Condition\x12,\n$num_training_examples_without_weight\x18\x04 \x01(\x03\x12)\n!num_training_examples_with_weight\x18\x05 \x01(\x01\x12\x16\n\x0bsplit_score\x18\x06 \x01(\x02:\x01\x30\x12\x30\n(num_pos_training_examples_without_weight\x18\x07 \x01(\x03\x12-\n%num_pos_training_examples_with_weight\x18\x08 \x01(\x01\"\xb6\x03\n\x04Node\x12`\n\nclassifier\x18\x01 \x01(\x0b\x32J.yggdrasil_decision_forests.model.decision_tree.proto.NodeClassifierOutputH\x00\x12^\n\tregressor\x18\x02 \x01(\x0b\x32I.yggdrasil_decision_forests.model.decision_tree.proto.NodeRegressorOutputH\x00\x12X\n\x06uplift\x18\x05 \x01(\x0b\x32\x46.yggdrasil_decision_forests.model.decision_tree.proto.NodeUpliftOutputH\x00\x12V\n\tcondition\x18\x03 \x01(\x0b\x32\x43.yggdrasil_decision_forests.model.decision_tree.proto.NodeCondition\x12\x30\n(num_pos_training_examples_without_weight\x18\x04 \x01(\x03\x42\x08\n\x06output')
 
 _globals = globals()
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'yggdrasil_decision_forests.model.decision_tree.decision_tree_pb2', _globals)
```

## Comparing `yggdrasil_decision_forests/model/random_forest/random_forest_pb2.py` & `ydf/proto/model/random_forest/random_forest_pb2.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,16 +7,16 @@
 from google.protobuf import symbol_database as _symbol_database
 from google.protobuf.internal import builder as _builder
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
-from yggdrasil_decision_forests.metric import metric_pb2 as yggdrasil__decision__forests_dot_metric_dot_metric__pb2
-from yggdrasil_decision_forests.model import abstract_model_pb2 as yggdrasil__decision__forests_dot_model_dot_abstract__model__pb2
+from ydf.proto.metric import metric_pb2 as yggdrasil__decision__forests_dot_metric_dot_metric__pb2
+from ydf.proto.model import abstract_model_pb2 as yggdrasil__decision__forests_dot_model_dot_abstract__model__pb2
 
 
 DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\nByggdrasil_decision_forests/model/random_forest/random_forest.proto\x12\x34yggdrasil_decision_forests.model.random_forest.proto\x1a.yggdrasil_decision_forests/metric/metric.proto\x1a\x35yggdrasil_decision_forests/model/abstract_model.proto\"\xc7\x03\n\x06Header\x12\x17\n\x0fnum_node_shards\x18\x01 \x01(\x05\x12\x11\n\tnum_trees\x18\x02 \x01(\x03\x12\'\n\x19winner_take_all_inference\x18\x03 \x01(\x08:\x04true\x12q\n\x16out_of_bag_evaluations\x18\x04 \x03(\x0b\x32Q.yggdrasil_decision_forests.model.random_forest.proto.OutOfBagTrainingEvaluations\x12]\n\x19mean_decrease_in_accuracy\x18\x05 \x03(\x0b\x32:.yggdrasil_decision_forests.model.proto.VariableImportance\x12Y\n\x15mean_increase_in_rmse\x18\x06 \x03(\x0b\x32:.yggdrasil_decision_forests.model.proto.VariableImportance\x12!\n\x0bnode_format\x18\x07 \x01(\t:\x0cTFE_RECORDIO\x12\x18\n\x10num_pruned_nodes\x18\x08 \x01(\x03\"\x86\x01\n\x1bOutOfBagTrainingEvaluations\x12\x17\n\x0fnumber_of_trees\x18\x01 \x01(\x05\x12N\n\nevaluation\x18\x02 \x01(\x0b\x32:.yggdrasil_decision_forests.metric.proto.EvaluationResults\"k\n\x1bRandomForestSerializedModel\x12L\n\x06header\x18\x01 \x01(\x0b\x32<.yggdrasil_decision_forests.model.random_forest.proto.Header:\xb3\x01\n\x1erandom_forest_serialized_model\x12\x37.yggdrasil_decision_forests.model.proto.SerializedModel\x18\xea\x07 \x01(\x0b\x32Q.yggdrasil_decision_forests.model.random_forest.proto.RandomForestSerializedModel')
 
 _globals = globals()
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'yggdrasil_decision_forests.model.random_forest.random_forest_pb2', _globals)
```

## Comparing `yggdrasil_decision_forests/utils/distribution_pb2.py` & `ydf/proto/utils/distribution_pb2.py`

 * *Files identical despite different names*

## Comparing `yggdrasil_decision_forests/utils/fold_generator_pb2.py` & `ydf/proto/utils/fold_generator_pb2.py`

 * *Files identical despite different names*

## Comparing `yggdrasil_decision_forests/utils/model_analysis_pb2.py` & `ydf/proto/utils/model_analysis_pb2.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,19 +7,19 @@
 from google.protobuf import symbol_database as _symbol_database
 from google.protobuf.internal import builder as _builder
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
-from yggdrasil_decision_forests.dataset import data_spec_pb2 as yggdrasil__decision__forests_dot_dataset_dot_data__spec__pb2
-from yggdrasil_decision_forests.dataset import example_pb2 as yggdrasil__decision__forests_dot_dataset_dot_example__pb2
-from yggdrasil_decision_forests.model import abstract_model_pb2 as yggdrasil__decision__forests_dot_model_dot_abstract__model__pb2
-from yggdrasil_decision_forests.model import prediction_pb2 as yggdrasil__decision__forests_dot_model_dot_prediction__pb2
-from yggdrasil_decision_forests.utils import partial_dependence_plot_pb2 as yggdrasil__decision__forests_dot_utils_dot_partial__dependence__plot__pb2
+from ydf.proto.dataset import data_spec_pb2 as yggdrasil__decision__forests_dot_dataset_dot_data__spec__pb2
+from ydf.proto.dataset import example_pb2 as yggdrasil__decision__forests_dot_dataset_dot_example__pb2
+from ydf.proto.model import abstract_model_pb2 as yggdrasil__decision__forests_dot_model_dot_abstract__model__pb2
+from ydf.proto.model import prediction_pb2 as yggdrasil__decision__forests_dot_model_dot_prediction__pb2
+from ydf.proto.utils import partial_dependence_plot_pb2 as yggdrasil__decision__forests_dot_utils_dot_partial__dependence__plot__pb2
 
 
 DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n5yggdrasil_decision_forests/utils/model_analysis.proto\x12\x35yggdrasil_decision_forests.utils.model_analysis.proto\x1a\x32yggdrasil_decision_forests/dataset/data_spec.proto\x1a\x30yggdrasil_decision_forests/dataset/example.proto\x1a\x35yggdrasil_decision_forests/model/abstract_model.proto\x1a\x31yggdrasil_decision_forests/model/prediction.proto\x1a>yggdrasil_decision_forests/utils/partial_dependence_plot.proto\"\xe0\x0b\n\x07Options\x12\x16\n\x0bnum_threads\x18\x01 \x01(\x05:\x01\x36\x12V\n\x03pdp\x18\x02 \x01(\x0b\x32I.yggdrasil_decision_forests.utils.model_analysis.proto.Options.PlotConfig\x12V\n\x03\x63\x65p\x18\x03 \x01(\x0b\x32I.yggdrasil_decision_forests.utils.model_analysis.proto.Options.PlotConfig\x12\x7f\n\x1cpermuted_variable_importance\x18\x07 \x01(\x0b\x32Y.yggdrasil_decision_forests.utils.model_analysis.proto.Options.PermutedVariableImportance\x12<\n-include_model_structural_variable_importances\x18\x11 \x01(\x08:\x05\x66\x61lse\x12\x1a\n\x0c\x66igure_width\x18\x08 \x01(\x05:\x04\x31\x36\x30\x30\x12\x17\n\nplot_width\x18\t \x01(\x05:\x03\x35\x33\x30\x12\x18\n\x0bplot_height\x18\n \x01(\x05:\x03\x34\x35\x30\x12\x62\n\rreport_header\x18\x0b \x01(\x0b\x32K.yggdrasil_decision_forests.utils.model_analysis.proto.Options.ReportHeader\x12g\n\x10table_of_content\x18\x0c \x01(\x0b\x32M.yggdrasil_decision_forests.utils.model_analysis.proto.Options.TableOfContent\x12`\n\x0creport_setup\x18\r \x01(\x0b\x32J.yggdrasil_decision_forests.utils.model_analysis.proto.Options.ReportSetup\x12Y\n\x08\x64\x61taspec\x18\x0e \x01(\x0b\x32G.yggdrasil_decision_forests.utils.model_analysis.proto.Options.Dataspec\x12j\n\x11model_description\x18\x0f \x01(\x0b\x32O.yggdrasil_decision_forests.utils.model_analysis.proto.Options.ModelDescription\x12Q\n\x04plot\x18\x10 \x01(\x0b\x32\x43.yggdrasil_decision_forests.utils.model_analysis.proto.Options.Plot\x12\x16\n\x0ehtml_id_prefix\x18\x12 \x01(\t\x1a`\n\nPlotConfig\x12\x15\n\x07\x65nabled\x18\x01 \x01(\x08:\x04true\x12\x1b\n\x10\x65xample_sampling\x18\x02 \x01(\x02:\x01\x31\x12\x1e\n\x12num_numerical_bins\x18\x03 \x01(\x05:\x02\x35\x30\x1aJ\n\x1aPermutedVariableImportance\x12\x15\n\x07\x65nabled\x18\x01 \x01(\x08:\x04true\x12\x15\n\nnum_rounds\x18\x02 \x01(\x05:\x01\x31\x1a%\n\x0cReportHeader\x12\x15\n\x07\x65nabled\x18\x01 \x01(\x08:\x04true\x1a\'\n\x0eTableOfContent\x12\x15\n\x07\x65nabled\x18\x01 \x01(\x08:\x04true\x1a$\n\x0bReportSetup\x12\x15\n\x07\x65nabled\x18\x01 \x01(\x08:\x04true\x1a!\n\x08\x44\x61taspec\x12\x15\n\x07\x65nabled\x18\x01 \x01(\x08:\x04true\x1a)\n\x10ModelDescription\x12\x15\n\x07\x65nabled\x18\x01 \x01(\x08:\x04true\x1a,\n\x04Plot\x12$\n\x15show_interactive_menu\x18\x01 \x01(\x08:\x05\x66\x61lse\"\xaf\x03\n\x0e\x41nalysisResult\x12Q\n\x07pdp_set\x18\x01 \x01(\x0b\x32@.yggdrasil_decision_forests.utils.proto.PartialDependencePlotSet\x12Q\n\x07\x63\x65p_set\x18\x02 \x01(\x0b\x32@.yggdrasil_decision_forests.utils.proto.PartialDependencePlotSet\x12|\n\x14variable_importances\x18\x03 \x03(\x0b\x32^.yggdrasil_decision_forests.utils.model_analysis.proto.AnalysisResult.VariableImportancesEntry\x1ay\n\x18VariableImportancesEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12L\n\x05value\x18\x02 \x01(\x0b\x32=.yggdrasil_decision_forests.model.proto.VariableImportanceSet:\x02\x38\x01\"\xe0\x02\n\x18StandaloneAnalysisResult\x12\\\n\rcore_analysis\x18\x01 \x01(\x0b\x32\x45.yggdrasil_decision_forests.utils.model_analysis.proto.AnalysisResult\x12\x14\n\x0c\x64\x61taset_path\x18\x02 \x01(\t\x12\x12\n\nmodel_path\x18\x03 \x01(\t\x12N\n\tdata_spec\x18\x04 \x01(\x0b\x32;.yggdrasil_decision_forests.dataset.proto.DataSpecification\x12\x15\n\rlabel_col_idx\x18\x05 \x01(\x05\x12:\n\x04task\x18\x06 \x01(\x0e\x32,.yggdrasil_decision_forests.model.proto.Task\x12\x19\n\x11model_description\x18\x07 \x01(\t\"\x86\x01\n\x19PredictionAnalysisOptions\x12\x1e\n\x12numerical_num_bins\x18\x01 \x01(\x05:\x02\x35\x30\x12\x17\n\nplot_width\x18\x02 \x01(\x05:\x03\x34\x30\x30\x12\x18\n\x0bplot_height\x18\x03 \x01(\x05:\x03\x33\x30\x30\x12\x16\n\x0ehtml_id_prefix\x18\x04 \x01(\t\"\xad\x03\n\x18PredictionAnalysisResult\x12N\n\tdata_spec\x18\x01 \x01(\x0b\x32;.yggdrasil_decision_forests.dataset.proto.DataSpecification\x12\x15\n\rlabel_col_idx\x18\x02 \x01(\x05\x12:\n\x04task\x18\x03 \x01(\x0e\x32,.yggdrasil_decision_forests.model.proto.Task\x12\x62\n\x11\x66\x65\x61ture_variation\x18\x04 \x01(\x0b\x32G.yggdrasil_decision_forests.utils.model_analysis.proto.FeatureVariation\x12\x42\n\x07\x65xample\x18\x05 \x01(\x0b\x32\x31.yggdrasil_decision_forests.dataset.proto.Example\x12\x46\n\nprediction\x18\x06 \x01(\x0b\x32\x32.yggdrasil_decision_forests.model.proto.Prediction\"n\n\x10\x46\x65\x61tureVariation\x12Z\n\x05items\x18\x01 \x03(\x0b\x32K.yggdrasil_decision_forests.utils.model_analysis.proto.FeatureVariationItem\"\x8a\x06\n\x14\x46\x65\x61tureVariationItem\x12]\n\x04\x62ins\x18\x01 \x03(\x0b\x32O.yggdrasil_decision_forests.utils.model_analysis.proto.FeatureVariationItem.Bin\x12i\n\nattributes\x18\x02 \x03(\x0b\x32U.yggdrasil_decision_forests.utils.model_analysis.proto.FeatureVariationItem.Attribute\x1aM\n\x03\x42in\x12\x46\n\nprediction\x18\x01 \x01(\x0b\x32\x32.yggdrasil_decision_forests.model.proto.Prediction\x1a\xd8\x03\n\tAttribute\x12\x12\n\ncolumn_idx\x18\x02 \x01(\x05\x12t\n\tnumerical\x18\x03 \x01(\x0b\x32_.yggdrasil_decision_forests.utils.model_analysis.proto.FeatureVariationItem.Attribute.NumericalH\x00\x12x\n\x0b\x63\x61tegorical\x18\x04 \x01(\x0b\x32\x61.yggdrasil_decision_forests.utils.model_analysis.proto.FeatureVariationItem.Attribute.CategoricalH\x00\x12p\n\x07\x62oolean\x18\x05 \x01(\x0b\x32].yggdrasil_decision_forests.utils.model_analysis.proto.FeatureVariationItem.Attribute.BooleanH\x00\x1a\x1f\n\tNumerical\x12\x12\n\x06values\x18\x01 \x03(\x02\x42\x02\x10\x01\x1a!\n\x0b\x43\x61tegorical\x12\x12\n\nnum_values\x18\x01 \x01(\x05\x1a\t\n\x07\x42ooleanB\x06\n\x04type')
 
 _globals = globals()
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'yggdrasil_decision_forests.utils.model_analysis_pb2', _globals)
```

## Comparing `yggdrasil_decision_forests/utils/partial_dependence_plot_pb2.py` & `ydf/proto/utils/partial_dependence_plot_pb2.py`

 * *Files 3% similar despite different names*

```diff
@@ -7,16 +7,16 @@
 from google.protobuf import symbol_database as _symbol_database
 from google.protobuf.internal import builder as _builder
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
-from yggdrasil_decision_forests.dataset import example_pb2 as yggdrasil__decision__forests_dot_dataset_dot_example__pb2
-from yggdrasil_decision_forests.utils import distribution_pb2 as yggdrasil__decision__forests_dot_utils_dot_distribution__pb2
+from ydf.proto.dataset import example_pb2 as yggdrasil__decision__forests_dot_dataset_dot_example__pb2
+from ydf.proto.utils import distribution_pb2 as yggdrasil__decision__forests_dot_utils_dot_distribution__pb2
 
 
 DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n>yggdrasil_decision_forests/utils/partial_dependence_plot.proto\x12&yggdrasil_decision_forests.utils.proto\x1a\x30yggdrasil_decision_forests/dataset/example.proto\x1a\x33yggdrasil_decision_forests/utils/distribution.proto\"\x9b\x0c\n\x18PartialDependencePlotSet\x12\x64\n\x04pdps\x18\x01 \x03(\x0b\x32V.yggdrasil_decision_forests.utils.proto.PartialDependencePlotSet.PartialDependencePlot\x1a\x98\x0b\n\x15PartialDependencePlot\x12\x18\n\x10num_observations\x18\x01 \x01(\x01\x12l\n\x08pdp_bins\x18\x03 \x03(\x0b\x32Z.yggdrasil_decision_forests.utils.proto.PartialDependencePlotSet.PartialDependencePlot.Bin\x12|\n\x0e\x61ttribute_info\x18\x04 \x03(\x0b\x32\x64.yggdrasil_decision_forests.utils.proto.PartialDependencePlotSet.PartialDependencePlot.AttributeInfo\x1a\xea\x01\n\x10LabelAccumulator\x12m\n!classification_class_distribution\x18\x01 \x01(\x0b\x32@.yggdrasil_decision_forests.utils.proto.IntegerDistributionFloatH\x00\x12*\n\x1dsum_of_regression_predictions\x18\x02 \x01(\x01:\x01\x30H\x00\x12\'\n\x1asum_of_ranking_predictions\x18\x03 \x01(\x01:\x01\x30H\x00\x42\x12\n\x10prediction_value\x1aq\n\x15\x45valuationAccumulator\x12\x1e\n\x11sum_squared_error\x18\x01 \x01(\x01:\x01\x30H\x00\x12$\n\x17num_correct_predictions\x18\x02 \x01(\x01:\x01\x30H\x00\x42\x12\n\x10prediction_value\x1a\xe6\x03\n\x03\x42in\x12{\n\nprediction\x18\x01 \x01(\x0b\x32g.yggdrasil_decision_forests.utils.proto.PartialDependencePlotSet.PartialDependencePlot.LabelAccumulator\x12}\n\x0cground_truth\x18\x02 \x01(\x0b\x32g.yggdrasil_decision_forests.utils.proto.PartialDependencePlotSet.PartialDependencePlot.LabelAccumulator\x12\x80\x01\n\nevaluation\x18\x04 \x01(\x0b\x32l.yggdrasil_decision_forests.utils.proto.PartialDependencePlotSet.PartialDependencePlot.EvaluationAccumulator\x12`\n\x1b\x63\x65nter_input_feature_values\x18\x03 \x03(\x0b\x32;.yggdrasil_decision_forests.dataset.proto.Example.Attribute\x1a\xaf\x02\n\rAttributeInfo\x12\"\n\x1anum_bins_per_input_feature\x18\x01 \x01(\x05\x12\x15\n\rattribute_idx\x18\x02 \x01(\x05\x12!\n\x19num_observations_per_bins\x18\x03 \x03(\x01\x12\x1c\n\x14numerical_boundaries\x18\x04 \x03(\x02\x12\x82\x01\n\x05scale\x18\x05 \x01(\x0e\x32j.yggdrasil_decision_forests.utils.proto.PartialDependencePlotSet.PartialDependencePlot.AttributeInfo.Scale:\x07UNIFORM\"\x1d\n\x05Scale\x12\x0b\n\x07UNIFORM\x10\x00\x12\x07\n\x03LOG\x10\x01')
 
 _globals = globals()
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'yggdrasil_decision_forests.utils.partial_dependence_plot_pb2', _globals)
```

## Comparing `yggdrasil_decision_forests/utils/distribute/distribute_pb2.py` & `ydf/proto/utils/distribute/distribute_pb2.py`

 * *Files identical despite different names*

## Comparing `yggdrasil_decision_forests/utils/distribute/implementations/grpc/grpc_pb2.py` & `ydf/proto/utils/distribute/implementations/grpc/grpc_pb2.py`

 * *Files 1% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 from google.protobuf import symbol_database as _symbol_database
 from google.protobuf.internal import builder as _builder
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
-from yggdrasil_decision_forests.utils.distribute import distribute_pb2 as yggdrasil__decision__forests_dot_utils_dot_distribute_dot_distribute__pb2
+from ydf.proto.utils.distribute import distribute_pb2 as yggdrasil__decision__forests_dot_utils_dot_distribute_dot_distribute__pb2
 
 
 DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\nKyggdrasil_decision_forests/utils/distribute/implementations/grpc/grpc.proto\x12+yggdrasil_decision_forests.distribute.proto\x1a<yggdrasil_decision_forests/utils/distribute/distribute.proto\"\xb2\x02\n\x07GRPCImp\x12X\n\x10socket_addresses\x18\x01 \x01(\x0b\x32<.yggdrasil_decision_forests.distribute.proto.SocketAddressesH\x00\x12?\n\x03\x62ns\x18\x02 \x01(\x0b\x32\x30.yggdrasil_decision_forests.distribute.proto.BnsH\x00\x12T\n\x0egrpc_addresses\x18\x05 \x01(\x0b\x32:.yggdrasil_decision_forests.distribute.proto.GrpcAddressesH\x00\x12\x17\n\x08use_loas\x18\x03 \x01(\x08:\x05\x66\x61lse\x12\x0b\n\x03key\x18\x04 \x01(\x05\x42\x10\n\x0eworker_address\"\"\n\rGrpcAddresses\x12\x11\n\taddresses\x18\x01 \x03(\t\"\x8f\x01\n\x0cWorkerConfig\x12\x14\n\x0cwelcome_blob\x18\x01 \x01(\x0c\x12\x18\n\x10worker_addresses\x18\x02 \x03(\t\x12\x13\n\x0bmanager_uid\x18\x03 \x01(\x04\x12\x13\n\x0bworker_name\x18\x04 \x01(\t\x12%\n\x1dparallel_execution_per_worker\x18\x05 \x01(\x05\"\x96\x01\n\x05Query\x12\x0c\n\x04\x62lob\x18\x01 \x01(\x0c\x12\x13\n\x0bmanager_uid\x18\x03 \x01(\x04\x12\x12\n\nworker_idx\x18\x04 \x01(\x05\x12P\n\rworker_config\x18\x05 \x01(\x0b\x32\x39.yggdrasil_decision_forests.distribute.proto.WorkerConfigJ\x04\x08\x02\x10\x03\"%\n\x06\x41nswer\x12\x0c\n\x04\x62lob\x18\x01 \x01(\x0c\x12\r\n\x05\x65rror\x18\x02 \x01(\t\",\n\rShutdownQuery\x12\x1b\n\x13kill_worker_manager\x18\x01 \x01(\x08\"0\n\x0bWorkerQuery\x12\x0c\n\x04\x62lob\x18\x01 \x01(\x0c\x12\x13\n\x0bmanager_uid\x18\x03 \x01(\x04\"+\n\x0cWorkerAnswer\x12\x0c\n\x04\x62lob\x18\x01 \x01(\x0c\x12\r\n\x05\x65rror\x18\x02 \x01(\t\"C\n\x18UpdateWorkerAddressQuery\x12\x12\n\nworker_idx\x18\x01 \x01(\x05\x12\x13\n\x0bnew_address\x18\x02 \x01(\t\"\x07\n\x05\x45mpty2\x84\x05\n\x06Server\x12p\n\x03Run\x12\x32.yggdrasil_decision_forests.distribute.proto.Query\x1a\x33.yggdrasil_decision_forests.distribute.proto.Answer\"\x00\x12\x82\x01\n\tWorkerRun\x12\x38.yggdrasil_decision_forests.distribute.proto.WorkerQuery\x1a\x39.yggdrasil_decision_forests.distribute.proto.WorkerAnswer\"\x00\x12|\n\x08Shutdown\x12:.yggdrasil_decision_forests.distribute.proto.ShutdownQuery\x1a\x32.yggdrasil_decision_forests.distribute.proto.Empty\"\x00\x12p\n\x04Ping\x12\x32.yggdrasil_decision_forests.distribute.proto.Empty\x1a\x32.yggdrasil_decision_forests.distribute.proto.Empty\"\x00\x12\x92\x01\n\x13UpdateWorkerAddress\x12\x45.yggdrasil_decision_forests.distribute.proto.UpdateWorkerAddressQuery\x1a\x32.yggdrasil_decision_forests.distribute.proto.Empty\"\x00:x\n\x04grpc\x12\x33.yggdrasil_decision_forests.distribute.proto.Config\x18\xe9\x07 \x01(\x0b\x32\x34.yggdrasil_decision_forests.distribute.proto.GRPCImp')
 
 _globals = globals()
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'yggdrasil_decision_forests.utils.distribute.implementations.grpc.grpc_pb2', _globals)
```

## Comparing `ydf-0.4.0.dist-info/LICENSE` & `ydf-0.4.1.dist-info/LICENSE`

 * *Ordering differences only*

 * *Files 8% similar despite different names*

```diff
@@ -1,243 +1,243 @@
-
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
-
-
-jszip
-Copyright (c) 2009-2016 Stuart Knightley, David Duponchel, Franz Buchinger, Antnio Afonso
-
-Permission is hereby granted, free of charge, to any person obtaining a copy
-of this software and associated documentation files (the "Software"), to deal
-in the Software without restriction, including without limitation the rights
-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the Software is
-furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in
-all copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
-THE SOFTWARE.
-
-rapidjson
-Permission is hereby granted, free of charge, to any person obtaining a copy
-of this software and associated documentation files (the "Software"), to deal
-in the Software without restriction, including without limitation the rights
-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the Software is
-furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in
-all copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
-THE SOFTWARE.
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
+
+jszip
+Copyright (c) 2009-2016 Stuart Knightley, David Duponchel, Franz Buchinger, Antnio Afonso
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in
+all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+THE SOFTWARE.
+
+rapidjson
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in
+all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+THE SOFTWARE.
```

## Comparing `ydf-0.4.0.dist-info/METADATA` & `ydf-0.4.1.dist-info/METADATA`

 * *Files 12% similar despite different names*

```diff
@@ -1,92 +1,92 @@
-Metadata-Version: 2.1
-Name: ydf
-Version: 0.4.0
-Summary: YDF (short for Yggdrasil Decision Forests) is a library for training, serving, evaluating and analyzing decision forest models such as Random Forest and Gradient Boosted Trees.
-Home-page: https://github.com/google/yggdrasil-decision-forests
-Author: Mathieu Guillame-Bert, Richard Stotz, Jan Pfeifer
-Author-email: decision-forests-contact@google.com
-License: Apache 2.0
-Project-URL: Documentation, https://ydf.readthedocs.io/
-Project-URL: Source, https://github.com/google/yggdrasil-decision-forests.git
-Project-URL: Tracker, https://github.com/google/yggdrasil-decision-forests/issues
-Keywords: machine learning decision forests random forest gradient boosted decision trees classification regression ranking uplift
-Classifier: Intended Audience :: Developers
-Classifier: Intended Audience :: Education
-Classifier: Intended Audience :: Science/Research
-Classifier: License :: OSI Approved :: Apache Software License
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Classifier: Programming Language :: Python :: 3.11
-Classifier: Programming Language :: Python :: 3 :: Only
-Classifier: Topic :: Scientific/Engineering
-Classifier: Topic :: Scientific/Engineering :: Mathematics
-Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
-Classifier: Topic :: Software Development
-Classifier: Topic :: Software Development :: Libraries
-Classifier: Topic :: Software Development :: Libraries :: Python Modules
-Requires-Python: >=3.8
-Description-Content-Type: text/markdown
-License-File: LICENSE
-Requires-Dist: numpy
-Requires-Dist: absl-py
-Requires-Dist: protobuf >=3.14
-Provides-Extra: pandas
-Requires-Dist: pandas ; extra == 'pandas'
-
-# Port of Yggdrasil / TensorFlow Decision Forests for Python
-
-The Python port of Yggdrasil Decision is a light-weight wrapper around Yggdrasil
-Decision Forests. It allows direct, fast access to YDF's methods and it also
-offers advanced import / export, evaluation and inspection methods. While the
-package is called YDF, the wrapping code is sometimes lovingly called *PYDF*.
-
-It is not a replacement for its sister project 
-[Tensorflow Decision Forests](https://github.com/tensorflow/decision-forests) 
-(TF-DF). Instead, it complements TF-DF for use cases that cannot be solved 
-through the Keras API.
-
-## Installation
-
-To install YDF, in Python, simply grab the package from pip:
-
-```
-pip install ydf
-```
-
-For build instructions, see INSTALLATION.md.
-
-## Usage Example
-
-```python
-import ydf
-import pandas as pd
-
-ds_path = "https://raw.githubusercontent.com/google/yggdrasil-decision-forests/main/yggdrasil_decision_forests/test_data/dataset"
-train_ds = pd.read_csv(f"{ds_path}/adult_train.csv")
-test_ds = pd.read_csv(f"{ds_path}/adult_test.csv")
-
-model = ydf.GradientBoostedTreesLearner(label="income").train(train_ds)
-
-print(model.evaluate(test_ds))
-
-model.save("my_model")
-
-loaded_model = ydf.load_model("my_model")
-```
-
-## Frequently Asked Questions
-
-*   **Is it PYDF or YDF?** The name of the library is simply ydf, and so is the
-    name of the corresponding Pip package. Internally, the team sometimes uses
-    the name *PYDF* because it fits so well.
-*   **What is the status of PYDF?** PYDF is currently in Alpha development. Most
-    parts already work well (training, evaluation, predicting, export), some new
-    features are yet to come. The API surface is mostly stable but may still 
-    change without notice.
-*   **Where is the documentation for PYDF?** The documentation is
-    available on https://ydf.readthedocs.org.
-*   **How should I pronounce PYDF?** The preferred pronunciation is 
-    "Py-dee-eff" / padif (IPA)
-
+Metadata-Version: 2.1
+Name: ydf
+Version: 0.4.1
+Summary: YDF (short for Yggdrasil Decision Forests) is a library for training, serving, evaluating and analyzing decision forest models such as Random Forest and Gradient Boosted Trees.
+Home-page: https://github.com/google/yggdrasil-decision-forests
+Author: Mathieu Guillame-Bert, Richard Stotz, Jan Pfeifer
+Author-email: decision-forests-contact@google.com
+License: Apache 2.0
+Project-URL: Documentation, https://ydf.readthedocs.io/
+Project-URL: Source, https://github.com/google/yggdrasil-decision-forests.git
+Project-URL: Tracker, https://github.com/google/yggdrasil-decision-forests/issues
+Keywords: machine learning decision forests random forest gradient boosted decision trees classification regression ranking uplift
+Classifier: Intended Audience :: Developers
+Classifier: Intended Audience :: Education
+Classifier: Intended Audience :: Science/Research
+Classifier: License :: OSI Approved :: Apache Software License
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
+Classifier: Programming Language :: Python :: 3 :: Only
+Classifier: Topic :: Scientific/Engineering
+Classifier: Topic :: Scientific/Engineering :: Mathematics
+Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
+Classifier: Topic :: Software Development
+Classifier: Topic :: Software Development :: Libraries
+Classifier: Topic :: Software Development :: Libraries :: Python Modules
+Requires-Python: >=3.8
+Description-Content-Type: text/markdown
+License-File: LICENSE
+Requires-Dist: numpy
+Requires-Dist: absl-py
+Requires-Dist: protobuf >=3.14
+Provides-Extra: pandas
+Requires-Dist: pandas ; extra == 'pandas'
+
+# Port of Yggdrasil / TensorFlow Decision Forests for Python
+
+The Python port of Yggdrasil Decision is a light-weight wrapper around Yggdrasil
+Decision Forests. It allows direct, fast access to YDF's methods and it also
+offers advanced import / export, evaluation and inspection methods. While the
+package is called YDF, the wrapping code is sometimes lovingly called *PYDF*.
+
+It is not a replacement for its sister project 
+[Tensorflow Decision Forests](https://github.com/tensorflow/decision-forests) 
+(TF-DF). Instead, it complements TF-DF for use cases that cannot be solved 
+through the Keras API.
+
+## Installation
+
+To install YDF, in Python, simply grab the package from pip:
+
+```
+pip install ydf
+```
+
+For build instructions, see INSTALLATION.md.
+
+## Usage Example
+
+```python
+import ydf
+import pandas as pd
+
+ds_path = "https://raw.githubusercontent.com/google/yggdrasil-decision-forests/main/yggdrasil_decision_forests/test_data/dataset"
+train_ds = pd.read_csv(f"{ds_path}/adult_train.csv")
+test_ds = pd.read_csv(f"{ds_path}/adult_test.csv")
+
+model = ydf.GradientBoostedTreesLearner(label="income").train(train_ds)
+
+print(model.evaluate(test_ds))
+
+model.save("my_model")
+
+loaded_model = ydf.load_model("my_model")
+```
+
+## Frequently Asked Questions
+
+*   **Is it PYDF or YDF?** The name of the library is simply ydf, and so is the
+    name of the corresponding Pip package. Internally, the team sometimes uses
+    the name *PYDF* because it fits so well.
+*   **What is the status of PYDF?** PYDF is currently in Alpha development. Most
+    parts already work well (training, evaluation, predicting, export), some new
+    features are yet to come. The API surface is mostly stable but may still 
+    change without notice.
+*   **Where is the documentation for PYDF?** The documentation is
+    available on https://ydf.readthedocs.org.
+*   **How should I pronounce PYDF?** The preferred pronunciation is 
+    "Py-dee-eff" / padif (IPA)
+
```

## Comparing `ydf-0.4.0.dist-info/RECORD` & `ydf-0.4.1.dist-info/RECORD`

 * *Files 20% similar despite different names*

```diff
@@ -1,115 +1,117 @@
-ydf/__init__.py,sha256=3ZVdWlK7GOWkVZPVGq8HPq32GyLCQNMej4MJVP569G8,2671
-ydf/api_test.py,sha256=YMm1lFuT78i2Ylw_wRMt-G2fyPeU9CbQarPBUp1l0js,6499
-ydf/version.py,sha256=wIqneop8CzlvkfPzy_g34zgDQSnTQkz-gTsBbLgdK5A,610
-ydf/cc/__init__.py,sha256=0aWRIYvAhUipL9ESDELU6myMkoplllYplp4pWmT191g,591
-ydf/cc/ydf.pyd,sha256=lPZGRRHRBICEFddk0ptdXUWFY-Q8TASoQh1-LYzhP3Q,11149312
-ydf/dataset/__init__.py,sha256=0aWRIYvAhUipL9ESDELU6myMkoplllYplp4pWmT191g,591
-ydf/dataset/dataset.py,sha256=R9swikRbFb3-kAuZl4Zil8NS9anSPtbWCrj6oOBVPRQ,24355
-ydf/dataset/dataset_test.py,sha256=AkubKbzCnmyfJEuJEUnZie9FNap4FMhXjw1zE8SLQh0,65185
-ydf/dataset/dataset_with_tf_test.py,sha256=Iwe20XjHKaopKVHe-n2mz-tCJf_jervrRu137VsN4q0,1762
-ydf/dataset/dataspec.py,sha256=IS1IPHBFovgnO9mL6Iyz1o9-PQcR7nqOZCOr7IgE2P8,23287
-ydf/dataset/dataspec_test.py,sha256=T2u2a-OQAaDpZiwP9JorFhnzsZr0NjSBGNIsgdKdqj8,13459
-ydf/dataset/io/__init__.py,sha256=0aWRIYvAhUipL9ESDELU6myMkoplllYplp4pWmT191g,591
-ydf/dataset/io/dataset_io.py,sha256=FDJwmXUFELjKepSNdrL8T6jaTC6MqfCM7ZGzfN0-ys8,5549
-ydf/dataset/io/dataset_io_test.py,sha256=_fKRDenuI3yyf4HluNaOIhZawBa5sEwPR9XCgPAwjaQ,1219
-ydf/dataset/io/dataset_io_types.py,sha256=q_ho0CXtXM3hNFCl-OiCjX-Nw1Jc7xkqeEupjismbHo,1718
-ydf/dataset/io/pandas_io.py,sha256=4uyqyzdiHXn7MLyMCsysVAuK64hsjnf4p--SK5pHd28,1917
-ydf/dataset/io/pandas_io_test.py,sha256=SrxqOuLThYq3mV3qXKYzvXWXcPT0UESIsyR4haBSl_Y,983
-ydf/dataset/io/tensorflow_io.py,sha256=WgRe9E2zxlQ4ZVqXpDhFt4hza-sEKGPm2sctF6cG8RM,1488
-ydf/learner/__init__.py,sha256=0aWRIYvAhUipL9ESDELU6myMkoplllYplp4pWmT191g,591
-ydf/learner/custom_loss.py,sha256=GPZ4kDgeTsptNNWGCazzrVyFQCuyLT_HXtTz1YFA9WA,12348
-ydf/learner/custom_loss_test.py,sha256=d5PSu0_Rk7LA5MD-CWt6NXVBfe_cGfkfdG3Uze1KMIA,25040
-ydf/learner/distributed_learner_test.py,sha256=sb3M_TgDh7dX8dPlkJ1Il2P5LRxIqHSqPLRCFN8rt9k,5141
-ydf/learner/generic_learner.py,sha256=1m1f58L2wZTTqOEBOu1OAncN51dFaum44ZXv-wdqTWo,20901
-ydf/learner/hyperparameters.py,sha256=nEXne6A5u77ypQq6NNkrRONBDSi_WBqSYc4D-Jl1OLU,3939
-ydf/learner/learner_test.py,sha256=6ZCt4KRfiBEPf1oDJfGGcMeOw1fQMi7dDum_ebuTEXI,30705
-ydf/learner/learner_with_tf_test.py,sha256=PlLuPLz_p2s-TzU5N9X-S2zonwnqtPHCJze3aPQG1dY,1574
-ydf/learner/specialized_learners.py,sha256=jC4s1xGHV_2B4YPHNYF6b3PrHl7OWwN39lQhzv0bdm8,127185
-ydf/learner/specialized_learners_pre_generated.py,sha256=ghPDYigfWnuMk-7QY-USHNTz2uIzIskvKKSUi8D39MI,129404
-ydf/learner/tuner.py,sha256=rU34SXOll_Q3eox1XBEastHLwGmY5cvPCp6XCjY0tDI,12862
-ydf/learner/tuner_test.py,sha256=BRRMGgX0dG_6c9I3JavFoB_Q_h4ZfxkDC4xpf5Hd46k,7252
-ydf/learner/worker.py,sha256=hcIm2B4mlPBxIIHYjIYTGK4lHpsvEqcm0GxQbjqEsqY,2155
-ydf/learner/worker_main.py,sha256=Fabh8CKx2QTL9kJX87pzJ2yGHwBfcEFjQhwpuztvbuY,879
-ydf/metric/__init__.py,sha256=0aWRIYvAhUipL9ESDELU6myMkoplllYplp4pWmT191g,591
-ydf/metric/display_metric.py,sha256=7ZcMa7F65TUzom0LxHAl9CPkd_XwU7VbJVuCiTc8M_M,12187
-ydf/metric/metric.py,sha256=KNFgwJGHygfEwEtloSRQYRRkkqkwilLKeMj3d4LBIxo,15907
-ydf/metric/metric_test.py,sha256=ujiDTdY_3sPAYIYmLI2YQE1_StlBVCTQGyE6qm2bRFc,8393
-ydf/model/__init__.py,sha256=0aWRIYvAhUipL9ESDELU6myMkoplllYplp4pWmT191g,591
-ydf/model/analysis.py,sha256=SNIhVvAuRdm0abqR9uZquo4kfxplUQufeVlfZ1SW-ZM,9665
-ydf/model/export_cc_generator.py,sha256=t3zBlUUN_imGOIe4_JAYWQpzYaNG734tBBfH50Jviu4,1467
-ydf/model/export_tf.py,sha256=fp8E4QyZKJRCgtfAgdCnIXpdwpX7DFeSo3YTAw3zUCA,22096
-ydf/model/generic_model.py,sha256=bEjBUVi7Y_WuYOKre2--3aPBWzezMYPfsFxXL_f1GGY,37036
-ydf/model/model_lib.py,sha256=64HlyQVj_rgz3U_6kpM9Y5-0EA2M4Zd3jwqU4YBV90s,7293
-ydf/model/model_metadata.py,sha256=4m3cea0_PadvE3s3PNROZfVhDu7PBYikLBsghei85J4,2005
-ydf/model/model_test.py,sha256=9DdkEGxmj97rkloY4R4EDtz9PUgpcA9NkGV1kAFGW9o,21913
-ydf/model/optimizer_logs.py,sha256=jE5WbwRoQfO-yCj7qV_6MPEreMALDqL_hZ_M1HLlZIY,2521
-ydf/model/optimizer_logs_test.py,sha256=FHUqLGWwylhpXJPy3n7xN_lcPqoVx9BCOOA8Ia6IkIM,2654
-ydf/model/template_cpp_export.py,sha256=Qoa5cqFncknEKwLbdjLbb4ghshJ0QbCedxnY9tpiRSU,8469
-ydf/model/tf_model_test.py,sha256=HRi5olcyhf5M8K7V5rNA8VExPFW7Ku9xXJnvHvCDkSU,43175
-ydf/model/decision_forest_model/__init__.py,sha256=0aWRIYvAhUipL9ESDELU6myMkoplllYplp4pWmT191g,591
-ydf/model/decision_forest_model/decision_forest_model.py,sha256=20cv_Em4RH_zg5LxXX9YwYAvMHx1YExisb-hW73_zkI,7760
-ydf/model/decision_forest_model/decision_forest_model_test.py,sha256=20TN4kEFWdFnIwNvy7dXyC1GjW_T2B5vCFrUW1_Xi24,5125
-ydf/model/gradient_boosted_trees_model/__init__.py,sha256=0aWRIYvAhUipL9ESDELU6myMkoplllYplp4pWmT191g,591
-ydf/model/gradient_boosted_trees_model/gradient_boosted_trees_model.py,sha256=_EZzBWII0hGJEpul_oJyVfir6QmN3ico6Hvm2Xfuodc,3644
-ydf/model/gradient_boosted_trees_model/gradient_boosted_trees_model_test.py,sha256=XFFU50Ff5q34xRcoBJo4PQgGUpJ72HDNah6AnwT1CdU,17156
-ydf/model/random_forest_model/__init__.py,sha256=0aWRIYvAhUipL9ESDELU6myMkoplllYplp4pWmT191g,591
-ydf/model/random_forest_model/random_forest_model.py,sha256=eu2Yue51pZKObjL_hqmBDFNQP7WM5PxfuZ3QZCfvsFs,5334
-ydf/model/random_forest_model/random_forest_model_test.py,sha256=2_ZHvdK8Rz4BMrr0JBmDevfjs5KdpUz8oIyA7cIAdxs,3485
-ydf/model/tree/__init__.py,sha256=Myp92InZhLWoERDDEK9avMSxFWrKgychdqSB9dNK9Lg,1801
-ydf/model/tree/condition.py,sha256=7-v47Ht4qOPUvcVQ6SztyUTKPhcRIhjVzdfbHBQhOH0,18431
-ydf/model/tree/condition_test.py,sha256=2UUe66T6eF6XYCnxqJn0wAP2OtVZdJ_2WuR1PCFWcC4,18098
-ydf/model/tree/node.py,sha256=ZyBZA7AbEmlmDFhCijxN0kY2wuPNfqPUm5U-g1Jz-nE,5846
-ydf/model/tree/node_test.py,sha256=3TKBcSOcORVMQYacrrxRrX8kqf6EVsyK08YnXzlnrUQ,1390
-ydf/model/tree/plot.py,sha256=kJmQQ8asa-bX_i_IXBf_3hKz7xkttj6-LhWYX2GCI-8,3726
-ydf/model/tree/plot_test.py,sha256=8edqwtl9g-5c7L9q6azb2nKn0oMjDW4kPahwKej09b0,2361
-ydf/model/tree/tree.py,sha256=5H7Bx3lLYWcSt6LjDi1Pxkko8cjundGvqzx5bZUf9Zw,5317
-ydf/model/tree/tree_test.py,sha256=9FqLYY3sEXLqBO3lyAFdZbSsbOHPCnY1WAWO_jR9voc,5604
-ydf/model/tree/value.py,sha256=yOMv-WwuOj8bw9lyodJNvRcfPRSNCXQqJ5cHUmf9zDk,7017
-ydf/model/tree/value_test.py,sha256=zyJ-XZSo2Zcul2lulqt4o-ZXXP6tgkGjoXY4gCwKehQ,4799
-ydf/utils/__init__.py,sha256=0aWRIYvAhUipL9ESDELU6myMkoplllYplp4pWmT191g,591
-ydf/utils/documentation.py,sha256=2W5BPH0fGHv3pKgX4a6AyxugdphXTBizq5Gfh-pbuW8,1020
-ydf/utils/html.py,sha256=UPwwcCleQ4mDZsJT5g5BLGsKtnuqwrhlBRyA1IJugxU,2982
-ydf/utils/html_test.py,sha256=mEYB1X28q3w59kTlopz_zxegpMIw5CGeNxbY-17WMyI,1634
-ydf/utils/log.py,sha256=z8lrIrcXOA-QxY4PGt33rg3AYqtcSF_kQHiEwPG2q2k,6172
-ydf/utils/paths.py,sha256=Ciy71sWeAfVAZYA98GaaazBMJatTAyVG5eEim8Y3LDg,1840
-ydf/utils/paths_test.py,sha256=iEWx9q8HpBTMN-mSOcrh1B2IILOWVBh5vc3DalzM7nY,1903
-ydf/utils/string_lib.py,sha256=EdDULtlPnAEoiPwaaMkIQCqQ_2oEPu1FAwCSNafy53A,4648
-ydf/utils/string_lib_test.py,sha256=HlhocHejvqr-TEIzWXrVLyhiOYJK5jGsBmlTt_8t2sA,4196
-ydf/utils/test_utils.py,sha256=UDjcFSL_F_d-EXq5NX4A9Ne2DPXFTZXx8aupN_Rok-E,6127
-ydf/utils/test_utils_test.py,sha256=ImsriUkWDXzbOpZdttPUujopn60ukYYtEBxEfxHVg_8,3031
-yggdrasil_decision_forests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-yggdrasil_decision_forests/dataset/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-yggdrasil_decision_forests/dataset/data_spec_pb2.py,sha256=TFHrODTrw0QA34YOo3X8Og3oCd0BKmn-l2x81WFmQ6w,12217
-yggdrasil_decision_forests/dataset/example_pb2.py,sha256=ch2LYxTDAsjrTbWmhMOr1zksAIU8WZhpxt9Yyu5znBc,2952
-yggdrasil_decision_forests/dataset/weight_pb2.py,sha256=5NF0txlSuRcqEW5gWFfBC1P51JzD9V6bbKFG2DvSqzA,3286
-yggdrasil_decision_forests/learner/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-yggdrasil_decision_forests/learner/abstract_learner_pb2.py,sha256=OIdnHByo33_JzbyGh6kGMNNRyN1Ff-bYoFbe0YnVlMI,6638
-yggdrasil_decision_forests/learner/hyperparameters_optimizer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-yggdrasil_decision_forests/learner/hyperparameters_optimizer/hyperparameters_optimizer_pb2.py,sha256=GRoOL_srr4eSgrsPzAc_F0tAcZwYiqlC62005SsvDVw,4481
-yggdrasil_decision_forests/learner/hyperparameters_optimizer/optimizers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-yggdrasil_decision_forests/learner/hyperparameters_optimizer/optimizers/random_pb2.py,sha256=lTdOYCnzaZPznWqFKtgHh_2r5okrVbRU6wRGYAv5rdE,1840
-yggdrasil_decision_forests/metric/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-yggdrasil_decision_forests/metric/metric_pb2.py,sha256=UdsQNDlcYffKNLgTSCdQi_K3PNIQVMWWrNPBP7F1CAM,17906
-yggdrasil_decision_forests/model/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-yggdrasil_decision_forests/model/abstract_model_pb2.py,sha256=KccVf0_HwXPZSkv11REPy3pmyaN6sn6OG6-7TFWayj4,5226
-yggdrasil_decision_forests/model/hyperparameter_pb2.py,sha256=_US7AMNv7SMKLzhcfR4nvD3ciTYLyRsHp4NznYGLPRM,7941
-yggdrasil_decision_forests/model/prediction_pb2.py,sha256=ZA-Qbh_JuLzT0y2FTfqhjjqLbr05hTFqoLs-FtHFEDk,3413
-yggdrasil_decision_forests/model/decision_tree/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-yggdrasil_decision_forests/model/decision_tree/decision_tree_pb2.py,sha256=4DtLGLHE7BKSmX8abU0od45kgDy7jy9fD7EHHzS8ltw,7492
-yggdrasil_decision_forests/model/random_forest/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-yggdrasil_decision_forests/model/random_forest/random_forest_pb2.py,sha256=x0YBDnLUWZ_t5vA4l_wzTO2-neuFH7NdP95Od-AcjF8,2920
-yggdrasil_decision_forests/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-yggdrasil_decision_forests/utils/distribution_pb2.py,sha256=-EeHWF5Vam15akZTHb-CWviwDYgwhjfb5rJPiTOX4_8,2958
-yggdrasil_decision_forests/utils/fold_generator_pb2.py,sha256=KcKD6TWlcMoOf2wmxw6UiEomFAZho8kUuhsAhoZINBA,3094
-yggdrasil_decision_forests/utils/model_analysis_pb2.py,sha256=8wnB-9bjGXaf-3LE2zHOkUuxVi6Oyn_5YMRxN0nxyRg,10351
-yggdrasil_decision_forests/utils/partial_dependence_plot_pb2.py,sha256=nXz_DA85gUX2DvR5XtQLRqZNSweMbVmcB4KAnMJFxK8,4789
-yggdrasil_decision_forests/utils/distribute/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-yggdrasil_decision_forests/utils/distribute/distribute_pb2.py,sha256=7_xeBOU88I3Pjid0P7Esp-CgBge13R38LWxTXMisi9c,1935
-yggdrasil_decision_forests/utils/distribute/implementations/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-yggdrasil_decision_forests/utils/distribute/implementations/grpc/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-yggdrasil_decision_forests/utils/distribute/implementations/grpc/grpc_pb2.py,sha256=oy8qq5NpufEJK-0JF6Bh8STt2eG1qy_-fIxXCIcBw7s,4791
-ydf-0.4.0.dist-info/LICENSE,sha256=0tFI2lyBCPcz1aglkX4x-p_W5EmSoyEdWR2_NGNK8Ns,13759
-ydf-0.4.0.dist-info/METADATA,sha256=4qtiETtG0MEt9tw8aplTRRenaUneeQtsEdHJenmK1UU,3890
-ydf-0.4.0.dist-info/WHEEL,sha256=Z6c-bE0pUM47a70GvqO_SvH_XXU0lm62gEAKtoNJ08A,100
-ydf-0.4.0.dist-info/top_level.txt,sha256=mSb0WiTnfMx7SmC2n2488UZsbf2MXrONDAEvNHMa50A,31
-ydf-0.4.0.dist-info/RECORD,,
+ydf-0.4.1.dist-info/RECORD,,
+ydf-0.4.1.dist-info/LICENSE,sha256=Ubf6odEh0jNmfeBHl95MlWL1omO2PDWSSnVUxdi_urM,13516
+ydf-0.4.1.dist-info/top_level.txt,sha256=JBIxud3tcFzrrZx-TWxmqSRw8uMkpY2n41ijNFTvbCk,4
+ydf-0.4.1.dist-info/METADATA,sha256=1Mng00PG2AbcZ_nyEtrJujYOvSRWb7O55dgmSZjvKwY,3798
+ydf-0.4.1.dist-info/WHEEL,sha256=rY0Y6THYM7EImsHfF-zs67o8pQciAsMw9_YuSvftjrQ,148
+ydf/__init__.py,sha256=jSpSCNnSngt-SVf7r-qaOmWbUnErCpGU3z8sVRlRUZ0,2685
+ydf/version.py,sha256=JvnhaNffXiL72K4KQXv4rT2vFVlpbRIE-IIPXkI1h0Y,595
+ydf/api_test.py,sha256=nnCq2tgq9C2puVfzTNw2ro07YOCSz-fy30nDYO_HlTE,6303
+ydf/learner/tuner_test.py,sha256=EETixJy1ZSX4Jun3sDT_xZHScz3PLduOoEBCnJOTiUw,6986
+ydf/learner/hyperparameters.py,sha256=XhvPib7gEH0YKFRfQ6ornC18gt9iAbwbEAnCYzgbTeo,3806
+ydf/learner/generic_learner.py,sha256=VVY1XwxxGRyMho9MhzO0DeAua79f9N9K53sZyFc_QQg,20199
+ydf/learner/specialized_learners_pre_generated.py,sha256=YYAenUq0ZTm8yTLpElwIH_pKHz2T4wODEI4-P8FCfuo,126989
+ydf/learner/learner_with_tf_test.py,sha256=ctNArUChM9ahm-hzcAP-Cm8ufMGoFQs38zvy8y2K1Ao,1823
+ydf/learner/tuner.py,sha256=MKFzH72wCZWDF5m4jR5rRbZYUw_5QZh1hyjnzAX3AiI,12450
+ydf/learner/worker.py,sha256=5slXNifrx110SrCOJCfjGxoJi6SZxcVHzKboURlip3s,2083
+ydf/learner/custom_loss.py,sha256=ERYVsM5PL5NXscu5V1Vsyo6IorQV966LNxS6qlrT90w,12000
+ydf/learner/__init__.py,sha256=A_euuTAB0Q4ALZsJL3oM_pptZ7DMR9HBuY-DG20pPuA,577
+ydf/learner/specialized_learners.py,sha256=33MSOkQ9PHl54mxwsfPlA1RacF5sViCnfL64ABX-lV0,124984
+ydf/learner/worker_main.py,sha256=bd8HXaGXxMrXL8CFLAdE-WOFfyCOIbLj7hFW8Lbv4Qw,850
+ydf/learner/distributed_learner_test.py,sha256=Q56WOcyRpnLv67BdKhxge-rv5Eg0hOch4dO7kEUjHRo,4972
+ydf/learner/custom_loss_test.py,sha256=STjLh3QkQthRWcYppuClk8m-z_AKNWFLgtiwyvxPdv4,24327
+ydf/learner/learner_test.py,sha256=mslMr4z7WRf8u4imiuHqPg17qw3zpr9ri26L4SRDAN4,29761
+ydf/utils/test_utils.py,sha256=ih4ps-jr71c0PhqHY1kFyFxBP93tz5mNNPi-2eW_V94,5918
+ydf/utils/test_utils_test.py,sha256=TTxa3Mb5PPUPEGk1vtucm7EuRBugNlKmriwfruN5RwM,2947
+ydf/utils/html.py,sha256=GcBR0WEa5rehME4Vj4tH2hI_Dq0DUc9slnBQiqyusA4,2864
+ydf/utils/paths.py,sha256=HJ7NhJhCYIO1WEfxCxf15zcXPzFtp7ES7Stftt_HK8k,1790
+ydf/utils/string_lib_test.py,sha256=nMcfBK6FciEOBi_g0XmvyGVySOVzL2ANuC1o0tA5DyA,4063
+ydf/utils/paths_test.py,sha256=nTGV2D8LEx3QiWbYEOrikAOWPZKYmTwofJ0nDGujtLo,1852
+ydf/utils/documentation.py,sha256=omX5VSZ1Zqse-GcOE22P-MyH7BCYLMBiXloxnFzbwn8,994
+ydf/utils/html_test.py,sha256=_P7oHPw2TZ1vBOfHlY9kNl1vPsmoYGxBHzBFApm1WQ0,1579
+ydf/utils/__init__.py,sha256=A_euuTAB0Q4ALZsJL3oM_pptZ7DMR9HBuY-DG20pPuA,577
+ydf/utils/string_lib.py,sha256=JdUNvOWBJD0Tv2lMxhO1cXcnjcjqPU0PT6D6bxj-mFU,4497
+ydf/utils/log.py,sha256=AE-8YbxidydbbC-28lLOVL4qqfDZrIRWoqAQHU4bvh8,5940
+ydf/model/optimizer_logs_test.py,sha256=K1rrfCshqHrbqfpytHctgku3mDMUXa0ZxsINMk92BbI,2544
+ydf/model/model_test.py,sha256=bcNChXciUdRZpVeAcTgRJV9bGxMIL77Rlju2-pNsHtE,21291
+ydf/model/template_cpp_export.py,sha256=d8IL-A8RASGxWObQqagUHk5Q82v__9n8dX1sr7L116g,8212
+ydf/model/model_lib.py,sha256=jsXu9s6tX6IFz-3aT2ifm-XXB35zBDystOj8yOTu5ME,7060
+ydf/model/analysis.py,sha256=j4L1M_vr_77PwX_gFKhhEFBUEuteSiT9p6ESy7YqeBk,9317
+ydf/model/__init__.py,sha256=A_euuTAB0Q4ALZsJL3oM_pptZ7DMR9HBuY-DG20pPuA,577
+ydf/model/export_cc_generator.py,sha256=6Nzj5nPugDuVxJdzN8CUjveEZPNbIf3t3fFMKUZEla4,1417
+ydf/model/export_tf.py,sha256=nFjZuGpRZxY3l6vXFeDfnOjXREeogZi81tnZ4kBegUU,21406
+ydf/model/model_metadata.py,sha256=U8BmZxRKeJlTOjuSqIvF1KszsuEgIlmY32KmwYI5ksg,1930
+ydf/model/generic_model.py,sha256=iIHSpqQMKsgyRd-nNMSgjUqCZ5piDxVC0gpewbRrnGs,35981
+ydf/model/tf_model_test.py,sha256=-zVOqmIPGj4srGBNLOxkdb7VA03RdjChc7gaw9mSBOY,41994
+ydf/model/export_jax.py,sha256=L4b8Vqfn2L-A_NvcnmWGRq4gJHNgCO2d_rv5VUMAnE0,4223
+ydf/model/jax_model_test.py,sha256=b4V5i5Wk1fpX4y5OXmEHhW89NbB0HmjkPVJVlmCdR3c,6756
+ydf/model/optimizer_logs.py,sha256=4i6_Pn6yJKtG7By9Wo_l2l9u982aq0khjUf7Y8YGKxU,2402
+ydf/model/decision_forest_model/decision_forest_model.py,sha256=sdUt1IMkUhT6q1OA4m6a-qKE5ow0R6zqN2iKKkPQrWE,7519
+ydf/model/decision_forest_model/__init__.py,sha256=A_euuTAB0Q4ALZsJL3oM_pptZ7DMR9HBuY-DG20pPuA,577
+ydf/model/decision_forest_model/decision_forest_model_test.py,sha256=tlu2RAk1bGiv_3y5OlzIyhnPbj6XXtR4cfMnNLVSnMg,4983
+ydf/model/gradient_boosted_trees_model/gradient_boosted_trees_model_test.py,sha256=rOl2AmGmaLxhG84V2WnGGA_LMirIPjMCnabK0WKv6yM,16644
+ydf/model/gradient_boosted_trees_model/gradient_boosted_trees_model.py,sha256=YlGEEcHId5Um1AGj4FbdfJZxrOt96i5wxINQNMcJgtg,3526
+ydf/model/gradient_boosted_trees_model/__init__.py,sha256=A_euuTAB0Q4ALZsJL3oM_pptZ7DMR9HBuY-DG20pPuA,577
+ydf/model/tree/plot_test.py,sha256=vCtvmHQ-Id-0ais3y5lGqIAxVqDZMIYPkmSZxSDxfQM,2278
+ydf/model/tree/condition.py,sha256=RWuD5CFMCw33XBAPddYJJYYe4Y7lTfomXRLxmH8gLlA,17824
+ydf/model/tree/condition_test.py,sha256=fx7F77zIvih9XSNUvqC955xqC4J14okFp6-aTz_nOTc,17539
+ydf/model/tree/node_test.py,sha256=kxD_xfPtMLq1eHfjaSMiwALM39fsk_w2j4Pp9olOyH4,1350
+ydf/model/tree/tree_test.py,sha256=CfHcxpODt0Iu49gJQNsLW8MOMS5qByDCa_IY0TiTXvM,5414
+ydf/model/tree/node.py,sha256=hQX4DRQcLzP2cvwpayRo547IT441BB9BmQKVwQXLB4s,5625
+ydf/model/tree/value_test.py,sha256=91Ycjjlsp8km4A72D8f0EVkP2NFirlW0Pdooq4z6MzI,4615
+ydf/model/tree/__init__.py,sha256=rSD-sQ_Ab7JYo_4A57kwo6YmMqFHWb71NtKD1Rz5BxI,1755
+ydf/model/tree/value.py,sha256=fDXarLhl7qEvvva3DKnxDDSUxpxWUQPrfoMdg5eYN4w,6771
+ydf/model/tree/plot.py,sha256=IO_6FZ7-mDzBcagXDMgA3pLfEaklzBQCdwFaCBWx2Ig,3603
+ydf/model/tree/tree.py,sha256=jw8PajwLOrz7vVPGPsMlw2pW1Ct3ycfqez1Vfl1Cpmc,5113
+ydf/model/random_forest_model/random_forest_model.py,sha256=SOFVqaG297wd4VK0l7Z-GYQIYy70R0n4Rv0N-Cl3fu0,5158
+ydf/model/random_forest_model/__init__.py,sha256=A_euuTAB0Q4ALZsJL3oM_pptZ7DMR9HBuY-DG20pPuA,577
+ydf/model/random_forest_model/random_forest_model_test.py,sha256=Xq4kiB8pGgYrapGB3gxvxF3S_QwBYeck1i5X7cKQkwU,3379
+ydf/metric/__init__.py,sha256=A_euuTAB0Q4ALZsJL3oM_pptZ7DMR9HBuY-DG20pPuA,577
+ydf/metric/metric.py,sha256=8OW64U7w4Tu5elWWGSb4HObBTlG_2npbdDDeUABzV50,15362
+ydf/metric/display_metric.py,sha256=V9UzQOdGqqqI2fJBl5lJ6pzjJDCpRS0E2VuCmBfudac,11702
+ydf/metric/metric_test.py,sha256=s6L-Aewo1c6lpTh0BXDvDbWuxupONgd_ZgBpIAfpRcs,8045
+ydf/dataset/dataspec.py,sha256=nuBSas3mqoBGFXBj-d8DzehB_6ki9WJK5MSvZ1Mfkl4,22625
+ydf/dataset/dataset_test.py,sha256=FvbAeJE6zr0fZYdpCYhXA_lyJNhRMqN53qaSB5DgZOQ,63339
+ydf/dataset/dataset.py,sha256=jstmmbsLSErfwkhzkISxl9W24_2SEaOH-Lpt6jme0iQ,23726
+ydf/dataset/__init__.py,sha256=A_euuTAB0Q4ALZsJL3oM_pptZ7DMR9HBuY-DG20pPuA,577
+ydf/dataset/dataset_with_tf_test.py,sha256=0zvBdm0j6SCYU_MfIG-smMj0LRZesTLb0bmwGZ8mIlY,1692
+ydf/dataset/dataspec_test.py,sha256=rRYCqShPvQC81VKvQSgx--PlH5ZootEtdeVRk0xfEOE,13051
+ydf/dataset/io/pandas_io.py,sha256=mMlUD7JXaIp-NnKzOPu8xgcdvWibL235siSNhFUbC8g,1853
+ydf/dataset/io/dataset_io_types.py,sha256=4dww9d4GlPVhVLAx9muP4U29WmftJgLy2l8fJaj7c8g,1669
+ydf/dataset/io/__init__.py,sha256=A_euuTAB0Q4ALZsJL3oM_pptZ7DMR9HBuY-DG20pPuA,577
+ydf/dataset/io/dataset_io.py,sha256=BAuMrGnFRAf3f1_yyUI4jH7VKpVGaq608aR-yvkjaqA,5377
+ydf/dataset/io/pandas_io_test.py,sha256=ova2MIvOxRAoFA8AUTMAPp0C4LXt3NEADdj3_ID6t3s,952
+ydf/dataset/io/tensorflow_io.py,sha256=QD5kOcimhzAZ_guh6wrx6BGkkv8AfYvgc2LwYuwVAl0,1773
+ydf/dataset/io/dataset_io_test.py,sha256=bRk7TwvxB93FaRvlRW3mTKQl6sRo5drmzffMS8DGgHU,1180
+ydf/proto/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+ydf/proto/learner/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+ydf/proto/learner/abstract_learner_pb2.py,sha256=Xid7GLFyJ1Hr8ILGji-fh0VoNeACC8odUz6RHEpcHxo,6553
+ydf/proto/learner/hyperparameters_optimizer/hyperparameters_optimizer_pb2.py,sha256=-Eu-SO0kOm5hwtBxSPS1b4p8QeKUqp7GeBupD0A1368,4430
+ydf/proto/learner/hyperparameters_optimizer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+ydf/proto/learner/hyperparameters_optimizer/optimizers/random_pb2.py,sha256=qJZ6pFC_PKpkrwyp6RoxFOagxtDF_8r1e6vdfuN6tO4,1823
+ydf/proto/learner/hyperparameters_optimizer/optimizers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+ydf/proto/utils/model_analysis_pb2.py,sha256=VDgQIa0QhZcKAySehCE1jiMQY9AdYrR0CfcYMpxsoV0,10266
+ydf/proto/utils/fold_generator_pb2.py,sha256=KcKD6TWlcMoOf2wmxw6UiEomFAZho8kUuhsAhoZINBA,3094
+ydf/proto/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+ydf/proto/utils/partial_dependence_plot_pb2.py,sha256=Pym8XHvK9ngQitjtDdFAZMDN03vVMMm4DmMCCEf8LmA,4755
+ydf/proto/utils/distribution_pb2.py,sha256=-EeHWF5Vam15akZTHb-CWviwDYgwhjfb5rJPiTOX4_8,2958
+ydf/proto/utils/distribute/distribute_pb2.py,sha256=7_xeBOU88I3Pjid0P7Esp-CgBge13R38LWxTXMisi9c,1935
+ydf/proto/utils/distribute/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+ydf/proto/utils/distribute/implementations/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+ydf/proto/utils/distribute/implementations/grpc/grpc_pb2.py,sha256=9Nu5IrZkOXZNdkuiTOrH8XsNbxyXphquasSUmaCibys,4774
+ydf/proto/utils/distribute/implementations/grpc/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+ydf/proto/model/hyperparameter_pb2.py,sha256=_US7AMNv7SMKLzhcfR4nvD3ciTYLyRsHp4NznYGLPRM,7941
+ydf/proto/model/prediction_pb2.py,sha256=P6XfPm5SCqeQgcZYlzfW9vrFQ5qLd78oYR-A654O9D8,3396
+ydf/proto/model/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+ydf/proto/model/abstract_model_pb2.py,sha256=Q_1mY8Twx0ukv8JpPsXuNWMqrtmuwWJnTPFI6fnKxuk,5192
+ydf/proto/model/decision_tree/decision_tree_pb2.py,sha256=7j1utiNfhB2oDQ346ztwzts0rJZFhrUwhyEQe8jestQ,7475
+ydf/proto/model/decision_tree/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+ydf/proto/model/random_forest/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+ydf/proto/model/random_forest/random_forest_pb2.py,sha256=Q7XImAA_Fw27h6zdCdmlt5qS9ts1o5Gc8vNKR4-4ofw,2886
+ydf/proto/metric/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+ydf/proto/metric/metric_pb2.py,sha256=fWF8YBqMw3x0F1psET_M9TvgrPQWRWwVMnDWa0RxUSg,17821
+ydf/proto/dataset/weight_pb2.py,sha256=5NF0txlSuRcqEW5gWFfBC1P51JzD9V6bbKFG2DvSqzA,3286
+ydf/proto/dataset/data_spec_pb2.py,sha256=TFHrODTrw0QA34YOo3X8Og3oCd0BKmn-l2x81WFmQ6w,12217
+ydf/proto/dataset/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+ydf/proto/dataset/example_pb2.py,sha256=ch2LYxTDAsjrTbWmhMOr1zksAIU8WZhpxt9Yyu5znBc,2952
+ydf/cc/ydf.so,sha256=7mBUbxoBf8ptb9w4eZQVGM3RzGnIG1mqmetDNf3c0jw,28886744
+ydf/cc/__init__.py,sha256=A_euuTAB0Q4ALZsJL3oM_pptZ7DMR9HBuY-DG20pPuA,577
```

