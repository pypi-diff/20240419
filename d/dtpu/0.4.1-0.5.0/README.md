# Comparing `tmp/dtpu-0.4.1-cp38-abi3-win_amd64.whl.zip` & `tmp/dtpu-0.5.0-cp38-abi3-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,41 +1,43 @@
-Zip file size: 2419204 bytes, number of entries: 39
--rw-r--r--  4.6 unx     1524 b- defN 24-Mar-08 15:20 dtpu-0.4.1.dist-info/METADATA
--rw-r--r--  4.6 unx       94 b- defN 24-Mar-08 15:20 dtpu-0.4.1.dist-info/WHEEL
--rw-r--r--  4.6 unx      195 b- defN 24-Mar-08 15:20 dtpu-0.4.1.dist-info/entry_points.txt
--rw-r--r--  4.6 unx    11558 b- defN 24-Mar-08 15:20 dtpu-0.4.1.dist-info/license_files/LICENSE
--rw-r--r--  4.6 unx    15713 b- defN 24-Mar-08 15:20 text_utils/api/cli.py
--rw-r--r--  4.6 unx    13386 b- defN 24-Mar-08 15:20 text_utils/api/processor.py
--rw-r--r--  4.6 unx     7100 b- defN 24-Mar-08 15:20 text_utils/api/server.py
--rw-r--r--  4.6 unx     6615 b- defN 24-Mar-08 15:20 text_utils/api/table.py
--rw-r--r--  4.6 unx    56880 b- defN 24-Mar-08 15:20 text_utils/api/trainer.py
--rw-r--r--  4.6 unx     7861 b- defN 24-Mar-08 15:20 text_utils/api/utils.py
--rw-r--r--  4.6 unx      451 b- defN 24-Mar-08 15:20 text_utils/api/__init__.py
--rw-r--r--  4.6 unx     1078 b- defN 24-Mar-08 15:20 text_utils/cli/create_continuation_index.py
--rw-r--r--  4.6 unx     1782 b- defN 24-Mar-08 15:20 text_utils/cli/create_dictionary.py
--rw-r--r--  4.6 unx     1885 b- defN 24-Mar-08 15:20 text_utils/cli/train_bpe.py
--rw-r--r--  4.6 unx        0 b- defN 24-Mar-08 15:20 text_utils/cli/__init__.py
--rw-r--r--  4.6 unx     5731 b- defN 24-Mar-08 15:20 text_utils/configuration.py
--rw-r--r--  4.6 unx     1951 b- defN 24-Mar-08 15:20 text_utils/distributed.py
--rw-r--r--  4.6 unx     2399 b- defN 24-Mar-08 15:20 text_utils/hook.py
--rw-r--r--  4.6 unx    16641 b- defN 24-Mar-08 15:20 text_utils/inference/__init__.py
--rw-r--r--  4.6 unx     2895 b- defN 24-Mar-08 15:20 text_utils/io.py
--rw-r--r--  4.6 unx     2268 b- defN 24-Mar-08 15:20 text_utils/logging.py
--rw-r--r--  4.6 unx     1104 b- defN 24-Mar-08 15:20 text_utils/mask.py
--rw-r--r--  4.6 unx     9805 b- defN 24-Mar-08 15:20 text_utils/modules/decoder.py
--rw-r--r--  4.6 unx    10202 b- defN 24-Mar-08 15:20 text_utils/modules/embedding.py
--rw-r--r--  4.6 unx    12741 b- defN 24-Mar-08 15:20 text_utils/modules/encoder.py
--rw-r--r--  4.6 unx     1426 b- defN 24-Mar-08 15:20 text_utils/modules/grouping.py
--rw-r--r--  4.6 unx     1777 b- defN 24-Mar-08 15:20 text_utils/modules/head.py
--rw-r--r--  4.6 unx     5147 b- defN 24-Mar-08 15:20 text_utils/modules/loss.py
--rw-r--r--  4.6 unx     3186 b- defN 24-Mar-08 15:20 text_utils/modules/moe.py
--rw-r--r--  4.6 unx     3285 b- defN 24-Mar-08 15:20 text_utils/modules/optimizer.py
--rw-r--r--  4.6 unx     8522 b- defN 24-Mar-08 15:20 text_utils/modules/scheduler.py
--rw-r--r--  4.6 unx      331 b- defN 24-Mar-08 15:20 text_utils/modules/utils.py
--rw-r--r--  4.6 unx        0 b- defN 24-Mar-08 15:20 text_utils/modules/__init__.py
--rw-r--r--  4.6 unx        0 b- defN 24-Mar-08 15:20 text_utils/py.typed
--rw-r--r--  4.6 unx     9301 b- defN 24-Mar-08 15:20 text_utils/tensorboard.py
--rw-r--r--  4.6 unx       23 b- defN 24-Mar-08 15:20 text_utils/version.py
--rw-r--r--  4.6 unx      364 b- defN 24-Mar-08 15:20 text_utils/__init__.py
--rwxr-xr-x  4.6 unx  6122496 b- defN 24-Mar-08 15:20 text_utils/_internal.pyd
--rw-r--r--  4.6 unx     3206 b- defN 24-Mar-08 15:20 dtpu-0.4.1.dist-info/RECORD
-39 files, 6350923 bytes uncompressed, 2414130 bytes compressed:  62.0%
+Zip file size: 2423389 bytes, number of entries: 41
+-rw-r--r--  4.6 unx     1542 b- defN 24-Apr-19 12:52 dtpu-0.5.0.dist-info/METADATA
+-rw-r--r--  4.6 unx       94 b- defN 24-Apr-19 12:52 dtpu-0.5.0.dist-info/WHEEL
+-rw-r--r--  4.6 unx      195 b- defN 24-Apr-19 12:52 dtpu-0.5.0.dist-info/entry_points.txt
+-rw-r--r--  4.6 unx    11558 b- defN 24-Apr-19 12:52 dtpu-0.5.0.dist-info/license_files/LICENSE
+-rw-r--r--  4.6 unx    12567 b- defN 24-Apr-19 12:52 text_utils/api/cli.py
+-rw-r--r--  4.6 unx    11688 b- defN 24-Apr-19 12:52 text_utils/api/processor.py
+-rw-r--r--  4.6 unx     7100 b- defN 24-Apr-19 12:52 text_utils/api/server.py
+-rw-r--r--  4.6 unx     6620 b- defN 24-Apr-19 12:52 text_utils/api/table.py
+-rw-r--r--  4.6 unx    56880 b- defN 24-Apr-19 12:52 text_utils/api/trainer.py
+-rw-r--r--  4.6 unx     7861 b- defN 24-Apr-19 12:52 text_utils/api/utils.py
+-rw-r--r--  4.6 unx      451 b- defN 24-Apr-19 12:52 text_utils/api/__init__.py
+-rw-r--r--  4.6 unx     1086 b- defN 24-Apr-19 12:52 text_utils/cli/create_continuation_index.py
+-rw-r--r--  4.6 unx     1782 b- defN 24-Apr-19 12:52 text_utils/cli/create_dictionary.py
+-rw-r--r--  4.6 unx     1885 b- defN 24-Apr-19 12:52 text_utils/cli/train_bpe.py
+-rw-r--r--  4.6 unx        0 b- defN 24-Apr-19 12:52 text_utils/cli/__init__.py
+-rw-r--r--  4.6 unx     5731 b- defN 24-Apr-19 12:52 text_utils/configuration.py
+-rw-r--r--  4.6 unx     2278 b- defN 24-Apr-19 12:52 text_utils/constraints.py
+-rw-r--r--  4.6 unx     1951 b- defN 24-Apr-19 12:52 text_utils/distributed.py
+-rw-r--r--  4.6 unx     2399 b- defN 24-Apr-19 12:52 text_utils/hook.py
+-rw-r--r--  4.6 unx     8116 b- defN 24-Apr-19 12:52 text_utils/inference/utils.py
+-rw-r--r--  4.6 unx    13033 b- defN 24-Apr-19 12:52 text_utils/inference/__init__.py
+-rw-r--r--  4.6 unx     2895 b- defN 24-Apr-19 12:52 text_utils/io.py
+-rw-r--r--  4.6 unx     2268 b- defN 24-Apr-19 12:52 text_utils/logging.py
+-rw-r--r--  4.6 unx     1104 b- defN 24-Apr-19 12:52 text_utils/mask.py
+-rw-r--r--  4.6 unx     9805 b- defN 24-Apr-19 12:52 text_utils/modules/decoder.py
+-rw-r--r--  4.6 unx    10202 b- defN 24-Apr-19 12:52 text_utils/modules/embedding.py
+-rw-r--r--  4.6 unx    12741 b- defN 24-Apr-19 12:52 text_utils/modules/encoder.py
+-rw-r--r--  4.6 unx     1426 b- defN 24-Apr-19 12:52 text_utils/modules/grouping.py
+-rw-r--r--  4.6 unx     1777 b- defN 24-Apr-19 12:52 text_utils/modules/head.py
+-rw-r--r--  4.6 unx     5147 b- defN 24-Apr-19 12:52 text_utils/modules/loss.py
+-rw-r--r--  4.6 unx     3186 b- defN 24-Apr-19 12:52 text_utils/modules/moe.py
+-rw-r--r--  4.6 unx     3285 b- defN 24-Apr-19 12:52 text_utils/modules/optimizer.py
+-rw-r--r--  4.6 unx     8522 b- defN 24-Apr-19 12:52 text_utils/modules/scheduler.py
+-rw-r--r--  4.6 unx      331 b- defN 24-Apr-19 12:52 text_utils/modules/utils.py
+-rw-r--r--  4.6 unx        0 b- defN 24-Apr-19 12:52 text_utils/modules/__init__.py
+-rw-r--r--  4.6 unx        0 b- defN 24-Apr-19 12:52 text_utils/py.typed
+-rw-r--r--  4.6 unx     9301 b- defN 24-Apr-19 12:52 text_utils/tensorboard.py
+-rw-r--r--  4.6 unx       23 b- defN 24-Apr-19 12:52 text_utils/version.py
+-rw-r--r--  4.6 unx      364 b- defN 24-Apr-19 12:52 text_utils/__init__.py
+-rwxr-xr-x  4.6 unx  6175232 b- defN 24-Apr-19 12:52 text_utils/_internal.pyd
+-rw-r--r--  4.6 unx     3374 b- defN 24-Apr-19 12:52 dtpu-0.5.0.dist-info/RECORD
+41 files, 6405800 bytes uncompressed, 2418055 bytes compressed:  62.3%
```

## zipnote {}

```diff
@@ -1,17 +1,17 @@
-Filename: dtpu-0.4.1.dist-info/METADATA
+Filename: dtpu-0.5.0.dist-info/METADATA
 Comment: 
 
-Filename: dtpu-0.4.1.dist-info/WHEEL
+Filename: dtpu-0.5.0.dist-info/WHEEL
 Comment: 
 
-Filename: dtpu-0.4.1.dist-info/entry_points.txt
+Filename: dtpu-0.5.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: dtpu-0.4.1.dist-info/license_files/LICENSE
+Filename: dtpu-0.5.0.dist-info/license_files/LICENSE
 Comment: 
 
 Filename: text_utils/api/cli.py
 Comment: 
 
 Filename: text_utils/api/processor.py
 Comment: 
@@ -42,20 +42,26 @@
 
 Filename: text_utils/cli/__init__.py
 Comment: 
 
 Filename: text_utils/configuration.py
 Comment: 
 
+Filename: text_utils/constraints.py
+Comment: 
+
 Filename: text_utils/distributed.py
 Comment: 
 
 Filename: text_utils/hook.py
 Comment: 
 
+Filename: text_utils/inference/utils.py
+Comment: 
+
 Filename: text_utils/inference/__init__.py
 Comment: 
 
 Filename: text_utils/io.py
 Comment: 
 
 Filename: text_utils/logging.py
@@ -108,11 +114,11 @@
 
 Filename: text_utils/__init__.py
 Comment: 
 
 Filename: text_utils/_internal.pyd
 Comment: 
 
-Filename: dtpu-0.4.1.dist-info/RECORD
+Filename: dtpu-0.5.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## text_utils/api/cli.py

```diff
@@ -1,14 +1,14 @@
 import argparse
 from io import TextIOWrapper
 import sys
 import time
 import logging
 import warnings
-from typing import Iterator, Iterable, Union, Optional, Type
+from typing import Iterator, Iterable, Union, Type
 try:
     import readline  # noqa
 except ImportError:
     # readline is e.g. not available on Windows
     pass
 
 import torch
@@ -29,22 +29,23 @@
     def parser(
         cls,
         name: str,
         description: str
     ) -> argparse.ArgumentParser:
         parser = argparse.ArgumentParser(name, description)
         model_group = parser.add_mutually_exclusive_group()
+        default_model = cls.text_processor_cls.default_model()
         model_group.add_argument(
             "-m",
             "--model",
             choices=[
                 model.name for model in
                 cls.text_processor_cls.available_models()
             ],
-            default=cls.text_processor_cls.default_model().name,
+            default=None if default_model is None else default_model.name,
             help=f"Name of the model to use for {cls.text_processor_cls.task}"
         )
         model_group.add_argument(
             "-e",
             "--experiment",
             type=str,
             default=None,
@@ -70,28 +71,14 @@
             "-i",
             "--interactive",
             action="store_true",
             default=None,
             help="Start an interactive session where your command line input is processed"
         )
         parser.add_argument(
-            "-if",
-            "--input-format",
-            choices=cls.text_processor_cls.supported_input_formats(),
-            default=cls.text_processor_cls.supported_input_formats()[0],
-            help="Format of the text input"
-        )
-        parser.add_argument(
-            "-of",
-            "--output-format",
-            choices=cls.text_processor_cls.supported_output_formats(),
-            default=cls.text_processor_cls.supported_output_formats()[0],
-            help="Format of the text output"
-        )
-        parser.add_argument(
             "-o",
             "--out-path",
             type=str,
             default=None,
             help="Path where processed text should be saved to"
         )
         parser.add_argument(
@@ -136,22 +123,14 @@
         parser.add_argument(
             "-l",
             "--list",
             action="store_true",
             help="List all available models with short descriptions"
         )
         parser.add_argument(
-            "--precision",
-            choices=["auto", "fp32", "fp16", "bfp16"],
-            default="auto",
-            help="Choose the precision for inference, fp16 or bfp16 can result in faster runtimes when running on a "
-            "new GPU that supports lower precision, but it can be slower on older GPUs. Auto will set the precision to "
-            "the precision used for training if it is available, otherwise it will use fp32."
-        )
-        parser.add_argument(
             "-v",
             "--version",
             action="store_true",
             help=f"Print name and version of the underlying {cls.text_processor_cls.task} library"
         )
         parser.add_argument(
             "--force-download",
@@ -190,92 +169,68 @@
             "--log-level",
             type=str,
             choices=["none", "info", "debug"],
             default="none",
             help="Sets the logging level for the underlying loggers"
         )
         parser.add_argument(
-            "--lang",
-            type=str,
-            default=None,
-            help="Specify the language of the input, only allowed if the chosen model supports multiple languages. "
-            "This language setting is ignored if the input format already specifies a language for each input."
-        )
-        parser.add_argument(
             "--profile",
             type=str,
             default=None,
             help="Run CLI with cProfile profiler on and output stats to this file"
         )
         return parser
 
     def __init__(self, args: argparse.Namespace):
         self.args = args
 
     def version(self) -> str:
         raise NotImplementedError
 
-    def parse_input(self, ipt: str, lang: Optional[str]) -> data.InferenceData:
-        item = data.InferenceData.from_str(ipt, self.args.input_format)
-        if item.language is None and lang is not None:
-            item.language = lang
-        return item
-
-    def format_output(self, item: data.InferenceData) -> Iterable[str]:
-        return [item.to_str(self.args.output_format)]
+    def format_output(self, output: str) -> Iterable[str]:
+        return [output]
 
     def _run_with_profiling(self, file: str) -> None:
         import cProfile
         cProfile.runctx("self.run()", globals(), locals(), file)
 
     def process_iter(
         self,
-        text_processor: TextProcessor,
-        iter: Iterator[data.InferenceData]
-    ) -> Iterator[data.InferenceData]:
+        processor: TextProcessor,
+        iter: Iterator[str]
+    ) -> Iterator[str]:
         raise NotImplementedError
 
     def process_file(
         self,
-        text_processor: TextProcessor,
-        path: str,
-        lang: Optional[str],
-        out_file: Union[str, TextIOWrapper]
+        processor: TextProcessor,
+        input_file: str,
+        output_file: str | TextIOWrapper
     ) -> None:
         raise NotImplementedError
 
     def setup(self) -> TextProcessor:
         device = self.args.device or (
-            "cuda" if torch.cuda.is_available() else "cpu")
+            "cuda" if torch.cuda.is_available()
+            else "cpu"
+        )
         if self.args.experiment:
             cor = self.text_processor_cls.from_experiment(
                 experiment_dir=self.args.experiment,
                 device=device
             )
         else:
             cor = self.text_processor_cls.from_pretrained(
                 model=self.args.model,
                 device=device,
                 download_dir=self.args.download_dir,
                 cache_dir=self.args.cache_dir,
                 force_download=self.args.force_download
             )
 
-        if self.args.lang is not None:
-            supported_languages = cor.supported_languages()
-            assert supported_languages is not None, \
-                f"language {self.args.lang} specified but model does not " \
-                f"support multiple languages"
-            assert self.args.lang in supported_languages, \
-                f"the model supports the languages {supported_languages}, " \
-                f"but {self.args.lang} was specified"
-
-        if self.args.precision != "auto":
-            cor.set_precision(self.args.precision)
-
         return cor
 
     @staticmethod
     def inference_data_size(item: data.InferenceData) -> int:
         return len(item.text.encode("utf8"))
 
     def run(self) -> None:
@@ -318,28 +273,27 @@
         for d in self.cor.devices:
             if d.type == "cuda":
                 torch.cuda.reset_peak_memory_stats(d)
 
         start = time.perf_counter()
         if self.args.process is not None:
             self.args.progress = False
-            ipt = self.parse_input(self.args.process, self.args.lang)
-            opt = next(self.process_iter(self.cor, iter([ipt])))
+            opt = next(self.process_iter(self.cor, iter([self.args.process])))
             for line in self.format_output(opt):
                 print(line)
 
         elif self.args.file is not None:
             if self.args.out_path is None:
                 out = sys.stdout
                 assert isinstance(out, TextIOWrapper)
             else:
                 assert isinstance(self.args.out_path, str)
                 out = self.args.out_path
 
-            self.process_file(self.cor, self.args.file, self.args.lang, out)
+            self.process_file(self.cor, self.args.file, out)
 
             if self.args.report:
                 for d in self.cor.devices:
                     if d.type == "cuda":
                         torch.cuda.synchronize(d)
                 end = time.perf_counter()
 
@@ -348,79 +302,64 @@
                 report = generate_report(
                     self.cor.task,
                     self.cor.name,
                     self.cor.model,
                     num_lines,
                     num_bytes,
                     end - start,
-                    self.cor._precision_dtype,
                     self.args.batch_size,
                     not self.args.unsorted,
                     self.cor.devices,
-                    batch_max_tokens=self.args.batch_max_tokens,
+                    next(self.cor.model.parameters()).dtype,
+                    self.args.batch_max_tokens,
                 )
                 print(report)
 
         elif self.args.interactive:
             self.args.progress = False
             while True:
-                ipt = self.parse_input(input(">> "), self.args.lang)
+                ipt = data.InferenceData(input(">> "))
                 opt = next(self.process_iter(self.cor, iter([ipt])))
                 for line in self.format_output(opt):
                     print(line)
 
         else:
             if sys.stdin.isatty():
                 return
 
             try:
-                if self.args.unsorted:
-                    # correct lines from stdin as they come
-                    input_it = (
-                        self.parse_input(line.rstrip("\r\n"), self.args.lang)
-                        for line in sys.stdin
-                    )
-                    sized_it = ProgressIterator(
-                        input_it,
-                        self.inference_data_size
-                    )
-                    outputs = self.process_iter(self.cor, sized_it)
-                    for opt in outputs:
-                        for line in self.format_output(opt):
-                            print(line)
-                else:
-                    # read stdin completely, then potentially sort and correct
-                    inputs = [
-                        self.parse_input(line.rstrip("\r\n"), self.args.lang)
-                        for line in sys.stdin
-                    ]
-                    sized_it = ProgressIterator(
-                        iter(inputs),
-                        self.inference_data_size
-                    )
-                    outputs = self.process_iter(self.cor, sized_it)
-                    for opt in outputs:
-                        for line in self.format_output(opt):
-                            print(line)
+                # correct lines from stdin as they come
+                input_it = (
+                    line.rstrip("\r\n")
+                    for line in sys.stdin
+                )
+                sized_it = ProgressIterator(
+                    input_it,
+                    self.inference_data_size
+                )
+                outputs = self.process_iter(self.cor, sized_it)
+                for opt in outputs:
+                    for line in self.format_output(opt):
+                        print(line)
 
                 if self.args.report:
                     for d in self.cor.devices:
                         if d.type == "cuda":
                             torch.cuda.synchronize(d)
 
                     report = generate_report(
                         self.cor.task,
                         self.cor.name,
                         self.cor.model,
                         sized_it.num_items,
                         sized_it.total_size,
                         time.perf_counter() - start,
-                        self.cor._precision_dtype,
                         self.args.batch_size,
                         not self.args.unsorted,
                         self.cor.devices,
-                        batch_max_tokens=self.args.batch_max_tokens,
+                        next(self.cor.model.parameters()).dtype,
+                        self.args.batch_max_tokens,
                     )
                     print(report)
 
             except BrokenPipeError:
                 pass
```

## text_utils/api/processor.py

```diff
@@ -1,17 +1,17 @@
 import collections
 import math
 import sys
 import os
 import pprint
-from typing import Dict, List, Optional, Union, Tuple, Iterator, Any
+from typing import Iterator, Any
 
 from tqdm import tqdm
 import torch
-from torch import autocast, nn
+from torch import nn
 from torch.backends import cudnn, cuda
 
 from text_utils import (
     api,
     logging,
     configuration,
     io,
@@ -26,26 +26,29 @@
     ["name", "description", "tags"]
 )
 
 
 class TextProcessor:
     task: str
     pretrained: bool = False
+    devices: list[torch.device]
 
     @classmethod
     def _task_upper(cls) -> str:
         return cls.task.upper()
 
     @classmethod
-    def available_models(cls) -> List[ModelInfo]:
+    def available_models(cls) -> list[ModelInfo]:
         raise NotImplementedError
 
     @classmethod
-    def default_model(cls) -> ModelInfo:
+    def default_model(cls) -> ModelInfo | None:
         available_models = cls.available_models()
+        if len(available_models) == 0:
+            return None
         for info in available_models:
             if "default" in info.tags:
                 return info
         return available_models[0]
 
     @classmethod
     def _model_url(cls, model: str) -> str:
@@ -74,22 +77,25 @@
                 task_name
             )
         )
 
     @classmethod
     def from_pretrained(
         cls,
-        model: Optional[str] = None,
+        model: str | None = None,
         device: Device = "cuda",
-        download_dir: Optional[str] = None,
-        cache_dir: Optional[str] = None,
+        download_dir: str | None = None,
+        cache_dir: str | None = None,
         force_download: bool = False
     ):
         if model is None:
-            model = cls.default_model().name
+            default = cls.default_model()
+            assert default is not None, "no default model available"
+            model = default.name
+
         assert model is not None
         assert any(model == m.name for m in cls.available_models()), \
             f"model {model} does not match any of the available models:\n" \
             f"{pprint.pformat(cls.available_models())}"
 
         logger = logging.get_logger(f"{cls._task_upper()} DOWNLOAD")
         model_url = cls._model_url(model)
@@ -137,97 +143,73 @@
     @property
     def name(self) -> str:
         raise NotImplementedError
 
     @classmethod
     def _model_from_config(
         cls,
-        cfg: Dict[str, Any],
+        cfg: dict[str, Any],
         device: Device
     ) -> nn.Module:
         raise NotImplementedError
 
     @property
     def max_length(self) -> int:
         raise NotImplementedError
 
-    def supported_languages(self) -> Optional[List[str]]:
-        return None
-
-    @classmethod
-    def supported_input_formats(cls) -> List[str]:
-        return ["text", "text_language"]
-
-    @classmethod
-    def supported_output_formats(cls) -> List[str]:
-        return cls.supported_input_formats()
-
     def __init__(
         self,
         model: nn.Module,
-        cfg: Dict[str, Any],
+        cfg: dict[str, Any],
         device: Device = "cuda"
     ) -> None:
+        self.cfg = cfg
         self.logger = logging.get_logger(self._task_upper())
+        self.logger.debug(f"got config:\n{self.cfg}")
 
         torch.set_num_threads(len(os.sched_getaffinity(0)))
         torch.use_deterministic_algorithms(False)
         cudnn.benchmark = True
         cuda.matmul.allow_tf32 = True
 
         self.model = model
-        self.devices = get_devices(device)
         self.to(device)
 
-        self.cfg = cfg
-        self.logger.debug(f"got config:\n{self.cfg}")
-
-        self._precision_dtype: torch.dtype | None = torch.float32
-        self.set_precision(cfg["train"].get("precision", "fp32"))
-
         self._inference_loader_cfg = self._build_inference_loader_config()
 
-    def _build_inference_loader_config(self) -> Dict[str, Any]:
+    def _build_inference_loader_config(self) -> dict[str, Any]:
         raise NotImplementedError
 
-    def _prepare_batch(self, batch: data.InferenceBatch) -> Dict[str, Any]:
+    def _prepare_batch(self, batch: data.InferenceBatch) -> dict[str, Any]:
         raise NotImplementedError
 
-    def _inference(self, inputs: Dict[str, Any]) -> list[Any]:
+    @torch.inference_mode()
+    def _inference(self, inputs: dict[str, Any]) -> Iterator[Any]:
         raise NotImplementedError
 
-    @torch.inference_mode()
     def _run_model(self, batch: data.InferenceBatch) -> list[Any]:
         inputs = self._prepare_batch(batch)
-
-        # handle special case here, because autocasting is not supported on CPU
-        # for any other dtype than bfp16 yet
-        if self.devices[0].type == "cpu" and self._precision_dtype != torch.bfloat16:
-            return self._inference(inputs)
-
-        with autocast(
-            device_type=self.devices[0].type,
-            dtype=self._precision_dtype
-        ):
-            return self._inference(inputs)
+        # by default only return the last and no intermediate outputs
+        *_, last = self._inference(inputs)
+        return last
 
     def _process_results(
         self,
-        items: List[data.InferenceItem],
-        outputs: List[Any]
+        items: list[data.InferenceItem],
+        outputs: list[Any]
     ) -> data.InferenceData:
         raise NotImplementedError
 
     def _get_loader(
         self,
-        inputs: Union[Tuple[List[str], Optional[List[str]]], Iterator[data.InferenceData]],
+        inputs: list[str] | Iterator[data.InferenceData],
         batch_size: int = 16,
-        batch_max_tokens: Optional[int] = None,
+        batch_max_tokens: int | None = None,
         sort: bool = True,
-        num_threads: Optional[int] = None,
+        num_threads: int | None = None,
         **kwargs: Any
     ) -> data.InferenceLoader:
         if num_threads is None:
             num_threads = min(len(os.sched_getaffinity(0)), 4)
 
         if batch_max_tokens is None:
             batch_limit = max(1, batch_size)
@@ -249,15 +231,15 @@
             "batch_limit": batch_limit,
             "buffer_size": buffer_size,
             "prefetch_factor": prefetch_factor,
             "batch_limit_type": batch_limit_type,
             "sort": sort
         })
         self._inference_loader_cfg.update(kwargs)
-        if isinstance(inputs, tuple):
+        if isinstance(inputs, list):
             files, languages = inputs
             loader = data.InferenceLoader.from_files(
                 files=files,
                 languages=languages,
                 **self._inference_loader_cfg
             )
         elif isinstance(inputs, Iterator):
@@ -265,16 +247,16 @@
             self._inference_loader_cfg["num_threads"] = 0
             loader = data.InferenceLoader.from_iterator(
                 inputs,
                 **self._inference_loader_cfg
             )
         else:
             raise ValueError(
-                f"unknown input type {type(inputs)}, must either be a tuple of "
-                f"files and languages or an iterator over sequence language pairs"
+                f"unknown input type {type(inputs)}, must either be a list of "
+                f"files and an iterator over strings"
             )
 
         return loader
 
     def _pbar(
         self,
         progress_desc: str,
@@ -294,15 +276,15 @@
     def _process_sorted(
         self,
         loader: data.InferenceLoader,
         progress_desc: str,
         progress_total: int,
         progress_unit: str = "seq",
         show_progress: bool = False,
-    ) -> List[data.InferenceData]:
+    ) -> list[data.InferenceData]:
         results = {}
         pbar = self._pbar(
             progress_desc,
             progress_total,
             progress_unit,
             show_progress
         )
@@ -368,33 +350,7 @@
     def to(self, device: Device) -> "TextProcessor":
         self.devices = get_devices(device)
         assert len(self.devices) == 1, \
             "only a single device supported by default, implement custom to() if you need " \
             "multi-device support"
         self.model = self.model.to(self.devices[0])
         return self
-
-    def set_precision(self, precision: str) -> "TextProcessor":
-        assert precision in {"fp32", "fp16", "bfp16"}
-        if not all(d.type == self.devices[0].type for d in self.devices):
-            self.logger.info(
-                "autocasting only supported if all devices are of the same type"
-            )
-            self._precision_dtype = None
-            return self
-
-        if precision == "fp32":
-            precision_dtype = torch.float32
-        elif precision == "fp16":
-            precision_dtype = torch.float16
-        else:
-            precision_dtype = torch.bfloat16
-
-        if any(device.type == "cpu" for device in self.devices) and precision == "fp16":
-            self.logger.info(
-                "setting precision to bfp16 instead of fp16, "
-                "because fp16 is not supported on CPU yet"
-            )
-            precision_dtype = torch.bfloat16
-
-        self._precision_dtype = precision_dtype
-        return self
```

## text_utils/api/table.py

```diff
@@ -136,29 +136,32 @@
 def generate_report(
     task: str,
     model_name: str,
     model: nn.Module,
     input_size: int,
     input_size_bytes: int,
     runtime: float,
-    precision: torch.dtype | None,
     batch_size: int,
     sort_by_length: bool,
     devices: list[torch.device],
+    precision: torch.dtype | str | None = None,
     batch_max_tokens: int | None = None,
     file_path: str | None = None
 ) -> str | None:
+    if precision is None:
+        precision = next(model.parameters()).dtype
+
     if precision == torch.float16:
         precision_str = "fp16"
     elif precision == torch.bfloat16:
         precision_str = "bfp16"
-    elif precision == torch.float32 or precision is None:
+    elif precision == torch.float32:
         precision_str = "fp32"
     else:
-        raise ValueError("expected precision to be one of torch.float16, torch.bfloat16 or torch.float32")
+        precision_str = str(precision)
 
     devices = [d for d in devices if d.type == "cuda"]
     devices.append(torch.device("cpu"))
     report = generate_table(
         headers=[["REPORT", task]],
         data=[
             ["Model", model_name],
```

## text_utils/cli/create_continuation_index.py

```diff
@@ -15,25 +15,25 @@
 
 def create(args: argparse.Namespace):
     dir = os.path.dirname(args.output_file)
     if dir != "":
         os.makedirs(dir, exist_ok=True)
 
     start = time.perf_counter()
-    continuations.Continuations.build_from_file(
+    continuations.ContinuationIndex.build_from_file(
         args.input_file,
         args.output_file
     )
     end = time.perf_counter()
     print(f"Continuations trie built in {end - start:.2f} seconds")
 
     start = time.perf_counter()
     # empty continuations for testing
     conts = []
-    continuations.Continuations.load_with_continuations(
+    continuations.ContinuationIndex.load_with_continuations(
         args.output_file,
         conts
     )
     end = time.perf_counter()
     print(f"Continuations trie loaded in {end - start:.2f} seconds")
```

## text_utils/inference/__init__.py

```diff
@@ -1,218 +1,69 @@
-from typing import Callable, Tuple, List, Optional, Union, Dict, Any
-import copy
+from typing import Callable, Iterator, Any
 
 import torch
 
 from torch.nn.utils import rnn
 
-
-class Beam:
-    def __init__(
-        self,
-        token_ids: List[int],
-        log_probs: List[float],
-        info: Optional[Dict[str, Any]] = None
-    ) -> None:
-        self.token_ids: List[int] = token_ids
-        self.log_probs: List[float] = log_probs
-        self.info: Dict[str, Any] = info or {}
-
-    @staticmethod
-    def from_beam(
-        other: "Beam",
-        log_p: float,
-        token_id: int
-    ) -> "Beam":
-        return Beam(
-            other.token_ids + [token_id],
-            other.log_probs + [log_p],
-            copy.deepcopy(other.info)
-        )
-
-    def truncate_prefix(
-        self,
-        length: int
-    ) -> "Beam":
-        return Beam(
-            self.token_ids[length:],
-            self.log_probs[length:],
-        )
-
-    @property
-    def log_prob(self) -> float:
-        return sum(self.log_probs)
-
-    def __lt__(self, other: "Beam") -> bool:
-        return len(self) < len(other)
-
-    def __len__(self) -> int:
-        return len(self.token_ids)
-
-    def __repr__(self) -> str:
-        return f"Beam(token_ids={self.token_ids}, log_prob={self.log_prob:.4f})"
-
-
-# maps from token ids, length, and other kwargs to distribution over next token id and other info
-DecodeFn = Callable[..., Tuple[torch.Tensor, Dict[str, Any]]]
-# selects indices and scores from given token distributions
-IdxSelectFn = Callable[
-    [
-        # distributions over next token id, shape [batch_size, vocab_size]
-        torch.Tensor,
-        # indices of input batch elements for which the next token is selected
-        List[int]
-    ],
-    Tuple[
-        # indices of selected token
-        torch.Tensor,
-        # scores (log_probs) of selected token
-        torch.Tensor
-    ]
-]
-StopFn = Callable[
-    [
-        # selected token ids, shape [batch_size]
-        torch.Tensor,
-        # indices of input batch elements which are checked for stopping
-        List[int]
-    ],
-    torch.Tensor
-]
-# selects (multiple) indices and scores from a given token distribution
-BeamSelectFn = Callable[
-    [
-        # distribution over next token ids, shape [batch_size, beam_size, vocab_size]
-        torch.Tensor,
-        # input beams
-        List[List[Beam]],
-        # indices of input batch elements
-        List[int]
-    ],
-    # new beam candidates, should be sorted descending by score
-    List[List[Beam]]
-]
-BeamStopFn = Callable[
-    [
-        # beam checked for stopping
-        Beam,
-        # idx of input batch element which is checked for stopping
-        int
-    ],
-    bool
-]
-# select specific elements for all the kwargs keys given the mask tensor
-MaskSelectFn = Callable[
-    [Dict[str, Any], torch.Tensor],
-    Dict[str, Any]
-]
-MaskUpdateFn = Callable[
-    [Dict[str, Any], Dict[str, Any], torch.Tensor],
-    None
-]
+from text_utils.inference.utils import (
+    DecodeFn,
+    MaskSelectFn,
+    MaskUpdateFn,
+    LogitFn,
+    SampleFn,
+    StopFn,
+    Beam,
+    BeamSampleFn,
+    BeamCandidateFn,
+    BeamStopFn,
+    beam_greedy,
+    default_beam_candidate_fn,
+    greedy
+)
 
 
 def eos_stop_fn(eos_token_id: int) -> StopFn:
-    def _stop(token_ids: torch.Tensor, _: List[int]) -> torch.Tensor:
+    def _stop(token_ids: torch.Tensor, _: list[int]) -> torch.Tensor:
         return token_ids == eos_token_id
 
     return _stop
 
 
-def greedy_select_fn() -> IdxSelectFn:
-    def _greedy(scores: torch.Tensor, _: List[int]) -> Tuple[torch.Tensor, torch.Tensor]:
-        assert scores.ndim == 2
-        indices = torch.argmax(scores, dim=1)
-        scores = torch.gather(scores, -1, indices.unsqueeze(-1)).squeeze(-1)
-        return indices, scores
-
-    return _greedy
-
-
-def sample_select_fn(sample_top_k: int) -> IdxSelectFn:
-    def _sample(scores: torch.Tensor, _: List[int]) -> Tuple[torch.Tensor, torch.Tensor]:
-        assert scores.ndim == 2
-        k = min(sample_top_k, scores.shape[-1])
-        sampled_indices = torch.randint(
-            k,
-            (len(scores), 1),
-            device=scores.device
-        )
-        top_k = torch.topk(scores, k, dim=-1)
-        indices = torch.gather(top_k.indices, -1, sampled_indices).squeeze(-1)
-        scores = torch.gather(top_k.values, -1, sampled_indices).squeeze(-1)
-        return indices, scores
-
-    return _sample
-
-
-def beam_select_fn(beam_width: int) -> BeamSelectFn:
-    def _beam(
-        scores: torch.Tensor,
-        batch_beams: List[List[Beam]],
-        _: List[int]
-    ) -> List[List[Beam]]:
-        num_beams = [len(b) for b in batch_beams]
-        assert scores.ndim == 2 and scores.shape[0] == sum(num_beams)
-        k = min(beam_width, scores.shape[1])
-        top_k = torch.topk(scores, k, dim=1)
-        top_k_indices = torch.split(top_k.indices, num_beams)
-        top_k_values = torch.split(top_k.values, num_beams)
-        batch_candidates = []
-        for beams, indices, values in zip(batch_beams, top_k_indices, top_k_values):
-            candidates = []
-            for idx, (token_ids, log_probs) in enumerate(zip(indices.tolist(), values.tolist())):
-                for token_id, log_p in zip(token_ids, log_probs):
-                    candidates.append((idx, token_id, log_p))
-            candidates = sorted(
-                candidates,
-                key=lambda item: -(beams[item[0]].log_prob + item[2]),
-            )[:2 * beam_width]
-            batch_candidates.append([
-                Beam.from_beam(beams[idx], log_p, token_id)
-                for idx, token_id, log_p in candidates
-            ])
-        return batch_candidates
-
-    return _beam
-
-
 def _sub_select(
-    inputs: Union[torch.Tensor, Dict[str, torch.Tensor]],
-    mask: Union[int, torch.Tensor]
-) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:
+    inputs: torch.Tensor | dict[str, torch.Tensor],
+    mask: int | torch.Tensor,
+) -> torch.Tensor | dict[str, torch.Tensor]:
     if isinstance(inputs, torch.Tensor):
         return inputs[mask]
     elif isinstance(inputs, dict):
         return {k: v[mask] for k, v in inputs.items()}
     else:
         raise ValueError(
             f"expected inputs to be of type tensor or dict of tensors, but got {type(inputs)}"
         )
 
 
 @torch.inference_mode()
 def search(
     decode_fn: DecodeFn,
-    initial_token_ids: List[List[int]],
+    initial_token_ids: list[list[int]],
     pad_token_id: int,
     max_length: int,
     stop_fn: StopFn,
     device: torch.device,
-    select_fn: Optional[IdxSelectFn] = None,
-    kwargs_select_fn: Optional[MaskSelectFn] = None,
-    kwargs_update_fn: Optional[MaskUpdateFn] = None,
+    sample_fn: SampleFn = greedy(),
+    logit_fns: list[LogitFn] | None = None,
+    kwargs_select_fn: MaskSelectFn | None = None,
+    kwargs_update_fn: MaskUpdateFn | None = None,
     return_full: bool = False,
+    yield_intermediate: bool = False,
     **kwargs: Any,
-) -> List[List[int]]:
+) -> Iterator[list[list[int]]]:
     batch_size = len(initial_token_ids)
-    assert batch_size > 0
-
-    if select_fn is None:
-        select_fn = greedy_select_fn()
+    assert batch_size > 0, "empty inputs"
 
     lengths = []
     padded_initial_token_ids = []
     for token_ids in initial_token_ids:
         num_tokens = len(token_ids)
         assert num_tokens <= max_length, "initial token ids cannot be longer than max length"
         padded_initial_token_ids.append(
@@ -239,14 +90,22 @@
         dtype=torch.bool,
     )
     smaller_max_length_mask[lengths >= max_length] = False
     non_stop_mask = torch.ones(batch_size, dtype=torch.bool)
     mask = non_stop_mask & smaller_max_length_mask
     indices = torch.arange(batch_size, dtype=torch.long)
 
+    def get_outputs() -> list[list[int]]:
+        outputs = []
+        for i in range(batch_size):
+            length = lengths[i]
+            start = 0 if return_full else initial_lengths[i]
+            outputs.append(token_ids[i][start:length].tolist())
+        return outputs
+
     # all sequences are at max length or stopped by stop_fn
     while torch.sum(mask) > 0:
         decoder_lengths = _sub_select(lengths, mask)
         assert isinstance(decoder_lengths, torch.Tensor)
         max_decoder_length = torch.max(decoder_lengths)  # type: ignore
         indices_mask = indices[mask]
 
@@ -274,21 +133,27 @@
             decoder_outputs = decoder_outputs[:, 0]
         else:
             decoder_outputs = decoder_outputs[
                 torch.arange(b, device=decoder_outputs.device),
                 decoder_lengths.to(decoder_outputs.device) - 1
             ]
 
-        log_softmax_scores = torch.log_softmax(
-            decoder_outputs,
-            dim=1
-        )
-
+        # process logits and sample
         batch_indices = indices_mask.tolist()
-        sel_ids, sel_lps = select_fn(log_softmax_scores, batch_indices)
+        for logit_fn in logit_fns or []:
+            decoder_outputs = logit_fn(decoder_outputs, batch_indices)
+        sel_ids = sample_fn(decoder_outputs, batch_indices)
+
+        # get log prob of selected ids
+        sel_lps = torch.gather(
+            torch.log_softmax(decoder_outputs, dim=-1),
+            -1,
+            sel_ids.unsqueeze(-1)
+        ).squeeze(-1)
+
         sel_ids = sel_ids.to(token_ids.device)
         sel_lps = sel_lps.to(log_prob.device)
         token_ids[mask, decoder_lengths] = sel_ids
         log_prob[mask, decoder_lengths] = sel_lps
 
         lengths[mask] += 1
 
@@ -301,106 +166,132 @@
 
         mask = non_stop_mask & smaller_max_length_mask
 
         if kwargs_update_fn is not None:
             update_mask = torch.arange(b)[mask[indices_mask]]
             kwargs_update_fn(kwargs, decoder_info, update_mask)
 
-    token_ids = token_ids.tolist()
+        if yield_intermediate:
+            yield get_outputs()
 
-    outputs = []
-    for i in range(batch_size):
-        length = lengths[i]
-        start = 0 if return_full else initial_lengths[i]
-        outputs.append(token_ids[i][start:length])
-    return outputs
+    yield get_outputs()
 
 
 def log_likelihood_score(
     normalize_by_length: bool = True,
     alpha: float = 1.0
-) -> Callable[[Beam, int], float]:
-    def _score(beam: Beam, length: int) -> float:
+) -> Callable[[Beam], float]:
+    def _score(beam: Beam) -> float:
         if normalize_by_length:
-            return beam.log_prob / (length ** alpha)
+            return beam.log_prob / (len(beam) ** alpha)
         else:
             return beam.log_prob
 
     return _score
 
 
 @torch.inference_mode()
 def beam_search(
     decode_fn: DecodeFn,
-    initial_token_ids: List[List[int]],
+    initial: list[list[int]] | list[Beam],
     pad_token_id: int,
     max_length: int,
     stop_fn: BeamStopFn,
     device: torch.device,
     normalize_by_length: bool,
     alpha: float,
     beam_width: int,
-    select_fn: Optional[BeamSelectFn] = None,
-    kwargs_select_fn: Optional[MaskSelectFn] = None,
-    kwargs_update_fn: Optional[MaskUpdateFn] = None,
+    sample_fn: BeamSampleFn = beam_greedy(),
+    candidate_fn: BeamCandidateFn = default_beam_candidate_fn(),
+    logit_fns: list[LogitFn] | None = None,
+    kwargs_select_fn: MaskSelectFn | None = None,
+    kwargs_update_fn: MaskUpdateFn | None = None,
     return_full: bool = False,
+    yield_intermediate: bool = False,
     **kwargs: Any
-) -> List[List[Beam]]:
-    batch_size = len(initial_token_ids)
+) -> Iterator[list[list[Beam]]]:
+    batch_size = len(initial)
 
     score_fn = log_likelihood_score(normalize_by_length, alpha)
-    if select_fn is None:
-        select_fn = beam_select_fn(beam_width)
 
-    beam_queues: List[List[Beam]] = [[] for _ in range(batch_size)]
+    beam_queues: list[list[Beam]] = [[] for _ in range(batch_size)]
 
-    search_depths: List[int] = []
-    current_beams: List[List[Beam]] = []
+    current_beams: list[list[Beam]] = []
     initial_lenghts = []
-    for b in range(batch_size):
-        # initialize beams
-        token_ids = initial_token_ids[b]
-        initial_lenghts.append(len(token_ids))
-        log_prob = [0.0] * len(token_ids)
-        beam = Beam(token_ids, log_prob)
+    for init in initial:
+        initial_lenghts.append(len(init))
+        if isinstance(init, Beam):
+            current_beams.append([init])
+            continue
+        beam = Beam(init, [0.0] * len(init))
         current_beams.append([beam])
-        search_depths.append(len(beam))
 
     stop_mask = [False for _ in range(batch_size)]
 
-    def get_indices_to_decode() -> List[int]:
+    def get_indices_to_decode() -> list[int]:
         indices_to_decode = []
-        for idx, (stop, search_depth, beams) in enumerate(zip(stop_mask, search_depths, current_beams)):
-            if not stop and search_depth < max_length and len(beams) > 0:
-                indices_to_decode.append(idx)
+        for idx, (stop, beams) in enumerate(zip(
+            stop_mask,
+            current_beams
+        )):
+            if stop or len(beams) == 0 or len(beams[0]) >= max_length:
+                continue
+            indices_to_decode.append(idx)
         return indices_to_decode
 
     indices_to_decode = get_indices_to_decode()
 
+    def get_outputs() -> list[list[Beam]]:
+        out_beams = []
+        for idx, (beam_queue, active_beams) in enumerate(zip(beam_queues, current_beams)):
+            beam_queue = sorted(
+                beam_queue,
+                key=lambda b: score_fn(b),
+                reverse=True
+            )[:beam_width]
+            if len(beam_queue) < beam_width:
+                active_beams = sorted(
+                    active_beams,
+                    key=lambda b: score_fn(b),
+                    reverse=True
+                )
+                beam_queue.extend(active_beams[:beam_width - len(beam_queue)])
+            pfx = 0 if return_full else initial_lenghts[idx]
+            out_beams.append([
+                beam.truncate_prefix(pfx)
+                for beam in beam_queue
+            ])
+        return out_beams
+
     while len(indices_to_decode) > 0:
         num_beams = []
+        beams = []
         decoder_mask = []
         decoder_token_ids = []
         decoder_lengths = []
         for idx in indices_to_decode:
             num_beams.append(len(current_beams[idx]))
             decoder_mask.extend([idx] * num_beams[-1])
             for beam in current_beams[idx]:
+                beams.append(beam)
                 decoder_lengths.append(len(beam))
                 decoder_token_ids.append(
                     torch.tensor(beam.token_ids, dtype=torch.long)
                 )
 
         decoder_token_ids = rnn.pad_sequence(
             decoder_token_ids,
             batch_first=True,
             padding_value=pad_token_id
         ).to(non_blocking=True, dtype=torch.long, device=device)
         decoder_mask = torch.tensor(decoder_mask, dtype=torch.long)
-        decoder_lengths_tensor = torch.tensor(decoder_lengths, dtype=torch.long)
+        decoder_lengths_tensor = torch.tensor(
+            decoder_lengths,
+            dtype=torch.long
+        )
 
         decoder_kwargs = kwargs_select_fn(
             kwargs,
             decoder_mask
         ) if kwargs_select_fn is not None else {}
         # always add a padding mask, indicating which tokens are padding
         # and the lengths of the sequence to the additional arguments
@@ -418,41 +309,57 @@
             decoder_outputs = decoder_outputs[:, 0]
         else:
             decoder_outputs = decoder_outputs[
                 torch.arange(b),
                 decoder_lengths_tensor - 1
             ]
 
-        log_softmax_scores = torch.log_softmax(decoder_outputs, dim=1)
-        beam_candidates = select_fn(
-            log_softmax_scores,
-            [current_beams[idx] for idx in indices_to_decode],
-            indices_to_decode
-        )
+        # apply logit functions
+        for logit_fn in logit_fns or []:
+            decoder_outputs = logit_fn(decoder_outputs, beams)
+
+        log_probs = torch.log_softmax(decoder_outputs, dim=-1)
 
         update_info = {}
-        for i, (idx, candidates) in enumerate(zip(indices_to_decode, beam_candidates)):
-            new_current_beams = []
-            for num, candidate in enumerate(candidates):
+        for i, (idx, log_probs) in enumerate(zip(
+            indices_to_decode,
+            torch.split(log_probs, num_beams)
+        )):
+            candidates: list[tuple[Beam, int, float]] = []
+            for beam_idx, beam in enumerate(current_beams[idx]):
+                for token_id in sample_fn(log_probs[beam_idx], beam_width).tolist():
+                    candidates.append((
+                        beam,
+                        token_id,
+                        log_probs[beam_idx, token_id].item()
+                    ))
+
+            new_beams = []
+            for num, (beam, token_id, log_prob) in enumerate(sorted(
+                candidates,
+                key=lambda item: item[0].log_prob + item[2],
+                reverse=True
+            )):
+                # update candidates
+                candidate = candidate_fn(beam, token_id, log_prob)
                 # only consider eos beams if they are in top beam_width beams
-                stop = stop_fn(candidate, idx)
+                stop = stop_fn(candidate)
                 if num < beam_width and stop:
                     # we record all stop beams, but only stop when the top beam should stop
                     # (because then we are sure there is no better candidate left to decode)
                     beam_queues[idx].append(candidate)
                     stop_mask[idx] |= num == 0
                 elif not stop:
-                    new_current_beams.append(candidate)
+                    new_beams.append(candidate)
 
-                if len(new_current_beams) >= beam_width:
+                if len(new_beams) >= beam_width:
                     break
 
-            current_beams[idx] = new_current_beams
-            search_depths[idx] += 1
-            update_info[idx] = (i, len(new_current_beams))
+            current_beams[idx] = new_beams
+            update_info[idx] = (i, len(new_beams))
 
         indices_to_decode = get_indices_to_decode()
 
         if kwargs_update_fn is not None:
             update_mask = []
             for idx in indices_to_decode:
                 if idx not in update_info:
@@ -461,24 +368,11 @@
                 update_mask.extend([i] * num)
             kwargs_update_fn(
                 kwargs,
                 decoder_info,
                 torch.tensor(update_mask, dtype=torch.long)
             )
 
-    out_beams = []
-    for idx, (beam_queue, active_beams) in enumerate(zip(beam_queues, current_beams)):
-        beam_queue = sorted(
-            beam_queue, key=lambda b: -score_fn(b, initial_lenghts[idx])
-        )[:beam_width]
-        if len(beam_queue) < beam_width:
-            active_beams = sorted(
-                active_beams,
-                key=lambda b: -score_fn(b, initial_lenghts[idx])
-            )
-            beam_queue.extend(active_beams[:beam_width - len(beam_queue)])
-        pfx = 0 if return_full else initial_lenghts[idx]
-        out_beams.append([
-            beam.truncate_prefix(pfx)
-            for beam in beam_queue
-        ])
-    return out_beams
+        if yield_intermediate:
+            yield get_outputs()
+
+    yield get_outputs()
```

## text_utils/version.py

```diff
@@ -1 +1 @@
-__version__ = "0.4.1"
+__version__ = "0.5.0"
```

## Comparing `dtpu-0.4.1.dist-info/METADATA` & `dtpu-0.5.0.dist-info/METADATA`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.3
 Name: dtpu
-Version: 0.4.1
+Version: 0.5.0
 Classifier: Programming Language :: Rust
 Classifier: Programming Language :: Python :: 3
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Topic :: Software Development :: Libraries
 Classifier: Topic :: Text Processing
 Classifier: Topic :: Utilities
 Requires-Dist: torch >=2.2
@@ -14,17 +14,17 @@
 Requires-Dist: pyyaml >=6.0
 Requires-Dist: tqdm >=4.60.0
 Requires-Dist: tensorboard >=2.16.0
 Requires-Dist: flask >=2.2.0
 Requires-Dist: requests >=2.30.0
 Requires-Dist: termcolor >=2.4.0
 Requires-Dist: pytest >=8.0.0 ; extra == 'test'
-Requires-Dist: bitsandbytes >=0.42 ; extra == '8bit'
+Requires-Dist: bitsandbytes >=0.42 ; extra == 'low-precision'
 Provides-Extra: test
-Provides-Extra: 8bit
+Provides-Extra: low-precision
 License-File: LICENSE
 Summary: Utilities for text processing tasks with Deep NLP
 Keywords: nlp,utilities,text,processing
 Author-email: Sebastian Walter <swalter@cs.uni-freiburg.de>
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown; charset=UTF-8; variant=GFM
 Project-URL: Github, https://github.com/ad-freiburg/text-utils
```

## Comparing `dtpu-0.4.1.dist-info/license_files/LICENSE` & `dtpu-0.5.0.dist-info/license_files/LICENSE`

 * *Files identical despite different names*

## Comparing `dtpu-0.4.1.dist-info/RECORD` & `dtpu-0.5.0.dist-info/RECORD`

 * *Files 21% similar despite different names*

```diff
@@ -1,26 +1,28 @@
-dtpu-0.4.1.dist-info/METADATA,sha256=xMcqTTrY4htc8YBM6t10JDi0Aoib01Ik1x1kHZ9MTHQ,1524
-dtpu-0.4.1.dist-info/WHEEL,sha256=xHx4MTFVGTioQqKTlQqO1-qdDr58L6Fx5_KoufP7tVc,94
-dtpu-0.4.1.dist-info/entry_points.txt,sha256=M7S1KCTSy6Ln41p70V3PWBLCTIPNTaimlbwBmp3adh0,195
-dtpu-0.4.1.dist-info/license_files/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
-text_utils/api/cli.py,sha256=cb5wN_s0hTbYVAHlUezykkIoLpmjFQ5wAg6e48d_GWM,15713
-text_utils/api/processor.py,sha256=xGkZw6FD3aXurg--SNKsHm8Qp1i4K3w0uLmrYqLI2_o,13386
+dtpu-0.5.0.dist-info/METADATA,sha256=GBTvn7err4jSOKrOsvYaMF5F-5gaq1tGk5SQrB_7XkE,1542
+dtpu-0.5.0.dist-info/WHEEL,sha256=_-pO1V0R3bpcg5FhWC82Cl79freIhF1O9I5uv0may5k,94
+dtpu-0.5.0.dist-info/entry_points.txt,sha256=M7S1KCTSy6Ln41p70V3PWBLCTIPNTaimlbwBmp3adh0,195
+dtpu-0.5.0.dist-info/license_files/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
+text_utils/api/cli.py,sha256=QJJqNwCma6Rtslu5swzzZxSAVDJN7-AMEZc2ybosXtg,12567
+text_utils/api/processor.py,sha256=HgC0DspppNEpXQYDzC0qm54t-4G0fEB2KAJDTg9nB10,11688
 text_utils/api/server.py,sha256=7Ofw5D5TuiY_zR1RQoHFCRfT1_NRO2MtKUo5A1w8VhY,7100
-text_utils/api/table.py,sha256=gUJdOmPr-JRyn5Ub7aD8UfzIrYJneVr_fTvpbDwoiu4,6615
+text_utils/api/table.py,sha256=Nht6S-MF68bUcl-phiJ23BQ5W5890PPl7as37CSP7qE,6620
 text_utils/api/trainer.py,sha256=aqg6gMH6CD2rh0GCeNjlv1fJ9JR8zi4qZF9eNFVM5qU,56880
 text_utils/api/utils.py,sha256=7efAFtVEWc20Q3rlXchdfcYYLBmqhQ4H1Ja4lCs-YNo,7861
 text_utils/api/__init__.py,sha256=YGA-AW0CBiu2J6K-zwRdAqE3qhzQrysvD2GqyA4o6dw,451
-text_utils/cli/create_continuation_index.py,sha256=7wni_cO_o_Y7Krn9f9tFO7G3Z9SrksnQjmCUPtJld4g,1078
+text_utils/cli/create_continuation_index.py,sha256=pYaO_N8tEzEqRQdYjV-vQNdzHf-TKcarh4mds9SrUO8,1086
 text_utils/cli/create_dictionary.py,sha256=uTyJu0-bB-FEm952oesZZ6suLLnXwqMaVs8P7jZav9o,1782
 text_utils/cli/train_bpe.py,sha256=2nQsph51c2ERZeBBidbcM7YluzCNQf34aPE1B5QE7zs,1885
 text_utils/cli/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 text_utils/configuration.py,sha256=KxH5KVMDdfvxErcA32uxs2EYjxwEeyRfzBgORv5iJTc,5731
+text_utils/constraints.py,sha256=5YEOUPKrgunsP-NMAi09HWv3rEgBzjMtKgfKa6bCTNQ,2278
 text_utils/distributed.py,sha256=7ZhZ4hzPtbjx0oKRIS8I4Ft7hmY4FEHdLU9AElPdmC0,1951
 text_utils/hook.py,sha256=8h-9dRGJLaGv7XlErB_Zs4a4JoQRjen3e4q6kBMVcSY,2399
-text_utils/inference/__init__.py,sha256=YWWGjvzhUR_8_X0REfDzJ23Nq3wzQZsvMR8t0rkvGTY,16641
+text_utils/inference/utils.py,sha256=XpcCB4x8zn-L4XaMYaSDPIk9AdP2T9VsMLMTjByWB68,8116
+text_utils/inference/__init__.py,sha256=trnWuhKCb8DD7UjErdWx_RN1HGI4e4O8uUiet8bECZc,13033
 text_utils/io.py,sha256=5WQy5PEFdTlt19mcTghF0w-ips0Y9dzmWTfXKCUbTB8,2895
 text_utils/logging.py,sha256=R2-tZe5FXiAqXE0CA6ctchY0EZ7nXtngZ3eC9HQjqLM,2268
 text_utils/mask.py,sha256=hH4n9VJASsPooTSEY6S5WoTSbrYwcbeESkuwzqpwWVM,1104
 text_utils/modules/decoder.py,sha256=BPOnR6ekc-8eEm8VkY4NKfi1jarwYuOtSL3yfgN58q8,9805
 text_utils/modules/embedding.py,sha256=HKqiPEbxynBW-ZW_8EJGVbNOofuj8GaQajxy45yZQ4c,10202
 text_utils/modules/encoder.py,sha256=_vUxAHl5rQzerUO5q6DuCL4_QpdbsAuzOajbzIjhGnQ,12741
 text_utils/modules/grouping.py,sha256=xecWmO946LWQFEZjP7hlBEeXxdVx0-fPx_nKhGZ0WP8,1426
@@ -29,11 +31,11 @@
 text_utils/modules/moe.py,sha256=V_8Zj7u4TJH92Zpa2zlP95ybqxzhOYZtA11Mwt0ue9U,3186
 text_utils/modules/optimizer.py,sha256=_G2mr1zBBGc708mrsxCUjwLEyk4G8v9Vlaxq6iwic08,3285
 text_utils/modules/scheduler.py,sha256=dBnY8Fdf09v0nAqJKP319hmZL3bkNDCPMyWY3wsk2Z4,8522
 text_utils/modules/utils.py,sha256=p0MxCScHWcVJwBH7dwrUQi03P_FY3E3Pin6JPRV1f6Y,331
 text_utils/modules/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 text_utils/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 text_utils/tensorboard.py,sha256=-cgGV7XYEcfKAPV6W1r1bjuRjyiJQ73HWzawIbdiD8E,9301
-text_utils/version.py,sha256=075iz_wmJWFZOiHbuinNMyz1WJHo7tp0N817twjxQkQ,23
+text_utils/version.py,sha256=pZzEdsuAE3uDnTsMODkFABqmk03QJAN-SEFrFRq4y1U,23
 text_utils/__init__.py,sha256=8R6IQdrOeDO-g2fNgnH62LfkL56dG2fsvqcEvAPU4Cw,364
-text_utils/_internal.pyd,sha256=qJcYgVFGLOsyE5H4SvNfqWqQQn_Kb3fR6V_w_D-NnuM,6122496
-dtpu-0.4.1.dist-info/RECORD,,
+text_utils/_internal.pyd,sha256=TIoaKvcyjUnq9vk6dVAFVDSokeFxaP6W8Pe2VNksy50,6175232
+dtpu-0.5.0.dist-info/RECORD,,
```

