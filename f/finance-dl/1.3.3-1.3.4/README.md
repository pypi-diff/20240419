# Comparing `tmp/finance_dl-1.3.3-py3-none-any.whl.zip` & `tmp/finance_dl-1.3.4-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,35 +1,36 @@
-Zip file size: 78598 bytes, number of entries: 33
--rw-r--r--  2.0 unx        0 b- defN 21-Mar-13 08:54 finance_dl/__init__.py
--rw-r--r--  2.0 unx    11450 b- defN 21-Mar-13 08:54 finance_dl/amazon.py
--rw-r--r--  2.0 unx     5552 b- defN 21-Mar-13 08:54 finance_dl/anthem.py
--rw-r--r--  2.0 unx      439 b- defN 21-Mar-13 08:54 finance_dl/chromedriver_wrapper.py
--rw-r--r--  2.0 unx     3168 b- defN 21-Mar-13 08:54 finance_dl/cli.py
--rw-r--r--  2.0 unx     6706 b- defN 21-Mar-13 08:54 finance_dl/comcast.py
--rw-r--r--  2.0 unx     2242 b- defN 21-Mar-13 08:54 finance_dl/csv_merge.py
--rw-r--r--  2.0 unx     5392 b- defN 21-Mar-13 08:54 finance_dl/discover.py
--rw-r--r--  2.0 unx     4703 b- defN 21-Mar-13 08:54 finance_dl/ebmud.py
--rw-r--r--  2.0 unx     2249 b- defN 21-Mar-13 08:54 finance_dl/google_login.py
--rw-r--r--  2.0 unx     5432 b- defN 21-Mar-13 08:54 finance_dl/google_purchases.py
--rw-r--r--  2.0 unx     3642 b- defN 21-Mar-13 08:54 finance_dl/google_takeout.py
--rw-r--r--  2.0 unx    11927 b- defN 21-Mar-13 08:54 finance_dl/healthequity.py
--rw-r--r--  2.0 unx    17838 b- defN 21-Mar-13 08:54 finance_dl/mint.py
--rw-r--r--  2.0 unx    15303 b- defN 21-Mar-13 08:54 finance_dl/ofx.py
--rw-r--r--  2.0 unx     1010 b- defN 21-Mar-13 08:54 finance_dl/ofx_rename.py
--rw-r--r--  2.0 unx     9925 b- defN 21-Mar-13 08:54 finance_dl/paypal.py
--rw-r--r--  2.0 unx     6356 b- defN 21-Mar-13 08:54 finance_dl/pge.py
--rw-r--r--  2.0 unx     6614 b- defN 21-Mar-13 08:54 finance_dl/radiusbank.py
--rw-r--r--  2.0 unx    13144 b- defN 21-Mar-13 08:54 finance_dl/schwab.py
--rw-r--r--  2.0 unx    16363 b- defN 21-Mar-13 08:54 finance_dl/scrape_lib.py
--rw-r--r--  2.0 unx     7352 b- defN 21-Mar-13 08:54 finance_dl/stockplanconnect.py
--rw-r--r--  2.0 unx     8308 b- defN 21-Mar-13 08:54 finance_dl/ultipro_google.py
--rw-r--r--  2.0 unx     6817 b- defN 21-Mar-13 08:54 finance_dl/update.py
--rw-r--r--  2.0 unx     9430 b- defN 21-Mar-13 08:54 finance_dl/usbank.py
--rw-r--r--  2.0 unx    11378 b- defN 21-Mar-13 08:54 finance_dl/venmo.py
--rw-r--r--  2.0 unx     7481 b- defN 21-Mar-13 08:54 finance_dl/waveapps.py
--rw-r--r--  2.0 unx    18092 b- defN 21-Mar-13 08:55 finance_dl-1.3.3.dist-info/LICENSE
--rw-r--r--  2.0 unx     6102 b- defN 21-Mar-13 08:55 finance_dl-1.3.3.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 21-Mar-13 08:55 finance_dl-1.3.3.dist-info/WHEEL
--rw-r--r--  2.0 unx      123 b- defN 21-Mar-13 08:55 finance_dl-1.3.3.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       11 b- defN 21-Mar-13 08:55 finance_dl-1.3.3.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2654 b- defN 21-Mar-13 08:55 finance_dl-1.3.3.dist-info/RECORD
-33 files, 227295 bytes uncompressed, 74412 bytes compressed:  67.3%
+Zip file size: 88810 bytes, number of entries: 34
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-19 21:54 finance_dl/__init__.py
+-rw-r--r--  2.0 unx    24118 b- defN 24-Apr-19 21:54 finance_dl/amazon.py
+-rw-r--r--  2.0 unx     5584 b- defN 24-Apr-19 21:54 finance_dl/anthem.py
+-rw-r--r--  2.0 unx      439 b- defN 24-Apr-19 21:54 finance_dl/chromedriver_wrapper.py
+-rw-r--r--  2.0 unx     3168 b- defN 24-Apr-19 21:54 finance_dl/cli.py
+-rw-r--r--  2.0 unx     6708 b- defN 24-Apr-19 21:54 finance_dl/comcast.py
+-rw-r--r--  2.0 unx     2242 b- defN 24-Apr-19 21:54 finance_dl/csv_merge.py
+-rw-r--r--  2.0 unx     5438 b- defN 24-Apr-19 21:54 finance_dl/discover.py
+-rw-r--r--  2.0 unx     4719 b- defN 24-Apr-19 21:54 finance_dl/ebmud.py
+-rw-r--r--  2.0 unx    10417 b- defN 24-Apr-19 21:54 finance_dl/gemini.py
+-rw-r--r--  2.0 unx     2249 b- defN 24-Apr-19 21:54 finance_dl/google_login.py
+-rw-r--r--  2.0 unx     5484 b- defN 24-Apr-19 21:54 finance_dl/google_purchases.py
+-rw-r--r--  2.0 unx     3642 b- defN 24-Apr-19 21:54 finance_dl/google_takeout.py
+-rw-r--r--  2.0 unx    13306 b- defN 24-Apr-19 21:54 finance_dl/healthequity.py
+-rw-r--r--  2.0 unx    17838 b- defN 24-Apr-19 21:54 finance_dl/mint.py
+-rw-r--r--  2.0 unx    17409 b- defN 24-Apr-19 21:54 finance_dl/ofx.py
+-rw-r--r--  2.0 unx     1054 b- defN 24-Apr-19 21:54 finance_dl/ofx_rename.py
+-rw-r--r--  2.0 unx    11107 b- defN 24-Apr-19 21:54 finance_dl/paypal.py
+-rw-r--r--  2.0 unx     6307 b- defN 24-Apr-19 21:54 finance_dl/pge.py
+-rw-r--r--  2.0 unx     6614 b- defN 24-Apr-19 21:54 finance_dl/radiusbank.py
+-rw-r--r--  2.0 unx    14266 b- defN 24-Apr-19 21:54 finance_dl/schwab.py
+-rw-r--r--  2.0 unx    16597 b- defN 24-Apr-19 21:54 finance_dl/scrape_lib.py
+-rw-r--r--  2.0 unx     7352 b- defN 24-Apr-19 21:54 finance_dl/stockplanconnect.py
+-rw-r--r--  2.0 unx     8794 b- defN 24-Apr-19 21:54 finance_dl/ultipro_google.py
+-rw-r--r--  2.0 unx     6817 b- defN 24-Apr-19 21:54 finance_dl/update.py
+-rw-r--r--  2.0 unx     9435 b- defN 24-Apr-19 21:54 finance_dl/usbank.py
+-rw-r--r--  2.0 unx    14740 b- defN 24-Apr-19 21:54 finance_dl/venmo.py
+-rw-r--r--  2.0 unx     7517 b- defN 24-Apr-19 21:54 finance_dl/waveapps.py
+-rw-r--r--  2.0 unx    18092 b- defN 24-Apr-19 21:55 finance_dl-1.3.4.dist-info/LICENSE
+-rw-r--r--  2.0 unx     7857 b- defN 24-Apr-19 21:55 finance_dl-1.3.4.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-19 21:55 finance_dl-1.3.4.dist-info/WHEEL
+-rw-r--r--  2.0 unx      123 b- defN 24-Apr-19 21:55 finance_dl-1.3.4.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       11 b- defN 24-Apr-19 21:55 finance_dl-1.3.4.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2733 b- defN 24-Apr-19 21:55 finance_dl-1.3.4.dist-info/RECORD
+34 files, 262269 bytes uncompressed, 84508 bytes compressed:  67.8%
```

## zipnote {}

```diff
@@ -21,14 +21,17 @@
 
 Filename: finance_dl/discover.py
 Comment: 
 
 Filename: finance_dl/ebmud.py
 Comment: 
 
+Filename: finance_dl/gemini.py
+Comment: 
+
 Filename: finance_dl/google_login.py
 Comment: 
 
 Filename: finance_dl/google_purchases.py
 Comment: 
 
 Filename: finance_dl/google_takeout.py
@@ -75,26 +78,26 @@
 
 Filename: finance_dl/venmo.py
 Comment: 
 
 Filename: finance_dl/waveapps.py
 Comment: 
 
-Filename: finance_dl-1.3.3.dist-info/LICENSE
+Filename: finance_dl-1.3.4.dist-info/LICENSE
 Comment: 
 
-Filename: finance_dl-1.3.3.dist-info/METADATA
+Filename: finance_dl-1.3.4.dist-info/METADATA
 Comment: 
 
-Filename: finance_dl-1.3.3.dist-info/WHEEL
+Filename: finance_dl-1.3.4.dist-info/WHEEL
 Comment: 
 
-Filename: finance_dl-1.3.3.dist-info/entry_points.txt
+Filename: finance_dl-1.3.4.dist-info/entry_points.txt
 Comment: 
 
-Filename: finance_dl-1.3.3.dist-info/top_level.txt
+Filename: finance_dl-1.3.4.dist-info/top_level.txt
 Comment: 
 
-Filename: finance_dl-1.3.3.dist-info/RECORD
+Filename: finance_dl-1.3.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## finance_dl/amazon.py

```diff
@@ -11,33 +11,49 @@
 - `credentials`: Required.  Must be a `dict` with `'username'` and `'password'`
   keys.
 
 - `output_directory`: Required.  Must be a `str` that specifies the path on the
   local filesystem where the output will be written.  If the directory does not
   exist, it will be created.
 
+- `dir_per_year`: Optional. If true (default is false), adds one subdirectory
+  to the output for each year's worth of transactions. Useful for filesystems
+  that struggle with very large directories. Probably not that useful for
+  actually finding anything, given the uselessness of Amazon's order ID
+  scheme.
+
 - `amazon_domain`: Optional.  Specifies the Amazon domain from which to download
-  orders.  Must be one of `'.com'` or `'.co.cuk'`.  Defaults to `'.com'`.
+  orders.  Must be one of `'.com'`, `'.co.cuk'` or `'.de'`.  Defaults to
+  `'.com'`.
 
 - `regular`: Optional.  Must be a `bool`.  If `True` (the default), download regular orders.
+   For domains other than `amazon_domain=".com"`, `True` downloads regular AND digital orders.
 
 - `digital`: Optional.  Must be a `bool` or `None`.  If `True`, download digital
-  orders.  Defaults to `None`, which is equivalent to `True` for
-  `amazon_domain=".com"`, and `False` for `amazon_domain=".co.uk"`.
+  orders. Effective only for `amazon_domain=".com"`. Defaults to `True` for
+  `amazon_domain=".com"`. For other domains, digital invoices are downloaded
+  tgehter with regular invoices since there is no separate menu on the amazon website.
 
 - `profile_dir`: Optional.  If specified, must be a `str` that specifies the
   path to a persistent Chrome browser profile to use.  This should be a path
   used solely for this single configuration; it should not refer to your normal
   browser profile.  If not specified, a fresh temporary profile will be used
   each time.
 
 - `order_groups`: Optional.  If specified, must be a list of strings specifying the Amazon
   order page "order groups" that will be scanned for orders to download. Order groups
   include years (e.g. '2020'), as well as 'last 30 days' and 'past 3 months'.
 
+- `download_preorder_invoices`: Optional. If specified and True, invoices for
+  preorders (i.e. orders that have not actually been charged yet) will be
+  skipped. Such preorder invoices are not typically useful for accounting
+  since they claim a card was charged even though it actually has not been
+  yet; they get replaced with invoices containing the correct information when
+  the order is actually fulfilled.
+
 Output format:
 ==============
 
 Each regular or digital order invoice is written in HTML format to the specified
 `output_directory` using the naming scheme `<order-id>.html`,
 e.g. `166-7926740-5141621.html` for a regular order invoice and
 `D56-5204779-4181560.html` for a digital order invoice.
@@ -61,229 +77,517 @@
 
 Interactive shell:
 ==================
 
 From the interactive shell, type: `self.run()` to start the scraper.
 
 """
-
+import dataclasses
 import urllib.parse
 import re
 import logging
 import os
+import datetime
+import dateutil.parser
+import bs4
 from selenium.webdriver.common.by import By
 from selenium.webdriver.support.ui import Select
 from selenium.webdriver.common.keys import Keys
 from atomicwrites import atomic_write
 from . import scrape_lib
+from typing import List, Optional
 
 logger = logging.getLogger('amazon_scrape')
 
 
-class Domain:
-    COM = 'com'
-    CO_UK = 'co.uk'
+@dataclasses.dataclass
+class Domain():
+    top_level: str
+
+    sign_in: str
+    sign_out: str
+
+    # Find invoices.
+    your_orders: str
+    invoice: str
+    invoice_link: List[str]
+    order_summary: str
+    order_summary_hidden: bool
+    next: str
+
+    # Confirm invoice page
+    grand_total: str
+    grand_total_digital: str
+    order_cancelled: str
+    pre_order: str
+
+    digital_order: str
+    regular_order_placed: str
+
+    # .COM: digital orders have own order list
+    # other domains: digital orders are in the regular order list
+    digital_orders_menu: bool
+    digital_orders_menu_text: Optional[str] = None
+    
+    fresh_fallback: Optional[str] = None
+
+
+class DOT_COM(Domain):
+    def __init__(self) -> None:
+        super().__init__(
+            top_level='com',
+            sign_in='Sign In',
+            sign_out='Sign Out',
+
+            your_orders='Your Orders',
+            invoice='Invoice',
+            invoice_link=["View order", "View invoice"],
+            # View invoice -> regular/digital order, View order -> Amazon Fresh
+            fresh_fallback="View order",
+            order_summary='Order Summary',
+            order_summary_hidden=False,
+            next='Next',
+
+            grand_total='Grand Total:',
+            grand_total_digital='Grand Total:',
+            order_cancelled='Order Canceled',
+            pre_order='Pre-order',
+
+            digital_order='Digital Order: (.*)',
+            regular_order_placed=r'(?:Subscribe and Save )?Order Placed:\s+([^\s]+ \d+, \d{4})',
+
+            digital_orders_menu=True,
+            digital_orders_menu_text='Digital Orders',
+            )
+
+    @staticmethod
+    def parse_date(date_str) -> datetime.date:
+        return dateutil.parser.parse(date_str).date()
+
+class DOT_CO_UK(Domain):
+    def __init__(self) -> None:
+        super().__init__(
+            top_level='co.uk',
+            sign_in='Sign in',
+            sign_out='Sign out',
+
+            your_orders='Your Orders',
+            invoice='Invoice',
+            invoice_link=["View order", "View invoice"],
+            # View invoice -> regular/digital order, View order -> Amazon Fresh
+            fresh_fallback="View order",
+            order_summary='Order Summary',
+            order_summary_hidden=False,
+            next='Next',
+
+            grand_total='Grand Total:',
+            grand_total_digital='Grand Total:',
+            order_cancelled='Order Canceled',
+            pre_order='Pre-order',
+
+            digital_order='Digital Order: (.*)',
+            regular_order_placed=r'(?:Subscribe and Save )?Order Placed:\s+([^\s]+ \d+, \d{4})',
+
+            digital_orders_menu=False,
+            )
+
+    @staticmethod
+    def parse_date(date_str) -> datetime.date:
+        return dateutil.parser.parse(date_str).date()
+
+class DOT_DE(Domain):
+    def __init__(self) -> None:
+        super().__init__(
+            top_level='de',
+            sign_in='Anmelden',
+            sign_out='Abmelden',
+
+            your_orders='Meine Bestellungen',
+            invoice='Rechnung',
+            invoice_link=["Bestelldetails anzeigen"],
+            fresh_fallback=None,
+            order_summary='Bestellübersicht',
+            order_summary_hidden=True,
+            next='Weiter',
+
+            grand_total='Gesamtsumme:',
+            grand_total_digital='Endsumme:',
+            order_cancelled='Order Canceled',
+            pre_order='Pre-order',
+
+            digital_order='Digitale Bestellung: (.*)',
+            regular_order_placed=r'(?:Getätigte Spar-Abo-Bestellung|Bestellung aufgegeben am):\s+(\d+\. [^\s]+ \d{4})',
+
+            digital_orders_menu=False,
+            )
+
+    class _parserinfo(dateutil.parser.parserinfo):
+        MONTHS=[
+            ('Jan', 'Januar'), ('Feb', 'Februar'), ('Mär', 'März'),
+            ('Apr', 'April'), ('Mai', 'Mai'), ('Jun', 'Juni'),
+            ('Jul', 'Juli'), ('Aug', 'August'), ('Sep', 'September'),
+            ('Okt', 'Oktober'), ('Nov', 'November'), ('Dez', 'Dezember')
+            ]
+    
+    @staticmethod
+    def parse_date(date_str) -> datetime.date:
+        return dateutil.parser.parse(date_str, parserinfo=DOT_DE._parserinfo(dayfirst=True)).date()
+
+DOMAINS = {
+    ".com": DOT_COM,
+    ".co.uk": DOT_CO_UK, 
+    ".de": DOT_DE
+    }
 
 
 class Scraper(scrape_lib.Scraper):
-    def __init__(self, credentials, output_directory, amazon_domain=Domain.COM, regular=True, digital=None, order_groups=None, **kwargs):
+    def __init__(self,
+                 credentials,
+                 output_directory,
+                 dir_per_year=False,
+                 amazon_domain: str = ".com",
+                 regular: bool = True,
+                 digital: Optional[bool] = None,
+                 order_groups: Optional[List[str]] = None,
+                 download_preorder_invoices: bool = False,
+                 **kwargs):
         super().__init__(**kwargs)
-        default_digital = True if amazon_domain == Domain.COM else False
+        if amazon_domain not in DOMAINS:
+          raise ValueError(f"Domain '{amazon_domain} not supported. Supported "
+                           f"domains: {list(DOMAINS)}")
+        self.domain = DOMAINS[amazon_domain]()
         self.credentials = credentials
         self.output_directory = output_directory
+        self.dir_per_year = dir_per_year
         self.logged_in = False
-        self.amazon_domain = amazon_domain
         self.regular = regular
-        self.digital = digital if digital is not None else default_digital
+        self.digital_orders_menu = digital if digital is not None else self.domain.digital_orders_menu
         self.order_groups = order_groups
+        self.download_preorder_invoices = download_preorder_invoices
 
     def check_url(self, url):
-        netloc_re = r'^([^\.@]+\.)*amazon.' + self.amazon_domain + '$'
+        netloc_re = r'^([^\.@]+\.)*amazon.' + self.domain.top_level + '$'
         result = urllib.parse.urlparse(url)
         if result.scheme != 'https' or not re.fullmatch(netloc_re, result.netloc):
             raise RuntimeError('Reached invalid URL: %r' % url)
 
     def check_after_wait(self):
         self.check_url(self.driver.current_url)
 
     def login(self):
         logger.info('Initiating log in')
-        self.driver.get('https://www.amazon.' + self.amazon_domain)
+        self.driver.get('https://www.amazon.' + self.domain.top_level)
         if self.logged_in:
             return
 
-        sign_out_links = self.find_elements_by_descendant_partial_text('Sign Out', 'a')
+        sign_out_links = self.find_elements_by_descendant_partial_text(self.domain.sign_out, 'a')
         if len(sign_out_links) > 0:
             logger.info('You must be already logged in!')
             self.logged_in = True
             return
 
         logger.info('Looking for sign-in link')
         sign_in_links, = self.wait_and_return(
-            lambda: self.find_visible_elements_by_descendant_partial_text('Sign in', 'a')
+            lambda: self.find_visible_elements_by_descendant_partial_text(self.domain.sign_in, 'a')
         )
 
         self.click(sign_in_links[0])
         logger.info('Looking for username link')
         (username, ), = self.wait_and_return(
             lambda: self.find_visible_elements(By.XPATH, '//input[@type="email"]')
         )
         username.send_keys(self.credentials['username'])
         username.send_keys(Keys.ENTER)
 
+        self.finish_login()
+
+    def finish_login(self):
         logger.info('Looking for password link')
         (password, ), = self.wait_and_return(
             lambda: self.find_visible_elements(By.XPATH, '//input[@type="password"]')
         )
         password.send_keys(self.credentials['password'])
 
         logger.info('Looking for "remember me" checkbox')
         (rememberMe, ) = self.wait_and_return(
             lambda: self.find_visible_elements(By.XPATH, '//input[@name="rememberMe"]')[0]
         )
         rememberMe.click()
 
-        password.send_keys(Keys.ENTER)
+        with self.wait_for_page_load():
+            password.send_keys(Keys.ENTER)
 
         logger.info('Logged in')
         self.logged_in = True
 
-    def get_invoice_path(self, order_id):
+    def get_invoice_path(self, year, order_id):
+        if self.dir_per_year:
+            return os.path.join(self.output_directory, str(year), order_id + '.html')
         return os.path.join(self.output_directory, order_id + '.html')
 
-    def get_orders(self, regular=True, digital=True):
+    def get_order_id(self, href) -> str:
+        m = re.match('.*[&?]orderID=((?:D)?[0-9\\-]+)(?:&.*)?$', href)
+        if m is None:
+            raise RuntimeError(
+                'Failed to parse order ID from href %r' % (href, ))
+        return m[1]
+
+    def get_orders(self, regular=True, digital_orders_menu=True):
         invoice_hrefs = []
         order_ids_seen = set()
+        order_ids_downloaded = frozenset([
+            name[:len(name)-5]
+            for _, _, files in os.walk(self.output_directory)
+            for name in files
+            if name.endswith('.html')
+        ])
 
         def get_invoice_urls():
             initial_iteration = True
             while True:
+                # break when there is no "next page"
 
-                def invoice_finder():
-                    return self.driver.find_elements(By.XPATH, '//a[contains(@href, "orderID=")]')
+                # Problem: different site structures depending on country
+                
+                # .com / .uk
+                # Order Summary buttons are directly visible and can be
+                # identified with href containing "orderID="
+                # but order summary may have different names, e.g. for Amazon Fresh orders
+                
+                # .de
+                # only link with href containing "orderID=" is "Bestelldetails anzeigen" (=Order Details)
+                # which is not helpful
+                # order summary is hidden behind submenu which requires a click to be visible
 
+                def invoice_finder():
+                    if not self.domain.order_summary_hidden:
+                        # order summary link is visible on page
+                        return self.driver.find_elements(
+                            By.XPATH, '//a[contains(@href, "orderID=")]')
+                    else:
+                        # order summary link is hidden in submenu for each order
+                        elements = self.driver.find_elements(By.XPATH, 
+                            '//a[@class="a-popover-trigger a-declarative"]')
+                        return [a for a in elements if a.text == self.domain.invoice]
+                
                 if initial_iteration:
                     invoices = invoice_finder()
                 else:
                     invoices, = self.wait_and_return(invoice_finder)
                 initial_iteration = False
 
-                order_ids = set()
-                for invoice_link in invoices:
+                last_order_id = None
+
+                def invoice_link_finder(invoice_link):
+                    if invoice_link.text not in self.domain.invoice_link:
+                        # skip invoice if label is not known
+                        # different labels are possible e.g. for regular orders vs. Amazon fresh
+                        if invoice_link.text != "":
+                            # log non-empty link texts -> may be new type
+                            logger.debug(
+                                'Skipping invoice due to unknown invoice_link.text: %s',
+                                invoice_link.text)
+                        return (False, False)
                     href = invoice_link.get_attribute('href')
-                    m = re.match('.*[&?]orderID=((?:D)?[0-9\\-]+)(?:&.*)?$', href)
-                    if m is None:
-                        raise RuntimeError(
-                            'Failed to parse order ID from href %r' % (href, ))
-                    order_id = m[1]
-                    if order_id in order_ids:
-                        continue
-                    order_ids.add(order_id)
-                    invoice_path = self.get_invoice_path(order_id)
-                    if order_id in order_ids_seen:
-                        logger.info('Skipping already-seen order id: %r',
-                                    order_id)
-                        continue
-                    if os.path.exists(invoice_path):
-                        logger.info('Skipping already-downloaded invoice: %r',
-                                    order_id)
-                        continue
-                    print_url = 'https://www.amazon.%s/gp/css/summary/print.html?ie=UTF8&orderID=%s' % (
-                        self.amazon_domain, order_id)
-                    invoice_hrefs.append((print_url, order_id))
-                    order_ids_seen.add(order_id)
+                    order_id = self.get_order_id(href)
+                    if self.domain.fresh_fallback is not None and invoice_link.text == self.domain.fresh_fallback:
+                        # Amazon Fresh order, construct link to invoice
+                        logger.info("   Found likely Amazon Fresh order. Falling back to direct invoice URL.")
+                        tokens = href.split("/")
+                        tokens = tokens[:4]
+                        tokens[-1] = f"gp/css/summary/print.html?orderID={order_id}"
+                        href = "/".join(tokens)
+                    return (order_id, href)
+
+                def invoice_link_finder_hidden():
+                        # submenu containing order summary takes some time to load after click
+                        # search for order summary link and compare order_id
+                        # repeat until order_id is different to last order_id
+                        summary_links = self.driver.find_elements(By.LINK_TEXT, 
+                            self.domain.order_summary)
+                        if summary_links:
+                            href = summary_links[0].get_attribute('href')
+                            order_id = self.get_order_id(href)
+                            if order_id != last_order_id:
+                                return (order_id, href)
+                        return False
+
+                for invoice_link in invoices:
+                    if not self.domain.order_summary_hidden:
+                        (order_id, href) = invoice_link_finder(invoice_link)
+                    else:
+                        invoice_link.click()
+                        (order_id, href), = self.wait_and_return(invoice_link_finder_hidden)
+                    if order_id:
+                        if order_id in order_ids_seen:
+                            logger.info('Skipping already-seen order id: %r', order_id)
+                            continue
+                        if order_id in order_ids_downloaded:
+                            logger.info('Skipping already-downloaded invoice: %r', order_id)
+                            continue
+                        logger.info('Found order \'{}\''.format(order_id))
+                        invoice_hrefs.append((href, order_id))
+                        order_ids_seen.add(order_id)
+                        last_order_id = order_id
 
                 # Find next link
                 next_links = self.find_elements_by_descendant_text_match(
-                    '. = "Next"', 'a', only_displayed=True)
+                    f'. = "{self.domain.next}"', 'a', only_displayed=True)
                 if len(next_links) == 0:
                     logger.info('Found no more pages')
                     break
                 if len(next_links) != 1:
                     raise RuntimeError('More than one next link found')
                 with self.wait_for_page_load():
+                    logging.info("Next page.")
                     self.click(next_links[0])
 
         def retrieve_all_order_groups():
             order_select_index = 0
 
             while True:
-                (order_filter,), = self.wait_and_return(
-                    lambda: self.find_visible_elements(By.XPATH, '//select[@name="orderFilter"]')
-                )
+                order_filter, = self.wait_and_locate((By.CSS_SELECTOR, '#time-filter, #orderFilter'))
                 order_select = Select(order_filter)
                 num_options = len(order_select.options)
                 if order_select_index >= num_options:
                     break
-                option_text = order_select.options[
-                    order_select_index].text.strip()
+                option = order_select.options[
+                    order_select_index]
+                option_text = option.text.strip()
                 order_select_index += 1
                 if option_text == 'Archived Orders':
                     continue
                 if self.order_groups is not None and option_text not in self.order_groups:
                     logger.info('Skipping order group: %r', option_text)
                     continue
                 logger.info('Retrieving order group: %r', option_text)
-                with self.wait_for_page_load():
-                    order_select.select_by_index(order_select_index)
+                if not option.is_selected():
+                    with self.wait_for_page_load():
+                        order_select.select_by_index(order_select_index - 1)
                 get_invoice_urls()
 
         if regular:
-            orders_text = "Your Orders" if self.amazon_domain == Domain.CO_UK else "Orders"
             # on co.uk, orders link is hidden behind the menu, hence not directly clickable
             (orders_link,), = self.wait_and_return(
-                lambda: self.find_elements_by_descendant_text_match('. = "{}"'.format(orders_text), 'a', only_displayed=False)
+                lambda: self.find_elements_by_descendant_text_match(f'. = "{self.domain.your_orders}"', 'a', only_displayed=False)
             )
             link = orders_link.get_attribute('href')
             scrape_lib.retry(lambda: self.driver.get(link), retry_delay=2)
 
             retrieve_all_order_groups()
 
-        if digital:
+        if digital_orders_menu:
+            # orders in separate Digital Orders list (relevant for .COM)
+            # other domains list digital orders within the regular order list
             (digital_orders_link,), = self.wait_and_return(
-                lambda: self.find_elements_by_descendant_text_match('contains(., "Digital Orders")', 'a', only_displayed=True)
+                lambda: self.find_elements_by_descendant_text_match(
+                    f'contains(., "{self.domain.digital_orders_menu_text}")', 'a', only_displayed=True)
             )
             scrape_lib.retry(lambda: self.click(digital_orders_link),
                              retry_delay=2)
             retrieve_all_order_groups()
 
         self.retrieve_invoices(invoice_hrefs)
 
     def retrieve_invoices(self, invoice_hrefs):
         for href, order_id in invoice_hrefs:
-            invoice_path = self.get_invoice_path(order_id)
-
             logger.info('Downloading invoice for order %r', order_id)
             with self.wait_for_page_load():
                 self.driver.get(href)
 
             # For digital orders, Amazon dynamically generates some of the information.
             # Wait until it is all generated.
             def get_source():
                 source = self.driver.page_source
-                if 'Grand Total:' in source:
+                if (
+                    self.domain.grand_total in source or
+                    self.domain.grand_total_digital in source or
+                    self.domain.order_cancelled in source
+                ):
                     return source
+                elif 'problem loading this order' in source:
+                    raise ValueError(f'Failed to retrieve information for order {order_id}')
+                elif self.find_visible_elements(By.XPATH, '//input[@type="password"]'):
+                    self.finish_login() # fallthrough
+
                 return None
 
             page_source, = self.wait_and_return(get_source)
+            if self.domain.pre_order in page_source and not self.download_preorder_invoices:
+                    # Pre-orders don't have enough information to download yet. Skip them.
+                    logger.info(f'Skipping pre-order invoice {order_id}')
+                    return
             if order_id not in page_source:
-                raise ValueError('Failed to retrieve information for order %r'
-                                 % (order_id, ))
+                raise ValueError(f'Failed to retrieve information for order {order_id}')
+
+            # extract order date
+            def get_date(source, order_id):
+                # code blocks taken from beancount-import/amazon-invoice.py
+                soup=bs4.BeautifulSoup(source, 'lxml')
+
+                def is_order_placed_node(node):
+                    # order placed information in page header (top left)
+                    m = re.fullmatch(self.domain.regular_order_placed, node.text.strip())
+                    return m is not None
+                
+                def is_digital_order_row(node):
+                    # information in heading of order table
+                    if node.name != 'tr':
+                        return False
+                    m = re.match(self.domain.digital_order, node.text.strip())
+                    if m is None:
+                        return False
+                    try:
+                        self.domain.parse_date(m.group(1))
+                        return True
+                    except:
+                        return False
+
+                if order_id.startswith('D'):
+                    # digital order
+                    node = soup.find(is_digital_order_row)
+                    regex = self.domain.digital_order
+                else:
+                    # regular order
+                    node = soup.find(is_order_placed_node)
+                    regex = self.domain.regular_order_placed
+                
+                m = re.fullmatch(regex, node.text.strip())
+                if m is None:
+                    return None
+                order_date = self.domain.parse_date(m.group(1))
+                return order_date
+
+            order_date = get_date(page_source, order_id)
+            if order_date is None: 
+                if self.dir_per_year:
+                    raise ValueError(f'Failed to get date for order {order_id}')
+                else:
+                    # date is not necessary, so just log
+                    logger.info(f'Failed to get date for order {order_id}')
+            else:
+                order_date = order_date.year
+            invoice_path = self.get_invoice_path(order_date, order_id)
+            if not os.path.exists(os.path.dirname(invoice_path)):
+                os.makedirs(os.path.dirname(invoice_path))
             with atomic_write(
                     invoice_path, mode='w', encoding='utf-8',
-                    newline='\n') as f:
+                    newline='\n', overwrite=True) as f:
                 # Write with Unicode Byte Order Mark to ensure content will be properly interpreted as UTF-8
                 f.write('\ufeff' + page_source)
             logger.info('  Wrote %s', invoice_path)
 
     def run(self):
         self.login()
         if not os.path.exists(self.output_directory):
             os.makedirs(self.output_directory)
-        self.get_orders(regular=self.regular, digital=self.digital)
+        self.get_orders(
+            regular=self.regular,
+            digital_orders_menu=self.digital_orders_menu
+            )
 
 
 def run(**kwargs):
     scrape_lib.run_with_scraper(Scraper, **kwargs)
 
 
 def interactive(**kwargs):
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## finance_dl/anthem.py

```diff
@@ -136,24 +136,24 @@
             pdf_path = os.path.join(self.output_directory,
                                     claim['claimNumber'] + '.pdf')
             json_path = os.path.join(self.output_directory,
                                      claim['claimNumber'] + '.json')
             if not os.path.exists(json_path):
                 with atomic_write(
                         json_path, mode='w', encoding='utf-8',
-                        newline='\n') as f:
+                        newline='\n', overwrite=True) as f:
                     f.write(json.dumps(claim, indent='  ').strip() + '\n')
             if not os.path.exists(pdf_path):
                 if not claim['eobLinkUrl'].startswith('https:/'): continue
                 downloads_needed.append((claim['eobLinkUrl'], pdf_path))
         for i, (url, pdf_path) in enumerate(downloads_needed):
             logger.info('Downloading EOB %d/%d', i + 1, len(downloads_needed))
             self.driver.get(url)
             download_result, = self.wait_and_return(self.get_downloaded_file)
-            with atomic_write(pdf_path, mode='wb') as f:
+            with atomic_write(pdf_path, mode='wb', overwrite=True) as f:
                 f.write(download_result[1])
 
     def run(self):
         self.login()
         self.save_documents()
```

## finance_dl/comcast.py

```diff
@@ -146,15 +146,15 @@
                     partial_text, 'button')
                 continue_link.click()
                 time.sleep(3.0)  # wait for overlay to go away
             except:
                 pass
         bills_link = get_bills_link()
 
-        self.driver.find_element_by_tag_name('body').send_keys(Keys.ESCAPE)
+        self.driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.ESCAPE)
         bills_link.click()
 
         def get_links():
             links, = self.wait_and_return(
                 lambda: self.driver.find_elements(By.XPATH, '//a[starts-with(text(), "View PDF")]'))
             return links
 
@@ -164,15 +164,15 @@
 
         for link in links:
             if not link.is_displayed():
                 continue
             cur_el = link
             bill_date = None
             while True:
-                parent = cur_el.find_element_by_xpath('..')
+                parent = cur_el.find_element(By.XPATH, '..')
                 if parent == cur_el:
                     break
                 try:
                     bill_date = dateutil.parser.parse(parent.text, fuzzy=True)
                     break
                 except:
                     cur_el = parent
```

## finance_dl/discover.py

```diff
@@ -53,14 +53,15 @@
 import re
 import datetime
 import time
 import logging
 import os
 import shutil
 from selenium.common.exceptions import NoSuchElementException, TimeoutException
+from selenium.webdriver.common.by import By
 from selenium.webdriver.common.keys import Keys
 
 from . import scrape_lib
 
 
 logger = logging.getLogger('discover_scrape')
 
@@ -81,19 +82,19 @@
         self.output_directory = output_directory
         os.makedirs(self.output_directory, exist_ok=True)
 
     def check_after_wait(self):
         check_url(self.driver.current_url)
 
     def find_account_last4(self):
-        return self.driver.find_element_by_xpath(XPATH_OF_LAST_FOUR_DIGITS).text
+        return self.driver.find_element(By.XPATH, XPATH_OF_LAST_FOUR_DIGITS).text
 
     def login(self):
         try:
-            account = self.driver.find_element_by_xpath(XPATH_OF_LAST_FOUR_DIGITS)
+            account = self.driver.find_element(By.XPATH, XPATH_OF_LAST_FOUR_DIGITS)
             logger.info("Already logged in")
         except NoSuchElementException:
             logger.info('Initiating log in')
             url = "https://portal.discover.com/customersvcs/universalLogin/ac_main"
             self.driver.get(url)
 
             (username, password), = self.wait_and_return(
```

## finance_dl/ebmud.py

```diff
@@ -124,15 +124,15 @@
                 (statement_date.strftime(output_date_format), ))
             if os.path.exists(statement_path):
                 logger.info('Skipping existing statement: %s', statement_path)
                 continue
             logger.info('Downloading %s', statement_path)
             self.click(row.find_element_by_tag_name('a'))
             download_result, = self.wait_and_return(self.get_downloaded_file)
-            with atomic_write(statement_path, mode='wb') as f:
+            with atomic_write(statement_path, mode='wb', overwrite=True) as f:
                 f.write(download_result[1])
             logger.info('Wrote %s', statement_path)
 
     def run(self):
         self.login()
         self.get_statements()
```

## finance_dl/google_purchases.py

```diff
@@ -106,15 +106,16 @@
         for i, (purchase_id, html_path) in enumerate(need_to_fetch):
             url = 'https://myaccount.google.com/purchases/detail?order_id=' + purchase_id
             logger.info('Fetching details %d/%d: %s', i, len(need_to_fetch), url)
             with self.wait_for_page_load():
                 self.driver.get(url)
             content = self.driver.page_source
             with atomic_write(
-                    html_path, mode='w', encoding='utf-8', newline='\n') as f:
+                    html_path, mode='w', encoding='utf-8', newline='\n',
+                    overwrite=True) as f:
                 # Write with Unicode Byte Order Mark to ensure content will be properly interpreted as UTF-8
                 f.write('\ufeff' + content)
             logger.info('Write details %d/%d: %s', i, len(need_to_fetch), html_path)
 
     def run(self):
         if not os.path.exists(self.output_directory):
             os.makedirs(self.output_directory)
@@ -129,15 +130,15 @@
             if m is None:
                 logger.info('Ignoring file in takeout archive: %s', name)
                 continue
             order_id = m.group(1)
             json_path = os.path.join(self.output_directory,
                                      'order_' + order_id + '.json')
             if not os.path.exists(json_path):
-                with atomic_write(json_path, mode='wb') as f:
+                with atomic_write(json_path, mode='wb', overwrite=True) as f:
                     f.write(takeout_zip.read(name))
             html_path = os.path.join(self.output_directory, order_id + '.html')
             if os.path.exists(html_path):
                 continue
             need_to_fetch.append((order_id, html_path))
         self._fetch_html_pages(need_to_fetch)
```

## finance_dl/healthequity.py

```diff
@@ -70,14 +70,17 @@
 import urllib.parse
 import re
 import datetime
 import time
 import logging
 import os
 import bs4
+import tempfile
+import openpyxl
+from openpyxl.cell.cell import MergedCell
 from selenium.webdriver.common.by import By
 from selenium.webdriver.support.ui import Select
 from selenium.webdriver.common.keys import Keys
 from . import scrape_lib
 from . import csv_merge
 
 logger = logging.getLogger('healthequity_scrape')
@@ -97,51 +100,74 @@
             return datetime.datetime.strptime(line, date_format).date()
         except:
             pass
     return None
 
 
 FUND_ACTIVITY_HEADERS = [
-    'Fund', 'Name', 'Shares (#)', 'Closing Price', 'Closing Value'
+    'Fund', 'Name', 'Class', 'Target %\nallocation', 'Est. %\nholding', 'Shares\nheld', 'Closing\nprice', 'Closing\nvalue'
 ]
 
+# For compatibility with beancount-import's healthequity plugin, write the old
+# format for balances.csv files. The three new columns are fairly useless
+# anyway, and the new (multiline) column titles are unambiguously worse even if
+# a human were to actually ever read these CSVs.
+OLD_FUND_ACTIVITY_HEADERS = [
+    'Fund','Name',None,None,None,'Shares (#)','Closing Price','Closing Value'
+]
 
 def write_balances(data, path):
     rows = []
     for entry in data:
-        keys = [x[0] for x in entry]
-        if keys == FUND_ACTIVITY_HEADERS:
+        keys, values = zip(*entry)
+        if list(keys) == FUND_ACTIVITY_HEADERS:
+            entry = [
+                (k, v.strip().split('\n')[0].strip('$'))
+                for (k, v) in zip(OLD_FUND_ACTIVITY_HEADERS, values)
+                if k
+            ]
             row_values = dict(entry)
             row_values['Fund'] = row_values['Fund'].strip().split()[0]
+            row_values['Name'] = row_values['Name'].strip().split('\n')[0]
             rows.append(row_values)
-    csv_merge.write_csv(FUND_ACTIVITY_HEADERS, rows, path)
+    csv_merge.write_csv([h for h in OLD_FUND_ACTIVITY_HEADERS if h], rows, path)
 
 
 def write_fund_activity(raw_transactions_data, path):
-    input_date_format = '%m/%d/%Y'
-    output_date_format = '%Y-%m-%d'
-    soup = bs4.BeautifulSoup(raw_transactions_data.decode('utf-8'), 'lxml')
+    def format_cell(c):
+        if c.is_date:
+            return c.value.strftime('%Y-%m-%d')
+        if c.number_format[0] == '$':
+            base = '${:,.2f}'.format(abs(c.value))
+            if c.value >= 0:
+                return base
+            else:
+                return '(%s)' % base
+        return str(c.value)
+
+    wb = None
+    with tempfile.NamedTemporaryFile(suffix='.xlsx') as xlsx:
+        xlsx.write(raw_transactions_data)
+        xlsx.flush()
+        wb = openpyxl.load_workbook(xlsx.name)
+
+    ws = wb.worksheets[0]
     headers = [
         'Date', 'Fund', 'Category', 'Description', 'Price', 'Amount', 'Shares',
         'Total Shares', 'Total Value'
     ]
     rows = []
-    for row in soup.find_all('tr'):
-        cells = [str(x.text).strip() for x in row.find_all('td')]
-        while cells and not cells[-1].strip():
-            del cells[-1]
-        if len(cells) == 1:
+    for row in ws.rows:
+        if any([isinstance(c, MergedCell) for c in row]):
             continue
-        assert len(cells) == len(headers)
+        assert len(row) == len(headers)
+        cells = [format_cell(c) for c in row]
         if cells == headers:
             continue
-        row_values = dict(zip(headers, cells))
-        row_values['Date'] = datetime.datetime.strptime(
-            row_values['Date'], input_date_format).strftime(output_date_format)
-        rows.append(row_values)
+        rows.append(dict(zip(headers, cells)))
     csv_merge.merge_into_file(filename=path, field_names=headers, data=rows,
                               sort_by=lambda x: x['Date'])
 
 
 def write_transactions(raw_transactions_data, path):
     input_date_format = '%m/%d/%Y'
     output_date_format = '%Y-%m-%d'
@@ -153,32 +179,38 @@
         cells = [str(x.text).strip() for x in row.find_all('td')]
         while cells and not cells[-1].strip():
             del cells[-1]
         if len(cells) <= 1:
             continue
         if cells[0] == 'TOTAL':
             continue
-        assert len(cells) == len(headers)
-        if cells == headers:
+        assert len(cells) >= len(headers), (cells, headers)
+        if cells[:len(headers)] == headers:
             continue
         row_values = dict(zip(headers, cells))
         # Sanitize whitespace in description
         row_values['Transaction'] = ' '.join(row_values['Transaction'].split())
+        # Remove duplicate tax year in description
+        row_values['Transaction'] = re.sub(r'(\(Tax year: \d+\)) *\1', r'\1', row_values['Transaction'])
         row_values['Cash Balance'] = row_values.pop('HSA Cash Balance')
 
         # Sanitize date_str
         date_str = row_values['Date']
         date_str = re.sub('\\(Available .*\\)', '', date_str)
 
         row_values['Date'] = datetime.datetime.strptime(
             date_str, input_date_format).strftime(output_date_format)
         rows.append(row_values)
     rows.reverse()
     csv_merge.merge_into_file(filename=path, field_names=output_headers,
-                              data=rows, sort_by=lambda x: x['Date'])
+                              data=rows, sort_by=lambda x: x['Date'],
+                              # Don't consider balance-after in comparing rows,
+                              # because txn order (and therefore running
+                              # balance) is not stable across visits
+                              compare_fields = output_headers[0:3])
 
 
 class Scraper(scrape_lib.Scraper):
     def __init__(self, credentials, output_directory, **kwargs):
         super().__init__(**kwargs)
         self.credentials = credentials
         self.output_directory = output_directory
@@ -201,15 +233,15 @@
         with self.wait_for_page_load():
             password.send_keys(Keys.ENTER)
         logger.info('Logged in')
         self.logged_in = True
 
     def download_transaction_history(self):
         (transactions_link, ), = self.wait_and_return(
-            lambda: self.find_visible_elements_by_descendant_partial_text('Transaction History', 'td'))
+            lambda: self.find_visible_elements(By.ID, 'viewAllLink'))
         scrape_lib.retry(transactions_link.click, retry_delay=2)
         (date_select, ), = self.wait_and_return(
             lambda: self.find_visible_elements_by_descendant_partial_text('All dates', 'select'))
         date_select = Select(date_select)
         with self.wait_for_page_load():
             date_select.select_by_visible_text('All dates')
 
@@ -240,36 +272,36 @@
         self.driver.refresh()
 
         return results
 
     def get_investment_balance(self):
         headers = FUND_ACTIVITY_HEADERS
         (table, ), = self.wait_and_return(
-            lambda: scrape_lib.find_table_by_headers(self, headers))
+            lambda: self.driver.find_elements(By.TAG_NAME, 'table'))
         data = scrape_lib.extract_table_data(table, headers)
         return data
 
     def go_to_investment_history(self):
         logger.info('Going to investment history')
         self.driver.get(
             'https://www.healthequity.com/Member/Investment/Desktop.aspx')
 
     def download_fund_activity(self):
         logger.info('Looking for fund activity link')
         (fund_activity_link,), = self.wait_and_return(
-            lambda: self.find_visible_elements(By.XPATH, '//a[contains(@href, "FundActivity")]'))
+            lambda: self.find_visible_elements(By.ID, 'EditPortfolioTab'))
         scrape_lib.retry(fund_activity_link.click, retry_delay=2)
-        logger.info('Selecting date ranage for fund activity')
+        logger.info('Selecting date range for fund activity')
         (start_date,), = self.wait_and_return(
-            lambda: self.find_visible_elements(By.XPATH, '//input[@type="text" and contains(@id, "dateSelectStart")]'))
+            lambda: self.find_visible_elements(By.XPATH, '//input[@type="text" and contains(@id, "startDate")]'))
         start_date.clear()
-        start_date.send_keys('01011900')
+        start_date.send_keys('01/01/1900\n')
         logger.info('Downloading fund activity')
         (download_link, ), = self.wait_and_return(
-            lambda: self.driver.find_elements_by_link_text('Download'))
+            lambda: self.find_visible_elements(By.ID, 'fundPerformanceDownload'))
         scrape_lib.retry(download_link.click, retry_delay=2)
         logger.info('Waiting for fund activity download')
         download_result, = self.wait_and_return(self.get_downloaded_file)
         return download_result[1]
 
     def download_data(self):
         raw_transactions = self.download_transaction_history()
```

## finance_dl/ofx.py

```diff
@@ -30,14 +30,22 @@
     where `id` is a random hex string obtained from e.g.:
     `openssl rand -hex 16`.
 
 - `output_directory`: Required.  Must be a `str` that specifies the path to the
   directory where OFX files are to be written.  If it does not exist, it will be
   created.
 
+- `acct_dir_map`: Optional. A `dict` that maps account numbers as found in the
+  OFX output to the directory name to use to hold OFX files for that account.
+  You can use this to give mnemonic names to the directories in case you have
+  multiple accounts with an institution and don't want to keep track of them
+  by number alone. If you supply a mapping for an account, it will be used
+  *exactly*; if that mapping is not a valid directory name, finance-dl will
+  fail.
+
 - `overlap_days`: Optional.  An `int` that specifies the number of days of
   overlap to use when retrieving additional transactions.  This is intended to
   reduce the chances of transactions being missed (and duplicate transactions
   can easily be filtered when processing the downloaded data).  The default
   value of `2` should be suitable in almost all cases.
 
 - `min_start_date`: Optional.  A `datetime.date` object specifying the earliest
@@ -97,16 +105,20 @@
             'username': 'XXXXXX',
             'password': 'XXXXXX',
         }
         return dict(
             module='finance_dl.ofx',
             ofx_params=ofx_params,
             output_directory=os.path.join(data_dir, 'vanguard'),
+            acct_dir_map={
+                '880012345': 'Roth IRA',
+                '880045678': 'FooCorp 401(k)',
+                '880078901': 'Taxable Account'
+            }
         )
-
 """
 
 import contextlib
 import warnings
 import datetime
 import os
 import time
@@ -116,15 +128,51 @@
 
 from atomicwrites import atomic_write
 import bs4
 import dateutil.parser
 import ofxclient.institution
 import ofxclient
 
-from beancount.ingest.importers.ofx import parse_ofx_time, find_child
+# find_child and parse_ofx_time were derived from implementation in beancount/ingest/importers/ofx.py{,test}
+# Copyright (C) 2016  Martin Blais
+# GNU GPLv2
+def find_child(node, name, conversion=None):
+    """Find a child under the given node and return its value.
+
+    Args:
+      node: A <STMTTRN> bs4.element.Tag.
+      name: A string, the name of the child node.
+      conversion: A callable object used to convert the value to a new data type.
+    Returns:
+      A string, or None.
+    """
+    child = node.find(name)
+    if not child:
+        return None
+    if not child.contents:
+        value = ''
+    else:
+        value = child.contents[0].strip()
+    if conversion:
+        value = conversion(value)
+    return value
+
+
+def parse_ofx_time(date_str):
+    """Parse an OFX time string and return a datetime object.
+
+    Args:
+      date_str: A string, the date to be parsed.
+    Returns:
+      A datetime.datetime instance.
+    """
+    if len(date_str) < 14:
+        return datetime.datetime.strptime(date_str[:8], '%Y%m%d')
+    return datetime.datetime.strptime(date_str[:14], '%Y%m%d%H%M%S')
+
 
 warnings.filterwarnings('ignore', message='split()', module='re')
 
 logger = logging.getLogger('ofx')
 
 # Discover hack. Must have at least 5 seconds between requests.
 last_request_time = 0.0
@@ -174,14 +222,20 @@
 def get_earliest_data(account, start_date, slowdown = False):
     """Try to retrieve earliest batch of account data, starting at `start_date'.
 
     Uses binary search to find the earliest point after start_date that yields a valid response.
 
     Returns ((startdate, enddate), data).
     """
+    # First try the actual start_date; if it works, no binary search is needed
+    data = download_account_data_starting_from(account, start_date)
+    date_range = get_ofx_date_range(data)
+    if date_range is not None:
+        return date_range, data
+
     logger.info(
         'Binary searching to find earliest data available for account %s.',
         account.number)
     lower_bound = start_date
     upper_bound = datetime.date.today()
     valid_data = None
     valid_date_range = None
@@ -264,15 +318,15 @@
     date_ranges.sort()
 
     def save_data(date_range, data):
         t = time.time()
         logger.info('Received data %s -- %s', date_range[0], date_range[1])
         filename = ('%s-%s--%d.ofx' % (date_range[0].strftime(date_format),
                                        date_range[1].strftime(date_format), t))
-        with atomic_write(os.path.join(output_dir, filename), mode='wb') as f:
+        with atomic_write(os.path.join(output_dir, filename), mode='wb', overwrite=True) as f:
             f.write(data)
         date_ranges.append((date_range[0].date(), date_range[1].date()))
         date_ranges.sort()
 
     if len(date_ranges) == 0:
         try:
             date_range, data = get_earliest_data(account,
@@ -312,34 +366,37 @@
             break
         if (datetime.date.today() - date_ranges[-1][0]
             ).days <= min_days_retrieved:
             break
 
 
 def save_all_account_data(inst: ofxclient.institution.Institution,
-                          output_dir: str, **kwargs):
+                          output_dir: str,
+                          acct_dir_map: dict={}, **kwargs):
     """Attempts to download data for all accounts.
 
     :param inst: The institution connection.
     :param output_dir: The base output directory in which to store the
         downloaded OFX files.  The data for each account is saved in a
         subdirectory of `output_dir`, with a name equal to the account number.
     :param kwargs: Additional arguments to pass to save_single_account_data.
     """
     accounts = inst.accounts()
     slowdown = 'Discover' in inst.org
     if slowdown:
         time.sleep(5)
     for a in accounts:
-        try:
-            name = sanitize_account_name(a.number)
-        except ValueError:
-            logger.warning('Account number is invalid path component: %r',
-                           name)
-            continue
+        if a.number in acct_dir_map:
+            name = acct_dir_map[a.number]
+        else:
+            try:
+                name = sanitize_account_name(a.number)
+            except ValueError:
+                name = 'blank'
+                logger.warning(f"Account number is invalid path component: {a.number}; using {name}")
         save_single_account_data(
             account=a, output_dir=os.path.join(output_dir, name), slowdown = slowdown, **kwargs)
 
 
 def connect(params: dict) -> ofxclient.institution.Institution:
     """Connects to an OFX server.
```

## finance_dl/ofx_rename.py

```diff
@@ -9,15 +9,17 @@
 
 def fix_name(path, dry_run):
     name = os.path.basename(path)
     d = os.path.dirname(path)
     date_format = '%Y%m%d'
 
     parts = name.split('-')
-    assert len(parts) == 4
+    if len(parts) != 4:
+      print("Skipping %r" % name)
+      return
 
     with open(path, 'rb') as f:
         date_range = get_ofx_date_range(f.read())
     new_parts = [
         date_range[0].strftime(date_format), date_range[1].strftime(date_format)
     ] + parts[2:]
     new_name = '-'.join(new_parts)
@@ -27,11 +29,11 @@
         if not dry_run:
             os.rename(path, new_path)
 
 
 if __name__ == '__main__':
     ap = argparse.ArgumentParser()
     ap.add_argument('paths', nargs='*')
-    args = ap.parse_args()
     ap.add_argument('--dry-run', action='store_true')
+    args = ap.parse_args()
     for path in args.paths:
         fix_name(path, dry_run=args.dry_run)
```

## finance_dl/paypal.py

```diff
@@ -47,14 +47,15 @@
 import datetime
 import os
 import time
 from selenium.webdriver.common.by import By
 from selenium.webdriver.support.ui import Select
 from selenium.webdriver.common.keys import Keys
 from selenium.common.exceptions import NoSuchElementException
+from requests.exceptions import HTTPError
 import jsonschema
 from atomicwrites import atomic_write
 from . import scrape_lib
 from . import google_login
 
 logger = logging.getLogger('paypal')
 
@@ -64,32 +65,38 @@
     '#schema': 'http://json-schema.org/draft-07/schema#',
     'description': 'JSON schema for the transaction list response.',
     'type': 'object',
     'required': ['data'],
     'properties': {
         'data': {
             'type': 'object',
-            'required': ['activity'],
+            'required': ['data'],
             'properties': {
-                'activity': {
+                'data': {
                     'type': 'object',
-                    'required': ['transactions'],
+                    'required': ['activity'],
                     'properties': {
-                        'transactions': {
-                            'type': 'array',
-                            'items': {
-                                'type': 'object',
-                                'required': ['id'],
-                                'properties': {
-                                    'id': {
-                                        'type': 'string',
-                                        'pattern': r'^[A-Za-z0-9\-]+$',
-                                    },
+                        'activity': {
+                            'type': 'object',
+                            'required': ['transactions'],
+                            'properties': {
+                                'transactions': {
+                                    'type': 'array',
+                                    'items': {
+                                        'type': 'object',
+                                        'required': ['id'],
+                                        'properties': {
+                                            'id': {
+                                                'type': 'string',
+                                                'pattern': r'^[A-Za-z0-9\-]+$',
+                                            },
+                                        },
+                                    }
                                 },
-                            }
+                            },
                         },
                     },
                 },
             },
         },
     },
 }
@@ -98,17 +105,17 @@
     '#schema': 'http://json-schema.org/draft-07/schema#',
     'description': 'JSON schema for the transaction details response.',
     'type': 'object',
     'required': ['data'],
     'properties': {
         'data': {
             'type': 'object',
-            'required': ['details'],
+            'required': ['amount'],
             'properties': {
-                'details': {
+                'amount': {
                     'type': 'object',
                 },
             },
         },
     },
 }
 
@@ -164,17 +171,17 @@
             })
 
     def get_csrf_token(self):
         if self.csrf_token is not None: return self.csrf_token
         logging.info('Getting CSRF token')
         self.driver.get('https://www.paypal.com/myaccount/transactions/')
         # Get CSRF token
-        body_element, = self.wait_and_locate((By.XPATH,
-                                              '//body[@data-token!=""]'))
-        self.csrf_token = body_element.get_attribute('data-token')
+        body_element, = self.wait_and_locate((By.ID, "__APP_DATA__"))
+        attribute_object = json.loads(body_element.get_attribute("innerHTML"))
+        self.csrf_token = attribute_object["_csrf"]
         return self.csrf_token
 
     def get_transaction_list(self):
         end_date = datetime.datetime.now().date() + datetime.timedelta(days=2)
         start_date = end_date - datetime.timedelta(days=365 * 10)
         date_format = '%Y-%m-%d'
         logging.info('Getting transaction list')
@@ -184,15 +191,15 @@
             'isClearFilterSelection=false&isClientSideFiltering=false&selectedCurrency=ALL&'
             'startDate=%s&endDate=%s' % (start_date.strftime(date_format),
                                          end_date.strftime(date_format)))
         resp = self.make_json_request(url)
         resp.raise_for_status()
         j = resp.json()
         jsonschema.validate(j, transaction_list_schema)
-        return j['data']['activity']['transactions']
+        return j['data']['data']['activity']['transactions']
 
     def save_transactions(self):
         transaction_list = self.get_transaction_list()
         logging.info('Got %d transactions', len(transaction_list))
         for transaction in transaction_list:
             transaction_id = transaction['id']
             output_prefix = os.path.join(self.output_directory, transaction_id)
@@ -202,51 +209,62 @@
                     invoice_url = (
                         'https://www.paypal.com/invoice/payerView/detailsInternal/'
                         + transaction_id + '?printPdfMode=true')
                     logging.info('Retrieving PDF %s', invoice_url)
                     r = self.driver.request('GET', invoice_url)
                     r.raise_for_status()
                     data = r.content
-                    with atomic_write(pdf_path, mode='wb') as f:
+                    with atomic_write(pdf_path, mode='wb', overwrite=True) as f:
                         f.write(data)
                 invoice_json_path = output_prefix + '.invoice.json'
                 if not os.path.exists(invoice_json_path):
                     with atomic_write(
                             invoice_json_path,
                             mode='w',
                             encoding='utf-8',
-                            newline='\n') as f:
+                            newline='\n',
+                            overwrite=True) as f:
                         f.write(json.dumps(transaction, indent='  '))
                 continue
             details_url = (
                 'https://www.paypal.com/myaccount/transactions/details/' +
                 transaction_id)
             inline_details_url = (
                 'https://www.paypal.com/myaccount/transactions/details/inline/'
                 + transaction_id)
             html_path = output_prefix + '.html'
             json_path = output_prefix + '.json'
-            if not os.path.exists(html_path):
-                logging.info('Retrieving HTML %s', details_url)
-                html_resp = self.driver.request('GET', details_url)
-                html_resp.raise_for_status()
-                with atomic_write(
-                        html_path, mode='w', encoding='utf-8',
-                        newline='\n') as f:
-                    # Write with Unicode Byte Order Mark to ensure content will be properly interpreted as UTF-8
-                    f.write('\ufeff' + html_resp.text)
             if not os.path.exists(json_path):
                 logging.info('Retrieving JSON %s', inline_details_url)
                 json_resp = self.make_json_request(inline_details_url)
                 json_resp.raise_for_status()
                 j = json_resp.json()
                 jsonschema.validate(j, transaction_details_schema)
-                with atomic_write(json_path, mode='wb') as f:
+                with atomic_write(json_path, mode='wb', overwrite=True) as f:
                     f.write(
-                        json.dumps(j['data']['details'], indent='  ').encode())
+                        json.dumps(j['data'], indent='  ', sort_keys=True).encode())
+            if not os.path.exists(html_path):
+                logging.info('Retrieving HTML %s', details_url)
+                html_resp = self.driver.request('GET', details_url)
+                try:
+                    html_resp.raise_for_status()
+                except HTTPError as e:
+                    # in rare cases no HTML detail page exists but JSON could be extracted
+                    # if JSON is present gracefully skip HTML download if it fails
+                    if os.path.exists(json_path):
+                        # HTML download failed but JSON present -> only log warning
+                        logging.warning('Retrieving HTML %s failed due to %s but JSON is already present. Continuing...', details_url, e)
+                    else:
+                        logging.error('Retrieving HTML %s failed due to %s and no JSON is present. Aborting...', details_url, e)
+                        raise e
+                with atomic_write(
+                        html_path, mode='w', encoding='utf-8',
+                        newline='\n', overwrite=True) as f:
+                    # Write with Unicode Byte Order Mark to ensure content will be properly interpreted as UTF-8
+                    f.write('\ufeff' + html_resp.text)
 
     def run(self):
         if not os.path.exists(self.output_directory):
             os.makedirs(self.output_directory)
         self.login()
         self.save_transactions()
```

## finance_dl/pge.py

```diff
@@ -100,23 +100,23 @@
     def check_after_wait(self):
         check_url(self.driver.current_url)
 
     def login(self):
         if self.logged_in:
             return
         logger.info('Initiating log in')
-        self.driver.get('https://www.pge.com/en/myhome/myaccount/index.page')
+        self.driver.get('https://m.pge.com/')
 
         (username, password), = self.wait_and_return(
             self.find_username_and_password_in_any_frame)
         logger.info('Entering username and password')
         username.send_keys(self.credentials['username'])
         password.send_keys(self.credentials['password'])
-        with self.wait_for_page_load():
-            password.send_keys(Keys.ENTER)
+        password.send_keys(Keys.ENTER)
+        self.wait_and_return(lambda: self.find_visible_elements(By.ID, 'arrowBillPaymentHistory'))
         logger.info('Logged in')
         self.logged_in = True
 
     def get_output_path(self, output_dir, date):
         journal_date_format = '%Y-%m-%d'
         return os.path.join(
             output_dir, '%s.bill.pdf' % (date.strftime(journal_date_format)))
@@ -133,15 +133,15 @@
             date = datetime.date(
                 year=int(m.group(3)), month=int(m.group(1)), day=int(
                     m.group(2)))
             new_path = self.get_output_path(output_dir, date)
             if os.path.exists(new_path):
                 logger.info('Skipping duplicate download: %s', date)
                 return False
-            tmp_path = new_path + '.tmp'
+            tmp_path = new_path.replace('.pdf', '.tmp.pdf')
             with open(tmp_path, 'wb') as f:
                 download_data = download_result[1]
                 f.write(download_data)
             os.rename(tmp_path, new_path)
             logger.info("Wrote %s", new_path)
             return True
 
@@ -153,23 +153,19 @@
 
     def get_bills(self, output_dir):
         logger.info('Sending escape')
         actions = ActionChains(self.driver)
         actions.send_keys(Keys.ESCAPE)
         actions.perform()
         logger.info('Looking for download link')
-        (bills_link, ), = self.wait_and_return(
-            lambda: self.find_visible_elements_by_descendant_partial_text('BILL & PAYMENT HISTORY', 'h2'))
+        (bills_link, ), = self.wait_and_return(lambda: self.find_visible_elements(By.ID, 'arrowBillPaymentHistory'))
         scrape_lib.retry(lambda: self.click(bills_link), retry_delay=2)
-        (more_link, ), = self.wait_and_return(
-            lambda: self.find_visible_elements_by_descendant_partial_text('View up to 24 months of activity', 'a'))
+        (more_link, ), = self.wait_and_return(lambda: self.find_visible_elements(By.ID, 'href-view-24month-history'))
         scrape_lib.retry(lambda: self.click(more_link), retry_delay=2)
-        links, = self.wait_and_return(
-            lambda: self.find_visible_elements(By.PARTIAL_LINK_TEXT, "View Bill PDF")
-        )
+        links, = self.wait_and_return(lambda: self.find_visible_elements(By.CSS_SELECTOR, ".utag-bill-history-view-bill-pdf"))
 
         for link in links:
             if not self.do_download_from_link(link, output_dir) and self.stop_early:
                 break
 
     def run(self):
         self.login()
```

## finance_dl/schwab.py

```diff
@@ -35,20 +35,25 @@
 import time
 from dataclasses import dataclass
 from typing import Any, List, Mapping, Optional, Set, Tuple
 from urllib.parse import urlencode
 
 from finance_dl import scrape_lib
 from selenium.webdriver.common.by import By
+from selenium.common.exceptions import NoSuchElementException
 from selenium.webdriver.common.keys import Keys
 from selenium.webdriver.support.ui import Select, WebDriverWait
 from selenium.webdriver.support import expected_conditions as EC
 
 logger = logging.getLogger(__name__)
 
+def sanitize(x):
+    x = x.replace(' ', '_')
+    x = re.sub('[^a-zA-Z0-9-_.]', '', x)
+    return x
 
 @dataclass(frozen=True)
 class Account:
     label: str
     number: str
 
 
@@ -58,14 +63,15 @@
     POSITIONS = 2
 
 
 class SchwabScraper(scrape_lib.Scraper):
     HISTORY_URL = "https://client.schwab.com/Apps/accounts/transactionhistory/"
     POSITIONS_URL = "https://client.schwab.com/Areas/Accounts/Positions"
     TXN_API_URL = "https://client.schwab.com/api/History/Brokerage/ExportTransaction"
+    BANK_TXN_API_URL = "https://client.schwab.com/api/History/Banking/ExportBankTransaction"
     POS_API_URL = "https://client.schwab.com/api/PositionV2/PositionsDataV2/Export"
     LOT_API_URL = "https://client.schwab.com/api/Cost/CostData/Export?"
     TRANSACTIONS_FILENAME_RE = re.compile(
         r"(?P<start>\d{4}-\d{2}-\d{2})_(?P<end>\d{4}-\d{2}-\d{2}).csv"
     )
     POSITIONS_FILENAME_RE = re.compile(r"(?P<date>\d{4}-\d{2}-\d{2}).csv")
     ONE_DAY = datetime.timedelta(days=1)
@@ -74,15 +80,15 @@
         self,
         credentials: Mapping[str, str],
         output_directory: str,
         min_start_date: datetime.date,
         **kwargs,
     ) -> None:
         self.lot_details = kwargs.pop("lot_details", False)
-        super().__init__(**kwargs)
+        super().__init__(use_seleniumrequests=True, **kwargs)
         self.credentials = credentials
         self.output_directory = output_directory
         self.min_start_date = min_start_date
         self.already_got_accounts: Set[Account] = set()
         self.current_page = PageType.NONE
 
     def run(self) -> None:
@@ -99,68 +105,77 @@
 
         while True:
             account = self.select_next_unseen_account(seen)
             if account is None:
                 break
             seen.add(account)
             account_dir, positions_dir = self.get_account_dirs(account)
-            self.download(account, account_dir, positions_dir)
-            if self.lot_details:
+            is_checking = self.download(account, account_dir, positions_dir)
+            if self.lot_details and not is_checking:
                 self.driver.get(self.POSITIONS_URL)
                 self.current_page = PageType.POSITIONS
                 self.download_lot_details(positions_dir)
                 self.driver.get(self.HISTORY_URL)
                 self.current_page = PageType.HISTORY
 
-    def download(self, account: Account, account_dir: str, positions_dir: str) -> None:
+    def download(self, account: Account, account_dir: str, positions_dir: str) -> bool:
         assert self.current_page == PageType.HISTORY
 
         logger.info(f"Checking account {account}")
 
         from_date = self.get_last_fetched_date(account_dir)
         if from_date is None:
             from_date = self.min_start_date
         else:
             from_date += self.ONE_DAY
         # Only download up to yesterday, so we can avoid overlap and not risk missing
         # any transactions.
         to_date = datetime.date.today() - self.ONE_DAY
+        is_checking = len(self.find_visible_elements(By.XPATH, '//a[text() = "Realized Gain / Loss"]')) == 0
         if to_date <= from_date:
             logger.info("No dates to download.")
-            return
-
-        logger.info("Downloading transactions.")
-
-        num_transaction_types = self.get_num_transaction_types()
-        transaction_filter = "|".join(map(str, range(num_transaction_types)))
+            return is_checking
+        if is_checking:
+            logger.info("Downloading banking transactions.")
+            from_str = from_date.strftime("%m/%d/%Y")
+            to_str = to_date.strftime("%m/%d/%Y")
+            account_str = account.number.replace("-", "")
+            url = self.BANK_TXN_API_URL +\
+                f"?AccountId={account_str}" +\
+                f"&FromDate={from_str}&ToDate={to_str}&SelectedFilters=AllTransactions&SortBy=Date" +\
+                f"&SortOrder=D&RecordsPerPage=400&GetDirection=F&dateRange=All"
+            dest_name = f"{from_date.strftime('%Y-%m-%d')}_{to_date.strftime('%Y-%m-%d')}.csv"
+            dest_path = os.path.join(account_dir, dest_name)
+            self.save_url(url, dest_path)
+        else:
+            logger.info("Downloading brokerage transactions.")
 
-        from_str = from_date.strftime("%m/%d/%Y")
-        to_str = to_date.strftime("%m/%d/%Y")
-        self.driver.get(
-            self.TXN_API_URL +
-            f"?sortSeq=1&sortVal=0&tranFilter={transaction_filter}"
-            f"&timeFrame=0&filterSymbol=&fromDate={from_str}&toDate={to_str}"
-            "&exportError=&invalidFromDate=&invalidToDate=&symbolExportValue="
-            "&includeOptions=N&displayTotal=true"
-        )
-        dest_name = f"{from_date.strftime('%Y-%m-%d')}_{to_date.strftime('%Y-%m-%d')}.csv"
-        dest_path = os.path.join(account_dir, dest_name)
-        self.get_file(f"{account.label}_Transactions_", dest_path)
-
-        logger.info("Downloading positions.")
-
-        self.driver.get(
-            self.POS_API_URL +
-            "?CalculateDayChangeIntraday=true"
-            "&firstColumn=symbolandDescriptionStacked&format=csv"
-        )
-        dest_name = datetime.date.today().strftime("%Y-%m-%d") + ".csv"
-        dest_path = os.path.join(positions_dir, dest_name)
+            num_transaction_types = self.get_num_transaction_types()
+            transaction_filter = "|".join(map(str, range(num_transaction_types)))
 
-        self.get_file(f"{account.label}-Positions-", dest_path)
+            from_str = from_date.strftime("%m/%d/%Y")
+            to_str = to_date.strftime("%m/%d/%Y")
+            url = self.TXN_API_URL +\
+                f"?sortSeq=1&sortVal=0&tranFilter={transaction_filter}" +\
+                f"&timeFrame=0&filterSymbol=&fromDate={from_str}&toDate={to_str}" +\
+                "&exportError=&invalidFromDate=&invalidToDate=&symbolExportValue=" +\
+                "&includeOptions=N&displayTotal=true"
+            dest_name = f"{from_date.strftime('%Y-%m-%d')}_{to_date.strftime('%Y-%m-%d')}.csv"
+            dest_path = os.path.join(account_dir, dest_name)
+            self.save_url(url, dest_path)
+
+            logger.info("Downloading positions.")
+
+            url = self.POS_API_URL +\
+                "?CalculateDayChangeIntraday=true" +\
+                "&firstColumn=symbolandDescriptionStacked&format=csv"
+            dest_name = datetime.date.today().strftime("%Y-%m-%d") + ".csv"
+            dest_path = os.path.join(positions_dir, dest_name)
+            self.save_url(url, dest_path)
+        return is_checking
 
     def download_lot_details(self, pos_dir: str) -> None:
         assert self.current_page == PageType.POSITIONS
 
         data_attr_to_param = {
             "itemissueid": "itemIssueId",
             "accountindex": "accountindex",
@@ -186,44 +201,43 @@
             logger.info("Lot details for this date already downloaded.")
             return
         else:
             os.makedirs(lots_dir)
 
         logger.info("Getting lot details.")
 
-        lot_rows = self.get_elements_wait("table.securityTable tr.data-row")
+        lot_rows = self.get_elements_wait("table.securityTable tr")
 
         for row in lot_rows:
             symbol = row.get_attribute("data-pulsr-symbol")
             if not symbol:
                 continue
             logger.info(f"  ...{symbol}")
-            link = row.find_element(By.CSS_SELECTOR, "td.costBasisColumn a")
+            try:
+                link = row.find_element(By.CSS_SELECTOR, "td.costBasisColumn a")
+            except NoSuchElementException:
+                # possibly options on this symbol
+                logger.warning(f"Nothing to do on {symbol}")
+                continue
             params = fixed.copy()
             for attr, param in data_attr_to_param.items():
                 params[param] = link.get_attribute(f"data-{attr}")
             qs = urlencode(params)
-            self.driver.get(f"{self.LOT_API_URL}{qs}")
+            # Necessary because options have spaces, and SPAC warrants have a slash
+            symbol = sanitize(symbol)
             dest_name = f"{symbol}.csv"
             dest_path = os.path.join(lots_dir, dest_name)
-            self.get_file("Lot-Details.CSV", dest_path)
-
-    def get_file(self, expected_prefix: str, dest_path: str) -> None:
-        self.wait_and_return(
-            lambda: self._get_file(expected_prefix, dest_path),
-            message=f"Didn't find downloaded file starting with {expected_prefix}.",
-        )
+            self.save_url(f"{self.LOT_API_URL}{qs}", dest_path)
 
-    def _get_file(self, expected_prefix: str, dest_path: str) -> bool:
-        for entry in os.scandir(self.download_dir):
-            if entry.name.startswith(expected_prefix):
-                shutil.move(entry.path, dest_path)
-                logger.info(f"Downloaded {dest_path}")
-                return True
-        return False
+    def save_url(self, url, dest_path):
+        response = self.driver.request('GET', url)
+        response.raise_for_status()
+        with open(dest_path, 'wb') as fout:
+            fout.write(response.content)
+        logger.info(f"Downloaded {dest_path}")
 
     def get_num_transaction_types(self) -> int:
         filter_link, = self.get_elements_wait("a.transaction-search-link")
         filter_link.click()
 
         checkbox_div, = self.get_elements_wait("div.transaction-filter-checkbox")
 
@@ -303,21 +317,21 @@
             return
 
         (login_frame,) = login_frames
 
         logger.info("Logging in.")
 
         self.driver.switch_to.frame(login_frame)
-
-        (username,) = self.get_elements("input#LoginId")
-
+        
+        username =  self.find_username_and_password()[0]
         username.send_keys(self.credentials["username"])
 
         logger.info("Looking for password field.")
-        (password,) = self.get_elements("input#Password")
+        
+        password = self.find_username_and_password()[1]
         password.send_keys(self.credentials["password"])
 
         password.send_keys(Keys.ENTER)
 
         logger.info("Waiting for app to load.")
 
         self.driver.switch_to.default_content()
```

## finance_dl/scrape_lib.py

```diff
@@ -19,20 +19,20 @@
 
 
 def all_conditions(*conditions):
     return lambda driver: all(condition(driver) for condition in conditions)
 
 
 def extract_table_data(table, header_names, single_header=False):
-    rows = table.find_elements_by_xpath('thead/tr | tbody/tr | tr')
+    rows = table.find_elements(By.XPATH, 'thead/tr | tbody/tr | tr')
     headers = []
     seen_data = False
     data = []
     for row in rows:
-        cell_elements = row.find_elements_by_xpath('th | td')
+        cell_elements = row.find_elements(By.XPATH, 'th | td')
         cell_values = [x.text.strip() for x in cell_elements]
         is_header_values = [x in header_names for x in cell_values if x]
         if len(is_header_values) == 0:
             is_header = True
         else:
             if any(is_header_values) != all(is_header_values):
                 raise RuntimeError('Header mismatch: %r' % (list(
@@ -139,26 +139,29 @@
             return
 
         original_sigint_handler = signal.getsignal(signal.SIGINT)
         signal.signal(signal.SIGINT, signal.SIG_IGN)
 
         self.chromedriver_bin = chromedriver_bin
         chrome_options = webdriver.ChromeOptions()
-        service_args = ['--verbose', '--log-path=/tmp/chromedriver.log', '--no-sandbox']
+        chrome_options.binary_location = os.getenv("CHROMEDRIVER_CHROME_BINARY")
+        log_path = os.getenv("TMPLOG", "/tmp/chromedriver.log")
+        service_args = ['--verbose', f'--log-path={log_path}', '--no-sandbox']
         caps = DesiredCapabilities.CHROME
         if capture_network_requests:
             caps['loggingPrefs'] = {'performance': 'ALL'}
             caps['goog:loggingPrefs'] = {'performance': 'ALL'}
         chrome_options.add_experimental_option('excludeSwitches', [
             'enable-automation',
             'load-extension',
             'load-component-extension',
             'ignore-certificate-errors',
             'test-type',
         ])
+        chrome_options.add_argument('--no-sandbox')
         if profile_dir is not None:
             chrome_options.add_argument('user-data-dir=%s' % profile_dir)
             if not os.path.exists(profile_dir):
                 os.makedirs(profile_dir)
         prefs = {}
         prefs['plugins.plugins_disabled'] = [
             'Chrome PDF Viewer', 'Chromium PDF Viewer'
@@ -188,15 +191,15 @@
         pass
 
     def get_downloaded_file(self):
         names = os.listdir(self.download_dir)
         partial_names = []
         other_names = []
         for name in names:
-            if name.endswith('.part') or name.endswith('.crdownload'):
+            if name.endswith('.part') or name.endswith('.crdownload') or name.startswith('.com.google.Chrome'):
                 partial_names.append(name)
             else:
                 other_names.append(name)
         if len(other_names) == 0:
             return None
         if len(other_names) > 1:
             raise RuntimeError(
@@ -210,15 +213,15 @@
             return None
         os.remove(path)
         return other_names[0], data
 
     # See http://www.obeythetestinggoat.com/how-to-get-selenium-to-wait-for-page-load-after-a-click.html
     @contextlib.contextmanager
     def wait_for_page_load(self, timeout=30):
-        old_page = self.driver.find_element_by_tag_name('html')
+        old_page = self.driver.find_element(By.TAG_NAME, 'html')
         yield
         WebDriverWait(self.driver, timeout).until(
             expected_conditions.staleness_of(old_page),
             message='waiting for page to load')
         self.check_after_wait()
 
     @contextlib.contextmanager
@@ -348,32 +351,32 @@
     def find_visible_elements_by_descendant_partial_text(
             self, text, element_name):
         return self.find_elements_by_descendant_partial_text(
             text, element_name, only_displayed=True)
 
     def find_elements_by_descendant_partial_text(self, text, element_name,
                                                  only_displayed=False):
-        all_elements = self.driver.find_elements_by_xpath(
+        all_elements = self.driver.find_elements(By.XPATH, 
             "//text()[contains(.,%r)]/ancestor::*[self::%s][1]" %
             (text, element_name))
         if only_displayed:
             return [x for x in all_elements if is_displayed(x)]
         return all_elements
 
     def find_elements_by_descendant_text_match(self, text_match, element_name,
                                                only_displayed=False):
-        all_elements = self.driver.find_elements_by_xpath(
+        all_elements = self.driver.find_elements(By.XPATH, 
             "//text()[%s]/ancestor::*[self::%s][1]" % (text_match,
                                                        element_name))
         if only_displayed:
             return [x for x in all_elements if is_displayed(x)]
         return all_elements
 
     def find_visible_elements_by_partial_text(self, text, element_name):
-        all_elements = self.driver.find_elements_by_xpath(
+        all_elements = self.driver.find_elements(By.XPATH, 
             "//%s[contains(.,%r)]" % (element_name, text))
         return [x for x in all_elements if is_displayed(x)]
 
     def find_visible_elements(self, by_method, locator):
         elements = self.driver.find_elements(by_method, locator)
         return [x for x in elements if is_displayed(x)]
```

## finance_dl/ultipro_google.py

```diff
@@ -11,14 +11,17 @@
 - `credentials`: Required.  Must be a `dict` with `'username'` and `'password'`
   keys.
 
 - `output_directory`: Required.  Must be a `str` that specifies the path on the
   local filesystem where the PDF pay statements will be written.  If the
   directory does not exist, it will be created.
 
+- `dir_per_year`: Optional. If specified and True, finance-dl will create one
+  subdirectory of the `output_directory` for each year of pay statements.
+
 - `profile_dir`: Optional.  If specified, must be a `str` that specifies the
   path to a persistent Chrome browser profile to use.  This should be a path
   used solely for this single configuration; it should not refer to your normal
   browser profile.  If not specified, a fresh temporary profile will be used
   each time.
 
 - `headless`: Optional.  If specified, must be a `bool`.  Defaults to `True`.
@@ -59,14 +62,15 @@
 ==================
 
 From the interactive shell, type: `self.run()` to start the scraper.
 
 """
 
 import datetime
+import glob
 import logging
 import os
 import re
 import urllib.parse
 from selenium.webdriver.common.by import By
 from selenium.webdriver.support.ui import Select
 from selenium.webdriver.common.keys import Keys
@@ -78,22 +82,24 @@
 output_date_format = '%Y-%m-%d'
 
 
 class Scraper(scrape_lib.Scraper):
     def __init__(self,
                  credentials,
                  output_directory,
+                 dir_per_year=False,
                  login_url='https://googlemypay.ultipro.com',
                  netloc_re=r'^([^\.@]+\.)*(ultipro.com|google.com)$',
                  **kwargs):
         super().__init__(**kwargs)
         self.credentials = credentials
         self.login_url = login_url
         self.netloc_re = netloc_re
         self.output_directory = output_directory
+        self.dir_per_year = dir_per_year
 
     def check_url(self, url):
         result = urllib.parse.urlparse(url)
         if result.scheme != 'https' or not re.fullmatch(self.netloc_re,
                                                         result.netloc):
             raise RuntimeError('Reached invalid URL: %r' % url)
 
@@ -139,15 +145,16 @@
             document_str = 'Document %r : %r' % (pay_date, document_number)
             if (pay_date, document_number) in existing_statements:
                 logger.info('  Found in existing')
                 continue
             if (pay_date, document_number) not in downloaded_statements:
                 logger.info('%s:  Downloading', document_str)
                 link = row.find_element_by_tag_name('a')
-                link.click()
+                with self.wait_for_page_load():
+                  link.click()
                 download_link, = self.wait_and_return(
                     lambda: self.find_element_in_any_frame(
                         By.XPATH,
                         '//input[@type="image" and contains(@title, "Download")]'
                     ))
                 download_link.click()
                 logger.info('%s: Waiting to get download', document_str)
@@ -155,43 +162,46 @@
                     self.get_downloaded_file)
                 name, data = download_result
                 if len(data) < 5000:
                     raise RuntimeError(
                         'Downloaded file size is invalid: %d' % len(data))
                 output_name = '%s.statement-%s.pdf' % (
                     pay_date.strftime('%Y-%m-%d'), document_number)
-                output_path = os.path.join(self.output_directory, output_name)
-                with atomic_write(output_path, mode='wb') as f:
+                if self.dir_per_year:
+                    output_path = os.path.join(self.output_directory, pay_date.strftime('%Y'), output_name)
+                else:
+                    output_path = os.path.join(self.output_directory, output_name)
+                if not os.path.exists(os.path.dirname(output_path)):
+                    os.makedirs(os.path.dirname(output_path))
+                with atomic_write(output_path, mode='wb', overwrite=True) as f:
                     f.write(data)
                 downloaded_statements.add((pay_date, document_number))
                 return True
             else:
                 logger.info('%s: Just downloaded', document_str)
         return False
 
     def get_existing_statements(self):
         existing_statements = set()
-        if os.path.exists(self.output_directory):
-            for name in os.listdir(self.output_directory):
-                m = re.fullmatch(
-                    r'([0-9]{4})-([0-9]{2})-([0-9]{2})\.statement-([0-9A-Z]+)\.pdf',
-                    name)
-                if m is not None:
-                    date = datetime.date(
-                        year=int(m.group(1)),
-                        month=int(m.group(2)),
-                        day=int(m.group(3)))
-                    statement_number = m.group(4)
-                    existing_statements.add((date, statement_number))
-                    logger.info('Found existing statement %r %r', date,
-                                statement_number)
-                else:
-                    logger.warning(
-                        'Ignoring extraneous file in existing statement directory: %r',
-                        os.path.join(self.output_directory, name))
+        for p in glob.glob(os.path.join(self.output_directory, '**', '*.statement-?*.pdf')):
+            m = re.fullmatch(
+                r'([0-9]{4})-([0-9]{2})-([0-9]{2})\.statement-([0-9A-Z]+)\.pdf',
+                os.path.basename(p))
+            if m is not None:
+                date = datetime.date(
+                    year=int(m.group(1)),
+                    month=int(m.group(2)),
+                    day=int(m.group(3)))
+                statement_number = m.group(4)
+                existing_statements.add((date, statement_number))
+                logger.info('Found existing statement %r %r', date,
+                            statement_number)
+            else:
+                logger.warning(
+                    f'Ignoring extraneous file in existing statement directory: {p}')
         return existing_statements
 
     def download_statements(self):
         if not os.path.exists(self.output_directory):
             os.makedirs(self.output_directory)
         existing_statements = self.get_existing_statements()
         downloaded_statements = set()
```

## finance_dl/usbank.py

```diff
@@ -142,44 +142,44 @@
             password.send_keys(Keys.ENTER)
         logger.info('Logged in')
         self.logged_in = True
 
     def find_account_link_in_any_frame(self):
         for frame in self.for_each_frame():
             try:
-                return self.driver.find_element_by_partial_link_text(self.account_name)
+                return self.driver.find_element(By.PARTIAL_LINK_TEXT, self.account_name)
             except:
                 pass
         raise NoSuchElementException()
 
 
     def find_download_page_in_any_frame(self):
         for frame in self.for_each_frame():
             try:
-                return self.driver.find_element_by_partial_link_text("Download Transactions")
+                return self.driver.find_element(By.PARTIAL_LINK_TEXT, "Download Transactions")
             except:
                 pass
         raise NoSuchElementException()
 
         
     def find_date_fields(self):
         for frame in self.for_each_frame():
             try:
-                fromDate = self.driver.find_element_by_id("FromDateInput")
-                toDate = self.driver.find_element_by_id("ToDateInput")
+                fromDate = self.driver.find_element(By.ID, "FromDateInput")
+                toDate = self.driver.find_element(By.ID, "ToDateInput")
                 return (fromDate, toDate)
             except:
                 pass
         raise NoSuchElementException()
 
 
     def find_download_link(self):
         for frame in self.for_each_frame():
             try:
-                return self.driver.find_elements_by_id("DTLLink")[0]
+                return self.driver.find_elements(By.ID, "DTLLink")[0]
             except:
                 pass
         raise NoSuchElementException()
 
 
 
     def download_ofx(self):
```

## finance_dl/venmo.py

```diff
@@ -78,15 +78,15 @@
 import re
 import dateutil.parser
 import datetime
 import logging
 import os
 import time
 from selenium.webdriver.common.by import By
-from selenium.common.exceptions import NoSuchElementException
+from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException, StaleElementReferenceException
 from selenium.webdriver.support.ui import Select
 from selenium.webdriver.common.keys import Keys
 
 from . import scrape_lib
 from . import csv_merge
 
 logger = logging.getLogger('venmo_scrape')
@@ -142,62 +142,112 @@
             self.earliest_history_date = dateutil.parser.parse(
                 earliest_history_date).date()
         self.logged_in = False
 
     def check_after_wait(self):
         check_url(self.driver.current_url)
 
+    def find_venmo_username(self):
+        for frame in self.for_each_frame():
+            try:
+                return self.driver.find_elements(By.XPATH, '//input[@type="text" or @type="email"]')
+            except NoSuchElementException:
+                pass
+        raise NoSuchElementException()
+
+    def find_venmo_password(self):
+        for frame in self.for_each_frame():
+            try:
+                return self.driver.find_elements(By.XPATH, '//input[@type="password"]')
+            except NoSuchElementException:
+                pass
+        raise NoSuchElementException()
+
+    def wait_for(self, condition_function):
+        start_time = time.time()
+        while time.time() < start_time + 3:
+            if condition_function():
+                return True
+            else:
+                time.sleep(0.1)
+        raise Exception(
+            'Timeout waiting for {}'.format(condition_function.__name__)
+        )
+
+    def click_through_to_new_page(self, button_text):
+        link = self.driver.find_element(By.XPATH, f'//button[@name="{button_text}"]')
+        link.click()
+
+        def link_has_gone_stale():
+            try:
+                # poll the link with an arbitrary call
+                link.find_elements(By.XPATH, 'doesnt-matter')
+                return False
+            except StaleElementReferenceException:
+                return True
+
+        self.wait_for(link_has_gone_stale)
+
     def login(self):
         if self.logged_in:
             return
         logger.info('Initiating log in')
         self.driver.get('https://venmo.com/account/sign-in')
 
-        (username, password), = self.wait_and_return(
-            self.find_username_and_password_in_any_frame)
-        logger.info('Entering username and password')
-        username.send_keys(self.credentials['username'])
+        #(username, password), = self.wait_and_return(
+        #    self.find_username_and_password_in_any_frame)
+        username = self.wait_and_return(self.find_venmo_username)[0][0]
+        try:
+            logger.info('Entering username')
+            username.send_keys(self.credentials['username'])
+            username.send_keys(Keys.ENTER)
+        except ElementNotInteractableException:
+            # indicates that username already filled in
+            logger.info("Skipped")
+        password = self.wait_and_return(self.find_venmo_password)[0][0]
+        logger.info('Entering password')
         password.send_keys(self.credentials['password'])
-        with self.wait_for_page_load():
-            password.send_keys(Keys.ENTER)
+        self.click_through_to_new_page("Sign in")
         logger.info('Logged in')
         self.logged_in = True
 
     def goto_statement(self, start_date, end_date):
         url_date_format = '%m-%d-%Y'
         with self.wait_for_page_load():
             self.driver.get(
                 'https://venmo.com/account/statement?end=%s&start=%s' %
                 (end_date.strftime(url_date_format),
                  start_date.strftime(url_date_format)))
 
     def download_csv(self):
         logger.info('Looking for CSV link')
         download_button, = self.wait_and_locate(
-            (By.XPATH, '//a[text() = "Download CSV"]'))
+            (By.XPATH, '//*[text() = "Download CSV"]'))
         self.click(download_button)
         logger.info('Waiting for CSV download')
         download_result, = self.wait_and_return(self.get_downloaded_file)
         logger.info('Got CSV download')
         return download_result[1]
 
     def get_balance(self, balance_type):
         try:
-            balance_node = self.driver.find_element(
-                By.XPATH, '//*[@class="%s"]/child::*[@class="balance-amt"]' %
+            balance_node =  self.driver.find_element(
+                By.XPATH, '//*[text() = "%s"]/following-sibling::*' %
                 balance_type)
             return balance_node.text
         except NoSuchElementException:
             return None
 
     def get_balances(self):
         def maybe_get_balance():
-            start_balance = self.get_balance('start-balance')
-            end_balance = self.get_balance('end-balance')
+            start_balance = self.get_balance('Beginning amount')
+            end_balance = self.get_balance('Ending amount')
             if start_balance is not None and end_balance is not None:
+                start_balance = start_balance.replace("\n", "")
+                end_balance = end_balance.replace("\n", "")
                 return (start_balance, end_balance)
             try:
                 error_node = self.driver.find_element(
                     By.XPATH, '//*[@class="account-statement-error"]')
                 error_text = error_node.text
                 logging.info('Saw error text: %s', error_text)
                 if error_text.startswith('Loading'):
@@ -206,34 +256,53 @@
             except NoSuchElementException:
                 return None
 
         result, = self.wait_and_return(maybe_get_balance)
         return result
 
     def write_csv(self, csv_result):
-        csv_reader = csv.DictReader(
-            io.StringIO(csv_result.decode(), newline=''))
+        # Skip first two lines because they are not useful
+        str_io = io.StringIO(csv_result.decode(), newline='')
+        io_iter = iter(str_io)
+        next(io_iter)
+        next(io_iter)
+        csv_reader = csv.DictReader(str_io)
         field_names = csv_reader.fieldnames
         rows = [row for row in csv_reader if row['Datetime'].strip()]
 
         # Make sure rows are valid transactions with a date
         good_rows = []
         for r in rows:
-            if r['Datetime'] != '':
+            if 'Datetime' not in r or r['Datetime'] != '':
                 good_rows.append(r)
             else:
                 logging.info('Invalid date in row: {}'.format(r))
 
         rows = good_rows
 
         def get_sort_key(row):
             return parse_csv_date(row['Datetime']).timestamp()
 
         transactions_file = os.path.join(self.output_directory,
                                          'transactions.csv')
+        # One time fix in case Username column present in existing file
+        if os.path.exists(transactions_file):
+            with open(transactions_file, 'r', newline='', encoding='utf-8') as f:
+                csv_reader = csv.DictReader(f)
+                old_field_names = csv_reader.fieldnames
+                if old_field_names[0] == 'Username':
+                    logging.info("Removing 'Username' column from old transactions file.")
+                    data = list(csv_reader)
+                    for r in data:
+                        r.pop('Username')
+                        r[''] = ''
+                    old_field_names[0] = ''
+                    os.rename(transactions_file, transactions_file + '.bak')
+                    logging.info(f"Backed up existing transactions file to {transactions_file}.bak")
+                    csv_merge.write_csv(old_field_names, data, transactions_file)
         csv_merge.merge_into_file(filename=transactions_file,
                                   field_names=field_names, data=rows,
                                   sort_by=get_sort_key)
 
     def get_existing_balances(self):
         if not os.path.exists(self.balances_path):
             return []
@@ -280,21 +349,28 @@
 
         start_date = self.get_start_date()
         logging.info('Fetching history starting from %s',
                      start_date.strftime('%Y-%m-%d'))
 
         while start_date <= self.latest_history_date:
             end_date = min(self.latest_history_date,
-                           start_date + datetime.timedelta(days=89))
+                           self.last_day_of_month(start_date))
             self.fetch_statement(start_date, end_date)
             start_date = end_date + datetime.timedelta(days=1)
 
             logger.debug('Venmo hack: waiting 5 seconds between requests')
             time.sleep(5)
 
+
+    def last_day_of_month(self, any_day):
+        # The day 28 exists in every month. 4 days later, it's always next month
+        next_month = any_day.replace(day=28) + datetime.timedelta(days=4)
+        # subtracting the number of the current day brings us back one month
+        return next_month - datetime.timedelta(days=next_month.day)
+
     def run(self):
         self.login()
         self.fetch_history()
 
 
 def run(**kwargs):
     scrape_lib.run_with_scraper(Scraper, **kwargs)
```

## finance_dl/waveapps.py

```diff
@@ -71,15 +71,15 @@
                 'token': 'XXXXXX',
             },
             output_directory=os.path.join(data_dir, 'waveapps'),
         )
 
 """
 
-from typing import List, Any
+from typing import List, Any, Optional
 import contextlib
 import logging
 import json
 import os
 
 import requests
 from atomicwrites import atomic_write
@@ -150,15 +150,15 @@
         response.raise_for_status()
         result = response.json()
         cur_list = result['results']
         logger.info('Received %d receipts', len(cur_list))
         receipts.extend(cur_list)
         return receipts
 
-    def save_receipts(self, receipts: List[Any], output_directory: str = None):
+    def save_receipts(self, receipts: List[Any], output_directory: Optional[str] = None):
         if not output_directory:
             output_directory = self.output_directory
         if not os.path.exists(output_directory):
             os.makedirs(output_directory)
         for receipt in receipts:
             output_prefix = os.path.join(output_directory,
                                          str(receipt['id']))
@@ -170,15 +170,15 @@
                 else:
                     image_path = '%s.%02d.jpg' % (output_prefix, image_i)
                 if not os.path.exists(image_path):
                     logger.info('Downloading receipt image %s', image_url)
                     r = requests.get(image_url)
                     r.raise_for_status()
                     data = r.content
-                    with atomic_write(image_path, mode='wb') as f:
+                    with atomic_write(image_path, mode='wb', overwrite=True) as f:
                         f.write(data)
             with atomic_write(
                     json_path,
                     mode='w',
                     overwrite=True,
                     encoding='utf-8',
                     newline='\n') as f:
```

## Comparing `finance_dl-1.3.3.dist-info/LICENSE` & `finance_dl-1.3.4.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `finance_dl-1.3.3.dist-info/METADATA` & `finance_dl-1.3.4.dist-info/METADATA`

 * *Files 23% similar despite different names*

```diff
@@ -1,28 +1,30 @@
 Metadata-Version: 2.1
 Name: finance-dl
-Version: 1.3.3
+Version: 1.3.4
 Summary: Tools for scraping personal financial data.
 Home-page: https://github.com/jbms/finance-dl
 Author: Jeremy Maitin-Shepard
 Author-email: jeremy@jeremyms.com
 License: GPLv2
 Platform: UNKNOWN
 Requires-Python: >=3.5
 Description-Content-Type: text/markdown
+License-File: LICENSE
 Requires-Dist: bs4
-Requires-Dist: mintapi (>=1.31)
+Requires-Dist: mintapi >=1.31
 Requires-Dist: ofxclient
 Requires-Dist: selenium
 Requires-Dist: ipython
 Requires-Dist: selenium-requests
 Requires-Dist: chromedriver-binary
-Requires-Dist: beancount (>=2.1.2)
-Requires-Dist: atomicwrites (>=1.3.0)
+Requires-Dist: beancount >=2.1.2
+Requires-Dist: atomicwrites >=1.3.0
 Requires-Dist: jsonschema
+Requires-Dist: python-dateutil
 
 Python package for scraping personal financial data from financial
 institutions.
 
 [![License: GPL v2](https://img.shields.io/badge/License-GPL%20v2-blue.svg)](LICENSE)
 [![PyPI](https://img.shields.io/pypi/v/finance-dl)](https://pypi.org/project/finance-dl)
 [![Build](https://github.com/jbms/finance-dl/workflows/Build/badge.svg)](https://github.com/jbms/finance-dl/actions?query=workflow%3ABuild)
@@ -68,14 +70,15 @@
   [Wave](https://waveapps.com), which is a free receipt-scanning
   website/mobile app.
 - [finance_dl.ultipro_google](finance_dl/ultipro_google.py): downloads
   Google employee payroll statements in PDF format from Ultipro.
 - [finance_dl.usbank](finance_dl/usbank.py): downloads data from US Bank credit cards in OFX format.
 - [finance_dl.radiusbank](finance_dl/radiusbank.py): downloads data from Radius Bank in QFX format.
 - [finance_dl.schwab](finance_dl/schwab.py): downloads data from Schwab Brokerage accounts in CSV format.
+- [finance_dl.gemini](finance_dl/gemini.py): downloads trades, transfers and balances from Gemini crypto exchange using REST API, stores in a custom CSV format.
 
 Setup
 ==
 
 To install the most recent published package from PyPi, simply type:
 
 ```shell
@@ -93,18 +96,20 @@
 ```shell
 pip install -e .
 ```
 
 Configuration
 ==
 
-Create a Python file like `example_finance_dl_config.py`.
+Create a configuration file called something like `finance_dl_config.py`.
+For a complete example of this file and some documentation, 
+see [example_finance_dl_config.py](example_finance_dl_config.py).
 
 Refer to the documentation of the individual scraper modules for
-details.
+further details.
 
 Basic Usage
 ==
 
 You can run a scraping configuration named `myconfig` as follows:
 
     python -m finance_dl.cli --config-module example_finance_dl_config --config myconfig
@@ -155,14 +160,40 @@
 
 then all specified configurations are run in parallel.
 
 To update all configurations, run:
 
     python -m finance_dl.update --config-module example_finance_dl_config --log-dir logs update --all
 
+Note on Chromedriver Versioning
+==
+
+Chromedriver and Chrome are very tightly coupled; their versions need to
+match. `finance_dl` uses Chromedriver from the `chromedriver_binary` Python
+package (not your system's installed Chromedriver binary). However,
+Chromedriver, by default, uses your system's installed version of Chrome.
+Depending on how you manage the two installations on your system, this
+combination may frequently end up causing `finance_dl` to fail with messages
+like
+
+    selenium.common.exceptions.SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version 97
+    Current browser version is 96.0.4664.45 with binary path /usr/bin/google-chrome
+
+In this event, you have a few options:
+
+1. Explicitly manage your version of the `chromedriver_binary` Python package
+   to match your installed version of Chrome;
+1. Explicitly manage your installed version of Chrome to match your version of
+   the `chromedriver_binary` Python package; or
+1. Install the version of Chrome matching your version of
+   `chromedriver_binary` somewhere other than your system's default Chrome
+   version, and set the environment variable `CHROMEDRIVER_CHROME_BINARY` to
+   point to it. (You can do this from within your finance_dl config script,
+   e.g. with a line like `os.environ["CHROMEDRIVER_CHROME_BINARY"] = "/usr/bin/google-chrome-beta"`).
+
 License
 ==
 
 Copyright (C) 2014-2018 Jeremy Maitin-Shepard.
 
 Distributed under the GNU General Public License, Version 2.0 only.
 See [LICENSE](LICENSE) file for details.
```

## Comparing `finance_dl-1.3.3.dist-info/RECORD` & `finance_dl-1.3.4.dist-info/RECORD`

 * *Files 13% similar despite different names*

```diff
@@ -1,33 +1,34 @@
 finance_dl/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-finance_dl/amazon.py,sha256=SK9Q2rUR4jwqoUaega6NBmtj8xhDI2McTVU1JYDwcQ0,11450
-finance_dl/anthem.py,sha256=yEYMsmhTL6Z4_F9ZU-BBtElFjVuVO1Jk-jGM6g4lGPM,5552
+finance_dl/amazon.py,sha256=WHB3Ut5FhcM_ByrYkgJvq71TdUhF96bkh7FOl7B1tt4,24118
+finance_dl/anthem.py,sha256=j-CKcfZTcgwJl7qIlHruJPxIfrj_BL2edPnd0P7kRsI,5584
 finance_dl/chromedriver_wrapper.py,sha256=XrAXZ3UnS-9F6sghJyiLYwqA_3eSVH71qSWqH84WUMo,439
 finance_dl/cli.py,sha256=FXZSDZJDQ42lPb1Txad9BUW3-U5DXthD6_rZG6q1Wv4,3168
-finance_dl/comcast.py,sha256=-0xpwdH12T_bh-fQ8unDS1qogCk0DRFgvCGxBK-1VTo,6706
+finance_dl/comcast.py,sha256=wkijeGpxrQRIVM4Iodz1AveDOEf-Kdk0v1RvFOdTeXE,6708
 finance_dl/csv_merge.py,sha256=M_rltORHoa0qKv9MhbxAabgceNAY21_9UPV-E1roU_g,2242
-finance_dl/discover.py,sha256=6cG0FlN50jD-pWYM6h_ebQhYj5QoA2NJRdk-MSYi2YQ,5392
-finance_dl/ebmud.py,sha256=vsbFGbDHc3x89UDvAgsB-NabGc_uUI9De75aV7kUxZM,4703
+finance_dl/discover.py,sha256=wJYezI7S4MJDaiPaOVVFI4a9Zn91v3fmSVcBag7PfmE,5438
+finance_dl/ebmud.py,sha256=k8ZjHybP_6yEjRVE3dK-6REnTBXEJxcDkdHoUPw7bJQ,4719
+finance_dl/gemini.py,sha256=NYQ7roBqMfXIiF_ZwvNkEfySemF1K7SFCdtnVm38P-M,10417
 finance_dl/google_login.py,sha256=BibTVtMn5V8kdY4E66RYl_PlfAlWzsf4oszkDGQnJ8k,2249
-finance_dl/google_purchases.py,sha256=0izS0qIklzq25PIO_cJUIY_ffa29ol233X5bScWOCQY,5432
+finance_dl/google_purchases.py,sha256=uiTqEowERH5_cX2Bg21MrRld_IMIHFqh07Da1IDs-Lg,5484
 finance_dl/google_takeout.py,sha256=pB5urqrFUgsv7M_v_lGP1xM9BPJrhP2aE-fXdmPvlXw,3642
-finance_dl/healthequity.py,sha256=Xed7HQkjX8MzFcPNAdwG6atVa-z7L03xUIKz8w2auwU,11927
+finance_dl/healthequity.py,sha256=mgbdZl9O6bOZQ2NtfInKXH06n8q4xBiTTLzAcu6dWjQ,13306
 finance_dl/mint.py,sha256=T-xgffLopLGEVCM-6ExBT5AUFs4dgfNNKdVcgqxxVW4,17838
-finance_dl/ofx.py,sha256=79Og3KZxLF-bExSZVFdIEldrQnylD11p06AlTTRt2Y8,15303
-finance_dl/ofx_rename.py,sha256=23S1nwZWN9c_LaP_dyUYvMzrw-HlDsko9KtScguG_ks,1010
-finance_dl/paypal.py,sha256=zAQfIvf7OW19OvJfayxjVuSFRFGrQ6-7xS3Y3Iz34TM,9925
-finance_dl/pge.py,sha256=k8Z9--4cITpsJG18N7pqGLKpONFfG7m8CKetzo51ISw,6356
+finance_dl/ofx.py,sha256=gUIhpH4jITgLLHj_PCCpe2cPMyPB0haQQXNaGhrbol8,17409
+finance_dl/ofx_rename.py,sha256=MVM3_VQaXknS543B4xUcr1KSYHe1JtQd_zlvEbRFJEc,1054
+finance_dl/paypal.py,sha256=ISlco-4Hz6pl6rUOqoyTLWjbgjMCqdwVFz1kOAh1pek,11107
+finance_dl/pge.py,sha256=wPFm6Ggfr1KCDwxdMT5jO4rLCeCeq2KlKtzLnV0Ljpo,6307
 finance_dl/radiusbank.py,sha256=eKj8CQiAOaWStthR8l6TvxROKDRA6jhhoulFTeJZB9g,6614
-finance_dl/schwab.py,sha256=_N3war95torSy7fb2hmiHbN-SFXNByqerYE4er1HXLs,13144
-finance_dl/scrape_lib.py,sha256=QopAp1jOEUvP35hXmiZiOxMyMS_m8OzbYQGp5GV0i-o,16363
+finance_dl/schwab.py,sha256=viDZKZ9BgmdmTq8apX1PcNDcgRJCfJ0W2_MfTVrfsnQ,14266
+finance_dl/scrape_lib.py,sha256=riuYRSAqoH4RLWvEoDQuKXXd86hS0GTWKupmmzV5mU8,16597
 finance_dl/stockplanconnect.py,sha256=aE5jm6NyfZsDH_zRlRIon8wz469AK0XGmE_D3HMZxjE,7352
-finance_dl/ultipro_google.py,sha256=PCCXJ1dwAyozhmPQ9Xpt89CPkjZOpPV7B4hOGewnX_Y,8308
+finance_dl/ultipro_google.py,sha256=FGE-tJFEKhJSjTFe2qXrG-1o2t2kTMvuWYIdTMi8hXk,8794
 finance_dl/update.py,sha256=cqXgsAJwBt9nr-T1QYkZoM7JcLEYjpGpj_y9Rxu065o,6817
-finance_dl/usbank.py,sha256=RsL0EZIHjnMnkB0uXnUdkDy3wHpTKpLjFQw95P-jbFA,9430
-finance_dl/venmo.py,sha256=kGLWquxBZQyyuHI6h1EyqSRsmoEYfAw1t7OdHnT_twE,11378
-finance_dl/waveapps.py,sha256=fYMqSOoxfDgKWTCirNMPDLB8ZCAvvu2UWkpYGe8PWio,7481
-finance_dl-1.3.3.dist-info/LICENSE,sha256=gXf5dRMhNSbfLPYYTY_5hsZ1r7UU1OaKQEAQUhuIBkM,18092
-finance_dl-1.3.3.dist-info/METADATA,sha256=uWE2arq5J-BVifBI_DXZotaY2R-cu0wqpOR-rSIGYho,6102
-finance_dl-1.3.3.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-finance_dl-1.3.3.dist-info/entry_points.txt,sha256=aQxjiIOpDH8k-JT5yLzztdLbHxMQIBLy_3_r973-Aw4,123
-finance_dl-1.3.3.dist-info/top_level.txt,sha256=K7TI22TtewvpbNVd-3dIQrVtqbzKFgra0d15voAj-HY,11
-finance_dl-1.3.3.dist-info/RECORD,,
+finance_dl/usbank.py,sha256=rSEDUmfUXP6wGyk6jYsAiHsyEbyAtFGO-F8KlvlHMUs,9435
+finance_dl/venmo.py,sha256=wMAz2eJRPACrlejVoyN3dpUDB8oEXSTlYJZ0Dj3KWFs,14740
+finance_dl/waveapps.py,sha256=w6YynyWNfSi1TFqaTcaPq9l351KXu2Y55rUcXhAsq-U,7517
+finance_dl-1.3.4.dist-info/LICENSE,sha256=gXf5dRMhNSbfLPYYTY_5hsZ1r7UU1OaKQEAQUhuIBkM,18092
+finance_dl-1.3.4.dist-info/METADATA,sha256=9nvd1u_CD4MmjYrZgv0lvadtpm3ngSJgEkmwdcqLTtM,7857
+finance_dl-1.3.4.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+finance_dl-1.3.4.dist-info/entry_points.txt,sha256=aQxjiIOpDH8k-JT5yLzztdLbHxMQIBLy_3_r973-Aw4,123
+finance_dl-1.3.4.dist-info/top_level.txt,sha256=K7TI22TtewvpbNVd-3dIQrVtqbzKFgra0d15voAj-HY,11
+finance_dl-1.3.4.dist-info/RECORD,,
```

